# System Environment Snapshot

This document provides a comprehensive snapshot of the system environment at the time of the last update.

## System Information

- **OS**: macOS (Darwin 24.4.0)
- **Platform**: Apple Silicon (arm64)
- **User**: hank
- **Directory**: /Users/hank/dotfiles
- **Date**: 2025-05-03

## Repository Information

- **Branch**: main
- **Last Commit**: f109687 "Add comprehensive READMEs and setup script"
- **Commit Date**: 2025-05-03

## Core Tools

| Tool | Version | Package Manager |
|------|---------|-----------------|
| Zsh | 5.2.37 | Homebrew |
| Neovim | 0.11.1 | Homebrew |
| Git | 2.49.0 | Homebrew |
| Homebrew | latest | System |
| Just | 1.40.0 | Homebrew |
| Starship | 1.23.0 | Homebrew |
| Yazi | 25.4.8 | Homebrew |
| Aerospace | 0.18.4-Beta | Homebrew |
| Ghostty | 1.1.3 | Homebrew |

## Terminal Environment

- **Shell**: Zsh
- **Terminal**: Ghostty
- **Prompt**: Starship with Tokyo Night theme
- **Multiplexer**: Zellij (0.42.2)
- **File Manager**: Yazi (25.4.8)
- **Window Manager**: Aerospace (0.18.4-Beta)

## Tool Integration

- **History Management**: Atuin (18.5.0)
- **Directory Navigation**: Zoxide (0.9.7)
- **Fuzzy Finder**: FZF (0.61.3)
- **Task Runner**: Just (1.40.0)
- **File Listing**: Eza (0.21.3)
- **Text Viewing**: Bat (0.25.0)
- **File Searching**: Ripgrep (14.1.1), Fd (10.2.0)

## Development Environment

- **Node.js**: 23.11.0 (via Volta 2.0.2)
- **Python**: 3.12.10, 3.13.3
- **Go**: 1.24.2
- **Rust**: via Rustup 1.28.1
- **Text Editors**: Neovim (0.11.1), VS Code (1.98.2), Cursor (0.48.8)

## Primary Configuration Locations

- **Dotfiles**: ~/dotfiles
- **Config**: ~/.config
- **Cache**: ~/.cache
- **Local Data**: ~/.local/share
- **State**: ~/.local/state

## Environment Variables

Key environment variables that affect the configuration:

```
DOTFILES=/Users/hank/dotfiles
XDG_CONFIG_HOME=/Users/hank/.config
XDG_CACHE_HOME=/Users/hank/.cache
XDG_DATA_HOME=/Users/hank/.local/share
XDG_STATE_HOME=/Users/hank/.local/state
ZDOTDIR=/Users/hank/.config/zsh
EDITOR=nvim
VISUAL=nvim
STARSHIP_CONFIG=/Users/hank/dotfiles/config/starship/starship.toml
ATUIN_CONFIG_DIR=/Users/hank/dotfiles/config/atuin
YAZI_CONFIG_DIR=/Users/hank/dotfiles/config/yazi
ZELLIJ_CONFIG_DIR=/Users/hank/dotfiles/config/zellij
```

## Dotfiles Catalog

Major configuration files and directories:

```
~/dotfiles/
├── config/              # Configuration files
│   ├── aerospace/       # Window manager
│   ├── ai/              # AI tools (Claude, Cline)
│   ├── atuin/           # Shell history
│   ├── bat/             # Cat replacement
│   ├── ghostty/         # Terminal emulator
│   ├── git/             # Git configuration
│   ├── hammerspoon/     # macOS automation
│   ├── nvim/            # Neovim configuration
│   ├── starship/        # Prompt configuration
│   ├── vscode/          # VS Code settings
│   ├── yazi/            # File manager
│   ├── zellij/          # Terminal multiplexer
│   └── zsh/             # Shell configuration
├── scripts/             # Utility scripts
│   └── setup.sh         # Setup script
├── justfile             # Task runner configuration
├── README.md            # Main documentation
└── CLAUDE.md            # Claude AI instructions
```

## Homebrew Status

- **Number of Formulae**: 283
- **Number of Casks**: 74
- **Core Packages**: See Brewfile.core
- **Full Package List**: See Brewfile.full

## Primary Functions

Key functions available in the shell environment:

- `has_command`: Check if a command exists
- `path_add`: Add a directory to PATH
- `f`: FZF-enhanced file finder
- `fgit`: FZF-enhanced git log browser
- `j`: Run Just commands
- `jf`: Fuzzy-find Just commands
- `ja <tool>`: Fuzzy-find tool-specific Just commands

## Update Process

Last update process:

1. Updated repository with `git pull`
2. Ran setup script with `./scripts/setup.sh`
3. Updated Homebrew packages with `brew update && brew upgrade`
4. Created symlinks for configuration files
5. Updated documentation with current versions
# Advanced Prompting Techniques

This document outlines sophisticated AI prompting methods to achieve superior results for complex tasks. These techniques can be combined with task-specific templates for maximum effectiveness.

## Chain of Thought Prompting

### Description
Guide the AI through explicit reasoning steps before reaching a conclusion. This produces more accurate results for complex reasoning tasks.

### Implementation
```markdown
# Chain of Thought Request

I need you to solve this problem step-by-step:

[Problem description]

Please:
1. First, identify the key elements of this problem
2. Break down the approach into clear logical steps
3. Execute each step, showing your work
4. Evaluate the result for correctness
5. Summarize the final answer

Format each step clearly and explain your reasoning at each stage.
```

### When to Use
- Complex logical problems
- Multi-step calculations
- Debugging scenarios
- Complex decision-making with multiple factors

## Recursive Refinement

### Description
Use a staged approach where the AI first generates a draft, then iteratively improves it through self-critique and revision.

### Implementation
```markdown
# Recursive Refinement Request

I need a [type of content] about [subject]. Please approach this in three stages:

## Stage 1: Initial Draft
Create a first version that covers the core elements.

## Stage 2: Self-Critique
Review your draft and identify:
- Areas that need more detail or clarity
- Potential inaccuracies or omissions
- Structural improvements
- Stronger examples or evidence

## Stage 3: Refined Version
Create an improved version that addresses all the issues identified in your critique.

Present both the critique and the final refined version.
```

### When to Use
- Complex writing tasks
- System designs that require optimization
- Code that needs to balance multiple objectives
- Analyses where accuracy is critical

## Persona-Based Prompting

### Description
Direct the AI to adopt specific expert perspectives to leverage specialized knowledge and approaches.

### Implementation
```markdown
# Expert Perspective Request

I need analysis on [topic] from multiple expert perspectives.

Please approach this sequentially as the following experts:

1. As a senior systems architect with 15+ years of experience:
   [Specific questions/aspects to address]

2. As a security specialist focused on zero-trust implementations:
   [Specific questions/aspects to address]

3. As a DevOps engineer who prioritizes maintainability and observability:
   [Specific questions/aspects to address]

4. As a synthesis expert, integrate the perspectives above into final recommendations.

For each perspective, note key insights, blind spots, and specific recommendations.
```

### When to Use
- Getting well-rounded analysis of complex problems
- Identifying potential issues from different viewpoints
- Challenging initial assumptions
- Complex design decisions with multiple stakeholders

## Template Decomposition

### Description
Break complex outputs into parameterized templates with clear placeholders for the AI to fill in.

### Implementation
```markdown
# Template Decomposition Request

I need a [document type] with the following structure. For each section, I've provided guidance on what to include:

# [TITLE: Create a concise, descriptive title]

## Executive Summary
[EXEC_SUMMARY: Provide a 3-5 sentence overview that captures the key points]

## Problem Statement
[PROBLEM: Describe the current situation, challenges, and why this needs to be addressed]

## Proposed Approach
[APPROACH: Detail the recommended solution with justification]

## Implementation Steps
[STEPS: List 5-7 concrete actions in chronological order with owners and timeline]

## Success Metrics
[METRICS: Define 3-5 measurable indicators of success]

Please complete each section according to the instructions in brackets, then remove the bracketed instructions from the final output.
```

### When to Use
- Standardized document creation
- Complex outputs with consistent structure
- When you need to ensure all components are included
- Content that follows established patterns or protocols

## Comparative Analysis Framework

### Description
Structure a detailed comparison across multiple dimensions with explicit evaluation criteria.

### Implementation
```markdown
# Comparative Analysis Request

Please evaluate these options against the specified criteria:

## Options to Evaluate
- [Option 1]
- [Option 2]
- [Option 3]

## Evaluation Criteria
For each criterion, rate options on a scale of 1-10 and provide specific justification:

1. [Criterion 1] (Weight: X%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

2. [Criterion 2] (Weight: Y%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

3. [Criterion 3] (Weight: Z%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

## Output Format
- Provide individual criterion scores with justification
- Calculate weighted total scores
- Rank options from best to worst
- Explain key differentiators
- Include sensitivity analysis (how would rankings change if weights shifted)
```

### When to Use
- Technology selection decisions
- Architecture or design alternatives
- Investment or prioritization decisions
- Strategic option evaluation

## Decision Tree Construction

### Description
Map out a complex decision process with conditional branches based on different scenarios.

### Implementation
```markdown
# Decision Tree Request

Help me build a decision tree for [scenario/problem].

## Initial Conditions
- [Describe starting state and key variables]

## Primary Decision Points
For each primary decision point:
1. Identify the key question at this stage
2. List all possible choices (2-4 options)
3. For each choice:
   - Describe immediate consequences
   - Identify subsequent decision points
   - Note critical dependencies or assumptions

## Evaluation Metrics
At each terminal node (end state), evaluate:
- [Metric 1]
- [Metric 2]
- [Metric 3]

## Visual Representation
Create a text-based diagram of the decision tree showing:
- Decision nodes (questions)
- Option branches
- Terminal nodes (outcomes)
- Critical path highlighting
```

### When to Use
- Complex decision processes with multiple variables
- Scenario planning
- Risk assessment
- Troubleshooting guides

## System Interaction Modeling

### Description
Model complex interactions between system components, focusing on interfaces, dependencies, and data flows.

### Implementation
```markdown
# System Interaction Model Request

Help me understand interactions between components in [system name].

## System Components
- [Component 1]: [Brief description of responsibility]
- [Component 2]: [Brief description of responsibility]
- [Component 3]: [Brief description of responsibility]

## For Each Interface Between Components
1. Identify direction of dependency
2. Describe data/control flow
3. Document interface contract (API, events, etc.)
4. Note synchronicity (sync/async)
5. Identify potential failure modes
6. Document retry/resilience patterns

## Sequence Diagrams
Create text-based sequence diagrams for these key flows:
- [Flow 1]
- [Flow 2]
- [Flow 3]

## Critical Path Analysis
Identify and analyze:
- Performance bottlenecks
- Single points of failure
- Scalability constraints
- Data consistency challenges
```

### When to Use
- Complex distributed system design
- API design and documentation
- Troubleshooting integration issues
- Performance optimization

## Layered Explanation

### Description
Structure explanations across multiple levels of abstraction, from conceptual to implementation details.

### Implementation
```markdown
# Layered Explanation Request

Please explain [topic/concept] using a layered approach:

## Layer 1: Conceptual Overview (ELI5)
Explain the core concept as you would to a non-technical person, using analogies and simplified models.

## Layer 2: Functional Understanding
Explain how this works from a functional perspective - what it does, major components, and how they interact.

## Layer 3: Technical Implementation
Provide implementation details including:
- Specific technologies/methods used
- Important algorithms or patterns
- Code examples where appropriate
- Performance considerations

## Layer 4: Advanced Considerations
Discuss:
- Edge cases and how they're handled
- Optimization techniques
- Alternative approaches
- Common misconceptions
```

### When to Use
- Explaining complex technical concepts
- Educational content
- Technical documentation for diverse audiences
- Knowledge transfer

## Edge Case Exploration

### Description
Systematically identify and address uncommon but important scenarios that might be overlooked.

### Implementation
```markdown
# Edge Case Exploration Request

For this [system/code/process], help me identify and address edge cases:

## System Description
[Brief description of the system/code/process]

## Normal Operation
[Description of typical usage patterns and expected behavior]

## Edge Case Categories
For each category, identify specific edge cases and how to handle them:

1. Input Boundaries
   - Minimum/maximum values
   - Empty/null inputs
   - Malformed inputs
   - Extremely large inputs

2. Resource Constraints
   - Memory limitations
   - CPU constraints
   - Network issues
   - Timeouts

3. Concurrent Operations
   - Race conditions
   - Deadlocks
   - Contention scenarios

4. Integration Points
   - External system failures
   - API contract changes
   - Versioning issues

5. State Transitions
   - Interrupted operations
   - Invalid state sequences
   - Recovery scenarios

## For Each Edge Case
1. Describe the scenario
2. Explain potential impact
3. Recommend detection method
4. Provide handling strategy
5. Suggest testing approach
```

### When to Use
- Critical system design
- Security-sensitive applications
- High-reliability requirements
- Complex state management scenarios

# Code Generation Base Template

## Task

Generate {{language}} code that accomplishes the following:

{{request}}

## Requirements

- The code should be complete, functional, and ready to use.
- Follow industry standard best practices for {{language}}.
- Include appropriate error handling for edge cases.
- Use clear, descriptive variable and function names.
- Add comments to explain complex logic or non-obvious design decisions.
- Optimize for readability and maintainability.

## Specific Guidelines

{{#if language == "typescript" || language == "javascript"}}
- Use modern ES syntax (ES6+)
- Prefer const/let over var
- Use explicit typing in TypeScript
- Follow common conventions like camelCase for variables
- Include JSDoc comments for public functions
{{/if}}

{{#if language == "python"}}
- Follow PEP 8 style guidelines
- Use type hints (for Python 3.6+)
- Include docstrings in Google or NumPy format
- Use list/dict comprehensions where appropriate
- Handle exceptions with specific error types
{{/if}}

{{#if language == "go"}}
- Follow Go conventions (gofmt-compatible)
- Use idiomatic error handling (return errors, don't panic)
- Provide clear doc comments for exported functions
- Use meaningful variable names over single-letter names
- Organize code in logical packages
{{/if}}

{{#if language == "rust"}}
- Use Result and Option types for error handling
- Include appropriate lifetime annotations
- Add doc comments with examples for public functions
- Follow Rust naming conventions (snake_case)
- Use pattern matching where appropriate
{{/if}}

{{#if language == "java" || language == "kotlin"}}
- Follow standard language conventions
- Include exception handling with specific types
- Use appropriate access modifiers
- Add JavaDoc comments for public methods
- Use modern language features where appropriate
{{/if}}

## Output Format

Provide only the complete code implementation without unnecessary explanation. Format it in a code block with the appropriate language tag. If you need to explain any aspects of the implementation, do so in comments within the code.

```{{language}}
// Your implementation will replace this line
```
# AI Master Configuration

## System Overview

This document serves as the central configuration for all AI interactions. The complete system consists of:

1. **Master Configuration** (this file) - Core principles and system overview
2. **Developer Profile** - Your personal context and technical preferences
3. **Response Formatting** - Output structure and visual presentation guidelines
4. **Task Templates** - Purpose-specific prompt templates for different scenarios
5. **Advanced Techniques** - Advanced prompting methods for complex tasks
6. **Tool-Specific Configs** - Model-specific optimization guidelines

## Core Principles

1. **Maximum Context Efficiency**
   - Provide just enough context for optimal output
   - Use reference links to previous conversations when building on past work
   - Clearly separate instructions from content

2. **Structured Output by Default**
   - All responses should follow consistent structural patterns
   - Visual elements should be used where they enhance clarity
   - Complex information should be organized in digestible segments

3. **Progressive Enhancement**
   - Start with base prompts and add specific modules as needed
   - Build complex prompts by combining simpler templates
   - Refine outputs through iterative instructions

4. **Tool Specialization**
   - Use optimal AI model for each specific task
   - Leverage unique capabilities of different AI systems
   - Chain tools together for complex workflows

## Usage Methodology

1. **Task Analysis**
   - Determine primary goal of interaction
   - Identify relevant template modules
   - Consider required output formats

2. **Prompt Assembly**
   - Include developer profile (abbreviated if appropriate)
   - Add task-specific template
   - Specify response format requirements
   - Include any specialized techniques

3. **Response Management**
   - Evaluate output quality against requirements
   - Use follow-up prompts for refinement
   - Extract and store reusable components

4. **Continuous Improvement**
   - Note effective prompt patterns
   - Update templates based on successful interactions
   - Develop specialized templates for recurring tasks

## Integration Framework

These templates support integration with:

- **Claude Models** (Sonnet, Opus, Haiku)
- **OpenAI Models** (GPT-4, GPT-4o)
- **DeepSeek Models** (DeepSeek Coder)
- **Open Source Models** (via Ollama integration)
- **AI-powered Development Tools** (Cursor, GitHub Copilot)
- **Document Processing Tools** (for export/import)

## Template Evolution Strategy

For maximum effectiveness, treat these templates as living documents:

1. When an AI generates exceptionally good output, capture the prompt structure
2. When complex tasks are completed successfully, document the prompt chain
3. When outputs require significant refinement, note the refinement pattern
4. Regularly update templates with new capabilities as AI models evolve

## Quick Reference

| Task Type | Primary Template | Secondary Templates | Optimal AI Model |
|-----------|------------------|---------------------|------------------|
| Code Generation | `coding-primary.md` | `visual-code.md`, `advanced-reasoning.md` | DeepSeek Coder/Claude Sonnet |
| Code Review | `code-analysis.md` | `structured-feedback.md` | Claude Sonnet w/Extended Thinking |
| System Design | `system-design.md` | `visual-architecture.md` | Claude Opus/GPT-4 |
| Data Analysis | `data-analysis.md` | `visual-data.md` | Claude Sonnet |
| Technical Writing | `technical-docs.md` | `doc-formatting.md` | Claude Sonnet |
| Research | `research-synthesis.md` | `citation-format.md` | GPT-4o with browsing |
# Advanced Prompting Techniques

## Context Window Optimization

1. **Reference Compression**
   - Summarize long code/documents before reference
   - Example: "Here's a summary of our 2000-line codebase: [concise description]. Now help me implement a new feature that..."

2. **Selective Information Sharing**
   - Share only relevant portions of large documents
   - Example: "From our API docs, here are the relevant endpoints: [paste relevant sections only]"

3. **Continuation Chaining**
   - Use explicit markers for continuing work across context limits
   - Example: "This is part 2 of our database schema design. Continuing from where we left off with the user authentication tables..."

## Multi-Tool Integration

1. **Tool Specialization**
   - Use different AI models for their strengths
   - Example: "I'll use Claude for system design, GitHub Copilot for implementation, and GPT-4 for documentation"

2. **Workflow Automation**
   - Create scripts or shortcuts for common AI interactions
   - Example: Shell aliases for common AI commands or VS Code snippets for prompt templates

3. **Output Processing**
   - Use one AI to review/improve another's output
   - Example: "Review this code generated by GitHub Copilot for security vulnerabilities and performance issues"

## Progressive Refinement

1. **Iterative Improvement**
   - Start with a basic version and refine in steps
   - Example: "First, generate a simple implementation. Then, we'll optimize it for performance."

2. **Targeted Follow-ups**
   - Use specific follow-up prompts to improve initial results
   - Example: "That's a good start. Now, let's enhance the error handling to cover these edge cases..."

3. **Comparative Analysis**
   - Request multiple approaches and compare pros/cons
   - Example: "Generate three different approaches to this problem, then analyze the tradeoffs"

## Structured Reasoning

1. **Chain-of-Thought Prompting**
   - Ask the AI to reason step-by-step before providing final answer
   - Example: "Think through this problem step-by-step, explaining your reasoning at each stage."

2. **Self-Critique**
   - Have the AI evaluate its own initial response before finalizing
   - Example: "Before finalizing your answer, review your solution for potential issues or optimizations."

3. **Expert Personas**
   - Frame requests as coming from specific expert perspectives
   - Example: "Analyze this code first as a security expert, then as a performance engineer."

## Advanced Output Control

1. **Format Specification**
   - Provide explicit templates for desired output format
   - Example: "Format your response as a JSON object with these specific fields..."

2. **Conditional Instructions**
   - Include context-dependent behavior in prompts
   - Example: "If the code is Python, include type hints. If JavaScript, use JSDoc comments."

3. **Precision vs. Creativity Control**
   - Specify required balance between deterministic answers and creative exploration
   - Example: "For this coding task, prioritize correctness over creativity. Follow established patterns exactly."
# Prompt Engineering Fundamentals

## General Best Practices

### Prompt Engineering Fundamentals

1. **Be Specific and Explicit**
   - Provide clear context, constraints, and expected outputs
   - Example: "Write a Python function that sorts a list of dictionaries by the 'date' key in ISO format (YYYY-MM-DD)"

2. **Use Structured Formats**
   - Bullet points for requirements
   - Numbered lists for sequential steps
   - Headers for organizing complex prompts
   - Example: "Requirements: • Must handle null values • Must be thread-safe • Must complete in O(n) time"

3. **Iterate and Refine**
   - Start with a basic prompt, then refine based on results
   - Save effective prompts as templates for reuse
   - Example follow-up: "The solution works but needs better error handling for invalid inputs"

4. **Leverage System Messages**
   - Set persistent instructions in system prompts or chat settings
   - Define roles and output preferences clearly
   - Example: "You are an expert Python developer focusing on clean, maintainable code with comprehensive error handling"

## Domain-Specific Strategies

### Coding Excellence

1. **Problem Decomposition**
   - Break complex problems into specific components
   - Request system design before implementation
   - Example: "First outline the architecture for this authentication system, then we'll implement each component"

2. **Request Multiple Approaches**
   - Ask for alternative implementations with tradeoffs
   - Example: "Show me two different approaches to solving this problem - one optimized for readability, one for performance"

3. **Pair Programming Flow**
   - Use AI as a pair programming partner
   - Alternate between generating code and reviewing/refactoring
   - Example: "Let's implement this feature step by step. First, the user authentication logic..."

4. **Test-Driven Development**
   - Request tests before or alongside implementation
   - Example: "Write unit tests for this function before implementing it"

5. **Debug Collaboratively**
   - Share error messages and stack traces verbatim
   - Provide context on what you've already tried
   - Example: "I'm getting this error when running the code: [paste error]. I've tried X and Y without success"

### Research & Search Optimization

1. **Question Refinement**
   - Start broad, then narrow through follow-ups
   - Example: "What are modern approaches to database sharding?" → "How would these sharding techniques apply to time-series data?"

2. **Comparative Analysis Requests**
   - Ask for structured comparisons of alternatives
   - Example: "Compare Redis, MongoDB, and PostgreSQL for storing session data with these criteria: scalability, persistence, query complexity, operational overhead"

3. **Synthesis Requests**
   - Request summary of complex topics with key takeaways
   - Example: "Summarize the key developments in transformer architecture since 2020, focusing on efficiency improvements"

4. **Knowledge Stacking**
   - Build on previous responses with targeted follow-ups
   - Example: "Based on these database options, what would be the implementation challenges for our specific traffic pattern?"
# Archive

This directory contains archived content that has been superseded by improved versions or consolidated into other files.

## Structure

- **Duplicates/**: Files with content that duplicates information available elsewhere
- **Low_Value/**: Files that provide minimal unique value
- **Merged_Originals/**: Original files that have been consolidated into more comprehensive resources

## Purpose

This archive exists to:
1. Preserve historical content for reference
2. Maintain a record of the organizational process
3. Provide context for the evolution of prompts and templates
4. Prevent duplicate content from cluttering active directories

## Note

Content in this archive should not be used directly. Instead, refer to the active directories for current, refined versions of these materials.
# Authentication System in Anterior Platform

This document explains how authentication works in the Anterior platform, focusing on the integration between the frontend applications and the Noggin gateway.

## Overview

The Anterior platform implements a multi-step authentication flow:

1. **Login Form** - Users enter credentials (email/password)
2. **OTP Verification** - One-time password sent to user
3. **JWT Tokens** - Upon verification, JWT tokens are issued
4. **Cookie-Based Auth** - Tokens stored in HTTP-only cookies
5. **Enterprise/User Context** - JWT carries enterprise and user information

This system provides secure authentication while maintaining multi-tenant isolation between enterprises.

## Authentication Flow

### 1. Login Process

The authentication process begins when a user visits the login page:

```
/auth/login.html
```

The login page is a static HTML file that presents a form for email and password entry. When submitted, the form sends a POST request to:

```
/auth/login
```

The backend then:
1. Validates the credentials
2. Generates a one-time password (OTP)
3. Redirects the user to the OTP verification page

### 2. OTP Verification

The OTP verification page is accessed at:

```
/auth/otp-verify.html
```

Users enter the OTP they received, and the form submits to:

```
/auth/otp/verify
```

Upon successful OTP verification, the backend:
1. Generates JWT access and refresh tokens
2. Sets HTTP-only cookies containing these tokens
3. Redirects the user to the requested application

### 3. Cookie Management

The system sets three cookies for authentication:

1. **access_token** - Short-lived JWT token (15 minutes)
   ```
   access_token=<JWT>; Path=/; HttpOnly; SameSite=Lax; Max-Age=900
   ```

2. **refresh_token** - Long-lived token for refreshing access (30 days)
   ```
   refresh_token=<JWT>; Path=/; HttpOnly; SameSite=Lax; Max-Age=43200
   ```

3. **auth_indicator** - Non-HTTP-only flag for frontend
   ```
   auth_indicator=true; Path=/; SameSite=Lax; Max-Age=900
   ```

The `HttpOnly` flag prevents JavaScript from accessing the tokens, protecting against XSS attacks, while the third cookie allows the frontend to know the user is authenticated.

## JWT Structure and Enterprise Context

The JWT tokens contain important claims:

```json
{
  "sub": "user123",        // User ID
  "exp": 1683721200,       // Expiration time
  "iat": 1683717600,       // Issued at time
  "entr": {                // Enterprise memberships
    "enterprise456": "admin"
  },
  "wksp": {                // Workspace memberships
    "workspace789": "enterprise456"
  }
}
```

Key components:
- `sub`: The user's unique identifier
- `entr`: Map of enterprise IDs to roles
- `wksp`: Map of workspace IDs to enterprise IDs

This structure enables:
1. Multi-enterprise support (users can belong to multiple enterprises)
2. Role-based access control within each enterprise
3. Workspace context within enterprises

### JWT Parsing and Context Extraction

The `jwt.ts` file handles extraction of enterprise and user information from the JWT payload:

```typescript
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
  if (!ctx.jwtPayload?.entr) {
    throw new HTTPException(401, { message: "Authentication required" });
  }

  // Get enterprise and user uids from the token.
  let enterpriseUid = undefined;
  let role = undefined;

  // Most tokens will have a single enterprise.
  if (Object.keys(ctx.jwtPayload.entr).length === 1) {
    enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
    role = ctx.jwtPayload.entr[enterpriseUid];
  } else {
    // If token contains multiple enterprises.
    // First check for the X-Anterior-Enterprise-Id header.
    const entHeader = ctx.req.headers.get("X-Anterior-Enterprise-Id");
    if (entHeader && ctx.jwtPayload.entr[entHeader]) {
      enterpriseUid = ctx.jwtPayload.entr[entHeader];
      role = ctx.jwtPayload.entr[enterpriseUid];
    } else {
      // Use the first enterprise in the map (random).
      enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
      role = ctx.jwtPayload.entr[enterpriseUid];
    }
  }

  // Get workspaceUid from claims or header if present.
  let workspaceUid = undefined;
  const wksp = ctx.jwtPayload.wksp;
  if (Object.keys(wksp).length === 1) {
    // Token has a single workspace.
    Object.entries(wksp).forEach(([wkId, entId]) => {
      if (entId !== enterpriseUid) {
        throw new HTTPException(401, {
          message: "Workspace in token does not match enterprise",
        });
      }
      workspaceUid = wkId;
    });
  } else {
    // Token claims contain multiple workspaces. Look for
    // the X-Anterior-Workspace-Id header.
    const wkspHeader = ctx.req.headers.get("X-Anterior-Workspace-Id");
    if (wkspHeader) {
      const entId = wksp[wkspHeader];
      if (!entId) {
        throw new HTTPException(401, {
          message: "Workspace from header not found in token",
        });
      }
      if (entId !== enterpriseUid) {
        throw new HTTPException(401, {
          message: "Workspace in header does not match enterprise",
        });
      }
      workspaceUid = wkspHeader;
    }
  }

  return {
    enterpriseUid,
    role,
    workspaceUid,
    userUid: ctx.jwtPayload.sub,
  };
}
```

## API Authentication

When making API requests, the frontend doesn't need to manually handle tokens since the cookies are automatically included in requests to the same domain.

For enterprise-specific contexts, the frontend can include headers:
- `X-Anterior-Enterprise-Id`: Selects which enterprise context to use
- `X-Anterior-Workspace-Id`: Selects a specific workspace within the enterprise

## Token Refresh Mechanism

While not fully implemented in the current code, the system has a token refresh endpoint at:

```
/auth/refresh
```

The refresh flow is designed to:
1. Use the refresh_token cookie to authenticate
2. Generate new access and refresh tokens
3. Set new cookies with updated expiration times

This allows for session persistence without requiring users to log in frequently.

## Frontend Integration

The FloNotes app integrates with this auth system through:

1. **Login Page** - Static HTML served from S3 through Noggin
2. **Client JavaScript** - Handles form submission and error display
3. **Auth Detection** - Checks for the `auth_indicator` cookie

When a user attempts to access the app, the frontend checks for the auth cookie and redirects to login if not present.

## Security Considerations

The auth system implements several security best practices:

1. **HTTP-Only Cookies** - Prevents JavaScript access to tokens
2. **SameSite Policy** - Restricts cookie use to same-site contexts
3. **JWT Expiration** - Short-lived access tokens (15 minutes)
4. **Enterprise Isolation** - Data access limited to authorized enterprises
5. **CSRF Protection** - Planned but not yet implemented

## E2E Testing Authentication

The Anterior platform includes a robust system for handling authentication in end-to-end (E2E) tests. This system is designed to be secure, reusable, and configurable for different testing scenarios.

### Auth State Management

The `AuthStateManager` class in `auth-state.ts` provides persistent storage of authentication state between test runs:

```typescript
export class AuthStateManager {
  private baseDir: string;
  private filename: string;
  private autoCreateDir: boolean;
  private expirationTimeMs: number;

  constructor(options: AuthStateManagerOptions = {}) {
    this.baseDir = options.baseDir || path.join(process.cwd(), '.auth');
    this.filename = options.filename || 'user.json';
    this.autoCreateDir = options.autoCreateDir !== false;
    // Default to 24 hours expiration
    this.expirationTimeMs = options.expirationTimeMs || 24 * 60 * 60 * 1000;

    if (this.autoCreateDir && !fs.existsSync(this.baseDir)) {
      fs.mkdirSync(this.baseDir, { recursive: true });
    }
  }

  // Saves the current browser authentication state to disk
  async save(context: BrowserContext): Promise<string> {
    const playwrightState = await context.storageState();
    const now = Date.now();

    const enhancedState: AuthState = {
      ...playwrightState,
      createdAt: now,
      expiresAt: now + this.expirationTimeMs
    };

    const filePath = this.getStoragePath();
    fs.writeFileSync(filePath, JSON.stringify(enhancedState, null, 2), 'utf-8');

    return filePath;
  }

  // Loads a saved authentication state
  load(): AuthState | null {
    const filePath = this.getStoragePath();

    if (!fs.existsSync(filePath)) {
      return null;
    }

    try {
      const fileContent = fs.readFileSync(filePath, 'utf-8');
      return JSON.parse(fileContent) as AuthState;
    } catch (error) {
      console.error(`Error loading auth state from ${filePath}:`, error);
      return null;
    }
  }

  // Checks if the saved auth state is still valid (not expired)
  isValid(): boolean {
    if (!this.exists()) return false;

    const state = this.load();
    if (!state) return false;

    const now = Date.now();

    return state.expiresAt
      ? state.expiresAt > now
      : state.createdAt
        ? (state.createdAt + this.expirationTimeMs) > now
        : false;
  }
}
```

This allows tests to:
1. Save authenticated browser state after login
2. Reuse that state in future test runs to avoid repeated logins
3. Automatically detect when saved auth is expired
4. Clean up old authentication state when needed

### OTP Handling

The `otp-handler.ts` module provides secure OTP handling for E2E tests:

```typescript
export const otpHandlerOptionsSchema = z.object({
  // Email associated with the OTP
  email: z.string().email(),

  // API endpoint for retrieving OTPs (if available in your environment)
  apiEndpoint: z.string().url().optional(),

  // Environment variable name for test OTP
  otpEnvVar: z.string().optional().default('ANT_TEST_OTP'),

  // Default OTP to use if other methods fail
  defaultOtp: z.string().optional().default('123456'),

  // Whether to obfuscate OTP in logs
  secureLogging: z.boolean().optional().default(true),

  // Optional Playwright request object for API calls
  request: z.any().optional()
});

// Retrieves an OTP for testing purposes with priority order:
// 1. From environment variable
// 2. From API endpoint
// 3. Default OTP
export async function getTestOtp(options: OtpHandlerOptions): Promise<string> {
  // Validate the options with zod schema
  const validatedOptions = otpHandlerOptionsSchema.parse(options);

  const {
    email,
    apiEndpoint,
    otpEnvVar,
    defaultOtp,
    secureLogging,
    request
  } = validatedOptions;

  // 1. Try from environment variable
  if (process.env[otpEnvVar]) {
    const otp = process.env[otpEnvVar];
    console.log(`Using OTP from environment variable ${otpEnvVar}${secureLogging ? ': [REDACTED]' : `: ${otp}`}`);
    return otp as string;
  }

  // 2. Try from API endpoint if provided
  if (apiEndpoint) {
    try {
      // Use Playwright request object if provided
      if (request) {
        const response = await request.get(`${apiEndpoint}?email=${encodeURIComponent(email)}`);
        if (response.ok()) {
          const data = await response.json() as OtpApiResponse;
          if (data && data.otp) {
            console.log(`Retrieved OTP from API${secureLogging ? ': [REDACTED]' : `: ${data.otp}`}`);
            return data.otp;
          }
        }
      } else {
        // Use native fetch if no request object provided
        const response = await fetch(`${apiEndpoint}?email=${encodeURIComponent(email)}`);
        if (response.ok) {
          const data = await response.json() as OtpApiResponse;
          if (data && data.otp) {
            console.log(`Retrieved OTP from API${secureLogging ? ': [REDACTED]' : `: ${data.otp}`}`);
            return data.otp;
          }
        }
      }
    } catch (error) {
      console.warn('Failed to retrieve OTP from API:', error);
    }
  }

  // 3. Use default OTP
  console.log(`Using default OTP for testing${secureLogging ? ': [REDACTED]' : `: ${defaultOtp}`}`);
  return defaultOtp;
}

// Fills and submits the OTP in a test environment
export async function submitOtp(page: Page, otp: string): Promise<boolean> {
  try {
    // Wait for OTP input field
    await page.waitForSelector('input[name="otp"]');

    // Fill the OTP field
    await page.fill('input[name="otp"]', otp);

    // Submit the form
    await Promise.all([
      page.click('button[type="submit"]'),
      page.waitForResponse(response =>
        response.url().includes('/auth') &&
        response.status() < 400
      )
    ]);

    // Check for auth cookie to verify success
    const cookies = await page.context().cookies();
    const hasAuthCookie = cookies.some(cookie => cookie.name === 'auth_indicator');

    return hasAuthCookie;
  } catch (error) {
    console.error('Error submitting OTP:', error);
    return false;
  }
}
```

An important enhancement in the current implementation is the support for using Playwright's `request` object, which provides more consistent network handling in tests.

### Test Utilities

The `test-utils.ts` file provides a complete login workflow for tests:

```typescript
export interface LoginOptions {
  authDir?: string;
  storeAuthState?: boolean;
  expirationTimeMs?: number;
  reuseAuth?: boolean;
  secureLogging?: boolean;
}

export async function login(
  page: Page,
  options?: LoginOptions,
) {
  const {
    ANT_NOGGIN_BASE_URL,
    ANT_TEST_USER_EMAIL,
    ANT_TEST_USER_PASSWORD,
    ANT_TEST_USER_OTP,
    ANT_TEST_USER_STORE_AUTH_STATE,
    ANT_TEST_USER_AUTH_DIR,
    ANT_TEST_USER_SECURE_LOGGING,
    ANT_TEST_USER_API_ENDPOINT,
  } = envVarsSchema.parse(process.env);

  const storeAuthState = options?.storeAuthState ?? ANT_TEST_USER_STORE_AUTH_STATE ?? false;
  const secureLogging = options?.secureLogging ?? ANT_TEST_USER_SECURE_LOGGING ?? false;

  const authDir = options?.authDir || ANT_TEST_USER_AUTH_DIR || path.join(process.cwd(), '.auth');
  if (storeAuthState && !fs.existsSync(authDir)) {
    fs.mkdirSync(authDir, { recursive: true });
  }

  const result: LoginResult = {
    success: false,
    redirected: false,
    authCookie: null,
    accessToken: null,
    storageStatePath: storeAuthState ? path.join(authDir, 'user.json') : null,
    error: null,
  };

  try {
    // Login with email and password
    await page.goto(`${ANT_NOGGIN_BASE_URL}/auth/login.html`);
    await page.waitForSelector('input[name="email"]');
    await page.fill('input[name="email"]', ANT_TEST_USER_EMAIL);
    await page.fill('input[name="password"]', ANT_TEST_USER_PASSWORD);
    await page.click('button[type="submit"]');

    // Wait for OTP verification page
    await page.waitForURL(/otp-verify\.html/);

    // Get OTP securely
    let otp = ANT_TEST_USER_OTP;
    if (!otp) {
      const otpOptions: OtpHandlerOptions = {
        email: ANT_TEST_USER_EMAIL,
        secureLogging: secureLogging,
        otpEnvVar: 'ANT_TEST_OTP',
        defaultOtp: '123456'
      };

      // Only add apiEndpoint if defined
      if (ANT_TEST_USER_API_ENDPOINT) {
        otpOptions.apiEndpoint = ANT_TEST_USER_API_ENDPOINT;
      }

      otp = await getTestOtp(otpOptions);
    } else if (secureLogging) {
      console.log('Using provided OTP: [REDACTED]');
    } else {
      console.log(`Using provided OTP: ${otp}`);
    }

    // Submit OTP
    const otpSuccess = await submitOtp(page, otp);

    if (!otpSuccess) {
      throw new Error('OTP verification failed');
    }

    // Check for redirect
    try {
      await Promise.race([
        page.waitForURL(/\/flonotes/, { timeout: 10000 }),
        page.waitForURL(/\/health/, { timeout: 10000 })
      ]);
      result.redirected = true;
    } catch (error) {
      console.warn('Failed to detect redirect, checking cookies instead');
    }

    // Verify authentication cookies
    const cookies = await page.context().cookies();
    result.authCookie = cookies.find(cookie => cookie.name === 'auth_indicator');
    result.accessToken = cookies.find(cookie => cookie.name === 'access_token');

    if (!result.authCookie) {
      throw new Error('Authentication failed: No auth_indicator cookie found');
    }

    // Store authentication state if requested
    if (storeAuthState && result.storageStatePath) {
      await page.context().storageState({ path: result.storageStatePath });
    }

    result.success = true;

  } catch (error) {
    result.error = error instanceof Error ? error : new Error(String(error));
    if (storeAuthState) {
      // Take a screenshot of failure for debugging
      const screenshotPath = path.join(authDir, 'auth-failure.png');
      await page.screenshot({ path: screenshotPath });
    }
  }

  return result;
}

export async function verifyAuthenticated(page: Page): Promise<boolean> {
  const cookies = await page.context().cookies();
  return !!cookies.find(cookie => cookie.name === 'auth_indicator');
}
```

### Test Fixtures for OTP Testing

The `otp-handler.spec.ts` file introduces test fixtures for better isolation when testing OTP functionality:

```typescript
// Create a test fixture with mockable OTP environment
const test = base.extend<{ mockOtpEnv: undefined }>({
  // Mock environment for OTP testing
  mockOtpEnv: [async ({ }, use) => {
    // Save the original environment variable
    const originalOtp = process.env['ANT_TEST_OTP'];

    // Clear the variable for tests
    delete process.env['ANT_TEST_OTP'];

    // Allow tests to use the mock environment
    await use(undefined);

    // Restore the original value after tests
    if (originalOtp) {
      process.env['ANT_TEST_OTP'] = originalOtp;
    } else {
      delete process.env['ANT_TEST_OTP'];
    }
  }, { scope: 'test' }]
});
```

This approach isolates environment variable changes to specific test runs, preventing test interference.

### PDF API Testing

The PDF API tests in `pdf.spec.ts` show how protected endpoints can be accessed in tests:

```typescript
test("Upload, process, and retrieve extracts for PDF documents", async () => {
  // Process PDF (would fail without auth)
  const processPdfResponse = await fetch(PROCESS_PDFS_URL, {
    method: "POST",
    body: formData,
  });

  // Test remains of the API flow
  const { stemUid, pdfUid } = await processPdfResponse.json();

  // Poll for processing completion
  await expect
    .poll(
      async () => {
        const statusResponse = await fetch(PDF_STATUS_URL(stemUid, pdfUid));
        return statusResponse.status === 200;
      },
      {
        timeout: POLLING_TIMEOUT,
        intervals: [POLLING_INTERVAL],
      }
    )
    .toBeTruthy();

  // Check status and extracts
  // Verify results
});
```

In production code, these requests would include auth cookies automatically. In tests, the cookies are either:
1. Set up before the test using the `login()` function
2. Added manually to the request for isolated API tests
3. Bypassed in tests with a test-specific authentication mechanism

### Test Configuration

Tests use environment variables for configuration:

```typescript
// Environment variables for test auth
const envVarsSchema = z.object({
  ANT_NOGGIN_BASE_URL: z.string().url(),
  ANT_TEST_USER_EMAIL: z.string().email().default('test.user@anterior.com'),
  ANT_TEST_USER_PASSWORD: z.string().default('password123'),
  ANT_TEST_USER_OTP: z.string().optional(),
  ANT_TEST_USER_STORE_AUTH_STATE: z.boolean().optional(),
  ANT_TEST_USER_AUTH_DIR: z.string().optional(),
  ANT_TEST_USER_SECURE_LOGGING: z.boolean().optional(),
  ANT_TEST_USER_API_ENDPOINT: z.string().url().optional(),
});
```

This allows tests to be configured for different environments without code changes.

### Auth Test Scenarios

The tests in `auth.spec.ts` and `auth-state.spec.ts` cover key auth scenarios:

1. **Basic Authentication**: Verifying that login works with valid credentials
2. **OTP Source Flexibility**: Testing different ways to provide OTP codes
3. **Auth State Expiration**: Ensuring expired auth is handled correctly
4. **Auth State Reuse**: Confirming that saved auth state can be reused across test runs

### Security in Tests

The testing framework implements several security best practices:

1. **OTP Redaction**: Option to hide sensitive OTPs in logs
2. **Isolated Test Directories**: Separate directories for test auth state
3. **Automatic Cleanup**: Removing sensitive files after tests complete
4. **Env Var Configuration**: Using environment variables for credentials
5. **Validation**: Schema validation for all inputs and configuration
6. **Test Fixtures**: Isolated environment changes for test runs

## Implementation Components

The auth system consists of these key files:

1. **app.ts** - Main auth endpoints and cookie handling logic
2. **jwt.ts** - JWT parsing and enterprise/user context extraction
3. **auth-state.ts** - Manages authentication state for testing
4. **otp-handler.ts** - Handles OTP verification in tests
5. **test-utils.ts** - Provides authentication utilities for E2E tests
6. **test-e2e/*.spec.ts** - Test files for auth functionality

## Future Improvements

The current implementation has some noted areas for enhancement:

1. **CSRF Protection** - To be enabled when client is ready
2. **Refresh Token Implementation** - Fully functional token refresh flow
3. **Multiple Enterprise Support** - Improved UI for enterprise switching
4. **Workspace Context** - Better integration of workspace selection

## Integration Instructions

To add auth to a new frontend app:

1. **Check Auth State**
   ```javascript
   function isAuthenticated() {
     return document.cookie.includes('auth_indicator=true');
   }
   ```

2. **Redirect to Login if Needed**
   ```javascript
   if (!isAuthenticated()) {
     const currentPath = encodeURIComponent(window.location.pathname);
     window.location.href = `/auth/login.html?redirectTo=${currentPath}`;
   }
   ```

3. **Handle API Requests with Authentication**
   ```javascript
   // No need to manually add tokens - cookies are sent automatically
   fetch('/api/endpoint', {
     credentials: 'include',  // Important: include cookies
     headers: {
       'Content-Type': 'application/json',
       // Optional: specify enterprise context
       'X-Anterior-Enterprise-Id': currentEnterpriseId
     }
   });
   ```

4. **Logout Function**
   ```javascript
   function logout() {
     // Clear cookies by setting Max-Age=0
     document.cookie = 'auth_indicator=; Path=/; Max-Age=0';
     window.location.href = '/auth/login.html';
   }
   ```

5. **Adding E2E Tests for Auth**
   ```typescript
   import { login, verifyAuthenticated } from '../auth/test-utils';

   test('your protected feature', async ({ page }) => {
     // Login before testing protected features
     const result = await login(page);
     expect(result.success).toBe(true);

     // Now run your test with authentication in place
     await page.goto('/your-protected-page');
     // ...test assertions...
   });
   ```

6. **Testing with Custom OTP Values**
   ```typescript
   import { test as base } from '@playwright/test';

   // Create a fixture for OTP testing
   const test = base.extend({
     mockOtp: [async ({}, use) => {
       // Set a custom OTP for test
       const originalOtp = process.env['ANT_TEST_OTP'];
       process.env['ANT_TEST_OTP'] = '555555';

       await use(undefined);

       // Restore original
       if (originalOtp) {
         process.env['ANT_TEST_OTP'] = originalOtp;
       } else {
         delete process.env['ANT_TEST_OTP'];
       }
     }, { scope: 'test' }]
   });

   test('login with specific OTP', async ({ page, mockOtp }) => {
     // Test will use the mocked OTP value
     const result = await login(page);
     expect(result.success).toBe(true);
   });
   ```
# Node.js Build Process in the Platform Repository

This document explains the Node.js build process in the platform repository, focusing on the Nix development environment, package management, and build steps.

## Nix Development Shells

The platform repository uses Nix to create reproducible development environments. There are several specialized development shells available:

| Shell | Command | Purpose |
|-------|---------|---------|
| `npm` | `nix develop .#npm` | Node.js environment with all dependencies for npm projects |
| `nodejs` | `nix develop .#nodejs` | General Node.js development environment |
| `noggin` | `nix develop .#noggin` | Specialized environment for the noggin service |

### `.#npm` Development Shell

When you run `nix develop .#npm`, you enter a specialized Nix shell that:

1. Includes all runtime dependencies for all JavaScript projects in the repository
2. Sets up Node.js with the correct version
3. Provides access to the `ant-npm-build-deptree` tool for dependency management

This shell is defined in the `flake.nix` file as a custom `devShells.npm` attribute that uses `pkgs.mkShell` to create a development environment with all JavaScript service dependencies included through `inputsFrom`.

## Build Process

### Initial Setup

When you run:

```bash
nix develop .#npm
```

You enter the NPM development shell. The shell welcome message provides guidance on what to do next.

### Installing Dependencies

```bash
npm ci --ignore-scripts
```

This command:
1. Installs all dependencies according to the `package-lock.json` file
2. Uses the `--ignore-scripts` flag to prevent execution of arbitrary scripts during installation
3. Sets up the node_modules structure for the NPM workspace

### Building Dependencies Tree

```bash
ant-npm-build-deptree noggin
```

The `ant-npm-build-deptree` tool:
1. Analyzes the dependency tree of the specified service (noggin in this case)
2. Builds all dependencies that the service requires
3. Ensures that local workspace dependencies are built in the correct order

### Building the Service

```bash
npm run --workspace gateways/noggin build
```

This command:
1. Runs the build script defined in the service's package.json
2. Compiles TypeScript to JavaScript
3. Copies static files to the dist directory
4. Makes the output scripts executable

## Core Computer Science Concepts

### Build Systems & Compilation Theory

#### Dependency Resolution and Topological Sorting

At the heart of the build process is **dependency resolution**, implementing a classic computer science algorithm:

1. **Directed Acyclic Graph (DAG)**: Dependencies form a DAG where vertices are packages and edges represent "depends on" relationships
2. **Topological Sort**: Packages must be built in an order such that no package is built before its dependencies
3. **Cycle Detection**: The system must detect and prevent circular dependencies, which would make a valid build order impossible

The `ant-npm-build-deptree` tool performs a sophisticated topological sort of the dependency graph to ensure correct build order.

#### Incremental Compilation

Modern build systems optimize performance through **incremental compilation**:

1. **File Timestamps**: Tracking last-modified timestamps to determine which files need recompilation
2. **Dependency Graphs**: Maintaining a graph of file dependencies to recompile only affected files
3. **Caching**: Storing intermediate build artifacts to avoid redundant work

TypeScript's compiler (`tsc`) implements incremental compilation by:
- Using `.tsbuildinfo` files to track file dependencies and timestamps
- Only recompiling files that have changed or are affected by changes
- Providing the `--incremental` flag to enable this optimization

#### Declarative vs. Imperative Build Systems

The platform repository uses both approaches:

1. **Declarative (Nix)**: Describes the desired end state rather than the steps to achieve it
   - Focuses on what should be built, not how
   - Enables reproducibility and parallel builds
   - Improves caching through functional purity

2. **Imperative (npm scripts)**: Describes a sequence of steps to achieve the desired result
   - More familiar to developers
   - Simpler for straightforward tasks
   - Used for local development workflows

### Environment Isolation & Containerization

#### Process Isolation

The build system leverages several levels of **process isolation**:

1. **Operating System Processes**: Each build command runs in an isolated OS process
   - Memory isolation prevents corruption between builds
   - Process limits contain resource usage
   - Signal handling allows for graceful termination

2. **Nix Environment Isolation**: Nix shells create isolated environments
   - Prevent "works on my machine" problems
   - Ensure reproducible builds across systems
   - Control exact versions of all tools and dependencies

3. **Container Isolation**: Docker provides runtime isolation
   - Namespace isolation for files, network, and processes
   - Resource constraints through cgroups
   - Image layering for efficient storage and distribution

This multi-level isolation implements the computer science principle of **least privilege**, granting each process only the permissions and resources it needs.

#### Virtual Memory and Resource Management

The build system interacts with OS-level **resource management**:

1. **Virtual Memory**: The TypeScript compiler and Node.js use virtual memory for:
   - Memory mapping of source files
   - Heap management for AST construction
   - Garbage collection during compilation

2. **Process Scheduling**: Build parallelism depends on OS scheduling
   - CPU core allocation affects build speed
   - I/O waits during file operations
   - Process priorities may impact performance

3. **File System Operations**: Build performance is heavily I/O-bound:
   - File caching at the OS level affects read performance
   - Write coalescing improves output performance
   - Directory traversal algorithms impact file discovery

### Programming Language Implementation

#### Just-In-Time vs. Ahead-of-Time Compilation

The TypeScript toolchain demonstrates the tradeoffs between compilation approaches:

1. **Just-In-Time (JIT) Compilation**:
   - Used by `tsx` during development
   - Compiles TypeScript code on-demand
   - Provides faster iteration cycles
   - Incurs runtime overhead for compilation

2. **Ahead-of-Time (AOT) Compilation**:
   - Used by `tsc` for production builds
   - Compiles all code before execution
   - Results in faster runtime performance
   - Enables more thorough optimization

3. **Transpilation**:
   - TypeScript to JavaScript conversion is a form of source-to-source translation
   - Type erasure removes static types while preserving runtime behavior
   - Downleveling converts newer JavaScript features to older syntax for compatibility

#### Type Systems and Static Analysis

TypeScript's type system implements several formal type theory concepts:

1. **Structural Type System**:
   - Types are compatible based on structure, not nominal declarations
   - Duck typing: "If it walks like a duck and quacks like a duck..."
   - Enables flexible code reuse without explicit inheritance

2. **Flow Analysis**:
   - Control flow-based type narrowing
   - Null and undefined checks
   - Discriminated unions for type refinement

3. **Generic Types**:
   - Parametric polymorphism allows type-safe code reuse
   - Type inference reduces annotation burden
   - Constraint-based type checking ensures valid operations

The TypeScript compiler (`tsc`) performs sophisticated static analysis, including:
- Data flow analysis to track variable values
- Control flow analysis to determine reachable code
- Call graph analysis to verify function usage

### Module Systems & Dynamic Loading

#### JavaScript Module Resolution

The platform's build system handles several module systems:

1. **CommonJS**:
   - Synchronous `require()` function
   - Module.exports for exposing functionality
   - Node.js's original module system

2. **ES Modules**:
   - Static import/export declarations
   - Asynchronous loading with dynamic import()
   - Tree-shakable by design

3. **TypeScript Module Resolution**:
   - Path mapping through tsconfig.json
   - Module resolution strategies (Node, Classic)
   - Declaration files (.d.ts) for type definitions

#### Module Bundling Algorithms

Modern JavaScript bundlers implement several algorithmic concepts:

1. **Tree Shaking**:
   - Dead code elimination based on static analysis
   - Reachability analysis from entry points
   - Preservation of side effects

2. **Code Splitting**:
   - Graph partitioning to separate code into chunks
   - Dynamic loading boundaries
   - Shared chunk identification

3. **Bundle Optimization**:
   - Constant folding and propagation
   - Function inlining
   - Scope hoisting

### Concurrency & Parallelism

#### Build Parallelization

The build system leverages parallelism at multiple levels:

1. **Task-Level Parallelism**:
   - Independent tasks run concurrently
   - Dependency graph determines execution order
   - Work stealing balances load across cores

2. **File-Level Parallelism**:
   - TypeScript compiler processes multiple files concurrently
   - I/O operations are interleaved
   - Thread pool manages worker threads

3. **Service-Level Parallelism**:
   - Microservices compile independently
   - Docker builds can run in parallel
   - Test suites execute concurrently

#### Event Loop Architecture

Node.js uses an event-driven, non-blocking I/O model:

1. **Single-Threaded Event Loop**:
   - Processes events from an event queue
   - Delegates I/O operations to the operating system
   - Executes callbacks when operations complete

2. **Worker Threads**:
   - Parallel JavaScript execution for CPU-bound tasks
   - Shared memory through ArrayBuffers
   - Message passing for communication

3. **Asynchronous Programming Patterns**:
   - Promises for composable asynchronous operations
   - Async/await for sequential-style asynchronous code
   - Event emitters for reactive programming

### Functional Programming Principles

#### Pure Functions and Immutability

The Nix build system embodies functional programming principles:

1. **Pure Functions**:
   - Build steps have no side effects
   - Given the same inputs, they always produce the same outputs
   - Enable reliable caching and parallelization

2. **Immutability**:
   - Build artifacts are immutable once created
   - No modification of existing files, only creation of new ones
   - Prevents race conditions during parallel builds

3. **Referential Transparency**:
   - Build steps can be replaced with their result
   - Hash-based addressing ensures content integrity
   - Cached results are semantically equivalent to rebuilding

#### Lazy Evaluation

Nix uses lazy evaluation to optimize builds:

1. **Deferred Computation**:
   - Only build what's needed
   - Skip unused dependencies
   - Evaluate expressions only when their values are required

2. **Thunks and Promises**:
   - Build steps are represented as deferred computations
   - Results are memoized after first evaluation
   - Dependencies are tracked for minimal rebuilds

## IDE and Terminal Integration

### Cursor IDE Integration

#### Terminal Shell in Cursor IDE

When using Cursor IDE's integrated terminal:

1. **Environment Inheritance**: Cursor's terminal inherits environment variables from the parent process that launched Cursor.
   - If you launched Cursor from a shell with an active Nix environment, that environment will be inherited
   - If not, you'll need to run `nix develop .#npm` explicitly in the Cursor terminal

2. **Shell Differences**: Cursor's terminal may use a different shell (bash) than your system default (zsh)
   - May require different configuration for shell integration
   - Shell prompt and completions might differ from your customized zsh setup

3. **Working Directory**: The terminal usually opens in the project root, which is convenient for running npm and nix commands

#### Cursor Debugger Integration

Cursor's debugger for Node.js/TypeScript offers advantages over terminal-based development:

1. **Source Mapping**: The debugger automatically maps between TypeScript source and compiled JavaScript
   - No need to manually trace between source and compiled code in the `dist` folder
   - Breakpoints set in `.ts` files work even though the runtime executes `.js` files

2. **Launch Configurations**: You can create custom debug configurations for different services
   - Can specify environment variables
   - Can set arguments and runtime flags
   - Can be shared with team via `.vscode/launch.json`

3. **Runtime vs Build-time**: The debugger runs the actual Node.js process, not the build process
   - For TypeScript projects, you typically need to build first, then debug
   - Or use configurations that run `tsx` directly for development (bypassing the build step)

### Comparison with Ghostty + zsh on macOS

Compared to using Ghostty terminal with zsh on macOS:

1. **Performance**: Ghostty is often faster and more responsive than embedded IDE terminals
   - More efficient rendering
   - Lower latency

2. **Environment Management**: A dedicated terminal session in Ghostty can maintain long-lived Nix environments
   - Less need to frequently re-enter development shells
   - Can maintain multiple environments in different tabs/windows

3. **Shell Configuration**: Your full zsh configuration in Ghostty is available
   - All your aliases, functions, and customizations work as expected
   - Complete shell history

4. **Terminal Features**: Ghostty offers advanced terminal features
   - Better color support
   - Split panes
   - Better font rendering

## TypeScript Build Pipeline

### TypeScript Tools and Their Roles

| Tool | Purpose | Usage |
|------|---------|-------|
| `tsc` | TypeScript compiler | Used in npm build scripts to compile TS to JS |
| `tsx` | TypeScript execution | Runs TS files directly without separate compile step |
| `tsup` | TS bundler | Used for building libraries with dependencies bundled |
| `esbuild` | Fast JS/TS bundler | Used under the hood by tsx and tsup |

### Build Process Flow

```
TypeScript Source (.ts)
       │
       ▼
  Compilation (tsc)
       │
       ▼
JavaScript Output (dist/*.js)
       │
       ▼
   Execution (node)
```

### Development Flow with `tsx`

```
TypeScript Source (.ts)
       │
       ▼
    tsx (runs directly)
```

### Folder Structure

- `src/`: Contains TypeScript source files
- `dist/`: Contains compiled JavaScript files after running `npm run build`
- `node_modules/`: Contains installed dependencies
- `node_modules/.bin/`: Contains executable scripts from dependencies

### When to Use Each Tool

- **`tsc`**: Used in the build process to generate production-ready JavaScript
  - Performs full type-checking
  - Creates `.js` and `.d.ts` files in the `dist` directory
  - Used in CI/CD and production builds

- **`tsx`**: Used during development for quick iteration
  - Runs TypeScript files directly without separate compilation step
  - Faster development cycle (no build step)
  - Used in `npm run start` scripts for local development

- **Node with `dist/`**: Used in production
  - Runs the compiled JavaScript files
  - No TypeScript overhead in production
  - More efficient execution

## Technical Implementation

### NPM Workspace Structure

The platform repository uses NPM workspaces, defined in the root `package.json`:

```json
"workspaces": [
  "experimental/clinical_tool/frontend",
  "gateways/noodle",
  "gateways/noggin",
  "lib/ts/lib-infra",
  "lib/ts/lib-platform",
  "services/*",
  "surfaces/*",
  "gen/ts/*",
  "experimental/*"
]
```

This workspace configuration allows for:
- Sharing dependencies across projects
- Local referencing of internal packages
- Centralized dependency management

### Nix Integration

The repository leverages the `npm2nix` approach for integrating NPM packages with Nix:

1. `npm2nix.nix` contains tooling to transform npm dependencies into Nix derivations
2. `anterior-js-service.nix` defines the structure for JavaScript services, including:
   - Development environment setup
   - Build steps
   - Docker image creation
   - Test configuration

### Docker Build Process

For production deployment, the system creates Docker images with:

1. Layered architecture for efficient caching
2. Minimal runtime dependencies
3. Proper entrypoint configuration with migrate & main scripts
4. Secrets management through SOPS

## Security Considerations

### Supply Chain Security

The build system implements several measures to protect against supply chain attacks:

1. **Deterministic Builds**:
   - Nix ensures reproducible builds with identical outputs
   - Cryptographic verification of input sources
   - Lockfiles pin exact dependency versions

2. **Dependency Auditing**:
   - Regular security scanning of dependencies
   - Explicit allowlisting of trusted dependencies
   - Version pinning to prevent unexpected updates

3. **Isolated Build Environments**:
   - Builds run in isolated contexts
   - Network access is restricted during builds
   - Only specified dependencies are available

### Principle of Least Privilege

The system implements the principle of least privilege through:

1. **Containerization**:
   - Services run with minimal capabilities
   - Resource limits prevent denial-of-service
   - Network policies restrict service communication

2. **Secret Management**:
   - Secrets are encrypted at rest
   - Runtime-only access to decrypted secrets
   - Scope-limited secrets per service

## Performance Optimization

### Build Time Optimization

Several strategies minimize build times:

1. **Caching**:
   - Nix's content-addressable store caches build artifacts
   - Cached results are reused when inputs haven't changed
   - Local and remote caching share results across developers

2. **Parallelization**:
   - Multiple CPU cores utilized concurrently
   - I/O operations overlapped with computation
   - Independent services built in parallel

3. **Incremental Builds**:
   - Only changed files are reprocessed
   - Dependency tracking determines affected modules
   - Build checkpoints prevent redundant work

### Runtime Performance

The TypeScript build process optimizes runtime performance through:

1. **Tree Shaking**:
   - Eliminating dead code
   - Removing development-only features
   - Reducing bundle size

2. **Code Optimization**:
   - Inlining functions for faster execution
   - Constant folding and propagation
   - Eliminating redundant code

3. **Module Chunking**:
   - Breaking code into loadable chunks
   - Lazy loading non-critical functionality
   - Optimizing initial load time

## Common Workflows

1. **Development with IDE Debugger**:
   ```bash
   nix develop .#npm
   npm ci --ignore-scripts
   ant-npm-build-deptree noggin
   # Then use Cursor's debugger to launch with breakpoints
   ```

2. **Development with tsx (fast iteration)**:
   ```bash
   nix develop .#npm
   npm ci --ignore-scripts
   ant-npm-build-deptree noggin
   npm run --workspace gateways/noggin start
   ```

3. **Production Build**:
   ```bash
   nix develop .#npm
   npm ci --ignore-scripts
   ant-npm-build-deptree noggin
   npm run --workspace gateways/noggin build
   node gateways/noggin/dist/index.js
   ```

4. **Building all services**:
   ```bash
   nix develop
   ant build dev
   ```

5. **Running services locally**:
   ```bash
   nix develop
   ant up
   ```

6. **Auto-rebuild on changes**:
   ```bash
   nix develop
   ant watch
   ```

## Software Design Patterns

The build system and TypeScript codebase implement several design patterns:

### Factory Pattern

Nix's derivation system implements the Factory pattern:

1. **Derivation as Factories**:
   - Create complex objects (build artifacts) through a common interface
   - Hide instantiation logic behind a consistent API
   - Allow specialization through parameters

2. **Builder Pattern Variation**:
   - Step-by-step construction of complex objects
   - Clear separation of construction from representation
   - Fluent interface for configuration

### Observer Pattern

The development workflow uses the Observer pattern:

1. **File Watchers**:
   - Files are observable subjects
   - Build tools subscribe to file changes
   - Change events trigger rebuilds

2. **Event-based Architecture**:
   - Components communicate through events
   - Loose coupling between producers and consumers
   - Asynchronous processing of build steps

### Decorator Pattern

TypeScript's type system enables compile-time decorators:

1. **Method and Class Decoration**:
   - Extend behavior without modifying source
   - Apply cross-cutting concerns consistently
   - Layer functionality through composition

2. **Higher-order Functions**:
   - Functions that take or return other functions
   - Wrap behavior with additional functionality
   - Enable composition of behaviors

## Conclusion

The platform repository's Node.js build process combines NPM workspaces with Nix for a reproducible development and build environment. This approach provides developer-friendly workflows while ensuring consistent builds across all environments.

Whether using Cursor IDE with its integrated debugger or a standalone terminal like Ghostty with zsh, the underlying build processes remain the same, though the developer experience differs in terms of convenience features, performance, and workflow integration.

The build system demonstrates sophisticated computer science principles, from dependency resolution and compilation theory to concurrency models and functional programming concepts. Understanding these underlying concepts helps developers leverage the system effectively and contribute to its evolution.
# Flopilot Chat and PDF Architecture

This document outlines the architecture and flow of Flopilot's chat functionality and PDF handling.

## Class Diagram

```mermaid
classDiagram
    class FloPilotRoot {
        -useState() input
        +render()
    }

    class PromptBox {
        -message: string
        -setMessage: function
        +handleAttachClick()
        +render()
    }

    class PromptController {
        +handleSend(message, stemUid, pdfUids)
    }

    class PdfProcessController {
        -useState() processedFiles
        +processPdf(files)
        +handlePdfChange(e)
        +handleRemovePdf()
        +isPdfProcessingMap
    }

    class Chat {
        +render()
    }

    class ChatController {
        -messages: array
        -input: string
        +handleInputChange(e)
        +handleSubmit(e)
        +append(message)
    }

    class ChatInput {
        -input: string
        -handleInputChange: function
        -handleSubmit: function
        +render()
    }

    class Message {
        -content: string
        -messageId: string
        +render()
    }

    class UserMessage {
        -id: string
        -content: string
        +render()
    }

    class AttachedFile {
        -file: File
        -isFileProcessing: boolean
        -onRemove: function
        +render()
    }

    class DataSourcesLabel {
        -hasAttachments: boolean
        +render()
    }

    class PdfStatus {
        -status: "processing" | "ready" | "error"
        -message?: string
    }

    class PdfProcess {
        -stemUid: string
        -pdfUids: string[]
    }

    class PdfExtracts {
        -result: object
        -extracts: array
    }

    FloPilotRoot --> PromptBox: contains
    FloPilotRoot --> SavedPromptsSection: contains
    PromptBox --> PromptController: uses
    PromptBox --> PdfProcessController: uses
    PromptBox --> AttachedFile: renders
    PromptBox --> DataSourcesLabel: renders
    Chat --> ChatController: uses
    Chat --> Message: renders
    Chat --> UserMessage: renders
    Chat --> ChatInput: contains
    PdfProcessController --> PdfStatus: uses
    PdfProcessController --> PdfProcess: uses
    ChatController --> Message: manages
```

## Sequence Diagram: Chat Flow

```mermaid
sequenceDiagram
    participant User
    participant PromptBox
    participant PdfProcessController
    participant Router
    participant ChatComponent
    participant ChatController
    participant Backend

    User->>PromptBox: Enters message
    User->>PromptBox: Clicks send
    PromptBox->>PromptController: handleSend(message, stemUid, pdfUids)
    PromptController->>Router: navigate to chat route with params
    Router->>ChatComponent: Render Chat component
    ChatComponent->>ChatController: Initialize with stemUid, pdfUids

    Note over ChatController: useEffect checks for initialMessage
    ChatController->>ChatController: append user message to messages
    ChatController->>Backend: POST request to /chat
    Backend-->>ChatController: Stream response
    ChatController->>ChatComponent: Update messages state
    ChatComponent->>Message: Render assistant message
    Message-->>User: Display message

    User->>ChatInput: Type response
    User->>ChatInput: Submit message
    ChatInput->>ChatController: handleSubmit(message)
    ChatController->>ChatController: append user message
    ChatController->>Backend: POST request to /chat
    Backend-->>ChatController: Stream response
    ChatController->>ChatComponent: Update messages state
    ChatComponent->>Message: Render assistant message
    Message-->>User: Display message
```

## Sequence Diagram: PDF Upload and Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant PromptBox
    participant PdfProcessController
    participant Backend
    participant Router

    User->>PromptBox: Click Attach button
    PromptBox->>User: Show file picker dialog
    User->>PromptBox: Select PDF file(s)
    PromptBox->>PdfProcessController: handlePdfChange(files)
    PdfProcessController->>PdfProcessController: Create FormData
    PdfProcessController->>Backend: POST /pdfs/process-pdfs
    Backend-->>PdfProcessController: Return stemUid and pdfUids
    PdfProcessController->>PdfProcessController: setProcessedFiles(result)

    loop For each PDF
        PdfProcessController->>Backend: GET /pdfs/pdf-status/{stemUid}/{pdfUid}
        Backend-->>PdfProcessController: Return status (processing/ready/error)

        alt If status is "processing"
            PdfProcessController->>PdfProcessController: Set refetch interval (5s)
            PdfProcessController->>PdfProcessController: Update isPdfProcessingMap
            PdfProcessController->>PromptBox: Re-render with processing status
        else If status is "ready"
            PdfProcessController->>PdfProcessController: Update isPdfProcessingMap
            PdfProcessController->>PromptBox: Re-render with ready status
        else If status is "error"
            PdfProcessController->>PdfProcessController: Throw error
            PdfProcessController->>PromptBox: Display error message
        end
    end

    User->>PromptBox: Enter message and click send
    PromptBox->>Router: Navigate to chat with stemUid and pdfUids
```

## Key Technical Details

### PDF Processing Flow
- PDF files are uploaded via FormData to `/pdfs/process-pdfs` endpoint
- Backend assigns a unique stemUid and pdfUids for each uploaded file
- Frontend polls the PDF status endpoint to check processing progress
- Processing states are tracked in isPdfProcessingMap for UI feedback

### Chat Flow with PDFs
- When navigating to the chat route, stemUid and pdfUids are passed as URL parameters
- ChatController initializes with these parameters and includes them in API calls
- The backend uses the PDFs as context for the chat conversation
- User messages and AI responses use the same chat component structure

### Data Architecture
- Each chat session has a unique stemUid identifier
- PDF documents have unique pdfUids linked to the session stemUid
- Messages are streamed from the backend using AI SDK's useChat hook
- PDF processing status is polled at 5-second intervals until complete

### Technologies Used
- **TanStack Router** for routing
- **React Query** for server state management and API calls
- **AI SDK** for chat functionality
- **Zod** for schema validation

### Architectural Patterns
- **Model-View-Controller (MVC)** pattern
- Clear separation between UI components (view), business logic (controllers), and data models
- Reactive UI updates based on state changes
- Polling pattern for long-running processes
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Structure

This repository contains multiple related projects:

- `/foundations/` - Monorepo with various Python projects, including the router project
- `/platform/` and `/platform-lib-api-extractor/` - Frontend and backend application codebases
- `/ant-pdf-processor/` - PDF processing service
- `/anterior/` - Various projects and scripts
- `/vibes/`, `/rowboat/`, `/awesome-keys/` - Utility projects and tools

## Key Commands

### Python Projects

Python projects use `uv` for environment management:

```bash
# Setup virtual environment
cd <project_dir>
uv venv
# Activate the virtual environment (follow printed instructions)
uv sync

# Run tests (unit tests that don't depend on external services)
pytest -m 'not dependencies'

# Run a specific test
pytest path/to/test_file.py::TestClass::test_function -v

# Run tests with markers
pytest -m "unit"
pytest -m "integration"

# Lint code
ruff check
ruff check --fix  # Auto-fix linting issues

# Format code
ruff format

# Setup pre-commit
python scripts/setup_precommit.py
```

### JavaScript/TypeScript Projects

JavaScript/TypeScript projects use NPM workspaces:

```bash
# Set up local JS environment
nix develop .#npm

# Install dependencies
npm install --strict-peer-deps true --prefer-dedupe true

# Add a new dependency
npm install --strict-peer-deps true --prefer-dedupe true -S your-new-dependency

# Build a workspace
npm run --workspace your/dir build

# Run unit tests
npm run --workspace your/dir test
```

### Nix Commands

```bash
# Build and check everything
nix flake check -L

# Format files
nix fmt

# Enter development shell
nix develop
```

### Docker Commands

```bash
# Build docker images
ant build dev

# Run services locally
ant up

# Watch for changes and rebuild/restart
ant watch

# Run with code regeneration
ant regen proto
ant regen models
ant regen openapi-client
```

### PDF Processor

For the ant-pdf-processor project:

```bash
# Install dependencies
poetry install
poetry shell

# Run the API locally
uvicorn ant_pdf_processor.main:app --reload

# Deploy to GCP Cloud Run
./build_and_deploy.sh
```

## High-Level Code Architecture

The repository contains several key components:

1. **Platform** - A monorepo with a microservices architecture that's transitioning away from gRPC/protobuf toward a more unified API gateway (Noggin). Contains:
   - Services in Python, Go, and TypeScript
   - Gateways (including Noggin API Gateway)
   - Surfaces (frontend applications, including Cortex)
   - Workflows (async tasks using Prefect)

2. **Foundations** - Contains core libraries and utilities:
   - Router - Core routing service for handling external requests
   - IAC - Infrastructure as code components
   - SDK - Client libraries for interacting with services

3. **PDF Processor** - A standalone service for processing PDFs, which can be deployed to GCP Cloud Run.

## Technology Stack

- **Languages**: Python 3.12, TypeScript, Go
- **Python Package Management**: uv
- **JavaScript Package Management**: NPM workspaces
- **Build System**: Nix, Docker
- **API Frameworks**: FastAPI
- **Frontend**: NextJS
- **CI/CD**: GitHub Actions, Nix
- **Infrastructure**: AWS, GCP Cloud Run
- **Async Workflow**: Prefect
- **Secrets Management**: SOPS with AWS KMS

## Development Patterns

### Python Conventions

- Python 3.12 is the standard version
- Use Google-style docstrings only when they provide genuine value
- Type annotations are required for all functions and variables
- Follow DRY principles and prefer OOP patterns when appropriate
- Functional core, imperative shell: keep business logic as pure functions

### JavaScript/TypeScript Conventions

- Separate build and run steps
- Use NPM workspaces for managing dependencies
- No bun lockfiles, only one package-lock.json at the root
- Unit tests must be runnable without environment variables or other services

### Testing Philosophy

- Test behavior, not implementation
- Prefer real implementations over mocks when possible
- Mock only external dependencies when necessary
- Use pytest fixtures effectively
- Keep test code clean and organized

### Code Organization

- Follow existing code conventions in the repository
- Respect module boundaries and dependency structure
- Keep related code together
- Provide appropriate documentation
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This repository contains multiple projects spanning different languages and technologies:

- **Brrr**: High-performance workflow scheduling system (Python)
- **Zellij**: Terminal workspace/multiplexer (Rust)
- **Obsidian-Vimrc-Support**: Plugin for Obsidian.md
- **Platform/Anterior**: Healthcare platform monorepo (multi-language)
- **Nixos-config**: NixOS configuration system
- Several smaller utilities and tools

## Common Commands

### Nix-based Development (Platform/Anterior)

```bash
# Enter development shell
nix develop

# Build Docker images for development
ant build dev

# Run services locally
ant up

# Run with auto-rebuild on file changes
ant watch

# Populate services with test data
ant populate

# Run all tests and sanity checks
nix flake check -L

# Load testing
ant load-test [API_IP_ADDRESS]
```

### Python Projects (Brrr, workflows)

```bash
# Install dependencies
uv pip install -e .

# Run tests
pytest
pytest -m 'not dependencies'  # Skip tests requiring external dependencies

# Run specific test
pytest tests/test_file.py::test_function -v

# Brrr demo
nix run github:nobssoftware/brrr#demo
curl 'http://localhost:8333/hello?greetee=John'
```

### Rust Projects (Zellij)

```bash
# Run debug build
cargo xtask run

# Run all tests
cargo xtask test

# Build release version
cargo build --release
```

### JavaScript/TypeScript Projects

```bash
# Install dependencies
npm install

# Run linting
npm run lint

# Run tests
npm run test

# Build
npm run build
```

## Architecture

### Brrr

- Queue & database-agnostic workflow scheduling system
- Minimal core with extension points for different backends
- Supports Redis and SQS as queues, DynamoDB as database
- Designed for horizontal scaling

### Platform/Anterior

- Microservices architecture with Docker containerization
- Services communicate via HTTP/REST APIs with OpenAPI specs
- Prefect for workflow orchestration
- Nix for reproducible builds
- Frontend surfaces: Cortex (React/TypeScript)

### Zellij

- Terminal multiplexer with workspace focus
- Plugin system supporting WebAssembly
- Multi-pane layout system with floating windows

## Development Workflows

### Environment Setup

1. Install Nix package manager
2. Run `nix develop` to enter development shell
3. Configure editor with EditorConfig support
4. For secrets, use SOPS with correct key access

### Secret Management

- Secrets stored in `secrets.json` and `secrets.prod.json`
- Use 1Password for secure storage
- Never commit unencrypted secrets

### Running Services

1. Run `ant up` to start all services
2. Use `ant watch` for development with auto-rebuild
3. Access specific service documentation in their respective README files

### Testing

Follow testing protocols specific to each project:
- Python: pytest with appropriate markers
- JavaScript: Jest/Vitest
- Rust: Cargo test suite

## Additional Resources

- Development guides in `/development-guides/` directory
- Architecture documentation in `/docs/` directory
- Service-specific README files in each service directory

# Platform Code-Generation Overview

> **TL;DR**  `schemas → codegen → gen → lib → app`.
>
> *Schemas are the specs* → *Codegen turns them into language-specific artefacts* → *Generated artefacts live in `gen/`* → *Hand-written helper libraries in `lib/` wrap those artefacts* → Everything above is consumed by services, gateways, workflows, and front-ends.

---

## 1  `schemas/` – Single Source of Truth

| Sub-dir | Kind of schema | Typical toolchain |
|---------|----------------|-------------------|
| `proto/` | Protobuf service & message definitions | **Buf** → Go/Python/TS gRPC stubs |
| `openapi/` | REST/OpenAPI specifications | **openapi-generator** → typed clients |
| `events/` | Event envelopes (e.g. Kafka, SNS) | Registry docs / Avro / JSON-Schema |
| `data/` | Raw data models (JSON-Schema) | Code-model generators |
| `workflows/` | Workflow DSL / state machines | Prefect, Temporal, etc. |

*Never* import from here at runtime – these files are language-agnostic specifications only.

## 2  `codegen/` – All Generators & Templates

The `codegen/` directory contains containerised scripts (usually run via `ant codegen`) that read from `schemas/` and write to `gen/`.

Key folders:

* **`codegen/proto/`** – Runs Buf to emit Go & Python protobuf stubs.
* **`codegen/models/`** – Converts JSON-Schema → Go structs / Pydantic / Zod.
* **`codegen/openapi-client/`** – Generates REST API clients for multiple languages.
* **`codegen/compose/`** – Stamps out many environment-specific Docker Compose files.

These scripts are *build-time only*; nothing inside `codegen/` is imported by application code.

## 3  `gen/` – Generated Artefacts (Checked-in)

`gen/` is the mounted output directory for all generators.  Layout:

```
 gen/
   ├─ go/       # Go structs, clients, mocks …
   ├─ python/   # Pydantic & FastAPI models …
   ├─ ts/       # Zod schemas, TypeScript types …
   ├─ compose/  # Pre-baked docker-compose YAMLs
   └─ docs/     # Reference documentation
```

Rules:

1. Never hand-edit files in `gen/`.
2. Always commit the diff so CI/CD & IDEs don’t require local regeneration.

## 4  `lib/{go,ts,python}/` – Hand-Written Shared Libraries

These packages sit *on top* of the raw generated code, providing ergonomics, helpers and business logic.  Examples:

* `lib/ts/lib-platform/…` – HTTP app framework, structured logging helpers, Zod utilities.
* `lib/go/prefect/…` – Convenience wrappers around Prefect-generated types.
* `lib/python/src/lib/…` – Cross-service Python utilities.

Application layers import from **both** `gen/` and `lib/`.

## 5  `compose/` vs `codegen/compose/`

* `codegen/compose/` – Templates + generation logic.
* `compose/` – The frozen YAMLs produced by that logic.

Same pattern: template → generated output.

## 6  Developer Workflow

```text
1. Edit a schema   (schemas/…)
2. Run codegen      ant codegen
3. Commit schema change + generated diff (gen/…)
4. Extend helpers   (lib/…) if needed
5. Use new types    in services, gateways, etc.
```

Keep this pipeline in mind whenever you wonder, “Where does this type/client come from?” – start at `schemas/` and follow the arrows downstream.
/Users/hank/dotfiles/AI/PDF.anterior.md /Users/hank/dotfiles/AI/PRD-2.md /Users/hank/dotfiles/AI/PRD.md /Users/hank/dotfiles/AI/PDF_CHAT.md /Users/hank/dotfiles/AI/NOGGIN.md /Users/hank/dotfiles/AI/NIX-SETUP.md /Users/hank/dotfiles/AI/Frontend-Backend-Integration.anterior.md /Users/hank/dotfiles/AI/DEMO.md /Users/hank/dotfiles/AI/CONVENTIONAL_COMMITS.md /Users/hank/dotfiles/AI/CODEGEN.md /Users/hank/dotfiles/AI/AUTH.md /Users/hank/dotfiles/AI/BUILD.NODE.md /Users/hank/dotfiles/AI/CHAT.md /Users/hank/dotfiles/AI/CLAUDE_HISTORY.md /Users/hank/dotfiles/AI/CLAUDE.local.md /Users/hank/dotfiles/AI/CLAUDE.md

# Anterior PDF Processing Architecture

This document details the PDF processing architecture used in the Anterior platform, specifically the integration between the `platform` repository's Noggin gateway and the `vibes` repository's frontend applications.

## Overview

The PDF processing system in Anterior follows a microservices architecture with:

1. **Backend Services** (in `platform` repo) - Handle PDF storage, processing, and extraction
2. **Frontend Applications** (in `vibes` repo) - Provide user interfaces for PDF upload, viewing, and analysis
3. **API Gateway** (Noggin) - Serves as the communication layer between frontends and backend services

This architecture allows for a clean separation of concerns while enabling sophisticated PDF processing capabilities across multiple applications.

## System Components

### 1. Noggin Gateway (`platform/gateways/noggin`)

The Noggin gateway acts as the primary API endpoint for all PDF operations, implemented in `src/pdfs/app.ts`.

#### Key Endpoints

| Endpoint                         | Method | Purpose                  | Parameters/Body                       | Response                                                           |
| -------------------------------- | ------ | ------------------------ | ------------------------------------- | ------------------------------------------------------------------ |
| `/process-pdf`                   | POST   | Upload and process a PDF | `file` (PDF blob), optional `stemUid` | `{ stemUid: string, pdfUid: string }`                              |
| `/pdf-status/:stemUid/:pdfUid`   | GET    | Check processing status  | URL params: stemUid, pdfUid           | `{ status: "processing" \| "ready" \| "error", message?: string }` |
| `/pdf-extracts/:stemUid/:pdfUid` | GET    | Retrieve extracted data  | URL params: stemUid, pdfUid           | JSON structure of extracted content                                |

#### Authentication & Authorization

- Uses JWT-based authentication via HTTP-only cookies
- Extracts enterprise and user information from the JWT token
- Associates PDFs with specific enterprises for multi-tenant isolation

#### Implementation Details

1. **PDF Upload Process**:

   ```typescript
   // Creates a unique PDF ID and optionally a stem ID
   const pdfUid = generateRandomId("pdf");
   const stemUid = ctx.body.stemUid || generateRandomId("stm");

   // Stores the raw PDF in S3
   await ctx.platform.blobs.storeBlob(s3InputKey, blob, "application/pdf");

   // Schedules asynchronous processing
   await ctx.platform.flows.schedule(flowParams);
   ```

2. **Storage Structure**:

   - Raw PDFs: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
   - Extracted data: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json`

3. **Error Handling**:
   - Maps common S3 errors to appropriate HTTP responses
   - Returns 404 for PDFs still processing or not found
   - Returns 500 for processing service failures

### 2. PDF Processing Flow

1. **Upload**: Frontend uploads PDF to Noggin
2. **Initial Storage**: Noggin stores raw PDF in S3
3. **Processing**: Noggin schedules an asynchronous workflow
4. **Extraction**: Workflow processes PDF and stores JSON extracts
5. **Status Check**: Frontend polls status endpoint until processing completes
6. **Retrieval**: Frontend fetches and displays extracted content

### 3. Vibes Apps Implementation

#### A. Flonotes App (`vibes/apps/flonotes`)

Flonotes provides advanced PDF viewing and annotation capabilities for clinical documentation.

**Key Components**:

1. **API Integration** (`src/services/noggin-api.ts`):

   - Complete client implementation for Noggin PDF endpoints
   - Handles authentication, error management, and retries
   - Implements status polling with exponential backoff

2. **PDF Processing** (`src/hooks/use-process-pdf.ts`):

   - Custom React hook that manages PDF processing state
   - Tracks upload progress, processing status, and results

3. **PDF Viewer** (`src/components/shell/pdf/`):

   - Rich PDF viewing UI with page navigation
   - Highlight and annotation capabilities
   - Citation linking between PDF content and notes

4. **IndexedDB Integration** (`src/indexed-db/db.ts`):
   - Local storage of PDF metadata and processing results
   - Enables offline viewing of previously processed PDFs

**Sample API Usage**:

```typescript
// Process a PDF and retrieve extractions
const extractsResponse = await processAndGetPdfExtracts(
  fileData, // PDF binary data as number[]
  fileName, // Original filename
  "clinical", // Document type (clinical or criteria)
  undefined, // Optional stemUid
  100, // Max polling retries
  5000, // Polling interval (ms)
);
```

#### B. Flopilot App (`vibes/apps/flopilot`)

Flopilot uses PDFs as context for AI-assisted interactions and chat functionality.

**Key Components**:

1. **Controller Pattern** (`src/controller/use-pdf-process-controller.ts`):

   - State management for PDF processing
   - Integration with chat context

2. **PDF Models** (`src/controller/models/`):

   - `pdf-extracts.ts` - Types for extracted PDF content
   - `pdf-status.ts` - Status response handling
   - `process-pdf.ts` - PDF upload request models

3. **Chat Integration**:

   - Uses processed PDFs as context for AI-assisted chats

   - Links relevant PDF content to chat messages

## Technical Details

### 1. Stem Concept

A "stem" (`stemUid`) represents a logical grouping of related documents:

- Multiple PDFs can share the same `stemUid`
- Enables contextual relationships between documents
- Used for organizing documents in a case or workflow

### 2. Processing Pipeline

1. **PDF Upload**:

   - Multipart form data upload with optional metadata
   - Returns immediately with tracking IDs before processing completes

2. **Asynchronous Processing**:

   - Uses workflow engine for reliable, scalable processing
   - Extracts text, structure, and semantic information
   - Produces standardized JSON output

3. **Status Checking**:

   - Simple polling mechanism for checking completion
   - Returns processing, ready, or error states
   - Includes error details when processing fails

4. **Extract Retrieval**:
   - Fetches JSON representation of processed PDF
   - Structure includes text blocks, page information, and metadata
   - May include domain-specific extractions (clinical findings, etc.)

### 3. JSON Extract Structure

```json
{
  "result": {
    "metadata": {
      "title": "Document Title",
      "author": "Author Name",
      "pages": 5,
      "creationDate": "2023-01-15T12:00:00Z"
    },
    "pages": [
      {
        "pageNumber": 1,
        "blocks": [
          {
            "id": "block-1",
            "text": "Example text content",
            "boundingBox": {
              "x1": 100,
              "y1": 100,
              "x2": 400,
              "y2": 150
            },
            "type": "paragraph"
          }
          // More blocks...
        ]
      }
      // More pages...
    ],
    "structure": {
      // Document structure information
    }
  }
}
```

## Integration Instructions

To integrate PDF processing in a new vibes app:

### 1. Setup API Client

Create a service file that interfaces with Noggin PDF endpoints:

```typescript
// Example minimal API client
export async function processPdf(
  fileData: number[],
  fileName: string,
): Promise<{ stemUid: string; pdfUid: string }> {
  const formData = new FormData();
  formData.append("file", new Blob([new Uint8Array(fileData)]), fileName);

  const response = await fetch(`${NOGGIN_HOST}/pdfs/process-pdf`, {
    method: "POST",
    body: formData,
    credentials: "include", // For auth cookies
  });

  if (!response.ok) {
    throw new Error(`PDF processing failed: ${response.status}`);
  }

  return await response.json();
}
```

### 2. Implement Status Polling

```typescript
export async function waitForProcessing(
  stemUid: string,
  pdfUid: string,
  maxRetries = 100,
): Promise<void> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await fetch(
      `${NOGGIN_HOST}/pdfs/pdf-status/${stemUid}/${pdfUid}`,
      {
        credentials: "include",
      },
    );

    if (!response.ok) {
      throw new Error(`Status check failed: ${response.status}`);
    }

    const status = await response.json();

    if (status.status === "ready") {
      return;
    }

    if (status.status === "error") {
      throw new Error(`Processing error: ${status.message}`);
    }

    // Wait before next check (with exponential backoff)
    await new Promise((resolve) =>
      setTimeout(resolve, Math.min(5000 * Math.pow(1.5, attempt), 30000)),
    );
  }

  throw new Error("Processing timed out");
}
```

### 3. Fetch Extractions

```typescript
export async function getExtractions(
  stemUid: string,
  pdfUid: string,
): Promise<any> {
  const response = await fetch(
    `${NOGGIN_HOST}/pdfs/pdf-extracts/${stemUid}/${pdfUid}`,
    {
      credentials: "include",
    },
  );

  if (!response.ok) {
    throw new Error(`Failed to get extractions: ${response.status}`);
  }

  return await response.json();
}
```

### 4. Complete Flow Example

```typescript
export async function processAndGetExtractions(file: File): Promise<any> {
  // Read file as array buffer
  const arrayBuffer = await file.arrayBuffer();
  const fileData = Array.from(new Uint8Array(arrayBuffer));

  // Upload and start processing
  const { stemUid, pdfUid } = await processPdf(fileData, file.name);

  // Wait for processing to complete
  await waitForProcessing(stemUid, pdfUid);

  // Get and return extractions
  return await getExtractions(stemUid, pdfUid);
}
```

## Deployment and Development Workflow

1. **Local Development**:

   - Start platform services: `nix develop` and `ant-all-services`
   - Build and deploy vibes app: `cd apps/your-app && npm i && ./deploy-local.sh`
   - Access via Noggin: `http://localhost:20701/your-app`

2. **Production Deployment**:
   - Vibes apps are built and uploaded to S3
   - Noggin serves them as static assets
   - API calls are proxied through Noggin to backend services

## Security Considerations

1. **Authentication**:

   - JWT tokens stored in HTTP-only cookies
   - Enterprise-specific isolation of PDFs
   - Role-based access controls for specific operations

2. **Data Protection**:

   - PDFs are stored in enterprise-specific S3 paths
   - All API calls require valid authentication
   - Processing results are isolated by enterprise

3. **Error Handling**:
   - User-friendly error messages
   - Detailed logging for debugging
   - Appropriate HTTP status codes

## Extending the System

To extend PDF processing capabilities:

1. **New Extraction Types**:

   - Add new processing flows in platform workflows
   - Extend the JSON schema for new data types
   - Update frontend components to display new data

2. **Additional Endpoints**:

   - Add new routes in Noggin's PDF app
   - Implement corresponding API clients in vibes apps

3. **Enhanced Visualization**:
   - Build new UI components for specialized PDF rendering
   - Implement domain-specific interactions (e.g., clinical annotations)

## Troubleshooting

### Common Issues

1. **PDF Upload Fails**:

   - Check authentication status (JWT token)
   - Verify file size within limits
   - Ensure file is valid PDF format

2. **Processing Never Completes**:

   - Check workflow service logs for errors
   - Verify S3 permissions are correct
   - Check for resource constraints in processing service

3. **Extract Retrieval Fails**:
   - Confirm processing has completed successfully
   - Verify S3 path is correct for the enterprise
   - Check for malformed JSON in extract file

### Debugging Tools

1. **API Request Logs**:

   - Check Noggin server logs for API request details
   - Look for HTTP status codes and error messages

2. **Storage Inspection**:

   - Use S3 tools to inspect raw PDFs and extracts
   - Verify file integrity and permissions

3. **Processing Status**:
   - Monitor workflow execution status
   - Check for failed or stalled workflows# Product Requirements Document

This document captures the requirements, preferences, and design principles for the dotfiles repository as expressed by the user in conversations.

## Core Requirements

### 1. Repository Structure

- **Modular Organization**: Configuration files must be organized in a modular structure by tool
- **Consistent Naming**: Directory names should match tool identifiers used throughout the repository
- **XDG Compliance**: Follow XDG Base Directory specification for all configurations
- **Documentation**: Each tool must have its own README with usage instructions

### 2. Installation and Setup

- **Idempotent Setup**: Setup script must be runnable multiple times without causing issues
- **Backup Mechanism**: Automatically back up existing configurations before overwriting
- **Minimal Dependencies**: Keep external dependencies to a minimum
- **Consistent Symlinking**: Use symlinks consistently (entire directories when possible)
- **Cross-platform Support**: Primary focus on macOS (Apple Silicon), but avoid architecture-specific code when possible

### 3. Tool Integration

- **Unified Task Runner**: Use Just to provide a consistent interface for commands
- **Fuzzy Finding**: Enable fuzzy search capabilities for commands
- **Hierarchical Commands**: Commands should be organized by tool and accessible via prefix
- **Shell Integration**: Integrate with ZSH for immediate access to commands

### 4. Visual and UX Preferences

- **Tokyo Night Theme**: Consistent Tokyo Night theme across all tools
- **Modern Terminal**: GPU-accelerated terminal with Ghostty
- **Vim-style Navigation**: Vim-inspired navigation in all supporting tools
- **Clean Interface**: Minimize visual clutter in terminal and UI
- **Performance Optimization**: Configure for Apple Silicon performance

### 5. Tool-Specific Requirements

#### Neovim

- LazyVim-based configuration
- LSP integration
- Consistent syntax highlighting
- Fuzzy file navigation
- Git integration

#### ZSH

- Fast startup times
- Comprehensive aliases
- Useful functions for navigation and productivity
- Integration with modern tools (Atuin, Zoxide, Starship)

#### Yazi

- Optimized for Apple Silicon
- Integration with Neovim
- Rich file previews
- Custom navigation shortcuts

#### Aerospace

- Vim-style window navigation
- Multiple workspace support
- Tokyo Night-inspired window gaps
- Modal key binding system

#### Starship

- Tokyo Night theme
- Right-side time and battery information
- Fast performance
- Informative but not cluttered

### 6. Documentation Standards

- **Consistent Format**: All documentation should follow a consistent format
- **Code Examples**: Include useful code examples for common operations
- **Installation Instructions**: Clear installation instructions for each tool
- **Customization Guide**: Document how to customize each configuration
- **Version Information**: Track versions of all tools

### 7. Version Control

- **Clean Commits**: Commits should have clear, descriptive messages
- **Atomic Changes**: Each commit should represent a single logical change
- **Version Tracking**: Maintain a VERSION.md file with current environment snapshot

## Design Principles

1. **Minimalism**: Keep configurations simple and focused
2. **Performance**: Optimize for speed and responsiveness
3. **Consistency**: Maintain consistent style and behaviors across tools
4. **Modularity**: Enable easy addition/removal of specific configurations
5. **Discoverability**: Make features and commands easy to discover
6. **Documentation**: Document everything thoroughly
7. **Maintainability**: Code should be clean and well-organized

## Constraints

1. **macOS-focused**: Primary platform is macOS on Apple Silicon
2. **ZSH-centric**: Shell configuration is centered around ZSH
3. **Modern tools**: Focus on modern, actively maintained tools
4. **Performance**: Must maintain fast shell startup times
5. **Backwards compatibility**: Support for older versions is not a priority

## Future Enhancements

1. **Additional Tool Integration**: Support for more development tools
2. **Automation**: Further automate setup and maintenance tasks
3. **Enhanced Documentation**: More comprehensive guides and examples
4. **Cross-platform**: Better support for Linux and other platforms
5. **Testing**: Add automated testing for configurations# AI Integration PRD

## Overview

This Product Requirements Document (PRD) defines the specifications for the AI integration system within the dotfiles repository. This system provides consistent, reusable components for leveraging Large Language Models (LLMs) in development workflows, following industry best practices as of May 2025.

## Goals

- Create a modular, composable AI tooling system
- Provide consistent interfaces across different LLM providers
- Optimize prompt design for efficiency and effectiveness
- Support multiple development workflows (coding, docs, testing)
- Enable programmatic access via shell and TypeScript
- Follow industry best practices for LLM utilization
- Support secure handling of credentials and context
- Maintain backward compatibility with existing tools

## Non-Goals

- Replacing specialized AI tools with custom implementations
- Supporting every possible LLM provider
- Handling training or fine-tuning of models
- Building a complete UI for AI interactions

## System Components

### 1. Directory Structure

```
config/ai/
├── core/                 # Core components and shared utilities
│   ├── config.yaml       # Central configuration (models, endpoints, etc.)
│   ├── constants.sh      # Shared constants for shell scripts
│   ├── types.ts          # TypeScript type definitions
│   └── utils.{sh,ts}     # Shared utility functions
├── providers/            # Provider-specific implementations
│   ├── anthropic/        # Anthropic/Claude specific configuration
│   ├── openai/           # OpenAI specific configuration
│   └── local/            # Local model configurations (Ollama, etc.)
├── prompts/              # Reusable prompt components
│   ├── base/             # Base prompt templates
│   ├── coding/           # Code-related prompts
│   ├── testing/          # Test generation prompts
│   └── docs/             # Documentation prompts
├── tools/                # CLI tools and integrations
│   ├── git/              # Git integrations
│   ├── ide/              # IDE integrations
│   └── api/              # API development tools
├── interfaces/           # Interface definitions and clients
│   ├── cli/              # Command-line interfaces
│   ├── ts/               # TypeScript library
│   └── bash/             # Bash utilities
└── templates/            # Output templates
    ├── code/             # Code templates
    ├── docs/             # Documentation templates
    └── configs/          # Configuration templates
```

### 2. Core Features

#### 2.1 Provider-Agnostic Interface

- Consistent command interface across providers
- Abstraction layer for different APIs
- Automatic fallback mechanisms
- Environment-aware provider selection

#### 2.2 Prompt Engineering System

- Modular, composable prompt components
- Versioned prompt templates
- Context-aware prompt generation
- Runtime prompt optimization

#### 2.3 Development Workflows

- Code generation and completion
- Code review and improvement
- Test generation
- Documentation generation
- Commit message generation
- API design and implementation

#### 2.4 Tool Integrations

- Git hooks and commands
- IDE integration (VS Code, Neovim)
- API tools (OpenAPI, client generation)
- Shell enhancements

#### 2.5 Configuration Management

- Environment-aware configuration
- Secure credential handling
- Provider-specific settings
- User preference management

## Technical Requirements

### 1. API Standards

- All API calls must implement proper error handling
- Rate limit compliance and retry mechanisms
- Secure token handling (no hardcoded tokens)
- Streaming support where applicable
- Proper timeout handling

### 2. Prompt Engineering

- All prompts must follow the OpenAI cookbook guidelines
- Prompts should be modular and composable
- Prompt versions should be tracked
- Prompts should include clear instructions and constraints
- System prompts should be separate from user prompts

### 3. Code Architecture

- TypeScript library must use proper types
- Shell scripts must follow best practices (ShellCheck compliance)
- Functions should be focused and reusable
- Proper logging and error reporting

### 4. Testing

- Unit tests for TypeScript components
- Integration tests for end-to-end workflows
- Prompt regression testing system
- Performance benchmarking for optimization

### 5. Security

- No credentials in code repositories
- Secure environment variable handling
- Proper input validation
- Content filtering for generated outputs

## Implementation Phases

### Phase 1: Core Infrastructure

- Define directory structure and interfaces
- Implement provider abstraction layer
- Create basic prompt templates
- Set up configuration system

### Phase 2: Tool Integration

- Implement Git integration
- Create IDE plugins/tools
- Develop API development tools
- Build shell utilities

### Phase 3: Advanced Features

- Implement prompt optimization
- Add multi-provider orchestration
- Create advanced workflow tools
- Performance optimization

## Success Metrics

- Reduction in development time for common tasks
- Improved code quality through AI assistance
- Consistent experience across different providers
- Modularity and reusability of components
- Performance and reliability of AI integrations

## Appendix

### A. Example Workflows

#### A.1 Code Generation

```bash
$ just ai:code typescript "Create a React component that displays a list of items with pagination"
```

#### A.2 Commit Message Generation

```bash
$ just ai:commit-msg
```

#### A.3 API Client Generation

```bash
$ just api:generate-client path/to/spec.yaml output-dir
```

### B. Provider Support Matrix

| Provider  | Models                   | Features                 | Best For                 |
| --------- | ------------------------ | ------------------------ | ------------------------ |
| Anthropic | Claude 3 Opus/Sonnet/etc | Strong reasoning         | Complex coding tasks     |
| OpenAI    | GPT-4/3.5                | Tool use, function calls | RAG, tool integration    |
| Gemini    | Gemini Pro/Ultra         | Large context window     | Documentation, analysis  |
| Local     | Mixtral, Llama 3, etc.   | Privacy, offline         | Quick tasks, prototyping |

### C. Prompt Engineering Guidelines

1. **Clarity**: Be specific and clear in instructions
2. **Context**: Provide necessary context for the task
3. **Examples**: Include examples for complex tasks
4. **Constraints**: Specify output format and limitations
5. **Roles**: Define clear roles for the model
6. **Iterative**: Design prompts to be iteratively improved
7. **Modular**: Break complex prompts into reusable components# PDF Chat System: Authentication and Network Flow

This document explains the complete flow of authentication and networking between the frontend application and backend services for the PDF chat feature. It covers the authentication process, PDF processing, and chat interaction.

## 1. Architecture Overview

The PDF chat system consists of the following components:

- **Frontend UI**: React application at `/vibes/apps/flopilot/src/`
- **Backend Services**: Node.js services at `/platform/gateways/noggin/src/`
  - PDF Processing Service: `/platform/gateways/noggin/src/pdfs/`
  - Chat Service: `/platform/gateways/noggin/src/chat/`
- **Storage**: S3-compatible storage for PDFs and extracted data

## 2. Authentication Flow

### 2.1 Login Process

The system uses a JWT-based authentication system:

1. User logs in via the login form (`/login.html`)
2. The system creates a one-time password (OTP) and redirects to verification page
3. After OTP verification, the server issues two tokens:
   - `access_token`: Short-lived JWT containing user and enterprise information
   - `refresh_token`: Long-lived token for refreshing access

These tokens are stored as cookies:

```
Cookie: auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

### 2.2 JWT Structure

The access token contains critical enterprise information:

```json
{
  "entr": {
    "ent_VRfC5vl0Fw5w_dcEVM-1w": "ADMIN"
  },
  "wksp": {
    "wksp_PnCmyFGy9fHwWgIOx_k5h": "ent_VRfC5vl0Fw5w_dcEVM-1w",
    "wksp_vBE6Z0LimBSHaF0bRZ5l2": "ent_VRfC5vl0Fw5w_dcEVM-1w"
  },
  "iss": "https://co-helm.com",
  "sub": "usr_BYZnJljHgUz_y4CLsSmTU",
  "aud": ["access"],
  "exp": 1746662704,
  "nbf": 1746661804,
  "iat": 1746661804,
  "jti": "jwt_kOtkgt504p1WtEZJpA83w"
}
```

The `enterpriseUid` (`ent_VRfC5vl0Fw5w_dcEVM-1w`) is extracted from this token and is required for all authenticated operations.

### 2.3 Token Extraction

Backend services extract the enterprise information using the `parseEnterpriseUserFromJwt` function from `/platform/gateways/noggin/src/auth/jwt.ts`:

```typescript
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
  if (!ctx.jwtPayload?.entr) {
    throw new HTTPException(401, { message: "Authentication required" });
  }

  // Get enterprise and user uids from the token.
  let enterpriseUid = undefined;
  let role = undefined;

  // Most tokens will have a single enterprise.
  if (Object.keys(ctx.jwtPayload.entr).length === 1) {
    enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
    role = ctx.jwtPayload.entr[enterpriseUid];
  } else {
    // Logic for multiple enterprises...
  }

  // ...

  return {
    enterpriseUid,
    role,
    workspaceUid,
    userUid: ctx.jwtPayload.sub,
  };
}
```

## 3. PDF Processing Flow

### 3.1 PDF Upload

The process begins when a user uploads PDFs through the UI in `prompt-box.tsx`:

```typescript
// From /vibes/apps/flopilot/src/view/prompt-box.tsx
const { processedFiles, onPdfChange, onRemovePdf, isPdfProcessingMap } =
  usePdfProcessController();
```

The `usePdfProcessController` hook handles the PDF upload:

```typescript
// From /vibes/apps/flopilot/src/controller/use-pdf-process-controller.ts
const { mutateAsync: processPdf } = useMutation({
  mutationKey: PDF_PROCESS_KEY,
  mutationFn: async (files: File[]) => {
    const formData = new FormData();
    files.forEach((file) => {
      formData.append("files[]", file);
    });

    const response = await fetch(
      `${env.VITE_NOGGIN_HOST}/process-pdfs`, // Note: No pdfs/ prefix
      {
        method: "POST",
        body: formData,
        credentials: "include", // Explicitly include credentials
      },
    );
    // ... process response
  },
});
```

This request includes the authentication cookies thanks to the `credentials: "include"` option:

```
"cookie": "auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```

### 3.2 Backend PDF Processing

The backend endpoint `/pdfs/process-pdfs` in `/platform/gateways/noggin/src/pdfs/app.ts` handles the request:

1. It extracts the `enterpriseUid` from the JWT using `parseEnterpriseUserFromJwt`
2. Generates a `stemUid` and `pdfUid` for each uploaded file
3. Stores each PDF in S3 using a path structure: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
4. Schedules a background flow for processing the PDF
5. Returns the `stemUid` and `pdfUids` to the client

```typescript
// From /platform/gateways/noggin/src/pdfs/app.ts
app.endpoint({
  method: "POST",
  body: processStemPdfSchema,
  response: stemPdfUidResponseSchema,
  route: "/process-pdfs",
  async handler(ctx) {
    const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
    // ... process PDFs and store in S3
    return {
      stemUid,
      pdfUids: results.map((result) => result.pdfUid),
    };
  },
});
```

### 3.3 PDF Status Polling

After upload, the frontend polls the status of PDF processing:

```typescript
// From /vibes/apps/flopilot/src/controller/use-pdf-process-controller.ts
const combinedQueries = useQueries({
  queries: (processedFiles ?? []).map(({ pdfUid, stemUid }) => ({
    queryKey: [PDF_PROCESS_STATUS_KEY, pdfUid],
    queryFn: async () => {
      const response = await fetch(
        `${env.VITE_NOGGIN_HOST}/pdf-status/${stemUid}/${pdfUid}`, // No pdfs/ prefix
        {
          method: "GET",
          credentials: "include", // Explicitly include credentials
        },
      );
      // ... process response
    },
    refetchInterval: (query) => {
      if (query.state.data?.status === "processing") {
        return 5_000; // Poll every 5 seconds while processing
      }
      return false;
    },
  })),
});
```

The backend status endpoint again extracts the `enterpriseUid` from the JWT and checks if the processed file exists in the proper S3 path.

## 4. Chat Interaction Flow

### 4.1 Chat Request

Once PDFs are processed, users can chat about them. The chat request is made through the `useChatController` hook:

```typescript
// From /vibes/apps/flopilot/src/controller/use-chat-controller.ts
// Custom fetcher function to ensure credentials are included
const customFetcher = async (url: string, options: RequestInit) => {
  return fetch(url, {
    ...options,
    credentials: "include", // This ensures cookies are sent with the request
  });
};

export function useChatController() {
  // ...
  const { messages, input, append, handleInputChange, handleSubmit, error } =
    useChat({
      api: `${env.VITE_NOGGIN_HOST}/`, // Direct to root endpoint, not /chat
      streamProtocol: "text",
      body: {
        stemUid,
        pdfUids,
      },
      fetcher: customFetcher, // Use our custom fetcher that includes credentials
      onError: (error) => {
        console.error("Chat error:", error);
      },
    });
  // ...
}
```

### 4.2 Authentication Issue and Solution

**The Critical Issue**:

The AI SDK's `useChat` hook doesn't include cookies by default, unlike direct fetch calls from the browser. This caused 401 Unauthorized errors in the chat endpoint.

The solution is to implement a custom fetcher that explicitly includes credentials:

```typescript
const customFetcher = async (url: string, options: RequestInit) => {
  return fetch(url, {
    ...options,
    credentials: "include", // This ensures cookies are sent with the request
  });
};
```

This custom fetcher needs to be applied to all API requests to ensure authentication cookies are properly sent:

```typescript
// For chat endpoints
useChat({
  api: `${env.VITE_NOGGIN_HOST}/chat`,
  fetcher: customFetcher,
  // ...
});

// For PDF endpoints
const response = await fetch(`${env.VITE_NOGGIN_HOST}/pdfs/process-pdfs`, {
  method: "POST",
  body: formData,
  credentials: "include", // Ensure cookies are sent
});
```

````

### 4.3 Backend Chat Processing

The chat endpoint handler in `/platform/gateways/noggin/src/chat/app.ts` requires the `enterpriseUid` to fetch PDF extracts from S3:

```typescript
async function getExtractsByPdfUid(
  ctx: RequestContext<ChatAppContext, "/", {}, unknown, unknown, ChatRequestBody>
) {
  const enterpriseUid = ctx.auth?.enterpriseUid;
  if (!enterpriseUid) {
    // The types think we can have an undefined auth context
    throw new HTTPException(401, { message: "Unauthorized" });
  }

  const { stemUid, pdfUids } = ctx.body;
  const s3Keys = pdfUids.map((pdfUid) => {
    return {
      s3Key: buildS3PdfPath({
        enterpriseUid,
        stemUid,
        pdfUid,
        filename: "extracts.json",
      }),
      pdfUid,
    };
  });

  // Fetch and process PDF extracts from S3
  // ...
}
````

## 5. S3 Path Structure

The system uses a consistent S3 path structure for both raw PDFs and their extracted content:

```
stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf
stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
```

This structure is created by the `buildS3PdfPath` function used in both PDF and chat services:

```typescript
function buildS3PdfPath({
  enterpriseUid,
  stemUid,
  pdfUid,
  filename,
}: BuildS3PdfPathArgs): string {
  const basePath = `stems/${enterpriseUid}/${stemUid}/${pdfUid}`;
  return `${basePath}/${filename}`;
}
```

## 6. Flow Diagram

### Authentication and PDF Processing:

```
User → Login → JWT Token Issued
↓
Upload PDF → /pdfs/process-pdfs → Extract enterpriseUid from JWT
↓
Generate stemUid and pdfUid → Store PDF in S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf
↓
Schedule PDF processing flow → Process PDF → Store extracts in S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
↓
Frontend polls /pdfs/pdf-status/{stemUid}/{pdfUid} → Wait until status is "ready"
```

### Chat Flow:

```
User sends message → useChat hook with custom fetcher → POST to /chat
↓
Backend extracts enterpriseUid from JWT → Get PDF extracts from S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
↓
Format extracts and prompt → Send to LLM → Return response to user
```

## 7. Network Logs Analysis

The server logs show the full request/response cycle:

1. **Authentication**: Successful authentication with OTP verification

   ```
   [19:50:04.477] INFO (52868): Request completed
       res: {
         "status": 200,
         "headers": {}
       }
       req: {
         "url": "/otp/verify",
         "method": "POST",
         ...
   ```

2. **PDF Upload**: Successful upload with authentication cookies

   ```
   [19:50:24.719] INFO (52868): Request completed
       res: {
         "status": 200,
         "headers": {}
       }
       req: {
         "url": "/process-pdfs",  // Server log shows just the path portion
         "method": "POST",
         "headers": {
           ...
           "cookie": "auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
           ...
         },
   ```

3. **PDF Status Polling**: Successful status checks

   ```
   [19:50:24.743] INFO (52868): Request completed
       res: {
         "status": 200,
         "headers": {}
       }
       req: {
         "url": "/pdf-status/stm_4b5aaa396d_391899ebd3/pdf_a67de8344c_c8d574d08b",  // Server log shows just the path portion
         "method": "GET",
         ...
   ```

4. **Chat Request Failure**: The 401 error when cookies aren't properly included
   ```
   [19:50:48.504] ERROR (52868): Unauthorized
       res: {
         "status": 401,
         "headers": {}
       }
       req: {
         "url": "/",  // Server log shows path as root, but full URL is likely /chat
         "method": "POST",
         ...
   ```

## 8. Conclusion

The PDF Chat system requires consistent authentication across all endpoints. The critical issue needed to be fixed:

**Authentication Issue**: The AI SDK's fetch implementation wasn't including credentials by default. By adding a custom fetcher that explicitly includes credentials with `credentials: "include"`, we ensure the auth token is passed to the chat endpoint, allowing it to extract the `enterpriseUid` and access the correct PDF extracts in S3.

**Note about server logs**: The server logs show the paths without the full URL prefix, but in the frontend code, we still need to use the complete paths with prefixes:

- Frontend uses: `/pdfs/process-pdfs` while server logs show: `/process-pdfs`
- Frontend uses: `/pdfs/pdf-status/${stemUid}/${pdfUid}` while server logs show: `/pdf-status/...`
- Frontend uses: `/chat` while server logs show: `/`

By adding the credentials to all fetch requests, we ensure:

1. All requests are properly authenticated with the JWT token
2. The backend can extract the enterpriseUid from the JWT
3. The backend can correctly access the S3 paths containing the PDF extracts

The entire system is built around the concept of enterprise-scoped data, with all storage paths containing the enterprise ID extracted from the JWT. This ensures data isolation between enterprises while maintaining a consistent access pattern across services.# Noggin Gateway & Frontend-Backend Integration

This document provides a comprehensive overview of the Noggin gateway and how it integrates frontend applications with backend services in the Anterior platform.

## Architecture Overview

The Anterior platform uses a sophisticated architecture with:

1. **Backend Services** (in `platform` repo) - Microservices handling business logic
2. **Frontend Applications** (in `vibes` repo) - React/TypeScript user interfaces
3. **Noggin Gateway** (`platform/gateways/noggin`) - Central API gateway and static app server
4. **S3 Storage** - Stores frontend assets and user files (like PDFs)

This architecture enables:

- Clean separation of concerns
- Consistent API patterns
- Centralized authentication
- Reliable static app serving
- Enterprise-level isolation

## Core Components

### 1. App Framework (`lib-platform/src/app.ts`)

The foundation of all backend services is the `App` class in `lib-platform/src/app.ts`:

```typescript
export class App<TServiceMap extends ServiceMap, TAppContext extends object> {
  public readonly hono: Hono<{ Variables: TAppContext & BaseContext }>;
  public readonly logger: Logger;
  public readonly trpc = initTRPC...

  constructor(
    public readonly baseCtx: TAppContext,
    public readonly options: AppOptions,
    public readonly auth?: (req: Request) => Promise<{ userUid: string; enterpriseUid: string } | Response>
  ) { ... }
}
```

Key features:

- Built on [Hono](https://hono.dev/) for HTTP routing and handling
- Type-safe API endpoint definitions with schemas
- JWT authentication middleware
- Standardized error handling
- CORS support

### 2. Static App Serving

The `createStaticApp` function in `app.ts` enables serving frontend applications:

```typescript
export function createStaticApp({
  prefix,
  blobs,
  index = "index.html",
  env,
  shouldGenerateCSP = false,
}: {
  prefix: Path | "";
  blobs: ReadOnlyBlobStore;
  index?: string;
  env?: Record<string, string>;
  shouldGenerateCSP?: boolean;
}): Hono { ... }
```

This function:

1. Creates a Hono app that serves static files from S3
2. Handles path prefixing for route matching
3. Sets appropriate MIME types for files
4. Performs environment variable substitution
5. Applies Content Security Policy headers

### 3. Noggin Gateway Structure

The Noggin gateway serves two primary purposes:

1. Provide API endpoints for backend services
2. Serve frontend applications from S3

Each subdirectory contains app-specific endpoints:

- `src/pdfs/app.ts` - PDF processing API
- `src/auth/app.ts` - Authentication services
- `src/notes/app.ts` - Notes API
- App-specific APIs: `src/flonotes/app.ts`, `src/flopilot/app.ts`

Apps are dynamically loaded using `importAndMountNestedDirectoryApps`.

## PDF Processing Architecture

### PDF API Endpoints

The Noggin gateway provides these key PDF endpoints:

| Endpoint                         | Method | Purpose                  | Parameters/Body                       | Response                                                           |
| -------------------------------- | ------ | ------------------------ | ------------------------------------- | ------------------------------------------------------------------ |
| `/process-pdf`                   | POST   | Upload and process a PDF | `file` (PDF blob), optional `stemUid` | `{ stemUid: string, pdfUid: string }`                              |
| `/pdf-status/:stemUid/:pdfUid`   | GET    | Check processing status  | URL params: stemUid, pdfUid           | `{ status: "processing" \| "ready" \| "error", message?: string }` |
| `/pdf-extracts/:stemUid/:pdfUid` | GET    | Retrieve extracted data  | URL params: stemUid, pdfUid           | JSON structure of extracted content                                |

### PDF Processing Flow

1. **Upload**: Frontend uploads PDF to Noggin
2. **Initial Storage**: Noggin stores raw PDF in S3
3. **Processing**: Noggin schedules an asynchronous workflow
4. **Extraction**: Workflow processes PDF and stores JSON extracts
5. **Status Check**: Frontend polls status endpoint until processing completes
6. **Retrieval**: Frontend fetches and displays extracted content

### Storage Structure

PDFs are stored with this S3 path structure:

- Raw PDFs: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
- Extracted data: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json`

The "stem" concept represents a logical grouping of related documents, enabling contextual relationships.

## Frontend Deployment & Integration

### Deployment Flow

Frontend applications are deployed using the following process:

1. **Build**: The app is built using Vite

   ```bash
   npm run build
   ```

2. **Configure**: Environment variables are set

   ```bash
   export VITE_NOGGIN_HOST="http://localhost:20701"
   ```

3. **Upload**: Assets are uploaded to S3

   ```bash
   aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
     --endpoint-url "$LOCALSTACK_ENDPOINT" \
     --delete
   ```

4. **Access**: Apps are accessed through Noggin
   ```
   http://localhost:20701/flonotes
   ```

### S3 Directory Structure

Frontend assets are stored in S3 with this structure:

- `vibes/flonotes/` - FloNotes app assets
- `vibes/flopilot/` - FloPilot app assets
- `vibes/atlas/` - Atlas app assets

### API Communication

Frontend apps make API calls through Noggin:

```typescript
// Example from flonotes
export async function processPdf(
  fileData: number[],
  fileName: string,
  documentType: "clinical" | "criteria",
  stemUid?: string,
): Promise<ProcessPdfResponse> {
  // Builds and sends a multipart/form-data request to Noggin
  const response = await apiRequest(API_ENDPOINTS.processPdf, {
    method: "POST",
    body: formData,
  });

  return data;
}
```

## Frontend Application Examples

### 1. FloNotes App (`vibes/apps/flonotes`)

FloNotes provides advanced PDF viewing and annotation capabilities for clinical documentation.

**Key Components**:

1. **API Integration** (`src/services/noggin-api.ts`):

   - Complete client implementation for Noggin PDF endpoints
   - Handles authentication, error management, and retries
   - Implements status polling with exponential backoff

2. **PDF Processing** (`src/hooks/use-process-pdf.ts`):

   - Custom React hook that manages PDF processing state
   - Tracks upload progress, processing status, and results

3. **PDF Viewer** (`src/components/shell/pdf/`):
   - Rich PDF viewing UI with page navigation
   - Highlight and annotation capabilities
   - Citation linking between PDF content and notes

### 2. FloPilot App (`vibes/apps/flopilot`)

FloPilot uses PDFs as context for AI-assisted interactions and chat functionality.

**Key Components**:

1. **Controller Pattern** (`src/controller/use-pdf-process-controller.ts`):

   - State management for PDF processing
   - Integration with chat context

2. **PDF Models** (`src/controller/models/`):

   - `pdf-extracts.ts` - Types for extracted PDF content
   - `pdf-status.ts` - Status response handling
   - `process-pdf.ts` - PDF upload request models

3. **Chat Integration**:
   - Uses processed PDFs as context for AI-assisted chats
   - Links relevant PDF content to chat messages

## Security Model

The Anterior platform implements robust security:

1. **Authentication**:

   - JWT tokens stored in HTTP-only cookies
   - Enterprise-specific isolation of resources
   - Role-based access controls

2. **Data Protection**:

   - Resources are stored in enterprise-specific S3 paths
   - All API calls require valid authentication
   - Processing results are isolated by enterprise

3. **Network Security**:
   - Controlled CORS for cross-origin requests
   - Content Security Policy for frontend applications
   - HTTPS enforcement in production

## Development Workflow

### Local Development Setup

1. Start the platform services:

   ```bash
   cd platform
   nix develop
   ant-all-services
   ```

2. Build and deploy a frontend app:

   ```bash
   cd vibes/apps/flonotes
   npm i
   ./deploy-local.sh
   ```

3. Access the app through Noggin:
   ```
   http://localhost:20701/flonotes
   ```

### Advanced Development

For faster development:

1. Stop Noggin in the process-compose screen
2. Copy environment variables from logs
3. Run Noggin separately:
   ```bash
   cd gateways/noggin
   source .env && npm run start | npx pino-pretty
   ```

## Integration Instructions

To integrate a new frontend app with Noggin:

### 1. Create API Client

```typescript
// Example minimal API client
export async function processPdf(
  fileData: number[],
  fileName: string,
): Promise<{ stemUid: string; pdfUid: string }> {
  const formData = new FormData();
  formData.append("file", new Blob([new Uint8Array(fileData)]), fileName);

  const response = await fetch(`${NOGGIN_HOST}/pdfs/process-pdf`, {
    method: "POST",
    body: formData,
    credentials: "include", // For auth cookies
  });

  if (!response.ok) {
    throw new Error(`PDF processing failed: ${response.status}`);
  }

  return await response.json();
}
```

### 2. Implement Status Polling

```typescript
export async function waitForProcessing(
  stemUid: string,
  pdfUid: string,
  maxRetries = 100,
): Promise<void> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await fetch(
      `${NOGGIN_HOST}/pdfs/pdf-status/${stemUid}/${pdfUid}`,
      {
        credentials: "include",
      },
    );

    if (!response.ok) {
      throw new Error(`Status check failed: ${response.status}`);
    }

    const status = await response.json();

    if (status.status === "ready") {
      return;
    }

    if (status.status === "error") {
      throw new Error(`Processing error: ${status.message}`);
    }

    // Wait before next check (with exponential backoff)
    await new Promise((resolve) =>
      setTimeout(resolve, Math.min(5000 * Math.pow(1.5, attempt), 30000)),
    );
  }

  throw new Error("Processing timed out");
}
```

### 3. Create Deployment Script

```bash
#!/bin/bash
set -euo pipefail

# App configuration
APP_PREFIX="vibes/your-app"
BASE_PATH="/your-app"
BUCKET_NAME="local-bucket"
LOCALSTACK_ENDPOINT="http://localhost:4566"
VITE_NOGGIN_HOST="http://localhost:20701"

# Build frontend with environment variables
export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
export BASE_PATH="${BASE_PATH}"
npm run build

# Deploy to S3
aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
  --endpoint-url "$LOCALSTACK_ENDPOINT" \
  --no-verify-ssl \
  --delete

echo "Deployed app to $VITE_NOGGIN_HOST$BASE_PATH"
```

## Troubleshooting

### Common Issues

1. **PDF Upload Fails**:

   - Check authentication status (JWT token)
   - Verify file size within limits
   - Ensure file is valid PDF format

2. **Processing Never Completes**:

   - Check workflow service logs for errors
   - Verify S3 permissions are correct
   - Check for resource constraints in processing service

3. **Static Assets Not Loading**:

   - Verify S3 upload was successful
   - Check for path/prefix configuration mismatches
   - Confirm Noggin is running and can access S3

4. **API Errors**:
   - Check Noggin logs for detailed error information
   - Verify authentication cookies are present
   - Check network requests in browser developer tools

### Debugging Steps

1. **Connection Verification**:

   ```bash
   # Check LocalStack availability
   curl -s --head "http://localhost:4566"
   ```

2. **Authentication Check**:

   ```javascript
   // Check for auth_indicator cookie
   document.cookie.includes("auth_indicator=true");
   ```

3. **S3 Content Verification**:

   ```bash
   aws s3 ls s3://local-bucket/vibes/flonotes/ \
     --endpoint-url "http://localhost:4566" --no-verify-ssl
   ```

4. **API Call Testing**:
   ```bash
   curl -v -H "Cookie: auth_token=YOUR_JWT_TOKEN" \
     "http://localhost:20701/auth/me"
   ```

## Extending the System

To add new functionality to the Noggin gateway:

1. **New API Endpoint**:

   - Create a new app.ts file in the appropriate directory
   - Define endpoints using the App framework
   - Add necessary validation schemas

2. **New Frontend App**:

   - Build the frontend app with Vite
   - Create deployment script
   - Deploy to S3 with the correct prefix

3. **New PDF Processing Capabilities**:

   - Add new processing flows in platform workflows
   - Extend the JSON schema for new data types
   - Update frontend components to display new data# Nix Setup in Anterior Platform

This document provides a detailed explanation of the Nix setup in the Anterior Platform, covering the flake.nix file, related Nix files, build processes, and how they connect to the codebase.

## Table of Contents

1. [Overview](#overview)
2. [Key Components](#key-components)
3. [Flake Inputs](#flake-inputs)
4. [Flake Modules](#flake-modules)
5. [Packages and Services](#packages-and-services)
6. [Build Process](#build-process)
7. [Developer Environment](#developer-environment)
8. [Darwin Configuration](#darwin-configuration)
9. [Testing and CI](#testing-and-ci)
10. [Common Commands](#common-commands)

## Overview

The Anterior Platform uses Nix for:

1. **Reproducible Builds**: Ensuring consistent build environments across developers and CI
2. **Service Definition**: Declaratively specifying services and their dependencies
3. **Docker Image Generation**: Creating consistent Docker images for deployment
4. **Development Environments**: Providing standardized developer shells
5. **macOS Configuration**: Managing macOS developer machines via nix-darwin

The central file is `flake.nix`, which defines all inputs, outputs, and modules. The system supports multiple languages (Go, Python, TypeScript/JavaScript) and uses specialized builders for each.

## Key Components

The Nix setup consists of several interconnected components:

### 1. Main Files

- **flake.nix**: The entry point defining inputs, outputs, and system configurations
- **nix/anterior-services.nix**: Defines the service framework for building services
- **nix/anterior-python.nix**: Python-specific configuration using uv2nix
- **nix/anterior-services-process-compose.nix**: Configuration for local service orchestration
- **nix/ant-darwin-module.nix**: macOS configuration for developer machines

### 2. Service Frameworks

The platform uses specialized builders for different language ecosystems:

- **Go**: Using `buildGoModule` with vendor hashes
- **Python**: Using `uv2nix` to build from uv.lock files
- **JavaScript**: Using custom npm2nix implementation

### 3. Build Artifacts

- **Docker Images**: For each service in production and test variants
- **Development Shells**: Language-specific development environments
- **Executables**: Direct access to service binaries

## Flake Inputs

The `flake.nix` file begins by declaring all external dependencies:

```nix
inputs = {
  nixpkgs.url = "github:nixos/nixpkgs/nixpkgs-unstable";

  # Terraform providers with specific version pin
  nixpkgs-terraform-providers-bin = {
    url = "github:cohelm/nixpkgs-terraform-providers-bin/18e042762d12cc1e9452e426d4cb1097e7686fb2";
    inputs.nixpkgs.follows = "nixpkgs";
  };

  # Pinned to specific version for WeasyPrint compatibility
  nixpkgs_weasyprint_62_3.url = "github:nixos/nixpkgs/29c58cf78dc44b209e40e037b280cbbfc015ef39";

  # Nix registry and module system
  flakeregistry = {
    url = "github:NixOS/flake-registry";
    flake = false;
  };
  flake-parts.url = "flake-parts";

  # Development shell and service management
  devshell.url = "github:numtide/devshell";
  process-compose-flake.url = "github:Platonic-Systems/process-compose-flake";
  services-flake.url = "github:juspay/services-flake";

  # Python packaging
  pyproject-nix = {
    url = "github:cohelm/pyproject.nix/fake-sdk-compat";
    inputs.nixpkgs.follows = "nixpkgs";
  };
  uv2nix = {
    url = "github:pyproject-nix/uv2nix";
    inputs.pyproject-nix.follows = "pyproject-nix";
    inputs.nixpkgs.follows = "nixpkgs";
  };
  pyproject-build-systems = {
    url = "github:pyproject-nix/build-system-pkgs";
    inputs.pyproject-nix.follows = "pyproject-nix";
    inputs.uv2nix.follows = "uv2nix";
    inputs.nixpkgs.follows = "nixpkgs";
  };

  # macOS configuration
  nix-darwin = {
    url = "nix-darwin";
    inputs.nixpkgs.follows = "nixpkgs";
  };

  # Utilities
  globset = {
    url = "github:pdtpartners/globset";
    inputs.nixpkgs-lib.follows = "nixpkgs";
  };

  # External services
  brrr = {
    url = "github:cohelm/brrr/v0.1.0";
    inputs.pyproject-build-systems.follows = "pyproject-build-systems";
    inputs.pyproject-nix.follows = "pyproject-nix";
    inputs.nixpkgs.follows = "nixpkgs";
    inputs.uv2nix.follows = "uv2nix";
  };

  # CI tools
  ant-ci-tools.url = "github:cohelm/brrr/ci-tools";
}
```

Key insights:

1. **Input Pinning**: Most inputs are pinned to specific versions or commits
2. **Input Redirection**: Many inputs use `follows` to ensure consistent nixpkgs versions
3. **Custom Forks**: Several inputs are forks maintained by Cohelm (e.g., `cohelm/pyproject.nix`)

## Flake Modules

The flake uses a modular approach with `flake-parts`, which allows splitting configurations into separate modules:

```nix
flake-parts.lib.mkFlake { inherit inputs; } {
  systems = [ "aarch64-linux" "aarch64-darwin" "x86_64-linux" ];
  imports = [
    ./nix/anterior-python.nix
    ./nix/anterior-libpy.nix
    ./nix/anterior-services.nix
    ./nix/anterior-standalones.nix
    inputs.ant-ci-tools.flakeModules.checkBuildAll
    inputs.devshell.flakeModule
    inputs.process-compose-flake.flakeModule
    antFlakeAllSystems
    antFlakeLinux
  ];
}
```

Key modules:

1. **anterior-python.nix**: Configures Python environments using uv2nix
2. **anterior-services.nix**: Defines framework for all services
3. **anterior-standalones.nix**: Builds standalone executables
4. **antFlakeAllSystems**: Cross-platform configuration shared across systems
5. **antFlakeLinux**: Linux-specific configuration for Docker image building

## Packages and Services

The service definitions are collected in `config.anterior.services`, organized by language:

```nix
anterior = {
  inherit python;
  services = {
    python.hello-world = {
      inherit python;
      path = ./services/hello-world;
      subdirectory = "services/hello-world";
    };

    go.api = {
      inherit go;
      path = ./services/api;
      subdirectory = "services/api";
      vendorHash = "sha256-8qZAAvhgGHiWkQ6czjNLTtzsz/ftcF/7YqHx6Rps4bg=";
    };

    javascript.noggin = {
      inherit nodejs;
      subdirectory = "gateways/noggin";
    };

    # Other services...
  };
};
```

Each service declaration specifies:

- **Language**: Python, Go, or JavaScript
- **Path**: Location in the filesystem
- **Subdirectory**: Relative path in the repository
- **Language-specific fields**: Such as vendorHash for Go

### How Services are Built

The service definitions are processed by language-specific builders:

1. **Go Services**:

   - Built using `buildGoModule`
   - Vendor dependencies are verified with vendorHash
   - Creates both executables and Docker images

2. **Python Services**:

   - Built using uv2nix from uv.lock files
   - Includes virtualenv with all dependencies
   - Containerized with all runtime dependencies

3. **JavaScript Services**:
   - Built using custom npm2nix implementation
   - Handles workspace dependencies
   - Manages native dependencies and prebuilt binaries

## Build Process

### Docker Image Generation

For each service, multiple Docker images are created:

```nix
docker-image-derivs = lib.flatten (lib.mapAttrsToList (name: service: let
  p = service.outputs.package;
in [
  { inherit name; image = p.docker-prod; }
  { name = name + "-test"; image = p.docker-test; }
]) (s.go // s.python // s.javascript));
```

The system creates helper outputs:

1. **all-docker-images**: A file listing all Docker image outputs
2. **all-docker-tags**: Maps service names to Docker image tags
3. **all-docker-derivations**: Maps image names to their derivation paths

### Building Docker Images

When you run `ant-build-docker`, it:

1. Determines which images need to be built
2. Builds them in parallel using Nix
3. Loads them into the local Docker daemon
4. Tags them with `:latest` for use in docker-compose

```nix
command = let
  linuxEquivalent = "${pkgs.hostPlatform.parsed.cpu.name}-linux";
in ''
  build="$(mktemp)"
  tags="$(mktemp)"
  trap "rm -f $build $tags" EXIT

  nix build --no-link --print-out-paths .#packages.${linuxEquivalent}.all-docker-derivations | xargs -r cat | while read name drv; do
    if ! docker image inspect "$name" &>/dev/null; then
      printf "%s\t%s^out\n" "$name" "$drv" >> "$build"
    fi
    printf "docker tag %q %q_latest\n" "$name" "${name%_*}" >> "$tags"
  done

  <"$build" cut -f 2 | xargs -r nix build --no-link --print-out-paths | xargs -r -n 1 docker load -i
  bash "$tags"
''
```

### Linux Builder for macOS

Since Docker images are Linux-only, macOS developers need a Linux builder to build them. This is configured via nix-darwin:

```nix
nix.linux-builder = {
  config.virtualisation.cores = 6;
  config.virtualisation.memorySize = lib.mkOverride 69 6000;
  config.virtualisation.diskSize = lib.mkForce (100 * 1000);
  maxJobs = 12;
};
```

## Developer Environment

### DevShells

The system provides multiple development shells:

1. **Default Shell**: For repository management and meta-commands

   ```nix
   devshells.default = {
     name = "anterior";
     packages = devPackages { inherit pkgs; };
     motd = ''
       This is a "meta" devshell with commands for managing the
       repository and builds, not so much for actual dev of actual
       features. For an overview of the entire flake, run 'nix flake
       show'.

       Services managed by this flake:
       ${items2mdList (
         (map (go: "${go} (go)") (builtins.attrNames config.anterior.services.go))
         ++ (map (py: "${py} (python)") (builtins.attrNames config.anterior.services.python))
         ++ (map (js: "${js} (js)") (builtins.attrNames config.anterior.services.javascript))
       )}

       Extra tools on the path:
       ${items2mdList (map (p: "${p.pname or p.name} (${p.version})") packages)}
     '';
     # Commands and environment variables...
   }
   ```

2. **npm Shell**: For JavaScript development

   ```nix
   devShells.npm = pkgs.mkShell {
     name = "anterior-npm-devshell";
     inputsFrom = lib.mapAttrsToList (_: v: v.outputs.package) config.anterior.services.javascript;
     # Configuration...
   };
   ```

3. **Service-Specific Shells**: For each individual service
   - `nix develop .#hello-world`
   - `nix develop .#api`
   - etc.

### Common Commands

The default devshell provides useful commands:

```nix
commands = [
  {
    category = "maintenance";
    name = "system-prune";
    help = "Prune build cache for Docker & Nix";
    # Command implementation...
  }
  {
    name = "ant-admin";
    category = "run";
    help = "Anterior admin tool (/admin)";
    # Command implementation...
  }
  {
    category = "build";
    name = "ant-build-docker";
    help = "Build and load Docker images for every Nix service";
    # Command implementation...
  }
  # Other commands...
];
```

## Darwin Configuration

For macOS developers, the flake provides several nix-darwin configurations:

```nix
darwinConfigurations = {
  anterior-base = inputs.nix-darwin.lib.darwinSystem {
    modules = [
      self.darwinModules.anterior-managed-nixdarwin
      self.darwinModules.anterior-fancy
    ];
  };

  anterior-beefy = inputs.nix-darwin.lib.darwinSystem {
    modules = [
      self.darwinModules.anterior-managed-nixdarwin
      self.darwinModules.anterior-fancy
      ({ nix.linux-builder.config.virtualisation.memorySize = lib.mkForce 12000;})
    ];
  };

  anterior-bootstrap = inputs.nix-darwin.lib.darwinSystem {
    modules = [
      self.darwinModules.anterior-managed-nixdarwin
      self.darwinModules.anterior-core
    ];
  };

  anterior-nobuilder = self.darwinConfigurations.anterior-base.extendModules {
    modules = [
      { nix.linux-builder.enable = lib.mkForce false; }
    ];
  };
};
```

Each configuration serves a specific purpose:

- **anterior-base**: Standard development configuration
- **anterior-beefy**: For machines with more memory (prevents OOM issues)
- **anterior-bootstrap**: Minimal configuration to bootstrap the system
- **anterior-nobuilder**: Configuration without Linux builder for troubleshooting

## Testing and CI

The Nix setup includes testing capabilities:

```nix
checks = {
  lint = pkgs.callPackage ./nix/anterior-lint-ci.nix {
    go-services = let
      all = builtins.attrValues config.anterior.services.go;
    in
      lib.remove config.anterior.services.go.admin all;
    py-services = builtins.attrValues config.anterior.services.python;
    inherit (self'.packages) ruff;
  };
};
```

This enables:

- Running `nix flake check` to verify the repository
- Integrating with CI for automated testing
- Linting Go and Python services

## Local Development with Process Compose

For local development, the platform uses process-compose:

```nix
process-compose.ant-all-services = {
  imports = [
    inputs.services-flake.processComposeModules.default
    inputs.brrr.processComposeModules.dynamodb
    self.processComposeModules.gotenberg
    self.processComposeModules.localstack
    self.processComposeModules.prefect
    ./nix/anterior-services-process-compose.nix
  ];
  services.anterior-platform = {
    enable = true;
    serviceDefinitions = config.anterior.services;
  };
};
```

This enables:

1. Running all services locally with `ant-all-services`
2. Including dependencies like DynamoDB, Localstack, and Prefect
3. Integrating with external services

## Common Commands

Here are the most common commands for working with the Nix system:

### Setup

```bash
# Bootstrap a new macOS machine
darwin-rebuild switch --flake .#anterior-bootstrap

# Switch to full configuration
darwin-rebuild switch --flake .#anterior-base

# Set up Nix cache for faster builds
ant-setup-nix-cache
```

### Building and Running

```bash
# Build Docker images for all services
ant-build-docker

# Run all services locally
ant-all-services

# Run a specific service
nix run .#hello-world

# Enter development shell for a service
nix develop .#hello-world
```

### Development

```bash
# Enter default devshell
nix develop

# Enter npm devshell for JavaScript development
nix develop .#npm

# Lint code
ant-lint

# Clean build caches (use sparingly)
system-prune
```

### Testing

```bash
# Run all checks
nix flake check

# Run a specific service test
cd services/hello-world && pytest
```

## Advanced Features

### Environment Variables

The system sets several environment variables:

```nix
env = [
  {
    name = "ANT_ROOT";
    eval = "$(git rev-parse --show-toplevel)";
  }
  {
    name = "UV_PYTHON_DOWNLOADS";
    value = "never";
  }
  {
    name = "UV_NO_SYNC";
    value = "1";
  }
  {
    name = "SOPS_DISABLE_VERSION_CHECK";
    value = "1";
  }
];
```

### Secrets Management

The system integrates with 1Password for secrets:

```nix
command = ''
  set -euo pipefail
  ${lib.getExe self'.packages.ant-check-1password}
  account=${lib.escapeShellArg _1passwordAccount}
  CACHIX_AUTH_TOKEN="$(op read --account "$account" "op://Engineering/cachix/Envvars/CACHIX_AUTH_TOKEN")"
  export CACHIX_AUTH_TOKEN
  CACHIX_SIGNING_KEY="$(op read --account "$account" "op://Engineering/cachix/Envvars/CACHIX_SIGNING_KEY")"
  export CACHIX_SIGNING_KEY
  cachix use anterior-private
'';
```

## Conclusion

The Nix setup in Anterior Platform provides a comprehensive solution for:

1. **Reproducible Builds**: Ensuring consistency across environments
2. **Service Management**: Declaratively defining and building services
3. **Developer Experience**: Providing tailored development environments
4. **macOS Configuration**: Managing developer machines
5. **Local Development**: Running services locally for testing

The system's modular design allows for easy extension and maintenance, while specialized builders for each language ecosystem ensure optimal builds and deployments.# Frontend-Backend Integration in Anterior Platform

This document explains how the Vibes frontend applications integrate with the Platform backend through the Noggin service, with a focus on app deployment, serving, and API integration.

## Overview

The Anterior platform uses a sophisticated architecture to deploy and serve frontend applications:

1. **Vibes Repository**: Contains React/TypeScript frontend applications
2. **Platform Repository**: Houses the backend services including Noggin gateway
3. **S3 Storage**: Stores the built frontend assets
4. **Noggin Gateway**: Serves the frontend apps and proxies API requests

The key components involved in this integration are:

- `@platform/lib/ts/lib-platform/src/app.ts` - Core application framework
- `@platform/gateways/noggin/` - API gateway and frontend server
- `@vibes/apps/*/deploy-local.sh` - Frontend deployment scripts

## Application Framework (`lib-platform/src/app.ts`)

The `app.ts` file defines the core `App` class that powers all backend services in the platform. Key features:

```typescript
export class App<TServiceMap extends ServiceMap, TAppContext extends object> {
  public readonly hono: Hono<{ Variables: TAppContext & BaseContext }>;
  public readonly logger: Logger;
  public readonly trpc = initTRPC...

  constructor(
    public readonly baseCtx: TAppContext,
    public readonly options: AppOptions,
    public readonly auth?: (req: Request) => Promise<{ userUid: string; enterpriseUid: string } | Response>
  ) { ... }
}
```

Notable components:

1. **Hono Backend**: The app uses [Hono](https://hono.dev/) for HTTP routing, middleware, and handling
2. **Endpoint Definition**: Type-safe API endpoint definitions with schemas
3. **Auth Middleware**: JWT authentication support
4. **Error Handling**: Standardized error responses
5. **CORS Support**: Cross-origin resource sharing configuration

### Static App Serving

At the end of `app.ts` (lines 871-1018), there's a `createStaticApp` function that is crucial for serving frontend applications:

```typescript
export function createStaticApp({
  prefix,
  blobs,
  index = "index.html",
  env,
  shouldGenerateCSP = false,
}: {
  prefix: Path | "";
  blobs: ReadOnlyBlobStore;
  index?: string;
  env?: Record<string, string>;
  shouldGenerateCSP?: boolean;
}): Hono { ... }
```

This function:

1. Creates a Hono app that serves static files from a blob store (S3)
2. Handles path prefixing to match the route structure
3. Applies proper MIME types based on file extensions
4. Supports environment variable substitution in served files
5. Applies Content Security Policy headers if needed

## Noggin Gateway Integration

The Noggin gateway (`@platform/gateways/noggin/`) serves two purposes:

1. Provide API endpoints for backend services
2. Serve frontend applications from S3

### Backend App Structure

Each subdirectory in Noggin contains its own `app.ts` file that defines a specific service:

- `src/pdfs/app.ts` - PDF processing API
- `src/auth/app.ts` - Authentication services
- `src/notes/app.ts` - Notes API
- `src/flonotes/app.ts` - FloNotes app-specific API
- `src/flopilot/app.ts` - FloPilot app-specific API

These services expose endpoints the frontend apps consume:

```typescript
export function createPdfApp(ctx: PdfAppContext, options: AppOptions): PdfApp {
  const app: PdfApp = new App(ctx, options);

  app.endpoint({
    method: "POST",
    body: processStemPdfSchema,
    response: stemPdfUidResponseSchema,
    route: "/process-pdf",
    async handler(ctx) {
      /* Implementation */
    },
  });

  // Other endpoints like /pdf-status/:stemUid/:pdfUid and /pdf-extracts/:stemUid/:pdfUid

  return app;
}
```

### Directory-Based App Loading

Noggin dynamically loads all `app.ts` files from its subdirectories using `importAndMountNestedDirectoryApps` (lines 741-797). This creates an API structure mirroring the directory structure:

```typescript
export async function importAndMountNestedDirectoryApps(
  logger: Logger,
  app: Hono,
  basePath: string,
  baseRoute: string = "",
) {
  // Recursively find all app.ts files in subdirectories
  // and mount them at their corresponding paths
}
```

## Frontend Deployment Flow

The frontend applications in the Vibes repository are deployed using shell scripts:

### Build Process

Each app in the `vibes/apps/` directory has a `deploy-local.sh` script that:

1. Builds the React/TypeScript app with Vite
2. Configures environmental variables
3. Uploads the built assets to S3
4. Makes them available through Noggin

Key deployment variables:

```bash
# From flonotes/deploy-local.sh or flopilot/deploy-local.sh
APP_PREFIX="vibes/flonotes"  # or "vibes/flopilot"
BASE_PATH="/flonotes"        # or "/flopilot"
BUCKET_NAME="local-bucket"
LOCALSTACK_ENDPOINT="http://localhost:4566"
VITE_NOGGIN_HOST="http://localhost:20701"
```

The deployment scripts use AWS CLI to upload the built assets to S3:

```bash
aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
  --endpoint-url "$LOCALSTACK_ENDPOINT" \
  --no-verify-ssl \
  --delete
```

### S3 Directory Structure

The frontend assets are stored in S3 with the following structure:

- `vibes/flonotes/` - FloNotes app assets
- `vibes/flopilot/` - FloPilot app assets
- `vibes/atlas/` - Atlas app assets

## URL Routing

When a user visits a Noggin URL like `http://localhost:20701/flonotes`, the following happens:

1. Request arrives at Noggin gateway
2. Noggin's static app server handles the route
3. The path is used to determine which app to serve (`/flonotes` → `vibes/flonotes/`)
4. The static app fetches the appropriate files from S3
5. The frontend app is served with the correct content types

## API Communication

Frontend apps communicate with backend services through the Noggin gateway:

1. Frontend makes API requests to paths like `/pdfs/process-pdf`
2. Noggin routes these to the appropriate backend services
3. JWT authentication cookies are automatically included in requests
4. Responses are returned to the frontend

For example, the FloNotes frontend uses API functions like:

```typescript
// From flonotes/src/services/noggin-api.ts
export async function processPdf(
  fileData: number[],
  fileName: string,
  documentType: "clinical" | "criteria",
  stemUid?: string,
): Promise<ProcessPdfResponse> {
  // Builds and sends a multipart/form-data request to Noggin
  const response = await apiRequest(API_ENDPOINTS.processPdf, {
    method: "POST",
    body: formData,
  });

  // Return parsed response
  return data;
}
```

## Environment Variables

Frontend apps receive environment variables through two mechanisms:

1. **Build-time**: Variables defined in deployment scripts

   ```bash
   export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
   export BASE_PATH="${BASE_PATH}"
   ```

2. **Run-time**: Text substitution in static files
   ```typescript
   // Text substitution in served files
   if (env) {
     for (const [key, value] of Object.entries(env)) {
       text = text.replaceAll(`$${key}`, value);
     }
   }
   ```

## Deployment Workflow

The complete workflow for deploying and running a Vibes frontend app:

1. Start the platform services

   ```bash
   cd platform
   nix develop
   ant-all-services
   ```

2. Build and deploy the frontend app

   ```bash
   cd vibes/apps/flonotes
   npm i
   ./deploy-local.sh
   ```

3. Access the app through Noggin
   ```
   http://localhost:20701/flonotes
   ```

## Security Model

The security model for frontend-backend integration includes:

1. **Authentication**: JWT tokens stored as HTTP-only cookies
2. **Authorization**: Enterprise and user-level permissions
3. **CORS**: Controlled cross-origin access
4. **CSP**: Content Security Policy for frontend applications

## Development Workflow

For faster development:

1. The Noggin service can be stopped in the process-compose screen
2. Environment variables can be copied from the logs
3. Noggin can be run separately with:
   ```bash
   cd gateways/noggin
   source .env && npm run start | npx pino-pretty
   ```

## Common Issues and Troubleshooting

1. **Connection Failures**: Ensure LocalStack is running

   ```bash
   # Check LocalStack availability
   curl -s --head "http://localhost:4566"
   ```

2. **Authentication Issues**: Verify cookies are being set properly

   ```javascript
   // Check for auth_indicator cookie
   document.cookie.includes("auth_indicator=true");
   ```

3. **Missing Assets**: Check S3 bucket contents

   ```bash
   aws s3 ls s3://local-bucket/vibes/flonotes/ --endpoint-url "http://localhost:4566" --no-verify-ssl
   ```

4. **API Errors**: Check Noggin logs for detailed error information# Noggin Chat: Smart PDF Citations Demo

## Overview

This demo showcases the new PDF citation feature in Noggin Chat. This feature allows the AI to reference specific parts of uploaded PDF documents, creating a more transparent and verifiable chat experience for users working with medical documentation.

## Key Features

### 1. Intelligent PDF Citations

The AI can now reference precise sections of uploaded PDFs in its responses. Each citation:

- Links directly to the source document
- Highlights the exact text being referenced
- Shows the page number and position in the document

### 2. PDF Navigation Through AI

Users can click on citations to jump directly to the referenced section in the PDF, allowing for:

- Quick verification of AI responses against source material
- Efficient document exploration guided by AI insights
- Better understanding of complex medical documentation

### 3. Transparent AI Responses

All AI claims about document content can now be traced back to their source:

- Citations are presented inline within the AI's response
- Multiple documents can be referenced in a single response
- Users can easily distinguish between AI analysis and direct quotes

## Technical Implementation

The citation system works through a sophisticated pipeline:

1. **Document Processing**

   - PDFs are processed to extract text blocks with precise coordinate information
   - Text extracts are organized by document ID and page number

2. **Intelligent LLM Prompting**

   - The system builds structured XML prompts containing relevant document extracts
   - LLM is instructed to include citations using a consistent XML format

3. **Citation Processing**

   - LLM responses containing citation XML tags are parsed
   - Citation references are mapped back to the original document sections
   - Content offsets are calculated to maintain precise text references

4. **Response Formatting**
   - Citations are transformed into structured data
   - Response text is enhanced with metadata linking to source documents

## Demo Scenarios

### Scenario 1: Medical Criteria Verification

**User:** "What criteria need to be met for approval of this treatment?"

**Noggin Chat:**
"According to the guidelines, approval requires [three main criteria](#citation1):

1. Confirmed diagnosis by a specialist
2. Failure of at least two conventional therapies
3. No contraindications for the requested treatment

For patients under 18, [additional documentation](#citation2) from a pediatric specialist is required."

_Citations link directly to the relevant sections in the medical criteria documents_

### Scenario 2: Clinical Information Exploration

**User:** "Summarize the patient's cardiac history"

**Noggin Chat:**
"The patient has a history of [atrial fibrillation diagnosed in 2018](#citation1) with [two cardioversion procedures](#citation2) performed in 2019. They are currently on [anticoagulation therapy](#citation3) and have [stable cardiac function](#citation4) with an ejection fraction of 55% as of their last echocardiogram."

_Each citation links to the specific clinical note containing the information_

## Business Value

This feature delivers significant value by:

1. **Increasing Trust & Transparency**

   - Users can verify AI responses against source material
   - Reduces the risk of hallucinated or incorrect information

2. **Improving Efficiency**

   - Faster document navigation through AI-guided citations
   - Reduced time spent manually verifying information

3. **Enhancing Compliance**

   - Clear audit trail of information sources
   - Better documentation of decision-making rationale

4. **Supporting Better Decisions**
   - More informed clinical and operational decisions
   - Reduced risk of errors in document interpretation

## Technical Notes

- Citations use normalized coordinates (0-1) for PDF bounding boxes
- XML-based citation format: `<extract ids="pdf_ID#extract_ID">text</extract>`
- Content offsets track precise character positions in responses
- System handles multiple document references in a single response

# Modern Conventional Commits Workflow (2025)

This document provides a comprehensive guide to using the conventional commits workflow in your projects. This implementation includes fuzzy filtering, commitlint validation, and AI-assisted commit messages.

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Tools and Integrations](#tools-and-integrations)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)

## Overview

The Conventional Commits workflow implemented in this dotfiles repository provides:

- **Commitizen integration** with fuzzy filtering using `cz-git`
- **Commitlint validation** for enforcing conventional commit format
- **Husky hooks** for running pre-commit and commit-msg validations
- **AI-assisted commit messages** using Claude to analyze changes and generate commits
- **Fuzzy git commands** for interactive staging and committing
- **Shell aliases** for streamlined workflow

## Installation

All configuration is already set up in your dotfiles. The main setup script automatically configures the conventional commits workflow.

If you need to manually initialize or reinitialize the tooling:

```bash
# Initialize conventional commits tooling
cd ~/dotfiles/config/git/commitizen
npm install

# Run the setup script
node scripts/setup-hooks.js
```

Alternatively, use the helper function:

```bash
# Run the helper function from anywhere
git-setup-conv
```

This will:

1. Install required dependencies (commitizen, cz-git, commitlint, husky)
2. Set up git hooks for commit validation
3. Configure git to use the hooks

To source the shell aliases immediately:

```bash
source ~/dotfiles/config/zsh/git-aliases.zsh
```

## Usage

### Basic Workflow

```bash
# 1. Make changes to your code
# 2. Stage files using fuzzy add
gfa
# 3. Commit using commitizen with fuzzy filtering
gcz
# 4. Push your changes
git push
```

### Available Commands

| Command           | Description                                          |
| ----------------- | ---------------------------------------------------- |
| `git-cz` or `gcz` | Interactive commitizen prompt with fuzzy filtering   |
| `gczf`            | Commitizen without hook validation (for WIP commits) |
| `gfa`             | Fuzzy-find and stage files for commit                |
| `gfc`             | Fuzzy-find, stage, and commit in one command         |
| `gaic`            | Generate commit message using AI                     |
| `gclint`          | Run commitlint on last commit                        |
| `git-setup-conv`  | Re-initialize conventional commits setup             |
| `git-conv-help`   | Show quick reference for conventional commits        |

### AI-Assisted Commits

The `gaic` command will:

1. Analyze your staged changes
2. Generate a conventional commit message that describes the changes
3. Offer to use the generated message or modify it

```bash
# Stage changes first
git add .
# Use AI to generate commit message
gaic
```

## Configuration

The conventional commits setup uses the following configuration files:

- **`~/dotfiles/config/git/commitizen/.czrc`**: Commitizen configuration
- **`~/dotfiles/config/git/commitizen/commitlint.config.js`**: Commitlint rules
- **`~/dotfiles/config/git/gitconfig`**: Git aliases and core settings
- **`~/dotfiles/config/git/gitmessage`**: Commit message template
- **`~/dotfiles/config/zsh/git-aliases.zsh`**: Shell aliases and functions

### Customizing Commit Types

The default configuration includes standard conventional commit types plus a few custom ones:

- Standard: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, `build`, `ci`, `chore`, `revert`
- Custom: `wip` (work in progress), `deps` (dependency updates)

To add custom types, edit the following files:

- `commitlint.config.js`: Add to the `type-enum` rule
- `.czrc`: Add to the `types` object

### Customizing Commit Scopes

Allowed scopes are defined in the `commitlint.config.js` file under the `scope-enum` rule.

## Tools and Integrations

This setup integrates the following tools:

- **[Commitizen](https://github.com/commitizen/cz-cli)**: Interactive commit message wizard
- **[cz-git](https://github.com/Zhengqbbb/cz-git)**: Commitizen adapter with fuzzy filtering
- **[Commitlint](https://github.com/conventional-changelog/commitlint)**: Validate commit messages
- **[Husky](https://github.com/typicode/husky)**: Git hooks to validate commits
- **[Claude AI](https://claude.ai)**: AI-assisted commit message generation

## Best Practices

- **Use scopes consistently**: Choose meaningful scopes that map to areas of your codebase
- **Keep the subject line concise**: Aim for 50 characters or less
- **Use imperative mood**: Write "add feature" not "added feature"
- **Include a body for complex changes**: Explain why the change was made
- **Reference issues in footer**: Use "Fixes #123" or "Related to #456"
- **Mark breaking changes**: Use `!` suffix or `BREAKING CHANGE:` in the body

## Troubleshooting

### Commitizen not found

If you get `command not found: commitizen` or similar errors:

```bash
# Reinstall dependencies
cd ~/dotfiles/config/git/commitizen
npm install

# Make sure global node_modules bin is in your PATH
export PATH="$PATH:$(npm config get prefix)/bin"
```

### Commit validation failing

If your commits are being rejected:

```bash
# Check the commitlint rules
cat ~/dotfiles/config/git/commitizen/commitlint.config.js

# Test a commit message manually
echo "feat: my change" | npx --no-install commitlint

# Bypass validation for temporary commits
git commit --no-verify -m "WIP: temporary commit"
# Or use the alias
gczf
```

### Husky hooks not running

If hooks aren't executing:

````bash
# Check hooks path setting
git config --get core.hooksPath

# Run setup script again
cd ~/dotfiles/config/git/commitizen
node scripts/setup-hooks.js
```# Platform Code-Generation Overview

> **TL;DR**  `schemas → codegen → gen → lib → app`.
>
> *Schemas are the specs* → *Codegen turns them into language-specific artefacts* → *Generated artefacts live in `gen/`* → *Hand-written helper libraries in `lib/` wrap those artefacts* → Everything above is consumed by services, gateways, workflows, and front-ends.

---

## 1  `schemas/` – Single Source of Truth

| Sub-dir | Kind of schema | Typical toolchain |
|---------|----------------|-------------------|
| `proto/` | Protobuf service & message definitions | **Buf** → Go/Python/TS gRPC stubs |
| `openapi/` | REST/OpenAPI specifications | **openapi-generator** → typed clients |
| `events/` | Event envelopes (e.g. Kafka, SNS) | Registry docs / Avro / JSON-Schema |
| `data/` | Raw data models (JSON-Schema) | Code-model generators |
| `workflows/` | Workflow DSL / state machines | Prefect, Temporal, etc. |

*Never* import from here at runtime – these files are language-agnostic specifications only.

## 2  `codegen/` – All Generators & Templates

The `codegen/` directory contains containerised scripts (usually run via `ant codegen`) that read from `schemas/` and write to `gen/`.

Key folders:

* **`codegen/proto/`** – Runs Buf to emit Go & Python protobuf stubs.
* **`codegen/models/`** – Converts JSON-Schema → Go structs / Pydantic / Zod.
* **`codegen/openapi-client/`** – Generates REST API clients for multiple languages.
* **`codegen/compose/`** – Stamps out many environment-specific Docker Compose files.

These scripts are *build-time only*; nothing inside `codegen/` is imported by application code.

## 3  `gen/` – Generated Artefacts (Checked-in)

`gen/` is the mounted output directory for all generators.  Layout:

````

gen/
├─ go/ # Go structs, clients, mocks …
├─ python/ # Pydantic & FastAPI models …
├─ ts/ # Zod schemas, TypeScript types …
├─ compose/ # Pre-baked docker-compose YAMLs
└─ docs/ # Reference documentation

````

Rules:

1. Never hand-edit files in `gen/`.
2. Always commit the diff so CI/CD & IDEs don’t require local regeneration.

## 4  `lib/{go,ts,python}/` – Hand-Written Shared Libraries

These packages sit *on top* of the raw generated code, providing ergonomics, helpers and business logic.  Examples:

* `lib/ts/lib-platform/…` – HTTP app framework, structured logging helpers, Zod utilities.
* `lib/go/prefect/…` – Convenience wrappers around Prefect-generated types.
* `lib/python/src/lib/…` – Cross-service Python utilities.

Application layers import from **both** `gen/` and `lib/`.

## 5  `compose/` vs `codegen/compose/`

* `codegen/compose/` – Templates + generation logic.
* `compose/` – The frozen YAMLs produced by that logic.

Same pattern: template → generated output.

## 6  Developer Workflow

```text
1. Edit a schema   (schemas/…)
2. Run codegen      ant codegen
3. Commit schema change + generated diff (gen/…)
4. Extend helpers   (lib/…) if needed
5. Use new types    in services, gateways, etc.
````

Keep this pipeline in mind whenever you wonder, “Where does this type/client come from?” – start at `schemas/` and follow the arrows downstream.

# Authentication System in Anterior Platform

This document explains how authentication works in the Anterior platform, focusing on the integration between the frontend applications and the Noggin gateway.

## Overview

The Anterior platform implements a multi-step authentication flow:

1. **Login Form** - Users enter credentials (email/password)
2. **OTP Verification** - One-time password sent to user
3. **JWT Tokens** - Upon verification, JWT tokens are issued
4. **Cookie-Based Auth** - Tokens stored in HTTP-only cookies
5. **Enterprise/User Context** - JWT carries enterprise and user information

This system provides secure authentication while maintaining multi-tenant isolation between enterprises.

## Authentication Flow

### 1. Login Process

The authentication process begins when a user visits the login page:

```
/auth/login.html
```

The login page is a static HTML file that presents a form for email and password entry. When submitted, the form sends a POST request to:

```
/auth/login
```

The backend then:

1. Validates the credentials
2. Generates a one-time password (OTP)
3. Redirects the user to the OTP verification page

### 2. OTP Verification

The OTP verification page is accessed at:

```
/auth/otp-verify.html
```

Users enter the OTP they received, and the form submits to:

```
/auth/otp/verify
```

Upon successful OTP verification, the backend:

1. Generates JWT access and refresh tokens
2. Sets HTTP-only cookies containing these tokens
3. Redirects the user to the requested application

### 3. Cookie Management

The system sets three cookies for authentication:

1. **access_token** - Short-lived JWT token (15 minutes)

   ```
   access_token=<JWT>; Path=/; HttpOnly; SameSite=Lax; Max-Age=900
   ```

2. **refresh_token** - Long-lived token for refreshing access (30 days)

   ```
   refresh_token=<JWT>; Path=/; HttpOnly; SameSite=Lax; Max-Age=43200
   ```

3. **auth_indicator** - Non-HTTP-only flag for frontend
   ```
   auth_indicator=true; Path=/; SameSite=Lax; Max-Age=900
   ```

The `HttpOnly` flag prevents JavaScript from accessing the tokens, protecting against XSS attacks, while the third cookie allows the frontend to know the user is authenticated.

## JWT Structure and Enterprise Context

The JWT tokens contain important claims:

```json
{
  "sub": "user123", // User ID
  "exp": 1683721200, // Expiration time
  "iat": 1683717600, // Issued at time
  "entr": {
    // Enterprise memberships
    "enterprise456": "admin"
  },
  "wksp": {
    // Workspace memberships
    "workspace789": "enterprise456"
  }
}
```

Key components:

- `sub`: The user's unique identifier
- `entr`: Map of enterprise IDs to roles
- `wksp`: Map of workspace IDs to enterprise IDs

This structure enables:

1. Multi-enterprise support (users can belong to multiple enterprises)
2. Role-based access control within each enterprise
3. Workspace context within enterprises

### JWT Parsing and Context Extraction

The `jwt.ts` file handles extraction of enterprise and user information from the JWT payload:

```typescript
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
  if (!ctx.jwtPayload?.entr) {
    throw new HTTPException(401, { message: "Authentication required" });
  }

  // Get enterprise and user uids from the token.
  let enterpriseUid = undefined;
  let role = undefined;

  // Most tokens will have a single enterprise.
  if (Object.keys(ctx.jwtPayload.entr).length === 1) {
    enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
    role = ctx.jwtPayload.entr[enterpriseUid];
  } else {
    // If token contains multiple enterprises.
    // First check for the X-Anterior-Enterprise-Id header.
    const entHeader = ctx.req.headers.get("X-Anterior-Enterprise-Id");
    if (entHeader && ctx.jwtPayload.entr[entHeader]) {
      enterpriseUid = ctx.jwtPayload.entr[entHeader];
      role = ctx.jwtPayload.entr[enterpriseUid];
    } else {
      // Use the first enterprise in the map (random).
      enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
      role = ctx.jwtPayload.entr[enterpriseUid];
    }
  }

  // Get workspaceUid from claims or header if present.
  let workspaceUid = undefined;
  const wksp = ctx.jwtPayload.wksp;
  if (Object.keys(wksp).length === 1) {
    // Token has a single workspace.
    Object.entries(wksp).forEach(([wkId, entId]) => {
      if (entId !== enterpriseUid) {
        throw new HTTPException(401, {
          message: "Workspace in token does not match enterprise",
        });
      }
      workspaceUid = wkId;
    });
  } else {
    // Token claims contain multiple workspaces. Look for
    // the X-Anterior-Workspace-Id header.
    const wkspHeader = ctx.req.headers.get("X-Anterior-Workspace-Id");
    if (wkspHeader) {
      const entId = wksp[wkspHeader];
      if (!entId) {
        throw new HTTPException(401, {
          message: "Workspace from header not found in token",
        });
      }
      if (entId !== enterpriseUid) {
        throw new HTTPException(401, {
          message: "Workspace in header does not match enterprise",
        });
      }
      workspaceUid = wkspHeader;
    }
  }

  return {
    enterpriseUid,
    role,
    workspaceUid,
    userUid: ctx.jwtPayload.sub,
  };
}
```

## API Authentication

When making API requests, the frontend doesn't need to manually handle tokens since the cookies are automatically included in requests to the same domain.

For enterprise-specific contexts, the frontend can include headers:

- `X-Anterior-Enterprise-Id`: Selects which enterprise context to use
- `X-Anterior-Workspace-Id`: Selects a specific workspace within the enterprise

## Token Refresh Mechanism

While not fully implemented in the current code, the system has a token refresh endpoint at:

```
/auth/refresh
```

The refresh flow is designed to:

1. Use the refresh_token cookie to authenticate
2. Generate new access and refresh tokens
3. Set new cookies with updated expiration times

This allows for session persistence without requiring users to log in frequently.

## Frontend Integration

The FloNotes app integrates with this auth system through:

1. **Login Page** - Static HTML served from S3 through Noggin
2. **Client JavaScript** - Handles form submission and error display
3. **Auth Detection** - Checks for the `auth_indicator` cookie

When a user attempts to access the app, the frontend checks for the auth cookie and redirects to login if not present.

## Security Considerations

The auth system implements several security best practices:

1. **HTTP-Only Cookies** - Prevents JavaScript access to tokens
2. **SameSite Policy** - Restricts cookie use to same-site contexts
3. **JWT Expiration** - Short-lived access tokens (15 minutes)
4. **Enterprise Isolation** - Data access limited to authorized enterprises
5. **CSRF Protection** - Planned but not yet implemented

## E2E Testing Authentication

The Anterior platform includes a robust system for handling authentication in end-to-end (E2E) tests. This system is designed to be secure, reusable, and configurable for different testing scenarios.

### Auth State Management

The `AuthStateManager` class in `auth-state.ts` provides persistent storage of authentication state between test runs:

```typescript
export class AuthStateManager {
  private baseDir: string;
  private filename: string;
  private autoCreateDir: boolean;
  private expirationTimeMs: number;

  constructor(options: AuthStateManagerOptions = {}) {
    this.baseDir = options.baseDir || path.join(process.cwd(), ".auth");
    this.filename = options.filename || "user.json";
    this.autoCreateDir = options.autoCreateDir !== false;
    // Default to 24 hours expiration
    this.expirationTimeMs = options.expirationTimeMs || 24 * 60 * 60 * 1000;

    if (this.autoCreateDir && !fs.existsSync(this.baseDir)) {
      fs.mkdirSync(this.baseDir, { recursive: true });
    }
  }

  // Saves the current browser authentication state to disk
  async save(context: BrowserContext): Promise<string> {
    const playwrightState = await context.storageState();
    const now = Date.now();

    const enhancedState: AuthState = {
      ...playwrightState,
      createdAt: now,
      expiresAt: now + this.expirationTimeMs,
    };

    const filePath = this.getStoragePath();
    fs.writeFileSync(filePath, JSON.stringify(enhancedState, null, 2), "utf-8");

    return filePath;
  }

  // Loads a saved authentication state
  load(): AuthState | null {
    const filePath = this.getStoragePath();

    if (!fs.existsSync(filePath)) {
      return null;
    }

    try {
      const fileContent = fs.readFileSync(filePath, "utf-8");
      return JSON.parse(fileContent) as AuthState;
    } catch (error) {
      console.error(`Error loading auth state from ${filePath}:`, error);
      return null;
    }
  }

  // Checks if the saved auth state is still valid (not expired)
  isValid(): boolean {
    if (!this.exists()) return false;

    const state = this.load();
    if (!state) return false;

    const now = Date.now();

    return state.expiresAt
      ? state.expiresAt > now
      : state.createdAt
        ? state.createdAt + this.expirationTimeMs > now
        : false;
  }
}
```

This allows tests to:

1. Save authenticated browser state after login
2. Reuse that state in future test runs to avoid repeated logins
3. Automatically detect when saved auth is expired
4. Clean up old authentication state when needed

### OTP Handling

The `otp-handler.ts` module provides secure OTP handling for E2E tests:

```typescript
export const otpHandlerOptionsSchema = z.object({
  // Email associated with the OTP
  email: z.string().email(),

  // API endpoint for retrieving OTPs (if available in your environment)
  apiEndpoint: z.string().url().optional(),

  // Environment variable name for test OTP
  otpEnvVar: z.string().optional().default("ANT_TEST_OTP"),

  // Default OTP to use if other methods fail
  defaultOtp: z.string().optional().default("123456"),

  // Whether to obfuscate OTP in logs
  secureLogging: z.boolean().optional().default(true),

  // Optional Playwright request object for API calls
  request: z.any().optional(),
});

// Retrieves an OTP for testing purposes with priority order:
// 1. From environment variable
// 2. From API endpoint
// 3. Default OTP
export async function getTestOtp(options: OtpHandlerOptions): Promise<string> {
  // Validate the options with zod schema
  const validatedOptions = otpHandlerOptionsSchema.parse(options);

  const { email, apiEndpoint, otpEnvVar, defaultOtp, secureLogging, request } =
    validatedOptions;

  // 1. Try from environment variable
  if (process.env[otpEnvVar]) {
    const otp = process.env[otpEnvVar];
    console.log(
      `Using OTP from environment variable ${otpEnvVar}${secureLogging ? ": [REDACTED]" : `: ${otp}`}`,
    );
    return otp as string;
  }

  // 2. Try from API endpoint if provided
  if (apiEndpoint) {
    try {
      // Use Playwright request object if provided
      if (request) {
        const response = await request.get(
          `${apiEndpoint}?email=${encodeURIComponent(email)}`,
        );
        if (response.ok()) {
          const data = (await response.json()) as OtpApiResponse;
          if (data && data.otp) {
            console.log(
              `Retrieved OTP from API${secureLogging ? ": [REDACTED]" : `: ${data.otp}`}`,
            );
            return data.otp;
          }
        }
      } else {
        // Use native fetch if no request object provided
        const response = await fetch(
          `${apiEndpoint}?email=${encodeURIComponent(email)}`,
        );
        if (response.ok) {
          const data = (await response.json()) as OtpApiResponse;
          if (data && data.otp) {
            console.log(
              `Retrieved OTP from API${secureLogging ? ": [REDACTED]" : `: ${data.otp}`}`,
            );
            return data.otp;
          }
        }
      }
    } catch (error) {
      console.warn("Failed to retrieve OTP from API:", error);
    }
  }

  // 3. Use default OTP
  console.log(
    `Using default OTP for testing${secureLogging ? ": [REDACTED]" : `: ${defaultOtp}`}`,
  );
  return defaultOtp;
}

// Fills and submits the OTP in a test environment
export async function submitOtp(page: Page, otp: string): Promise<boolean> {
  try {
    // Wait for OTP input field
    await page.waitForSelector('input[name="otp"]');

    // Fill the OTP field
    await page.fill('input[name="otp"]', otp);

    // Submit the form
    await Promise.all([
      page.click('button[type="submit"]'),
      page.waitForResponse(
        (response) =>
          response.url().includes("/auth") && response.status() < 400,
      ),
    ]);

    // Check for auth cookie to verify success
    const cookies = await page.context().cookies();
    const hasAuthCookie = cookies.some(
      (cookie) => cookie.name === "auth_indicator",
    );

    return hasAuthCookie;
  } catch (error) {
    console.error("Error submitting OTP:", error);
    return false;
  }
}
```

An important enhancement in the current implementation is the support for using Playwright's `request` object, which provides more consistent network handling in tests.

### Test Utilities

The `test-utils.ts` file provides a complete login workflow for tests:

```typescript
export interface LoginOptions {
  authDir?: string;
  storeAuthState?: boolean;
  expirationTimeMs?: number;
  reuseAuth?: boolean;
  secureLogging?: boolean;
}

export async function login(page: Page, options?: LoginOptions) {
  const {
    ANT_NOGGIN_BASE_URL,
    ANT_TEST_USER_EMAIL,
    ANT_TEST_USER_PASSWORD,
    ANT_TEST_USER_OTP,
    ANT_TEST_USER_STORE_AUTH_STATE,
    ANT_TEST_USER_AUTH_DIR,
    ANT_TEST_USER_SECURE_LOGGING,
    ANT_TEST_USER_API_ENDPOINT,
  } = envVarsSchema.parse(process.env);

  const storeAuthState =
    options?.storeAuthState ?? ANT_TEST_USER_STORE_AUTH_STATE ?? false;
  const secureLogging =
    options?.secureLogging ?? ANT_TEST_USER_SECURE_LOGGING ?? false;

  const authDir =
    options?.authDir ||
    ANT_TEST_USER_AUTH_DIR ||
    path.join(process.cwd(), ".auth");
  if (storeAuthState && !fs.existsSync(authDir)) {
    fs.mkdirSync(authDir, { recursive: true });
  }

  const result: LoginResult = {
    success: false,
    redirected: false,
    authCookie: null,
    accessToken: null,
    storageStatePath: storeAuthState ? path.join(authDir, "user.json") : null,
    error: null,
  };

  try {
    // Login with email and password
    await page.goto(`${ANT_NOGGIN_BASE_URL}/auth/login.html`);
    await page.waitForSelector('input[name="email"]');
    await page.fill('input[name="email"]', ANT_TEST_USER_EMAIL);
    await page.fill('input[name="password"]', ANT_TEST_USER_PASSWORD);
    await page.click('button[type="submit"]');

    // Wait for OTP verification page
    await page.waitForURL(/otp-verify\.html/);

    // Get OTP securely
    let otp = ANT_TEST_USER_OTP;
    if (!otp) {
      const otpOptions: OtpHandlerOptions = {
        email: ANT_TEST_USER_EMAIL,
        secureLogging: secureLogging,
        otpEnvVar: "ANT_TEST_OTP",
        defaultOtp: "123456",
      };

      // Only add apiEndpoint if defined
      if (ANT_TEST_USER_API_ENDPOINT) {
        otpOptions.apiEndpoint = ANT_TEST_USER_API_ENDPOINT;
      }

      otp = await getTestOtp(otpOptions);
    } else if (secureLogging) {
      console.log("Using provided OTP: [REDACTED]");
    } else {
      console.log(`Using provided OTP: ${otp}`);
    }

    // Submit OTP
    const otpSuccess = await submitOtp(page, otp);

    if (!otpSuccess) {
      throw new Error("OTP verification failed");
    }

    // Check for redirect
    try {
      await Promise.race([
        page.waitForURL(/\/flonotes/, { timeout: 10000 }),
        page.waitForURL(/\/health/, { timeout: 10000 }),
      ]);
      result.redirected = true;
    } catch (error) {
      console.warn("Failed to detect redirect, checking cookies instead");
    }

    // Verify authentication cookies
    const cookies = await page.context().cookies();
    result.authCookie = cookies.find(
      (cookie) => cookie.name === "auth_indicator",
    );
    result.accessToken = cookies.find(
      (cookie) => cookie.name === "access_token",
    );

    if (!result.authCookie) {
      throw new Error("Authentication failed: No auth_indicator cookie found");
    }

    // Store authentication state if requested
    if (storeAuthState && result.storageStatePath) {
      await page.context().storageState({ path: result.storageStatePath });
    }

    result.success = true;
  } catch (error) {
    result.error = error instanceof Error ? error : new Error(String(error));
    if (storeAuthState) {
      // Take a screenshot of failure for debugging
      const screenshotPath = path.join(authDir, "auth-failure.png");
      await page.screenshot({ path: screenshotPath });
    }
  }

  return result;
}

export async function verifyAuthenticated(page: Page): Promise<boolean> {
  const cookies = await page.context().cookies();
  return !!cookies.find((cookie) => cookie.name === "auth_indicator");
}
```

### Test Fixtures for OTP Testing

The `otp-handler.spec.ts` file introduces test fixtures for better isolation when testing OTP functionality:

```typescript
// Create a test fixture with mockable OTP environment
const test = base.extend<{ mockOtpEnv: undefined }>({
  // Mock environment for OTP testing
  mockOtpEnv: [
    async ({}, use) => {
      // Save the original environment variable
      const originalOtp = process.env["ANT_TEST_OTP"];

      // Clear the variable for tests
      delete process.env["ANT_TEST_OTP"];

      // Allow tests to use the mock environment
      await use(undefined);

      // Restore the original value after tests
      if (originalOtp) {
        process.env["ANT_TEST_OTP"] = originalOtp;
      } else {
        delete process.env["ANT_TEST_OTP"];
      }
    },
    { scope: "test" },
  ],
});
```

This approach isolates environment variable changes to specific test runs, preventing test interference.

### PDF API Testing

The PDF API tests in `pdf.spec.ts` show how protected endpoints can be accessed in tests:

```typescript
test("Upload, process, and retrieve extracts for PDF documents", async () => {
  // Process PDF (would fail without auth)
  const processPdfResponse = await fetch(PROCESS_PDFS_URL, {
    method: "POST",
    body: formData,
  });

  // Test remains of the API flow
  const { stemUid, pdfUid } = await processPdfResponse.json();

  // Poll for processing completion
  await expect
    .poll(
      async () => {
        const statusResponse = await fetch(PDF_STATUS_URL(stemUid, pdfUid));
        return statusResponse.status === 200;
      },
      {
        timeout: POLLING_TIMEOUT,
        intervals: [POLLING_INTERVAL],
      },
    )
    .toBeTruthy();

  // Check status and extracts
  // Verify results
});
```

In production code, these requests would include auth cookies automatically. In tests, the cookies are either:

1. Set up before the test using the `login()` function
2. Added manually to the request for isolated API tests
3. Bypassed in tests with a test-specific authentication mechanism

### Test Configuration

Tests use environment variables for configuration:

```typescript
// Environment variables for test auth
const envVarsSchema = z.object({
  ANT_NOGGIN_BASE_URL: z.string().url(),
  ANT_TEST_USER_EMAIL: z.string().email().default("test.user@anterior.com"),
  ANT_TEST_USER_PASSWORD: z.string().default("password123"),
  ANT_TEST_USER_OTP: z.string().optional(),
  ANT_TEST_USER_STORE_AUTH_STATE: z.boolean().optional(),
  ANT_TEST_USER_AUTH_DIR: z.string().optional(),
  ANT_TEST_USER_SECURE_LOGGING: z.boolean().optional(),
  ANT_TEST_USER_API_ENDPOINT: z.string().url().optional(),
});
```

This allows tests to be configured for different environments without code changes.

### Auth Test Scenarios

The tests in `auth.spec.ts` and `auth-state.spec.ts` cover key auth scenarios:

1. **Basic Authentication**: Verifying that login works with valid credentials
2. **OTP Source Flexibility**: Testing different ways to provide OTP codes
3. **Auth State Expiration**: Ensuring expired auth is handled correctly
4. **Auth State Reuse**: Confirming that saved auth state can be reused across test runs

### Security in Tests

The testing framework implements several security best practices:

1. **OTP Redaction**: Option to hide sensitive OTPs in logs
2. **Isolated Test Directories**: Separate directories for test auth state
3. **Automatic Cleanup**: Removing sensitive files after tests complete
4. **Env Var Configuration**: Using environment variables for credentials
5. **Validation**: Schema validation for all inputs and configuration
6. **Test Fixtures**: Isolated environment changes for test runs

## Implementation Components

The auth system consists of these key files:

1. **app.ts** - Main auth endpoints and cookie handling logic
2. **jwt.ts** - JWT parsing and enterprise/user context extraction
3. **auth-state.ts** - Manages authentication state for testing
4. **otp-handler.ts** - Handles OTP verification in tests
5. **test-utils.ts** - Provides authentication utilities for E2E tests
6. **test-e2e/\*.spec.ts** - Test files for auth functionality

## Future Improvements

The current implementation has some noted areas for enhancement:

1. **CSRF Protection** - To be enabled when client is ready
2. **Refresh Token Implementation** - Fully functional token refresh flow
3. **Multiple Enterprise Support** - Improved UI for enterprise switching
4. **Workspace Context** - Better integration of workspace selection

## Integration Instructions

To add auth to a new frontend app:

1. **Check Auth State**

   ```javascript
   function isAuthenticated() {
     return document.cookie.includes("auth_indicator=true");
   }
   ```

2. **Redirect to Login if Needed**

   ```javascript
   if (!isAuthenticated()) {
     const currentPath = encodeURIComponent(window.location.pathname);
     window.location.href = `/auth/login.html?redirectTo=${currentPath}`;
   }
   ```

3. **Handle API Requests with Authentication**

   ```javascript
   // No need to manually add tokens - cookies are sent automatically
   fetch("/api/endpoint", {
     credentials: "include", // Important: include cookies
     headers: {
       "Content-Type": "application/json",
       // Optional: specify enterprise context
       "X-Anterior-Enterprise-Id": currentEnterpriseId,
     },
   });
   ```

4. **Logout Function**

   ```javascript
   function logout() {
     // Clear cookies by setting Max-Age=0
     document.cookie = "auth_indicator=; Path=/; Max-Age=0";
     window.location.href = "/auth/login.html";
   }
   ```

5. **Adding E2E Tests for Auth**

   ```typescript
   import { login, verifyAuthenticated } from "../auth/test-utils";

   test("your protected feature", async ({ page }) => {
     // Login before testing protected features
     const result = await login(page);
     expect(result.success).toBe(true);

     // Now run your test with authentication in place
     await page.goto("/your-protected-page");
     // ...test assertions...
   });
   ```

6. **Testing with Custom OTP Values**

   ````typescript
   import { test as base } from '@playwright/test';

   // Create a fixture for OTP testing
   const test = base.extend({
     mockOtp: [async ({}, use) => {
       // Set a custom OTP for test
       const originalOtp = process.env['ANT_TEST_OTP'];
       process.env['ANT_TEST_OTP'] = '555555';

       await use(undefined);

       // Restore original
       if (originalOtp) {
         process.env['ANT_TEST_OTP'] = originalOtp;
       } else {
         delete process.env['ANT_TEST_OTP'];
       }
     }, { scope: 'test' }]
   });

   test('login with specific OTP', async ({ page, mockOtp }) => {
     // Test will use the mocked OTP value
     const result = await login(page);
     expect(result.success).toBe(true);
   });
   ```# Node.js Build Process in the Platform Repository
   ````

This document explains the Node.js build process in the platform repository, focusing on the Nix development environment, package management, and build steps.

## Nix Development Shells

The platform repository uses Nix to create reproducible development environments. There are several specialized development shells available:

| Shell    | Command                | Purpose                                                    |
| -------- | ---------------------- | ---------------------------------------------------------- |
| `npm`    | `nix develop .#npm`    | Node.js environment with all dependencies for npm projects |
| `nodejs` | `nix develop .#nodejs` | General Node.js development environment                    |
| `noggin` | `nix develop .#noggin` | Specialized environment for the noggin service             |

### `.#npm` Development Shell

When you run `nix develop .#npm`, you enter a specialized Nix shell that:

1. Includes all runtime dependencies for all JavaScript projects in the repository
2. Sets up Node.js with the correct version
3. Provides access to the `ant-npm-build-deptree` tool for dependency management

This shell is defined in the `flake.nix` file as a custom `devShells.npm` attribute that uses `pkgs.mkShell` to create a development environment with all JavaScript service dependencies included through `inputsFrom`.

## Build Process

### Initial Setup

When you run:

```bash
nix develop .#npm
```

You enter the NPM development shell. The shell welcome message provides guidance on what to do next.

### Installing Dependencies

```bash
npm ci --ignore-scripts
```

This command:

1. Installs all dependencies according to the `package-lock.json` file
2. Uses the `--ignore-scripts` flag to prevent execution of arbitrary scripts during installation
3. Sets up the node_modules structure for the NPM workspace

### Building Dependencies Tree

```bash
ant-npm-build-deptree noggin
```

The `ant-npm-build-deptree` tool:

1. Analyzes the dependency tree of the specified service (noggin in this case)
2. Builds all dependencies that the service requires
3. Ensures that local workspace dependencies are built in the correct order

### Building the Service

```bash
npm run --workspace gateways/noggin build
```

This command:

1. Runs the build script defined in the service's package.json
2. Compiles TypeScript to JavaScript
3. Copies static files to the dist directory
4. Makes the output scripts executable

## Core Computer Science Concepts

### Build Systems & Compilation Theory

#### Dependency Resolution and Topological Sorting

At the heart of the build process is **dependency resolution**, implementing a classic computer science algorithm:

1. **Directed Acyclic Graph (DAG)**: Dependencies form a DAG where vertices are packages and edges represent "depends on" relationships
2. **Topological Sort**: Packages must be built in an order such that no package is built before its dependencies
3. **Cycle Detection**: The system must detect and prevent circular dependencies, which would make a valid build order impossible

The `ant-npm-build-deptree` tool performs a sophisticated topological sort of the dependency graph to ensure correct build order.

#### Incremental Compilation

Modern build systems optimize performance through **incremental compilation**:

1. **File Timestamps**: Tracking last-modified timestamps to determine which files need recompilation
2. **Dependency Graphs**: Maintaining a graph of file dependencies to recompile only affected files
3. **Caching**: Storing intermediate build artifacts to avoid redundant work

TypeScript's compiler (`tsc`) implements incremental compilation by:

- Using `.tsbuildinfo` files to track file dependencies and timestamps
- Only recompiling files that have changed or are affected by changes
- Providing the `--incremental` flag to enable this optimization

#### Declarative vs. Imperative Build Systems

The platform repository uses both approaches:

1. **Declarative (Nix)**: Describes the desired end state rather than the steps to achieve it

   - Focuses on what should be built, not how
   - Enables reproducibility and parallel builds
   - Improves caching through functional purity

2. **Imperative (npm scripts)**: Describes a sequence of steps to achieve the desired result
   - More familiar to developers
   - Simpler for straightforward tasks
   - Used for local development workflows

### Environment Isolation & Containerization

#### Process Isolation

The build system leverages several levels of **process isolation**:

1. **Operating System Processes**: Each build command runs in an isolated OS process

   - Memory isolation prevents corruption between builds
   - Process limits contain resource usage
   - Signal handling allows for graceful termination

2. **Nix Environment Isolation**: Nix shells create isolated environments

   - Prevent "works on my machine" problems
   - Ensure reproducible builds across systems
   - Control exact versions of all tools and dependencies

3. **Container Isolation**: Docker provides runtime isolation
   - Namespace isolation for files, network, and processes
   - Resource constraints through cgroups
   - Image layering for efficient storage and distribution

This multi-level isolation implements the computer science principle of **least privilege**, granting each process only the permissions and resources it needs.

#### Virtual Memory and Resource Management

The build system interacts with OS-level **resource management**:

1. **Virtual Memory**: The TypeScript compiler and Node.js use virtual memory for:

   - Memory mapping of source files
   - Heap management for AST construction
   - Garbage collection during compilation

2. **Process Scheduling**: Build parallelism depends on OS scheduling

   - CPU core allocation affects build speed
   - I/O waits during file operations
   - Process priorities may impact performance

3. **File System Operations**: Build performance is heavily I/O-bound:
   - File caching at the OS level affects read performance
   - Write coalescing improves output performance
   - Directory traversal algorithms impact file discovery

### Programming Language Implementation

#### Just-In-Time vs. Ahead-of-Time Compilation

The TypeScript toolchain demonstrates the tradeoffs between compilation approaches:

1. **Just-In-Time (JIT) Compilation**:

   - Used by `tsx` during development
   - Compiles TypeScript code on-demand
   - Provides faster iteration cycles
   - Incurs runtime overhead for compilation

2. **Ahead-of-Time (AOT) Compilation**:

   - Used by `tsc` for production builds
   - Compiles all code before execution
   - Results in faster runtime performance
   - Enables more thorough optimization

3. **Transpilation**:
   - TypeScript to JavaScript conversion is a form of source-to-source translation
   - Type erasure removes static types while preserving runtime behavior
   - Downleveling converts newer JavaScript features to older syntax for compatibility

#### Type Systems and Static Analysis

TypeScript's type system implements several formal type theory concepts:

1. **Structural Type System**:

   - Types are compatible based on structure, not nominal declarations
   - Duck typing: "If it walks like a duck and quacks like a duck..."
   - Enables flexible code reuse without explicit inheritance

2. **Flow Analysis**:

   - Control flow-based type narrowing
   - Null and undefined checks
   - Discriminated unions for type refinement

3. **Generic Types**:
   - Parametric polymorphism allows type-safe code reuse
   - Type inference reduces annotation burden
   - Constraint-based type checking ensures valid operations

The TypeScript compiler (`tsc`) performs sophisticated static analysis, including:

- Data flow analysis to track variable values
- Control flow analysis to determine reachable code
- Call graph analysis to verify function usage

### Module Systems & Dynamic Loading

#### JavaScript Module Resolution

The platform's build system handles several module systems:

1. **CommonJS**:

   - Synchronous `require()` function
   - Module.exports for exposing functionality
   - Node.js's original module system

2. **ES Modules**:

   - Static import/export declarations
   - Asynchronous loading with dynamic import()
   - Tree-shakable by design

3. **TypeScript Module Resolution**:
   - Path mapping through tsconfig.json
   - Module resolution strategies (Node, Classic)
   - Declaration files (.d.ts) for type definitions

#### Module Bundling Algorithms

Modern JavaScript bundlers implement several algorithmic concepts:

1. **Tree Shaking**:

   - Dead code elimination based on static analysis
   - Reachability analysis from entry points
   - Preservation of side effects

2. **Code Splitting**:

   - Graph partitioning to separate code into chunks
   - Dynamic loading boundaries
   - Shared chunk identification

3. **Bundle Optimization**:
   - Constant folding and propagation
   - Function inlining
   - Scope hoisting

### Concurrency & Parallelism

#### Build Parallelization

The build system leverages parallelism at multiple levels:

1. **Task-Level Parallelism**:

   - Independent tasks run concurrently
   - Dependency graph determines execution order
   - Work stealing balances load across cores

2. **File-Level Parallelism**:

   - TypeScript compiler processes multiple files concurrently
   - I/O operations are interleaved
   - Thread pool manages worker threads

3. **Service-Level Parallelism**:
   - Microservices compile independently
   - Docker builds can run in parallel
   - Test suites execute concurrently

#### Event Loop Architecture

Node.js uses an event-driven, non-blocking I/O model:

1. **Single-Threaded Event Loop**:

   - Processes events from an event queue
   - Delegates I/O operations to the operating system
   - Executes callbacks when operations complete

2. **Worker Threads**:

   - Parallel JavaScript execution for CPU-bound tasks
   - Shared memory through ArrayBuffers
   - Message passing for communication

3. **Asynchronous Programming Patterns**:
   - Promises for composable asynchronous operations
   - Async/await for sequential-style asynchronous code
   - Event emitters for reactive programming

### Functional Programming Principles

#### Pure Functions and Immutability

The Nix build system embodies functional programming principles:

1. **Pure Functions**:

   - Build steps have no side effects
   - Given the same inputs, they always produce the same outputs
   - Enable reliable caching and parallelization

2. **Immutability**:

   - Build artifacts are immutable once created
   - No modification of existing files, only creation of new ones
   - Prevents race conditions during parallel builds

3. **Referential Transparency**:
   - Build steps can be replaced with their result
   - Hash-based addressing ensures content integrity
   - Cached results are semantically equivalent to rebuilding

#### Lazy Evaluation

Nix uses lazy evaluation to optimize builds:

1. **Deferred Computation**:

   - Only build what's needed
   - Skip unused dependencies
   - Evaluate expressions only when their values are required

2. **Thunks and Promises**:
   - Build steps are represented as deferred computations
   - Results are memoized after first evaluation
   - Dependencies are tracked for minimal rebuilds

## IDE and Terminal Integration

### Cursor IDE Integration

#### Terminal Shell in Cursor IDE

When using Cursor IDE's integrated terminal:

1. **Environment Inheritance**: Cursor's terminal inherits environment variables from the parent process that launched Cursor.

   - If you launched Cursor from a shell with an active Nix environment, that environment will be inherited
   - If not, you'll need to run `nix develop .#npm` explicitly in the Cursor terminal

2. **Shell Differences**: Cursor's terminal may use a different shell (bash) than your system default (zsh)

   - May require different configuration for shell integration
   - Shell prompt and completions might differ from your customized zsh setup

3. **Working Directory**: The terminal usually opens in the project root, which is convenient for running npm and nix commands

#### Cursor Debugger Integration

Cursor's debugger for Node.js/TypeScript offers advantages over terminal-based development:

1. **Source Mapping**: The debugger automatically maps between TypeScript source and compiled JavaScript

   - No need to manually trace between source and compiled code in the `dist` folder
   - Breakpoints set in `.ts` files work even though the runtime executes `.js` files

2. **Launch Configurations**: You can create custom debug configurations for different services

   - Can specify environment variables
   - Can set arguments and runtime flags
   - Can be shared with team via `.vscode/launch.json`

3. **Runtime vs Build-time**: The debugger runs the actual Node.js process, not the build process
   - For TypeScript projects, you typically need to build first, then debug
   - Or use configurations that run `tsx` directly for development (bypassing the build step)

### Comparison with Ghostty + zsh on macOS

Compared to using Ghostty terminal with zsh on macOS:

1. **Performance**: Ghostty is often faster and more responsive than embedded IDE terminals

   - More efficient rendering
   - Lower latency

2. **Environment Management**: A dedicated terminal session in Ghostty can maintain long-lived Nix environments

   - Less need to frequently re-enter development shells
   - Can maintain multiple environments in different tabs/windows

3. **Shell Configuration**: Your full zsh configuration in Ghostty is available

   - All your aliases, functions, and customizations work as expected
   - Complete shell history

4. **Terminal Features**: Ghostty offers advanced terminal features
   - Better color support
   - Split panes
   - Better font rendering

## TypeScript Build Pipeline

### TypeScript Tools and Their Roles

| Tool      | Purpose              | Usage                                                 |
| --------- | -------------------- | ----------------------------------------------------- |
| `tsc`     | TypeScript compiler  | Used in npm build scripts to compile TS to JS         |
| `tsx`     | TypeScript execution | Runs TS files directly without separate compile step  |
| `tsup`    | TS bundler           | Used for building libraries with dependencies bundled |
| `esbuild` | Fast JS/TS bundler   | Used under the hood by tsx and tsup                   |

### Build Process Flow

```
TypeScript Source (.ts)
       │
       ▼
  Compilation (tsc)
       │
       ▼
JavaScript Output (dist/*.js)
       │
       ▼
   Execution (node)
```

### Development Flow with `tsx`

```
TypeScript Source (.ts)
       │
       ▼
    tsx (runs directly)
```

### Folder Structure

- `src/`: Contains TypeScript source files
- `dist/`: Contains compiled JavaScript files after running `npm run build`
- `node_modules/`: Contains installed dependencies
- `node_modules/.bin/`: Contains executable scripts from dependencies

### When to Use Each Tool

- **`tsc`**: Used in the build process to generate production-ready JavaScript

  - Performs full type-checking
  - Creates `.js` and `.d.ts` files in the `dist` directory
  - Used in CI/CD and production builds

- **`tsx`**: Used during development for quick iteration

  - Runs TypeScript files directly without separate compilation step
  - Faster development cycle (no build step)
  - Used in `npm run start` scripts for local development

- **Node with `dist/`**: Used in production
  - Runs the compiled JavaScript files
  - No TypeScript overhead in production
  - More efficient execution

## Technical Implementation

### NPM Workspace Structure

The platform repository uses NPM workspaces, defined in the root `package.json`:

```json
"workspaces": [
  "experimental/clinical_tool/frontend",
  "gateways/noodle",
  "gateways/noggin",
  "lib/ts/lib-infra",
  "lib/ts/lib-platform",
  "services/*",
  "surfaces/*",
  "gen/ts/*",
  "experimental/*"
]
```

This workspace configuration allows for:

- Sharing dependencies across projects
- Local referencing of internal packages
- Centralized dependency management

### Nix Integration

The repository leverages the `npm2nix` approach for integrating NPM packages with Nix:

1. `npm2nix.nix` contains tooling to transform npm dependencies into Nix derivations
2. `anterior-js-service.nix` defines the structure for JavaScript services, including:
   - Development environment setup
   - Build steps
   - Docker image creation
   - Test configuration

### Docker Build Process

For production deployment, the system creates Docker images with:

1. Layered architecture for efficient caching
2. Minimal runtime dependencies
3. Proper entrypoint configuration with migrate & main scripts
4. Secrets management through SOPS

## Security Considerations

### Supply Chain Security

The build system implements several measures to protect against supply chain attacks:

1. **Deterministic Builds**:

   - Nix ensures reproducible builds with identical outputs
   - Cryptographic verification of input sources
   - Lockfiles pin exact dependency versions

2. **Dependency Auditing**:

   - Regular security scanning of dependencies
   - Explicit allowlisting of trusted dependencies
   - Version pinning to prevent unexpected updates

3. **Isolated Build Environments**:
   - Builds run in isolated contexts
   - Network access is restricted during builds
   - Only specified dependencies are available

### Principle of Least Privilege

The system implements the principle of least privilege through:

1. **Containerization**:

   - Services run with minimal capabilities
   - Resource limits prevent denial-of-service
   - Network policies restrict service communication

2. **Secret Management**:
   - Secrets are encrypted at rest
   - Runtime-only access to decrypted secrets
   - Scope-limited secrets per service

## Performance Optimization

### Build Time Optimization

Several strategies minimize build times:

1. **Caching**:

   - Nix's content-addressable store caches build artifacts
   - Cached results are reused when inputs haven't changed
   - Local and remote caching share results across developers

2. **Parallelization**:

   - Multiple CPU cores utilized concurrently
   - I/O operations overlapped with computation
   - Independent services built in parallel

3. **Incremental Builds**:
   - Only changed files are reprocessed
   - Dependency tracking determines affected modules
   - Build checkpoints prevent redundant work

### Runtime Performance

The TypeScript build process optimizes runtime performance through:

1. **Tree Shaking**:

   - Eliminating dead code
   - Removing development-only features
   - Reducing bundle size

2. **Code Optimization**:

   - Inlining functions for faster execution
   - Constant folding and propagation
   - Eliminating redundant code

3. **Module Chunking**:
   - Breaking code into loadable chunks
   - Lazy loading non-critical functionality
   - Optimizing initial load time

## Common Workflows

1. **Development with IDE Debugger**:

   ```bash
   nix develop .#npm
   npm ci --ignore-scripts
   ant-npm-build-deptree noggin
   # Then use Cursor's debugger to launch with breakpoints
   ```

2. **Development with tsx (fast iteration)**:

   ```bash
   nix develop .#npm
   npm ci --ignore-scripts
   ant-npm-build-deptree noggin
   npm run --workspace gateways/noggin start
   ```

3. **Production Build**:

   ```bash
   nix develop .#npm
   npm ci --ignore-scripts
   ant-npm-build-deptree noggin
   npm run --workspace gateways/noggin build
   node gateways/noggin/dist/index.js
   ```

4. **Building all services**:

   ```bash
   nix develop
   ant build dev
   ```

5. **Running services locally**:

   ```bash
   nix develop
   ant up
   ```

6. **Auto-rebuild on changes**:
   ```bash
   nix develop
   ant watch
   ```

## Software Design Patterns

The build system and TypeScript codebase implement several design patterns:

### Factory Pattern

Nix's derivation system implements the Factory pattern:

1. **Derivation as Factories**:

   - Create complex objects (build artifacts) through a common interface
   - Hide instantiation logic behind a consistent API
   - Allow specialization through parameters

2. **Builder Pattern Variation**:
   - Step-by-step construction of complex objects
   - Clear separation of construction from representation
   - Fluent interface for configuration

### Observer Pattern

The development workflow uses the Observer pattern:

1. **File Watchers**:

   - Files are observable subjects
   - Build tools subscribe to file changes
   - Change events trigger rebuilds

2. **Event-based Architecture**:
   - Components communicate through events
   - Loose coupling between producers and consumers
   - Asynchronous processing of build steps

### Decorator Pattern

TypeScript's type system enables compile-time decorators:

1. **Method and Class Decoration**:

   - Extend behavior without modifying source
   - Apply cross-cutting concerns consistently
   - Layer functionality through composition

2. **Higher-order Functions**:
   - Functions that take or return other functions
   - Wrap behavior with additional functionality
   - Enable composition of behaviors

## Conclusion

The platform repository's Node.js build process combines NPM workspaces with Nix for a reproducible development and build environment. This approach provides developer-friendly workflows while ensuring consistent builds across all environments.

Whether using Cursor IDE with its integrated debugger or a standalone terminal like Ghostty with zsh, the underlying build processes remain the same, though the developer experience differs in terms of convenience features, performance, and workflow integration.

The build system demonstrates sophisticated computer science principles, from dependency resolution and compilation theory to concurrency models and functional programming concepts. Understanding these underlying concepts helps developers leverage the system effectively and contribute to its evolution.# Flopilot Chat and PDF Architecture

This document outlines the architecture and flow of Flopilot's chat functionality and PDF handling.

## Class Diagram

```mermaid
classDiagram
    class FloPilotRoot {
        -useState() input
        +render()
    }

    class PromptBox {
        -message: string
        -setMessage: function
        +handleAttachClick()
        +render()
    }

    class PromptController {
        +handleSend(message, stemUid, pdfUids)
    }

    class PdfProcessController {
        -useState() processedFiles
        +processPdf(files)
        +handlePdfChange(e)
        +handleRemovePdf()
        +isPdfProcessingMap
    }

    class Chat {
        +render()
    }

    class ChatController {
        -messages: array
        -input: string
        +handleInputChange(e)
        +handleSubmit(e)
        +append(message)
    }

    class ChatInput {
        -input: string
        -handleInputChange: function
        -handleSubmit: function
        +render()
    }

    class Message {
        -content: string
        -messageId: string
        +render()
    }

    class UserMessage {
        -id: string
        -content: string
        +render()
    }

    class AttachedFile {
        -file: File
        -isFileProcessing: boolean
        -onRemove: function
        +render()
    }

    class DataSourcesLabel {
        -hasAttachments: boolean
        +render()
    }

    class PdfStatus {
        -status: "processing" | "ready" | "error"
        -message?: string
    }

    class PdfProcess {
        -stemUid: string
        -pdfUids: string[]
    }

    class PdfExtracts {
        -result: object
        -extracts: array
    }

    FloPilotRoot --> PromptBox: contains
    FloPilotRoot --> SavedPromptsSection: contains
    PromptBox --> PromptController: uses
    PromptBox --> PdfProcessController: uses
    PromptBox --> AttachedFile: renders
    PromptBox --> DataSourcesLabel: renders
    Chat --> ChatController: uses
    Chat --> Message: renders
    Chat --> UserMessage: renders
    Chat --> ChatInput: contains
    PdfProcessController --> PdfStatus: uses
    PdfProcessController --> PdfProcess: uses
    ChatController --> Message: manages
```

## Sequence Diagram: Chat Flow

```mermaid
sequenceDiagram
    participant User
    participant PromptBox
    participant PdfProcessController
    participant Router
    participant ChatComponent
    participant ChatController
    participant Backend

    User->>PromptBox: Enters message
    User->>PromptBox: Clicks send
    PromptBox->>PromptController: handleSend(message, stemUid, pdfUids)
    PromptController->>Router: navigate to chat route with params
    Router->>ChatComponent: Render Chat component
    ChatComponent->>ChatController: Initialize with stemUid, pdfUids

    Note over ChatController: useEffect checks for initialMessage
    ChatController->>ChatController: append user message to messages
    ChatController->>Backend: POST request to /chat
    Backend-->>ChatController: Stream response
    ChatController->>ChatComponent: Update messages state
    ChatComponent->>Message: Render assistant message
    Message-->>User: Display message

    User->>ChatInput: Type response
    User->>ChatInput: Submit message
    ChatInput->>ChatController: handleSubmit(message)
    ChatController->>ChatController: append user message
    ChatController->>Backend: POST request to /chat
    Backend-->>ChatController: Stream response
    ChatController->>ChatComponent: Update messages state
    ChatComponent->>Message: Render assistant message
    Message-->>User: Display message
```

## Sequence Diagram: PDF Upload and Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant PromptBox
    participant PdfProcessController
    participant Backend
    participant Router

    User->>PromptBox: Click Attach button
    PromptBox->>User: Show file picker dialog
    User->>PromptBox: Select PDF file(s)
    PromptBox->>PdfProcessController: handlePdfChange(files)
    PdfProcessController->>PdfProcessController: Create FormData
    PdfProcessController->>Backend: POST /pdfs/process-pdfs
    Backend-->>PdfProcessController: Return stemUid and pdfUids
    PdfProcessController->>PdfProcessController: setProcessedFiles(result)

    loop For each PDF
        PdfProcessController->>Backend: GET /pdfs/pdf-status/{stemUid}/{pdfUid}
        Backend-->>PdfProcessController: Return status (processing/ready/error)

        alt If status is "processing"
            PdfProcessController->>PdfProcessController: Set refetch interval (5s)
            PdfProcessController->>PdfProcessController: Update isPdfProcessingMap
            PdfProcessController->>PromptBox: Re-render with processing status
        else If status is "ready"
            PdfProcessController->>PdfProcessController: Update isPdfProcessingMap
            PdfProcessController->>PromptBox: Re-render with ready status
        else If status is "error"
            PdfProcessController->>PdfProcessController: Throw error
            PdfProcessController->>PromptBox: Display error message
        end
    end

    User->>PromptBox: Enter message and click send
    PromptBox->>Router: Navigate to chat with stemUid and pdfUids
```

## Key Technical Details

### PDF Processing Flow

- PDF files are uploaded via FormData to `/pdfs/process-pdfs` endpoint
- Backend assigns a unique stemUid and pdfUids for each uploaded file
- Frontend polls the PDF status endpoint to check processing progress
- Processing states are tracked in isPdfProcessingMap for UI feedback

### Chat Flow with PDFs

- When navigating to the chat route, stemUid and pdfUids are passed as URL parameters
- ChatController initializes with these parameters and includes them in API calls
- The backend uses the PDFs as context for the chat conversation
- User messages and AI responses use the same chat component structure

### Data Architecture

- Each chat session has a unique stemUid identifier
- PDF documents have unique pdfUids linked to the session stemUid
- Messages are streamed from the backend using AI SDK's useChat hook
- PDF processing status is polled at 5-second intervals until complete

### Technologies Used

- **TanStack Router** for routing
- **React Query** for server state management and API calls
- **AI SDK** for chat functionality
- **Zod** for schema validation

### Architectural Patterns

- **Model-View-Controller (MVC)** pattern
- Clear separation between UI components (view), business logic (controllers), and data models
- Reactive UI updates based on state changes
- Polling pattern for long-running processes

# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This repository contains multiple projects spanning different languages and technologies:

- **Brrr**: High-performance workflow scheduling system (Python)
- **Zellij**: Terminal workspace/multiplexer (Rust)
- **Obsidian-Vimrc-Support**: Plugin for Obsidian.md
- **Platform/Anterior**: Healthcare platform monorepo (multi-language)
- **Nixos-config**: NixOS configuration system
- Several smaller utilities and tools

## Common Commands

### Nix-based Development (Platform/Anterior)

```bash
# Enter development shell
nix develop

# Build Docker images for development
ant build dev

# Run services locally
ant up

# Run with auto-rebuild on file changes
ant watch

# Populate services with test data
ant populate

# Run all tests and sanity checks
nix flake check -L

# Load testing
ant load-test [API_IP_ADDRESS]
```

### Python Projects (Brrr, workflows)

```bash
# Install dependencies
uv pip install -e .

# Run tests
pytest
pytest -m 'not dependencies'  # Skip tests requiring external dependencies

# Run specific test
pytest tests/test_file.py::test_function -v

# Brrr demo
nix run github:nobssoftware/brrr#demo
curl 'http://localhost:8333/hello?greetee=John'
```

### Rust Projects (Zellij)

```bash
# Run debug build
cargo xtask run

# Run all tests
cargo xtask test

# Build release version
cargo build --release
```

### JavaScript/TypeScript Projects

```bash
# Install dependencies
npm install

# Run linting
npm run lint

# Run tests
npm run test

# Build
npm run build
```

## Architecture

### Brrr

- Queue & database-agnostic workflow scheduling system
- Minimal core with extension points for different backends
- Supports Redis and SQS as queues, DynamoDB as database
- Designed for horizontal scaling

### Platform/Anterior

- Microservices architecture with Docker containerization
- Services communicate via HTTP/REST APIs with OpenAPI specs
- Prefect for workflow orchestration
- Nix for reproducible builds
- Frontend surfaces: Cortex (React/TypeScript)

### Zellij

- Terminal multiplexer with workspace focus
- Plugin system supporting WebAssembly
- Multi-pane layout system with floating windows

## Development Workflows

### Environment Setup

1. Install Nix package manager
2. Run `nix develop` to enter development shell
3. Configure editor with EditorConfig support
4. For secrets, use SOPS with correct key access

### Secret Management

- Secrets stored in `secrets.json` and `secrets.prod.json`
- Use 1Password for secure storage
- Never commit unencrypted secrets

### Running Services

1. Run `ant up` to start all services
2. Use `ant watch` for development with auto-rebuild
3. Access specific service documentation in their respective README files

### Testing

Follow testing protocols specific to each project:

- Python: pytest with appropriate markers
- JavaScript: Jest/Vitest
- Rust: Cargo test suite

## Additional Resources

- Development guides in `/development-guides/` directory
- Architecture documentation in `/docs/` directory
- Service-specific README files in each service directory
# Modern Conventional Commits Workflow (2025)

This document provides a comprehensive guide to using the conventional commits workflow in your projects. This implementation includes fuzzy filtering, commitlint validation, and AI-assisted commit messages.

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Tools and Integrations](#tools-and-integrations)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)

## Overview

The Conventional Commits workflow implemented in this dotfiles repository provides:

- **Commitizen integration** with fuzzy filtering using `cz-git`
- **Commitlint validation** for enforcing conventional commit format
- **Husky hooks** for running pre-commit and commit-msg validations
- **AI-assisted commit messages** using Claude to analyze changes and generate commits
- **Fuzzy git commands** for interactive staging and committing
- **Shell aliases** for streamlined workflow

## Installation

All configuration is already set up in your dotfiles. The main setup script automatically configures the conventional commits workflow.

If you need to manually initialize or reinitialize the tooling:

```bash
# Initialize conventional commits tooling
cd ~/dotfiles/config/git/commitizen
npm install

# Run the setup script
node scripts/setup-hooks.js
```

Alternatively, use the helper function:

```bash
# Run the helper function from anywhere
git-setup-conv
```

This will:
1. Install required dependencies (commitizen, cz-git, commitlint, husky)
2. Set up git hooks for commit validation
3. Configure git to use the hooks

To source the shell aliases immediately:

```bash
source ~/dotfiles/config/zsh/git-aliases.zsh
```

## Usage

### Basic Workflow

```bash
# 1. Make changes to your code
# 2. Stage files using fuzzy add
gfa
# 3. Commit using commitizen with fuzzy filtering
gcz
# 4. Push your changes
git push
```

### Available Commands

| Command | Description |
|---------|-------------|
| `git-cz` or `gcz` | Interactive commitizen prompt with fuzzy filtering |
| `gczf` | Commitizen without hook validation (for WIP commits) |
| `gfa` | Fuzzy-find and stage files for commit |
| `gfc` | Fuzzy-find, stage, and commit in one command |
| `gaic` | Generate commit message using AI |
| `gclint` | Run commitlint on last commit |
| `git-setup-conv` | Re-initialize conventional commits setup |
| `git-conv-help` | Show quick reference for conventional commits |

### AI-Assisted Commits

The `gaic` command will:
1. Analyze your staged changes
2. Generate a conventional commit message that describes the changes
3. Offer to use the generated message or modify it

```bash
# Stage changes first
git add .
# Use AI to generate commit message
gaic
```

## Configuration

The conventional commits setup uses the following configuration files:

- **`~/dotfiles/config/git/commitizen/.czrc`**: Commitizen configuration
- **`~/dotfiles/config/git/commitizen/commitlint.config.js`**: Commitlint rules
- **`~/dotfiles/config/git/gitconfig`**: Git aliases and core settings
- **`~/dotfiles/config/git/gitmessage`**: Commit message template
- **`~/dotfiles/config/zsh/git-aliases.zsh`**: Shell aliases and functions

### Customizing Commit Types

The default configuration includes standard conventional commit types plus a few custom ones:

- Standard: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, `build`, `ci`, `chore`, `revert`
- Custom: `wip` (work in progress), `deps` (dependency updates)

To add custom types, edit the following files:
- `commitlint.config.js`: Add to the `type-enum` rule
- `.czrc`: Add to the `types` object

### Customizing Commit Scopes

Allowed scopes are defined in the `commitlint.config.js` file under the `scope-enum` rule.

## Tools and Integrations

This setup integrates the following tools:

- **[Commitizen](https://github.com/commitizen/cz-cli)**: Interactive commit message wizard
- **[cz-git](https://github.com/Zhengqbbb/cz-git)**: Commitizen adapter with fuzzy filtering
- **[Commitlint](https://github.com/conventional-changelog/commitlint)**: Validate commit messages
- **[Husky](https://github.com/typicode/husky)**: Git hooks to validate commits
- **[Claude AI](https://claude.ai)**: AI-assisted commit message generation

## Best Practices

- **Use scopes consistently**: Choose meaningful scopes that map to areas of your codebase
- **Keep the subject line concise**: Aim for 50 characters or less
- **Use imperative mood**: Write "add feature" not "added feature"
- **Include a body for complex changes**: Explain why the change was made
- **Reference issues in footer**: Use "Fixes #123" or "Related to #456"
- **Mark breaking changes**: Use `!` suffix or `BREAKING CHANGE:` in the body

## Troubleshooting

### Commitizen not found

If you get `command not found: commitizen` or similar errors:

```bash
# Reinstall dependencies
cd ~/dotfiles/config/git/commitizen
npm install

# Make sure global node_modules bin is in your PATH
export PATH="$PATH:$(npm config get prefix)/bin"
```

### Commit validation failing

If your commits are being rejected:

```bash
# Check the commitlint rules
cat ~/dotfiles/config/git/commitizen/commitlint.config.js

# Test a commit message manually
echo "feat: my change" | npx --no-install commitlint

# Bypass validation for temporary commits
git commit --no-verify -m "WIP: temporary commit"
# Or use the alias
gczf
```

### Husky hooks not running

If hooks aren't executing:

```bash
# Check hooks path setting
git config --get core.hooksPath

# Run setup script again
cd ~/dotfiles/config/git/commitizen
node scripts/setup-hooks.js
```
# Modern Conventional Commits Workflow (2025)

This document provides a comprehensive guide to using the conventional commits workflow in your projects. This implementation includes fuzzy filtering, commitlint validation, and AI-assisted commit messages.

## Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Tools and Integrations](#tools-and-integrations)
- [Best Practices](#best-practices)
- [Troubleshooting](#troubleshooting)

## Overview

The Conventional Commits workflow implemented in this dotfiles repository provides:

- **Commitizen integration** with fuzzy filtering using `cz-git`
- **Commitlint validation** for enforcing conventional commit format
- **Husky hooks** for running pre-commit and commit-msg validations
- **AI-assisted commit messages** using Claude to analyze changes and generate commits
- **Fuzzy git commands** for interactive staging and committing
- **Shell aliases** for streamlined workflow

## Installation

All configuration is already set up in your dotfiles. The main setup script automatically configures the conventional commits workflow.

If you need to manually initialize or reinitialize the tooling:

```bash
# Initialize conventional commits tooling
cd ~/dotfiles/config/git/commitizen
npm install

# Run the setup script
node scripts/setup-hooks.js
```

Alternatively, use the helper function:

```bash
# Run the helper function from anywhere
git-setup-conv
```

This will:
1. Install required dependencies (commitizen, cz-git, commitlint, husky)
2. Set up git hooks for commit validation
3. Configure git to use the hooks

To source the shell aliases immediately:

```bash
source ~/dotfiles/config/zsh/git-aliases.zsh
```

## Usage

### Basic Workflow

```bash
# 1. Make changes to your code
# 2. Stage files using fuzzy add
gfa
# 3. Commit using commitizen with fuzzy filtering
gcz
# 4. Push your changes
git push
```

### Available Commands

| Command | Description |
|---------|-------------|
| `git-cz` or `gcz` | Interactive commitizen prompt with fuzzy filtering |
| `gczf` | Commitizen without hook validation (for WIP commits) |
| `gfa` | Fuzzy-find and stage files for commit |
| `gfc` | Fuzzy-find, stage, and commit in one command |
| `gaic` | Generate commit message using AI |
| `gclint` | Run commitlint on last commit |
| `git-setup-conv` | Re-initialize conventional commits setup |
| `git-conv-help` | Show quick reference for conventional commits |

### AI-Assisted Commits

The `gaic` command will:
1. Analyze your staged changes
2. Generate a conventional commit message that describes the changes
3. Offer to use the generated message or modify it

```bash
# Stage changes first
git add .
# Use AI to generate commit message
gaic
```

## Configuration

The conventional commits setup uses the following configuration files:

- **`~/dotfiles/config/git/commitizen/.czrc`**: Commitizen configuration
- **`~/dotfiles/config/git/commitizen/commitlint.config.js`**: Commitlint rules
- **`~/dotfiles/config/git/gitconfig`**: Git aliases and core settings
- **`~/dotfiles/config/git/gitmessage`**: Commit message template
- **`~/dotfiles/config/zsh/git-aliases.zsh`**: Shell aliases and functions

### Customizing Commit Types

The default configuration includes standard conventional commit types plus a few custom ones:

- Standard: `feat`, `fix`, `docs`, `style`, `refactor`, `perf`, `test`, `build`, `ci`, `chore`, `revert`
- Custom: `wip` (work in progress), `deps` (dependency updates)

To add custom types, edit the following files:
- `commitlint.config.js`: Add to the `type-enum` rule
- `.czrc`: Add to the `types` object

### Customizing Commit Scopes

Allowed scopes are defined in the `commitlint.config.js` file under the `scope-enum` rule.

## Tools and Integrations

This setup integrates the following tools:

- **[Commitizen](https://github.com/commitizen/cz-cli)**: Interactive commit message wizard
- **[cz-git](https://github.com/Zhengqbbb/cz-git)**: Commitizen adapter with fuzzy filtering
- **[Commitlint](https://github.com/conventional-changelog/commitlint)**: Validate commit messages
- **[Husky](https://github.com/typicode/husky)**: Git hooks to validate commits
- **[Claude AI](https://claude.ai)**: AI-assisted commit message generation

## Best Practices

- **Use scopes consistently**: Choose meaningful scopes that map to areas of your codebase
- **Keep the subject line concise**: Aim for 50 characters or less
- **Use imperative mood**: Write "add feature" not "added feature"
- **Include a body for complex changes**: Explain why the change was made
- **Reference issues in footer**: Use "Fixes #123" or "Related to #456"
- **Mark breaking changes**: Use `!` suffix or `BREAKING CHANGE:` in the body

## Troubleshooting

### Commitizen not found

If you get `command not found: commitizen` or similar errors:

```bash
# Reinstall dependencies
cd ~/dotfiles/config/git/commitizen
npm install

# Make sure global node_modules bin is in your PATH
export PATH="$PATH:$(npm config get prefix)/bin"
```

### Commit validation failing

If your commits are being rejected:

```bash
# Check the commitlint rules
cat ~/dotfiles/config/git/commitizen/commitlint.config.js

# Test a commit message manually
echo "feat: my change" | npx --no-install commitlint

# Bypass validation for temporary commits
git commit --no-verify -m "WIP: temporary commit"
# Or use the alias
gczf
```

### Husky hooks not running

If hooks aren't executing:

```bash
# Check hooks path setting
git config --get core.hooksPath

# Run setup script again
cd ~/dotfiles/config/git/commitizen
node scripts/setup-hooks.js
```
# Understanding @igg.py Functionality
_Exported on 5/21/2025 at 02:15:24 EDT from Cursor (0.50.5)_

---

**User**

explain @igg.py and how it works

---

**Cursor**

`workflows/src/guideline_intelligence/models/igg.py` defines a small, strongly-typed domain-model for representing a clinical guideline as a traversable graph that an LLM (or any other reasoning engine) can follow while auditing how a nurse would review a patient’s record.  Everything is expressed with Pydantic models, so you automatically get validation, (de-)serialization and JSON-schema generation “for free”.

Key pieces, in the order they appear:

1. OutcomeSentiment (StrEnum)
   • Encodes whether reaching a particular outcome is good, bad or neutral for the patient (`POSITIVE`, `NEGATIVE`, `NEUTRAL`).
   • StrEnum makes the value itself equal the name string (useful when emitting JSON).

2. GuidelineOutcome (BaseModel)
   • A leaf-node payload that says “if you stop here, this is the conclusion”.
   • Fields
     – description Natural-language explanation of the outcome.
     – tagline Short snake-case identifier (≤ 3 words) for UI chips or anchors.
     – sentiment One of the `OutcomeSentiment` values, used by the front-end to color-code the result (green/red/grey, etc.).

3. GuidelineCriterion (BaseModel)
   • The atomic decision checkpoints inside a guideline (e.g. “patient is ≥ 18 years old”).
   • Fields
     – id Human-readable, unique, lower-snake-case or section number (“10.1”, “contrast_ct_required_iii”, …).
     – description What is being checked, paraphrased or copied verbatim.
     – conditions List of individual, clearly separable clinical conditions that must all be evaluated.
     – citation `GuidelineCitation` (defined elsewhere) pin-points exactly where in the source document this criterion comes from.

4. NodeType (StrEnum) and BaseNode
   • Small discriminated-union helper: each node knows if it’s the graph root, another criterion, or a leaf outcome.

5. OutcomeNode(CriterionNode)
   • Two concrete node types inherit from `BaseNode`.
   • `OutcomeNode` carries a `GuidelineOutcome`.
   • `CriterionNode` carries a back-reference (`guideline_criterion_id`) to one of the criterion objects defined earlier.

6. Edge (BaseModel)
   • Directed connection in the graph (`origin → destination`).
   • origin: always a `CriterionNode` (you decide something and then move on).
   • destination: either the next `CriterionNode` or an `OutcomeNode` (terminates the path).
   • logic: free-text statement the LLM will evaluate to decide whether the edge should be followed (“ALL conditions met”, “IF age ≥ 65”, etc.).
   • description: very short label for UI (“Patient is male”).

7. IGG (Intelligent Guideline Graph)
   • The top-level object that groups everything.
   • `guideline_title` Human title (“NICE NG123 Headache guideline”).
   • `criteria` Flat list of all `GuidelineCriterion` objects (source-of-truth for each decision point).
   • `nodes` Flat list of all graph nodes (criteria and outcomes).
   • `edges` Flat list of all directed edges that join the nodes together.

   ➜ Design note in the docstring: a node does not store its outgoing edges directly. That keeps the node schema simple and avoids forcing the LLM to “know every child up-front”. Instead, you compute or fetch the relevant edges dynamically during traversal.

How it is typically used end-to-end
------------------------------------

1. A guideline author (or another model) populates the `criteria`, `nodes`, and `edges` lists to encode the paper/pdf guideline as a graph.
2. At runtime an inference agent (e.g. OpenAI function-calling or your bespoke engine) starts at the root node, presents the criterion to the LLM along with the patient’s EMR facts, and asks “Does the logic on edge X apply?”.
3. Based on the answer it chooses the correct `destination` node, continuing until it lands on an `OutcomeNode`.
4. The outcome (with sentiment) is shown to the nurse, and every edge decision forms an auditable trail.

Why the structure matters
-------------------------

• **Pydantic validation** guarantees every IGG you load is well-formed.
• **Separation of concerns** lets multiple teams work on content (criteria), logic (edges) and UI (outcome tags) independently.
• **Auditability**: every edge stores the literal logic string plus the originating citation, which is crucial for regulatory/clinical review.
• **LLM friendliness**: plain-text fields and StrEnums serialize naturally to JSON for function calls or prompts.

In short, `igg.py` is the typed contract for building, storing and traversing an intelligent, transparent representation of complex clinical guidelines.
# Frontend-Backend Integration in Anterior Platform

This document explains how the Vibes frontend applications integrate with the Platform backend through the Noggin service, with a focus on app deployment, serving, and API integration.

## Overview

The Anterior platform uses a sophisticated architecture to deploy and serve frontend applications:

1. **Vibes Repository**: Contains React/TypeScript frontend applications
2. **Platform Repository**: Houses the backend services including Noggin gateway
3. **S3 Storage**: Stores the built frontend assets
4. **Noggin Gateway**: Serves the frontend apps and proxies API requests

The key components involved in this integration are:

- `@platform/lib/ts/lib-platform/src/app.ts` - Core application framework
- `@platform/gateways/noggin/` - API gateway and frontend server
- `@vibes/apps/*/deploy-local.sh` - Frontend deployment scripts

## Application Framework (`lib-platform/src/app.ts`)

The `app.ts` file defines the core `App` class that powers all backend services in the platform. Key features:

```typescript
export class App<TServiceMap extends ServiceMap, TAppContext extends object> {
  public readonly hono: Hono<{ Variables: TAppContext & BaseContext }>;
  public readonly logger: Logger;
  public readonly trpc = initTRPC...

  constructor(
    public readonly baseCtx: TAppContext,
    public readonly options: AppOptions,
    public readonly auth?: (req: Request) => Promise<{ userUid: string; enterpriseUid: string } | Response>
  ) { ... }
}
```

Notable components:

1. **Hono Backend**: The app uses [Hono](https://hono.dev/) for HTTP routing, middleware, and handling
2. **Endpoint Definition**: Type-safe API endpoint definitions with schemas
3. **Auth Middleware**: JWT authentication support
4. **Error Handling**: Standardized error responses
5. **CORS Support**: Cross-origin resource sharing configuration

### Static App Serving

At the end of `app.ts` (lines 871-1018), there's a `createStaticApp` function that is crucial for serving frontend applications:

```typescript
export function createStaticApp({
  prefix,
  blobs,
  index = "index.html",
  env,
  shouldGenerateCSP = false,
}: {
  prefix: Path | "";
  blobs: ReadOnlyBlobStore;
  index?: string;
  env?: Record<string, string>;
  shouldGenerateCSP?: boolean;
}): Hono { ... }
```

This function:

1. Creates a Hono app that serves static files from a blob store (S3)
2. Handles path prefixing to match the route structure
3. Applies proper MIME types based on file extensions
4. Supports environment variable substitution in served files
5. Applies Content Security Policy headers if needed

## Noggin Gateway Integration

The Noggin gateway (`@platform/gateways/noggin/`) serves two purposes:

1. Provide API endpoints for backend services
2. Serve frontend applications from S3

### Backend App Structure

Each subdirectory in Noggin contains its own `app.ts` file that defines a specific service:

- `src/pdfs/app.ts` - PDF processing API
- `src/auth/app.ts` - Authentication services
- `src/notes/app.ts` - Notes API
- `src/flonotes/app.ts` - FloNotes app-specific API
- `src/flopilot/app.ts` - FloPilot app-specific API

These services expose endpoints the frontend apps consume:

```typescript
export function createPdfApp(ctx: PdfAppContext, options: AppOptions): PdfApp {
  const app: PdfApp = new App(ctx, options);

  app.endpoint({
    method: "POST",
    body: processStemPdfSchema,
    response: stemPdfUidResponseSchema,
    route: "/process-pdf",
    async handler(ctx) { /* Implementation */ }
  });

  // Other endpoints like /pdf-status/:stemUid/:pdfUid and /pdf-extracts/:stemUid/:pdfUid

  return app;
}
```

### Directory-Based App Loading

Noggin dynamically loads all `app.ts` files from its subdirectories using `importAndMountNestedDirectoryApps` (lines 741-797). This creates an API structure mirroring the directory structure:

```typescript
export async function importAndMountNestedDirectoryApps(
  logger: Logger,
  app: Hono,
  basePath: string,
  baseRoute: string = ""
) {
  // Recursively find all app.ts files in subdirectories
  // and mount them at their corresponding paths
}
```

## Frontend Deployment Flow

The frontend applications in the Vibes repository are deployed using shell scripts:

### Build Process

Each app in the `vibes/apps/` directory has a `deploy-local.sh` script that:

1. Builds the React/TypeScript app with Vite
2. Configures environmental variables
3. Uploads the built assets to S3
4. Makes them available through Noggin

Key deployment variables:

```bash
# From flonotes/deploy-local.sh or flopilot/deploy-local.sh
APP_PREFIX="vibes/flonotes"  # or "vibes/flopilot"
BASE_PATH="/flonotes"        # or "/flopilot"
BUCKET_NAME="local-bucket"
LOCALSTACK_ENDPOINT="http://localhost:4566"
VITE_NOGGIN_HOST="http://localhost:20701"
```

The deployment scripts use AWS CLI to upload the built assets to S3:

```bash
aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
  --endpoint-url "$LOCALSTACK_ENDPOINT" \
  --no-verify-ssl \
  --delete
```

### S3 Directory Structure

The frontend assets are stored in S3 with the following structure:

- `vibes/flonotes/` - FloNotes app assets
- `vibes/flopilot/` - FloPilot app assets
- `vibes/atlas/` - Atlas app assets

## URL Routing

When a user visits a Noggin URL like `http://localhost:20701/flonotes`, the following happens:

1. Request arrives at Noggin gateway
2. Noggin's static app server handles the route
3. The path is used to determine which app to serve (`/flonotes` → `vibes/flonotes/`)
4. The static app fetches the appropriate files from S3
5. The frontend app is served with the correct content types

## API Communication

Frontend apps communicate with backend services through the Noggin gateway:

1. Frontend makes API requests to paths like `/pdfs/process-pdf`
2. Noggin routes these to the appropriate backend services
3. JWT authentication cookies are automatically included in requests
4. Responses are returned to the frontend

For example, the FloNotes frontend uses API functions like:

```typescript
// From flonotes/src/services/noggin-api.ts
export async function processPdf(
  fileData: number[],
  fileName: string,
  documentType: "clinical" | "criteria",
  stemUid?: string,
): Promise<ProcessPdfResponse> {
  // Builds and sends a multipart/form-data request to Noggin
  const response = await apiRequest(API_ENDPOINTS.processPdf, {
    method: "POST",
    body: formData,
  });

  // Return parsed response
  return data;
}
```

## Environment Variables

Frontend apps receive environment variables through two mechanisms:

1. **Build-time**: Variables defined in deployment scripts
   ```bash
   export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
   export BASE_PATH="${BASE_PATH}"
   ```

2. **Run-time**: Text substitution in static files
   ```typescript
   // Text substitution in served files
   if (env) {
     for (const [key, value] of Object.entries(env)) {
       text = text.replaceAll(`$${key}`, value);
     }
   }
   ```

## Deployment Workflow

The complete workflow for deploying and running a Vibes frontend app:

1. Start the platform services
   ```bash
   cd platform
   nix develop
   ant-all-services
   ```

2. Build and deploy the frontend app
   ```bash
   cd vibes/apps/flonotes
   npm i
   ./deploy-local.sh
   ```

3. Access the app through Noggin
   ```
   http://localhost:20701/flonotes
   ```

## Security Model

The security model for frontend-backend integration includes:

1. **Authentication**: JWT tokens stored as HTTP-only cookies
2. **Authorization**: Enterprise and user-level permissions
3. **CORS**: Controlled cross-origin access
4. **CSP**: Content Security Policy for frontend applications

## Development Workflow

For faster development:

1. The Noggin service can be stopped in the process-compose screen
2. Environment variables can be copied from the logs
3. Noggin can be run separately with:
   ```bash
   cd gateways/noggin
   source .env && npm run start | npx pino-pretty
   ```

## Common Issues and Troubleshooting

1. **Connection Failures**: Ensure LocalStack is running
   ```bash
   # Check LocalStack availability
   curl -s --head "http://localhost:4566"
   ```

2. **Authentication Issues**: Verify cookies are being set properly
   ```javascript
   // Check for auth_indicator cookie
   document.cookie.includes("auth_indicator=true")
   ```

3. **Missing Assets**: Check S3 bucket contents
   ```bash
   aws s3 ls s3://local-bucket/vibes/flonotes/ --endpoint-url "http://localhost:4566" --no-verify-ssl
   ```

4. **API Errors**: Check Noggin logs for detailed error information
# Nix Setup in Anterior Platform

This document provides a detailed explanation of the Nix setup in the Anterior Platform, covering the flake.nix file, related Nix files, build processes, and how they connect to the codebase.

## Table of Contents

1. [Overview](#overview)
2. [Key Components](#key-components)
3. [Flake Inputs](#flake-inputs)
4. [Flake Modules](#flake-modules)
5. [Packages and Services](#packages-and-services)
6. [Build Process](#build-process)
7. [Developer Environment](#developer-environment)
8. [Darwin Configuration](#darwin-configuration)
9. [Testing and CI](#testing-and-ci)
10. [Common Commands](#common-commands)

## Overview

The Anterior Platform uses Nix for:

1. **Reproducible Builds**: Ensuring consistent build environments across developers and CI
2. **Service Definition**: Declaratively specifying services and their dependencies
3. **Docker Image Generation**: Creating consistent Docker images for deployment
4. **Development Environments**: Providing standardized developer shells
5. **macOS Configuration**: Managing macOS developer machines via nix-darwin

The central file is `flake.nix`, which defines all inputs, outputs, and modules. The system supports multiple languages (Go, Python, TypeScript/JavaScript) and uses specialized builders for each.

## Key Components

The Nix setup consists of several interconnected components:

### 1. Main Files

- **flake.nix**: The entry point defining inputs, outputs, and system configurations
- **nix/anterior-services.nix**: Defines the service framework for building services
- **nix/anterior-python.nix**: Python-specific configuration using uv2nix
- **nix/anterior-services-process-compose.nix**: Configuration for local service orchestration
- **nix/ant-darwin-module.nix**: macOS configuration for developer machines

### 2. Service Frameworks

The platform uses specialized builders for different language ecosystems:

- **Go**: Using `buildGoModule` with vendor hashes
- **Python**: Using `uv2nix` to build from uv.lock files
- **JavaScript**: Using custom npm2nix implementation

### 3. Build Artifacts

- **Docker Images**: For each service in production and test variants
- **Development Shells**: Language-specific development environments
- **Executables**: Direct access to service binaries

## Flake Inputs

The `flake.nix` file begins by declaring all external dependencies:

```nix
inputs = {
  nixpkgs.url = "github:nixos/nixpkgs/nixpkgs-unstable";

  # Terraform providers with specific version pin
  nixpkgs-terraform-providers-bin = {
    url = "github:cohelm/nixpkgs-terraform-providers-bin/18e042762d12cc1e9452e426d4cb1097e7686fb2";
    inputs.nixpkgs.follows = "nixpkgs";
  };

  # Pinned to specific version for WeasyPrint compatibility
  nixpkgs_weasyprint_62_3.url = "github:nixos/nixpkgs/29c58cf78dc44b209e40e037b280cbbfc015ef39";

  # Nix registry and module system
  flakeregistry = {
    url = "github:NixOS/flake-registry";
    flake = false;
  };
  flake-parts.url = "flake-parts";

  # Development shell and service management
  devshell.url = "github:numtide/devshell";
  process-compose-flake.url = "github:Platonic-Systems/process-compose-flake";
  services-flake.url = "github:juspay/services-flake";

  # Python packaging
  pyproject-nix = {
    url = "github:cohelm/pyproject.nix/fake-sdk-compat";
    inputs.nixpkgs.follows = "nixpkgs";
  };
  uv2nix = {
    url = "github:pyproject-nix/uv2nix";
    inputs.pyproject-nix.follows = "pyproject-nix";
    inputs.nixpkgs.follows = "nixpkgs";
  };
  pyproject-build-systems = {
    url = "github:pyproject-nix/build-system-pkgs";
    inputs.pyproject-nix.follows = "pyproject-nix";
    inputs.uv2nix.follows = "uv2nix";
    inputs.nixpkgs.follows = "nixpkgs";
  };

  # macOS configuration
  nix-darwin = {
    url = "nix-darwin";
    inputs.nixpkgs.follows = "nixpkgs";
  };

  # Utilities
  globset = {
    url = "github:pdtpartners/globset";
    inputs.nixpkgs-lib.follows = "nixpkgs";
  };

  # External services
  brrr = {
    url = "github:cohelm/brrr/v0.1.0";
    inputs.pyproject-build-systems.follows = "pyproject-build-systems";
    inputs.pyproject-nix.follows = "pyproject-nix";
    inputs.nixpkgs.follows = "nixpkgs";
    inputs.uv2nix.follows = "uv2nix";
  };

  # CI tools
  ant-ci-tools.url = "github:cohelm/brrr/ci-tools";
}
```

Key insights:

1. **Input Pinning**: Most inputs are pinned to specific versions or commits
2. **Input Redirection**: Many inputs use `follows` to ensure consistent nixpkgs versions
3. **Custom Forks**: Several inputs are forks maintained by Cohelm (e.g., `cohelm/pyproject.nix`)

## Flake Modules

The flake uses a modular approach with `flake-parts`, which allows splitting configurations into separate modules:

```nix
flake-parts.lib.mkFlake { inherit inputs; } {
  systems = [ "aarch64-linux" "aarch64-darwin" "x86_64-linux" ];
  imports = [
    ./nix/anterior-python.nix
    ./nix/anterior-libpy.nix
    ./nix/anterior-services.nix
    ./nix/anterior-standalones.nix
    inputs.ant-ci-tools.flakeModules.checkBuildAll
    inputs.devshell.flakeModule
    inputs.process-compose-flake.flakeModule
    antFlakeAllSystems
    antFlakeLinux
  ];
}
```

Key modules:

1. **anterior-python.nix**: Configures Python environments using uv2nix
2. **anterior-services.nix**: Defines framework for all services
3. **anterior-standalones.nix**: Builds standalone executables
4. **antFlakeAllSystems**: Cross-platform configuration shared across systems
5. **antFlakeLinux**: Linux-specific configuration for Docker image building

## Packages and Services

The service definitions are collected in `config.anterior.services`, organized by language:

```nix
anterior = {
  inherit python;
  services = {
    python.hello-world = {
      inherit python;
      path = ./services/hello-world;
      subdirectory = "services/hello-world";
    };

    go.api = {
      inherit go;
      path = ./services/api;
      subdirectory = "services/api";
      vendorHash = "sha256-8qZAAvhgGHiWkQ6czjNLTtzsz/ftcF/7YqHx6Rps4bg=";
    };

    javascript.noggin = {
      inherit nodejs;
      subdirectory = "gateways/noggin";
    };

    # Other services...
  };
};
```

Each service declaration specifies:
- **Language**: Python, Go, or JavaScript
- **Path**: Location in the filesystem
- **Subdirectory**: Relative path in the repository
- **Language-specific fields**: Such as vendorHash for Go

### How Services are Built

The service definitions are processed by language-specific builders:

1. **Go Services**:
   - Built using `buildGoModule`
   - Vendor dependencies are verified with vendorHash
   - Creates both executables and Docker images

2. **Python Services**:
   - Built using uv2nix from uv.lock files
   - Includes virtualenv with all dependencies
   - Containerized with all runtime dependencies

3. **JavaScript Services**:
   - Built using custom npm2nix implementation
   - Handles workspace dependencies
   - Manages native dependencies and prebuilt binaries

## Build Process

### Docker Image Generation

For each service, multiple Docker images are created:

```nix
docker-image-derivs = lib.flatten (lib.mapAttrsToList (name: service: let
  p = service.outputs.package;
in [
  { inherit name; image = p.docker-prod; }
  { name = name + "-test"; image = p.docker-test; }
]) (s.go // s.python // s.javascript));
```

The system creates helper outputs:

1. **all-docker-images**: A file listing all Docker image outputs
2. **all-docker-tags**: Maps service names to Docker image tags
3. **all-docker-derivations**: Maps image names to their derivation paths

### Building Docker Images

When you run `ant-build-docker`, it:

1. Determines which images need to be built
2. Builds them in parallel using Nix
3. Loads them into the local Docker daemon
4. Tags them with `:latest` for use in docker-compose

```nix
command = let
  linuxEquivalent = "${pkgs.hostPlatform.parsed.cpu.name}-linux";
in ''
  build="$(mktemp)"
  tags="$(mktemp)"
  trap "rm -f $build $tags" EXIT

  nix build --no-link --print-out-paths .#packages.${linuxEquivalent}.all-docker-derivations | xargs -r cat | while read name drv; do
    if ! docker image inspect "$name" &>/dev/null; then
      printf "%s\t%s^out\n" "$name" "$drv" >> "$build"
    fi
    printf "docker tag %q %q_latest\n" "$name" "${name%_*}" >> "$tags"
  done

  <"$build" cut -f 2 | xargs -r nix build --no-link --print-out-paths | xargs -r -n 1 docker load -i
  bash "$tags"
''
```

### Linux Builder for macOS

Since Docker images are Linux-only, macOS developers need a Linux builder to build them. This is configured via nix-darwin:

```nix
nix.linux-builder = {
  config.virtualisation.cores = 6;
  config.virtualisation.memorySize = lib.mkOverride 69 6000;
  config.virtualisation.diskSize = lib.mkForce (100 * 1000);
  maxJobs = 12;
};
```

## Developer Environment

### DevShells

The system provides multiple development shells:

1. **Default Shell**: For repository management and meta-commands
   ```nix
   devshells.default = {
     name = "anterior";
     packages = devPackages { inherit pkgs; };
     motd = ''
       This is a "meta" devshell with commands for managing the
       repository and builds, not so much for actual dev of actual
       features. For an overview of the entire flake, run 'nix flake
       show'.

       Services managed by this flake:
       ${items2mdList (
         (map (go: "${go} (go)") (builtins.attrNames config.anterior.services.go))
         ++ (map (py: "${py} (python)") (builtins.attrNames config.anterior.services.python))
         ++ (map (js: "${js} (js)") (builtins.attrNames config.anterior.services.javascript))
       )}

       Extra tools on the path:
       ${items2mdList (map (p: "${p.pname or p.name} (${p.version})") packages)}
     '';
     # Commands and environment variables...
   }
   ```

2. **npm Shell**: For JavaScript development
   ```nix
   devShells.npm = pkgs.mkShell {
     name = "anterior-npm-devshell";
     inputsFrom = lib.mapAttrsToList (_: v: v.outputs.package) config.anterior.services.javascript;
     # Configuration...
   };
   ```

3. **Service-Specific Shells**: For each individual service
   - `nix develop .#hello-world`
   - `nix develop .#api`
   - etc.

### Common Commands

The default devshell provides useful commands:

```nix
commands = [
  {
    category = "maintenance";
    name = "system-prune";
    help = "Prune build cache for Docker & Nix";
    # Command implementation...
  }
  {
    name = "ant-admin";
    category = "run";
    help = "Anterior admin tool (/admin)";
    # Command implementation...
  }
  {
    category = "build";
    name = "ant-build-docker";
    help = "Build and load Docker images for every Nix service";
    # Command implementation...
  }
  # Other commands...
];
```

## Darwin Configuration

For macOS developers, the flake provides several nix-darwin configurations:

```nix
darwinConfigurations = {
  anterior-base = inputs.nix-darwin.lib.darwinSystem {
    modules = [
      self.darwinModules.anterior-managed-nixdarwin
      self.darwinModules.anterior-fancy
    ];
  };

  anterior-beefy = inputs.nix-darwin.lib.darwinSystem {
    modules = [
      self.darwinModules.anterior-managed-nixdarwin
      self.darwinModules.anterior-fancy
      ({ nix.linux-builder.config.virtualisation.memorySize = lib.mkForce 12000;})
    ];
  };

  anterior-bootstrap = inputs.nix-darwin.lib.darwinSystem {
    modules = [
      self.darwinModules.anterior-managed-nixdarwin
      self.darwinModules.anterior-core
    ];
  };

  anterior-nobuilder = self.darwinConfigurations.anterior-base.extendModules {
    modules = [
      { nix.linux-builder.enable = lib.mkForce false; }
    ];
  };
};
```

Each configuration serves a specific purpose:
- **anterior-base**: Standard development configuration
- **anterior-beefy**: For machines with more memory (prevents OOM issues)
- **anterior-bootstrap**: Minimal configuration to bootstrap the system
- **anterior-nobuilder**: Configuration without Linux builder for troubleshooting

## Testing and CI

The Nix setup includes testing capabilities:

```nix
checks = {
  lint = pkgs.callPackage ./nix/anterior-lint-ci.nix {
    go-services = let
      all = builtins.attrValues config.anterior.services.go;
    in
      lib.remove config.anterior.services.go.admin all;
    py-services = builtins.attrValues config.anterior.services.python;
    inherit (self'.packages) ruff;
  };
};
```

This enables:
- Running `nix flake check` to verify the repository
- Integrating with CI for automated testing
- Linting Go and Python services

## Local Development with Process Compose

For local development, the platform uses process-compose:

```nix
process-compose.ant-all-services = {
  imports = [
    inputs.services-flake.processComposeModules.default
    inputs.brrr.processComposeModules.dynamodb
    self.processComposeModules.gotenberg
    self.processComposeModules.localstack
    self.processComposeModules.prefect
    ./nix/anterior-services-process-compose.nix
  ];
  services.anterior-platform = {
    enable = true;
    serviceDefinitions = config.anterior.services;
  };
};
```

This enables:
1. Running all services locally with `ant-all-services`
2. Including dependencies like DynamoDB, Localstack, and Prefect
3. Integrating with external services

## Common Commands

Here are the most common commands for working with the Nix system:

### Setup

```bash
# Bootstrap a new macOS machine
darwin-rebuild switch --flake .#anterior-bootstrap

# Switch to full configuration
darwin-rebuild switch --flake .#anterior-base

# Set up Nix cache for faster builds
ant-setup-nix-cache
```

### Building and Running

```bash
# Build Docker images for all services
ant-build-docker

# Run all services locally
ant-all-services

# Run a specific service
nix run .#hello-world

# Enter development shell for a service
nix develop .#hello-world
```

### Development

```bash
# Enter default devshell
nix develop

# Enter npm devshell for JavaScript development
nix develop .#npm

# Lint code
ant-lint

# Clean build caches (use sparingly)
system-prune
```

### Testing

```bash
# Run all checks
nix flake check

# Run a specific service test
cd services/hello-world && pytest
```

## Advanced Features

### Environment Variables

The system sets several environment variables:

```nix
env = [
  {
    name = "ANT_ROOT";
    eval = "$(git rev-parse --show-toplevel)";
  }
  {
    name = "UV_PYTHON_DOWNLOADS";
    value = "never";
  }
  {
    name = "UV_NO_SYNC";
    value = "1";
  }
  {
    name = "SOPS_DISABLE_VERSION_CHECK";
    value = "1";
  }
];
```

### Secrets Management

The system integrates with 1Password for secrets:

```nix
command = ''
  set -euo pipefail
  ${lib.getExe self'.packages.ant-check-1password}
  account=${lib.escapeShellArg _1passwordAccount}
  CACHIX_AUTH_TOKEN="$(op read --account "$account" "op://Engineering/cachix/Envvars/CACHIX_AUTH_TOKEN")"
  export CACHIX_AUTH_TOKEN
  CACHIX_SIGNING_KEY="$(op read --account "$account" "op://Engineering/cachix/Envvars/CACHIX_SIGNING_KEY")"
  export CACHIX_SIGNING_KEY
  cachix use anterior-private
'';
```

## Conclusion

The Nix setup in Anterior Platform provides a comprehensive solution for:

1. **Reproducible Builds**: Ensuring consistency across environments
2. **Service Management**: Declaratively defining and building services
3. **Developer Experience**: Providing tailored development environments
4. **macOS Configuration**: Managing developer machines
5. **Local Development**: Running services locally for testing

The system's modular design allows for easy extension and maintenance, while specialized builders for each language ecosystem ensure optimal builds and deployments.
# Prompt Engineering Guide

## Introduction to Prompt Engineering

Prompt engineering is the practice of designing effective inputs for large language models (LLMs) to produce desired outputs. It's an evolving discipline that combines elements of communication, programming, and psychology to reliably guide AI behavior.

## Core Principles for Beginners

### 1. Be Clear and Specific

- **State exactly what you want**: Define the task, expected format, and scope clearly
- **Avoid ambiguity**: Eliminate vague terms that could be interpreted multiple ways
- **Set boundaries**: Specify what the model should NOT do as well as what it should do

### 2. Provide Context

- **Establish background information**: Give relevant context the model needs
- **Define audience**: Specify who the response is for (experts, beginners, etc.)
- **State purpose**: Explain why you need this information and how it will be used

### 3. Use Examples (Few-Shot Learning)

- **Demonstrate desired outputs**: Show 1-3 examples of ideal responses
- **Illustrate edge cases**: Include examples of challenging scenarios
- **Maintain consistency**: Keep a similar structure across all examples

## Maximizing Accuracy

### 1. Knowledge Scaffolding

Break complex tasks into smaller steps:

```
1. First, identify the key facts in this medical case
2. Then, compare these facts to clinical guidelines
3. Finally, generate a recommendation based on this analysis
```

### 2. Chain-of-Thought Prompting

Ask the model to "think step by step" or "reason through this carefully":

```
Analyze this lab result by:
1. Identifying abnormal values
2. Explaining what each abnormality might indicate
3. Suggesting possible clinical implications
Think through each step carefully.
```

### 3. Self-Consistency Checks

Instruct the model to verify its own work:

```
After providing your analysis, review your reasoning for logical errors or unsupported claims.
```

## Improving Consistency

### 1. Use Clear Structural Templates

```
[TASK]: Summarize the following clinical note
[FORMAT]:
- Chief complaint:
- Key findings:
- Assessment:
- Plan:
[REQUIREMENTS]: Keep it under 100 words
[INPUT]: {text}
```

### 2. Control Parameters

- **Temperature**: Lower values (0.1-0.3) for factual, consistent responses
- **Response length**: Specify word/character count for consistent sizing
- **Sampling techniques**: Nucleus or top-k sampling for balanced control

### 3. Provide Evaluation Criteria

```
Your response will be evaluated on:
1. Factual accuracy (all claims must be supported by the document)
2. Completeness (all key information must be included)
3. Clarity (information must be organized logically)
```

## Enhancing Resiliency

### 1. Anticipate Variations

Prepare your prompt to handle different inputs:

```
If the input contains lab values, format them in a table.
If the input contains symptoms, list them in bullet points.
If the input is incomplete, identify what information is missing.
```

### 2. Error Handling

Instruct the model how to respond to ambiguity or insufficient information:

```
If you're uncertain about any aspect of your response, explicitly state your uncertainty.
If critical information is missing, indicate what additional data would be needed.
```

### 3. Guardrails and Constraints

Set clear boundaries for responses:

```
Only include information explicitly stated in the document. Do not introduce external information or assumptions.
```

## Getting Structured Responses

### 1. Define Output Format Explicitly

#### JSON Format

```
Return your analysis as a valid JSON object with the following structure:
{
  "diagnosis": string,
  "confidence": number (1-5),
  "supporting_evidence": string[],
  "alternative_explanations": string[]
}
```

#### XML Format

```
Format your response as XML using the following tags:
<assessment>
  <findings>List key findings here</findings>
  <diagnosis>Primary diagnosis here</diagnosis>
  <plan>Treatment plan here</plan>
</assessment>
```

#### Tabular Format

```
Present the medication information in a table with these columns:
- Medication name
- Dosage
- Frequency
- Purpose
- Side effects
```

### 2. Validation Instructions

```
Ensure your JSON response:
1. Contains all required fields
2. Uses the correct data types for each field
3. Has no extra fields not specified in the schema
```

### 3. Consistency Enforcement

```
Follow these rules consistently:
- Use present tense for all descriptions
- Express all measurements in metric units
- Format all dates as YYYY-MM-DD
```

## Maintaining Formal Structures

### 1. Schema Definitions

Define your structures using familiar formats:

```
Follow this JSON schema:
{
  "type": "object",
  "required": ["patient_id", "vital_signs", "assessment"],
  "properties": {
    "patient_id": { "type": "string" },
    "vital_signs": {
      "type": "object",
      "properties": {
        "temperature": { "type": "number" },
        "heart_rate": { "type": "number" },
        "blood_pressure": { "type": "string" }
      }
    },
    "assessment": { "type": "string" }
  }
}
```

### 2. Enumeration for Fixed Values

```
Use only these values for the 'severity' field:
- "mild"
- "moderate"
- "severe"
- "critical"
```

### 3. Versioning

```
This prompt uses response schema version 2.1.
Changes from v2.0:
- Added 'confidence_score' field
- 'complications' field is now optional
```

## Advanced Techniques

### 1. Role-Based Prompting

Assign a specific role to the model:

```
You are an experienced medical transcriptionist with specialty training in cardiology. Your task is to...
```

### 2. Multi-Step Workflows

Break complex tasks into stages:

```
This is a 3-step process:
STEP 1: Extract all medical terms from the text
STEP 2: Categorize each term as diagnosis, medication, or procedure
STEP 3: Generate a structured summary using these categorized terms
```

### 3. Reflection and Refinement

Have the model improve its own answers:

```
After generating your initial response, critique it for accuracy and completeness. Then provide an improved version addressing any deficiencies.
```

## Common Pitfalls to Avoid

- **Prompts that are too vague**: "Summarize this" vs. "Provide a 3-paragraph summary highlighting key diagnoses"
- **Contradictory instructions**: Asking for both brevity and exhaustive detail
- **Failing to specify constraints**: Not setting clear parameters on response length, format, or scope
- **Overly complex structures**: Creating schemas that are too detailed or nested
- **Ignoring edge cases**: Not considering how the prompt handles unusual inputs

## Testing and Iterating

The most effective prompts are developed through systematic testing:

1. Start with a basic version of your prompt
2. Test with diverse inputs
3. Identify failure modes and inconsistencies
4. Refine the prompt to address these issues
5. Document what works and why
# PostHog Implementation

This repository includes implementations for PostHog analytics in the FloNotes application.

## Key Components

1. **Core Analytics:**
   - Page view tracking
   - Template usage analytics
   - FloNote creation tracking
   - Session recordings

2. **User Feedback:**
   - Helpfulness ratings (thumbs up/down)
   - CSAT survey (5-point satisfaction rating)

## Testing

The `POSTHOG_TEST_PLAN.md` document contains detailed test cases to verify all analytics integrations.

## Setup

### 1. Install Dependencies

```bash
cd vibes/apps/flonotes
npm install
```

### 2. Configure Environment Variables

Create a `.env.local` file in the app directory:

```
VITE_POSTHOG_API_KEY=<your_posthog_api_key>
VITE_POSTHOG_HOST=https://app.posthog.com
VITE_POSTHOG_DEBUG=true  # Set to false in production
```

### 3. Run Local Development Server

```bash
npm run dev
```

## Deployment

The deployment scripts (`deploy-local.sh` and `deploy-aws.sh`) have been updated to include PostHog configuration.

For production:
```bash
./deploy-aws.sh <sso_profile> <bucket_name> <noggin_host_url>
```

For local testing:
```bash
./deploy-local.sh
```

## Documentation

- `POSTHOG.md` - Detailed implementation documentation
- `POSTHOG_TEST_PLAN.md` - Testing guidelines and test cases
- `POSTHOG_IMPLEMENTATION_SUMMARY.md` - Summary of implemented changes
- `posthog_implementation.patch` - Complete patch file with all changes

## Events Tracked

| Event | Description | Key Properties |
|-------|-------------|---------------|
| `$pageview` | Page visits | path, url |
| `template_reuse` | Template selection | template_name, pdf_count |
| `query_run_without_template` | Custom queries | has_pdfs, pdf_count |
| `flonote_created` | FloNote creation | template_name, pdf_count |
| `flonote_pdf_added` | PDF processing | pdf_uid, extract_count |
| `message_feedback` | Helpfulness ratings | message_id, rating |
| `csat_rating` | Satisfaction survey | session_id, rating |

## Components

- `posthog.ts` - Core initialization and tracking
- `helpfulness-rating.tsx` - Thumbs up/down UI
- `csat-survey.tsx` - 5-point rating UI
- `use-editor-callbacks-updated.ts` - Editor with PostHog tracking

## Resources

- [PostHog Documentation](https://posthog.com/docs)
- [PostHog React Integration](https://posthog.com/docs/libraries/react)
- [PostHog Event Tracking](https://posthog.com/docs/product-analytics/events)
- [PostHog Session Recording](https://posthog.com/docs/session-replay)
# Documentation and Reference

This directory contains guides, specifications, and reference materials related to AI prompting and tools.

## Structure

- User guides and tutorials
- Version information
- Style guides and conventions
- Project specifications

## Key Components

- Detailed documentation on AI capabilities
- Guides for implementing AI-powered workflows
- Reference materials for prompt engineering
- Specifications for AI-related projects

## Usage

These resources provide context and guidance for effective AI usage:

1. Consult relevant guides before creating new prompts
2. Reference specifications when developing AI-powered systems
3. Follow established conventions for consistency
4. Use version information to understand capability changes

## Best Practices

- Keep documentation updated as capabilities evolve
- Cross-reference related materials
- Provide concrete examples alongside conceptual explanations
- Document both successful patterns and known limitations
# Platform Code-Generation Overview

> **TL;DR**  `schemas → codegen → gen → lib → app`.
>
> *Schemas are the specs* → *Codegen turns them into language-specific artefacts* → *Generated artefacts live in `gen/`* → *Hand-written helper libraries in `lib/` wrap those artefacts* → Everything above is consumed by services, gateways, workflows, and front-ends.

---

## 1  `schemas/` – Single Source of Truth

| Sub-dir | Kind of schema | Typical toolchain |
|---------|----------------|-------------------|
| `proto/` | Protobuf service & message definitions | **Buf** → Go/Python/TS gRPC stubs |
| `openapi/` | REST/OpenAPI specifications | **openapi-generator** → typed clients |
| `events/` | Event envelopes (e.g. Kafka, SNS) | Registry docs / Avro / JSON-Schema |
| `data/` | Raw data models (JSON-Schema) | Code-model generators |
| `workflows/` | Workflow DSL / state machines | Prefect, Temporal, etc. |

*Never* import from here at runtime – these files are language-agnostic specifications only.

## 2  `codegen/` – All Generators & Templates

The `codegen/` directory contains containerised scripts (usually run via `ant codegen`) that read from `schemas/` and write to `gen/`.

Key folders:

* **`codegen/proto/`** – Runs Buf to emit Go & Python protobuf stubs.
* **`codegen/models/`** – Converts JSON-Schema → Go structs / Pydantic / Zod.
* **`codegen/openapi-client/`** – Generates REST API clients for multiple languages.
* **`codegen/compose/`** – Stamps out many environment-specific Docker Compose files.

These scripts are *build-time only*; nothing inside `codegen/` is imported by application code.

## 3  `gen/` – Generated Artefacts (Checked-in)

`gen/` is the mounted output directory for all generators.  Layout:

```
 gen/
   ├─ go/       # Go structs, clients, mocks …
   ├─ python/   # Pydantic & FastAPI models …
   ├─ ts/       # Zod schemas, TypeScript types …
   ├─ compose/  # Pre-baked docker-compose YAMLs
   └─ docs/     # Reference documentation
```

Rules:

1. Never hand-edit files in `gen/`.
2. Always commit the diff so CI/CD & IDEs don't require local regeneration.

## 4  `lib/{go,ts,python}/` – Hand-Written Shared Libraries

These packages sit *on top* of the raw generated code, providing ergonomics, helpers and business logic.  Examples:

* `lib/ts/lib-platform/…` – HTTP app framework, structured logging helpers, Zod utilities.
* `lib/go/prefect/…` – Convenience wrappers around Prefect-generated types.
* `lib/python/src/lib/…` – Cross-service Python utilities.

Application layers import from **both** `gen/` and `lib/`.

## 5  `compose/` vs `codegen/compose/`

* `codegen/compose/` – Templates + generation logic.
* `compose/` – The frozen YAMLs produced by that logic.

Same pattern: template → generated output.

## 6  Developer Workflow

```text
1. Edit a schema   (schemas/…)
2. Run codegen      ant codegen
3. Commit schema change + generated diff (gen/…)
4. Extend helpers   (lib/…) if needed
5. Use new types    in services, gateways, etc.
```

Keep this pipeline in mind whenever you wonder, "Where does this type/client come from?" – start at `schemas/` and follow the arrows downstream.
# Python Code Generation

## Task

Generate Python code that accomplishes the following:

{{request}}

## Requirements

- Write clean, idiomatic Python code (Python 3.8+)
- Follow PEP 8 style guidelines
- Include type hints (PEP 484)
- Add docstrings in Google or NumPy format
- Handle errors and edge cases appropriately
- Write testable and maintainable code
- Optimize for readability and clarity

## Python-Specific Guidelines

### Type Annotations

- Add type hints for function parameters and return values
- Use typing module's container types (List, Dict, Set, etc.)
- Consider using TypedDict for dictionaries with specific formats
- Use Optional[] for parameters that could be None
- Use Union[] for parameters with multiple possible types
- Use Protocol for structural typing when needed

### Functions and Classes

- Follow single responsibility principle
- Use descriptive function and variable names
- Make effective use of default parameters
- Use dataclasses for data containers when appropriate
- Implement proper magic methods in classes when needed
- Include type hints in class attributes and methods

### Documentation

- Include docstrings for all public modules, classes, and functions
- Use Google-style docstrings with Parameters, Returns, Raises sections
- Add examples for complex functions
- Note any assumptions or limitations

### Error Handling

- Use specific exception types rather than generic Exception
- Create custom exceptions for domain-specific errors
- Use context managers (with statements) for resource handling
- Implement proper try/except/finally blocks
- Add informative error messages

### Python Idioms

- Use list/dict/set comprehensions where appropriate
- Use generator expressions for large data processing
- Use tuple unpacking for multiple returns
- Use the walrus operator (:=) for assignment expressions (Python 3.8+)
- Use f-strings for string formatting
- Use pathlib for file path operations
- Use context managers for resource management

## Output Format

Provide the Python code with type hints and docstrings in a code block with the language tag. Include imports at the top, followed by class/function definitions, then implementation.

```python
# Your implementation will replace this line
```
# TypeScript Code Generation

## Task

Generate TypeScript code that accomplishes the following:

{{request}}

## Requirements

- Write idiomatic, modern TypeScript (ES2020+)
- Include proper TypeScript types and interfaces
- Follow TypeScript best practices
- Include appropriate error handling
- Add JSDoc comments for public functions and classes
- Follow consistent naming conventions
- Optimize for readability and maintainability

## TypeScript-Specific Guidelines

### Types and Interfaces

- Use explicit typing (avoid `any` type when possible)
- Prefer interfaces for object shapes and API contracts
- Use type aliases for unions, intersections, and complex types
- Use generics when creating reusable components
- Consider using utility types (Partial, Pick, Omit, etc.) where appropriate
- Use readonly properties when applicable

### Naming Conventions

- Use PascalCase for interfaces, types, classes, and enums
- Use camelCase for variables, functions, and methods
- Use ALL_CAPS for constants and static readonly properties
- Prefix interfaces for objects with 'I' only when necessary for clarity
- Prefix type parameters with 'T' (e.g., `<TItem>`)

### Functions and Methods

- Use arrow functions for callbacks and anonymous functions
- Use function declarations for named functions
- Specify return types explicitly
- Use parameter destructuring for objects
- Provide default parameters when applicable

### Error Handling

- Use typed error objects or custom error classes
- Implement proper try/catch/finally blocks
- Consider using Result/Either patterns for complex error cases
- Provide meaningful error messages

### Modern Features

- Use optional chaining (`?.`) and nullish coalescing (`??`)
- Use template literals for string interpolation
- Use object/array destructuring
- Use async/await for asynchronous code
- Use Map and Set collections when appropriate

### Code Organization

- Group related functions and types together
- Use namespaces or modules to organize code
- Separate interface declarations from implementations
- Export types and interfaces that will be used externally

## Output Format

Provide the TypeScript code with appropriate type definitions in a code block with the language tag. Include imports at the top, followed by type/interface definitions, then implementation.

```typescript
// Your implementation will replace this line
```
# Noggin Gateway & Frontend-Backend Integration Guide

This document provides a comprehensive overview of the Noggin gateway and how it integrates frontend applications with backend services in the Anterior platform.

## Architecture Overview

The Anterior platform uses a sophisticated architecture with:

1. **Backend Services** (in `platform` repo) - Microservices handling business logic
2. **Frontend Applications** (in `vibes` repo) - React/TypeScript user interfaces
3. **Noggin Gateway** (`platform/gateways/noggin`) - Central API gateway and static app server
4. **S3 Storage** - Stores frontend assets and user files (like PDFs)

This architecture enables:
- Clean separation of concerns
- Consistent API patterns
- Centralized authentication
- Reliable static app serving
- Enterprise-level isolation

## Core Components

### 1. App Framework (`lib-platform/src/app.ts`)

The foundation of all backend services is the `App` class in `lib-platform/src/app.ts`:

```typescript
export class App<TServiceMap extends ServiceMap, TAppContext extends object> {
  public readonly hono: Hono<{ Variables: TAppContext & BaseContext }>;
  public readonly logger: Logger;
  public readonly trpc = initTRPC...

  constructor(
    public readonly baseCtx: TAppContext,
    public readonly options: AppOptions,
    public readonly auth?: (req: Request) => Promise<{ userUid: string; enterpriseUid: string } | Response>
  ) { ... }
}
```

Key features:
- Built on [Hono](https://hono.dev/) for HTTP routing and handling
- Type-safe API endpoint definitions with schemas
- JWT authentication middleware
- Standardized error handling
- CORS support

### 2. Static App Serving

The `createStaticApp` function in `app.ts` enables serving frontend applications:

```typescript
export function createStaticApp({
  prefix,
  blobs,
  index = "index.html",
  env,
  shouldGenerateCSP = false,
}: {
  prefix: Path | "";
  blobs: ReadOnlyBlobStore;
  index?: string;
  env?: Record<string, string>;
  shouldGenerateCSP?: boolean;
}): Hono { ... }
```

This function:
1. Creates a Hono app that serves static files from S3
2. Handles path prefixing for route matching
3. Sets appropriate MIME types for files
4. Performs environment variable substitution
5. Applies Content Security Policy headers

### 3. Noggin Gateway Structure

The Noggin gateway serves two primary purposes:
1. Provide API endpoints for backend services
2. Serve frontend applications from S3

Each subdirectory contains app-specific endpoints:
- `src/pdfs/app.ts` - PDF processing API
- `src/auth/app.ts` - Authentication services
- `src/notes/app.ts` - Notes API
- App-specific APIs: `src/flonotes/app.ts`, `src/flopilot/app.ts`

Apps are dynamically loaded using `importAndMountNestedDirectoryApps`.

## PDF Processing Architecture

### PDF API Endpoints

The Noggin gateway provides these key PDF endpoints:

| Endpoint | Method | Purpose | Parameters/Body | Response |
|----------|--------|---------|-----------------|----------|
| `/process-pdf` | POST | Upload and process a PDF | `file` (PDF blob), optional `stemUid` | `{ stemUid: string, pdfUid: string }` |
| `/pdf-status/:stemUid/:pdfUid` | GET | Check processing status | URL params: stemUid, pdfUid | `{ status: "processing" \| "ready" \| "error", message?: string }` |
| `/pdf-extracts/:stemUid/:pdfUid` | GET | Retrieve extracted data | URL params: stemUid, pdfUid | JSON structure of extracted content |

### PDF Processing Flow

1. **Upload**: Frontend uploads PDF to Noggin
2. **Initial Storage**: Noggin stores raw PDF in S3
3. **Processing**: Noggin schedules an asynchronous workflow
4. **Extraction**: Workflow processes PDF and stores JSON extracts
5. **Status Check**: Frontend polls status endpoint until processing completes
6. **Retrieval**: Frontend fetches and displays extracted content

### Storage Structure

PDFs are stored with this S3 path structure:
- Raw PDFs: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
- Extracted data: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json`

The "stem" concept represents a logical grouping of related documents, enabling contextual relationships.

## Frontend Deployment & Integration

### Deployment Flow

Frontend applications are deployed using the following process:

1. **Build**: The app is built using Vite
   ```bash
   npm run build
   ```

2. **Configure**: Environment variables are set
   ```bash
   export VITE_NOGGIN_HOST="http://localhost:20701"
   ```

3. **Upload**: Assets are uploaded to S3
   ```bash
   aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
     --endpoint-url "$LOCALSTACK_ENDPOINT" \
     --delete
   ```

4. **Access**: Apps are accessed through Noggin
   ```
   http://localhost:20701/flonotes
   ```

### S3 Directory Structure

Frontend assets are stored in S3 with this structure:
- `vibes/flonotes/` - FloNotes app assets
- `vibes/flopilot/` - FloPilot app assets
- `vibes/atlas/` - Atlas app assets

### API Communication

Frontend apps make API calls through Noggin:

```typescript
// Example from flonotes
export async function processPdf(
  fileData: number[],
  fileName: string,
  documentType: "clinical" | "criteria",
  stemUid?: string,
): Promise<ProcessPdfResponse> {
  // Builds and sends a multipart/form-data request to Noggin
  const response = await apiRequest(API_ENDPOINTS.processPdf, {
    method: "POST",
    body: formData,
  });

  return data;
}
```

## Frontend Application Examples

### 1. FloNotes App (`vibes/apps/flonotes`)

FloNotes provides advanced PDF viewing and annotation capabilities for clinical documentation.

**Key Components**:

1. **API Integration** (`src/services/noggin-api.ts`):
   - Complete client implementation for Noggin PDF endpoints
   - Handles authentication, error management, and retries
   - Implements status polling with exponential backoff

2. **PDF Processing** (`src/hooks/use-process-pdf.ts`):
   - Custom React hook that manages PDF processing state
   - Tracks upload progress, processing status, and results

3. **PDF Viewer** (`src/components/shell/pdf/`):
   - Rich PDF viewing UI with page navigation
   - Highlight and annotation capabilities
   - Citation linking between PDF content and notes

### 2. FloPilot App (`vibes/apps/flopilot`)

FloPilot uses PDFs as context for AI-assisted interactions and chat functionality.

**Key Components**:

1. **Controller Pattern** (`src/controller/use-pdf-process-controller.ts`):
   - State management for PDF processing
   - Integration with chat context

2. **PDF Models** (`src/controller/models/`):
   - `pdf-extracts.ts` - Types for extracted PDF content
   - `pdf-status.ts` - Status response handling
   - `process-pdf.ts` - PDF upload request models

3. **Chat Integration**:
   - Uses processed PDFs as context for AI-assisted chats
   - Links relevant PDF content to chat messages

## Security Model

The Anterior platform implements robust security:

1. **Authentication**:
   - JWT tokens stored in HTTP-only cookies
   - Enterprise-specific isolation of resources
   - Role-based access controls

2. **Data Protection**:
   - Resources are stored in enterprise-specific S3 paths
   - All API calls require valid authentication
   - Processing results are isolated by enterprise

3. **Network Security**:
   - Controlled CORS for cross-origin requests
   - Content Security Policy for frontend applications
   - HTTPS enforcement in production

## Development Workflow

### Local Development Setup

1. Start the platform services:
   ```bash
   cd platform
   nix develop
   ant-all-services
   ```

2. Build and deploy a frontend app:
   ```bash
   cd vibes/apps/flonotes
   npm i
   ./deploy-local.sh
   ```

3. Access the app through Noggin:
   ```
   http://localhost:20701/flonotes
   ```

### Advanced Development

For faster development:
1. Stop Noggin in the process-compose screen
2. Copy environment variables from logs
3. Run Noggin separately:
   ```bash
   cd gateways/noggin
   source .env && npm run start | npx pino-pretty
   ```

## Integration Instructions

To integrate a new frontend app with Noggin:

### 1. Create API Client

```typescript
// Example minimal API client
export async function processPdf(fileData: number[], fileName: string): Promise<{stemUid: string, pdfUid: string}> {
  const formData = new FormData();
  formData.append("file", new Blob([new Uint8Array(fileData)]), fileName);

  const response = await fetch(`${NOGGIN_HOST}/pdfs/process-pdf`, {
    method: "POST",
    body: formData,
    credentials: "include" // For auth cookies
  });

  if (!response.ok) {
    throw new Error(`PDF processing failed: ${response.status}`);
  }

  return await response.json();
}
```

### 2. Implement Status Polling

```typescript
export async function waitForProcessing(stemUid: string, pdfUid: string, maxRetries = 100): Promise<void> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await fetch(`${NOGGIN_HOST}/pdfs/pdf-status/${stemUid}/${pdfUid}`, {
      credentials: "include"
    });

    if (!response.ok) {
      throw new Error(`Status check failed: ${response.status}`);
    }

    const status = await response.json();

    if (status.status === "ready") {
      return;
    }

    if (status.status === "error") {
      throw new Error(`Processing error: ${status.message}`);
    }

    // Wait before next check (with exponential backoff)
    await new Promise(resolve => setTimeout(resolve,
      Math.min(5000 * Math.pow(1.5, attempt), 30000)));
  }

  throw new Error("Processing timed out");
}
```

### 3. Create Deployment Script

```bash
#!/bin/bash
set -euo pipefail

# App configuration
APP_PREFIX="vibes/your-app"
BASE_PATH="/your-app"
BUCKET_NAME="local-bucket"
LOCALSTACK_ENDPOINT="http://localhost:4566"
VITE_NOGGIN_HOST="http://localhost:20701"

# Build frontend with environment variables
export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
export BASE_PATH="${BASE_PATH}"
npm run build

# Deploy to S3
aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
  --endpoint-url "$LOCALSTACK_ENDPOINT" \
  --no-verify-ssl \
  --delete

echo "Deployed app to $VITE_NOGGIN_HOST$BASE_PATH"
```

## Troubleshooting

### Common Issues

1. **PDF Upload Fails**:
   - Check authentication status (JWT token)
   - Verify file size within limits
   - Ensure file is valid PDF format

2. **Processing Never Completes**:
   - Check workflow service logs for errors
   - Verify S3 permissions are correct
   - Check for resource constraints in processing service

3. **Static Assets Not Loading**:
   - Verify S3 upload was successful
   - Check for path/prefix configuration mismatches
   - Confirm Noggin is running and can access S3

4. **API Errors**:
   - Check Noggin logs for detailed error information
   - Verify authentication cookies are present
   - Check network requests in browser developer tools

### Debugging Steps

1. **Connection Verification**:
   ```bash
   # Check LocalStack availability
   curl -s --head "http://localhost:4566"
   ```

2. **Authentication Check**:
   ```javascript
   // Check for auth_indicator cookie
   document.cookie.includes("auth_indicator=true")
   ```

3. **S3 Content Verification**:
   ```bash
   aws s3 ls s3://local-bucket/vibes/flonotes/ \
     --endpoint-url "http://localhost:4566" --no-verify-ssl
   ```

4. **API Call Testing**:
   ```bash
   curl -v -H "Cookie: auth_token=YOUR_JWT_TOKEN" \
     "http://localhost:20701/auth/me"
   ```

## Extending the System

To add new functionality to the Noggin gateway:

1. **New API Endpoint**:
   - Create a new app.ts file in the appropriate directory
   - Define endpoints using the App framework
   - Add necessary validation schemas

2. **New Frontend App**:
   - Build the frontend app with Vite
   - Create deployment script
   - Deploy to S3 with the correct prefix

3. **New PDF Processing Capabilities**:
   - Add new processing flows in platform workflows
   - Extend the JSON schema for new data types
   - Update frontend components to display new data
# Noggin Gateway & Frontend-Backend Integration

This document provides a comprehensive overview of the Noggin gateway and how it integrates frontend applications with backend services in the Anterior platform.

## Architecture Overview

The Anterior platform uses a sophisticated architecture with:

1. **Backend Services** (in `platform` repo) - Microservices handling business logic
2. **Frontend Applications** (in `vibes` repo) - React/TypeScript user interfaces
3. **Noggin Gateway** (`platform/gateways/noggin`) - Central API gateway and static app server
4. **S3 Storage** - Stores frontend assets and user files (like PDFs)

This architecture enables:
- Clean separation of concerns
- Consistent API patterns
- Centralized authentication
- Reliable static app serving
- Enterprise-level isolation

## Core Components

### 1. App Framework (`lib-platform/src/app.ts`)

The foundation of all backend services is the `App` class in `lib-platform/src/app.ts`:

```typescript
export class App<TServiceMap extends ServiceMap, TAppContext extends object> {
  public readonly hono: Hono<{ Variables: TAppContext & BaseContext }>;
  public readonly logger: Logger;
  public readonly trpc = initTRPC...

  constructor(
    public readonly baseCtx: TAppContext,
    public readonly options: AppOptions,
    public readonly auth?: (req: Request) => Promise<{ userUid: string; enterpriseUid: string } | Response>
  ) { ... }
}
```

Key features:
- Built on [Hono](https://hono.dev/) for HTTP routing and handling
- Type-safe API endpoint definitions with schemas
- JWT authentication middleware
- Standardized error handling
- CORS support

### 2. Static App Serving

The `createStaticApp` function in `app.ts` enables serving frontend applications:

```typescript
export function createStaticApp({
  prefix,
  blobs,
  index = "index.html",
  env,
  shouldGenerateCSP = false,
}: {
  prefix: Path | "";
  blobs: ReadOnlyBlobStore;
  index?: string;
  env?: Record<string, string>;
  shouldGenerateCSP?: boolean;
}): Hono { ... }
```

This function:
1. Creates a Hono app that serves static files from S3
2. Handles path prefixing for route matching
3. Sets appropriate MIME types for files
4. Performs environment variable substitution
5. Applies Content Security Policy headers

### 3. Noggin Gateway Structure

The Noggin gateway serves two primary purposes:
1. Provide API endpoints for backend services
2. Serve frontend applications from S3

Each subdirectory contains app-specific endpoints:
- `src/pdfs/app.ts` - PDF processing API
- `src/auth/app.ts` - Authentication services
- `src/notes/app.ts` - Notes API
- App-specific APIs: `src/flonotes/app.ts`, `src/flopilot/app.ts`

Apps are dynamically loaded using `importAndMountNestedDirectoryApps`.

## PDF Processing Architecture

### PDF API Endpoints

The Noggin gateway provides these key PDF endpoints:

| Endpoint | Method | Purpose | Parameters/Body | Response |
|----------|--------|---------|-----------------|----------|
| `/process-pdf` | POST | Upload and process a PDF | `file` (PDF blob), optional `stemUid` | `{ stemUid: string, pdfUid: string }` |
| `/pdf-status/:stemUid/:pdfUid` | GET | Check processing status | URL params: stemUid, pdfUid | `{ status: "processing" \| "ready" \| "error", message?: string }` |
| `/pdf-extracts/:stemUid/:pdfUid` | GET | Retrieve extracted data | URL params: stemUid, pdfUid | JSON structure of extracted content |

### PDF Processing Flow

1. **Upload**: Frontend uploads PDF to Noggin
2. **Initial Storage**: Noggin stores raw PDF in S3
3. **Processing**: Noggin schedules an asynchronous workflow
4. **Extraction**: Workflow processes PDF and stores JSON extracts
5. **Status Check**: Frontend polls status endpoint until processing completes
6. **Retrieval**: Frontend fetches and displays extracted content

### Storage Structure

PDFs are stored with this S3 path structure:
- Raw PDFs: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
- Extracted data: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json`

The "stem" concept represents a logical grouping of related documents, enabling contextual relationships.

## Frontend Deployment & Integration

### Deployment Flow

Frontend applications are deployed using the following process:

1. **Build**: The app is built using Vite
   ```bash
   npm run build
   ```

2. **Configure**: Environment variables are set
   ```bash
   export VITE_NOGGIN_HOST="http://localhost:20701"
   ```

3. **Upload**: Assets are uploaded to S3
   ```bash
   aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
     --endpoint-url "$LOCALSTACK_ENDPOINT" \
     --delete
   ```

4. **Access**: Apps are accessed through Noggin
   ```
   http://localhost:20701/flonotes
   ```

### S3 Directory Structure

Frontend assets are stored in S3 with this structure:
- `vibes/flonotes/` - FloNotes app assets
- `vibes/flopilot/` - FloPilot app assets
- `vibes/atlas/` - Atlas app assets

### API Communication

Frontend apps make API calls through Noggin:

```typescript
// Example from flonotes
export async function processPdf(
  fileData: number[],
  fileName: string,
  documentType: "clinical" | "criteria",
  stemUid?: string,
): Promise<ProcessPdfResponse> {
  // Builds and sends a multipart/form-data request to Noggin
  const response = await apiRequest(API_ENDPOINTS.processPdf, {
    method: "POST",
    body: formData,
  });

  return data;
}
```

## Frontend Application Examples

### 1. FloNotes App (`vibes/apps/flonotes`)

FloNotes provides advanced PDF viewing and annotation capabilities for clinical documentation.

**Key Components**:

1. **API Integration** (`src/services/noggin-api.ts`):
   - Complete client implementation for Noggin PDF endpoints
   - Handles authentication, error management, and retries
   - Implements status polling with exponential backoff

2. **PDF Processing** (`src/hooks/use-process-pdf.ts`):
   - Custom React hook that manages PDF processing state
   - Tracks upload progress, processing status, and results

3. **PDF Viewer** (`src/components/shell/pdf/`):
   - Rich PDF viewing UI with page navigation
   - Highlight and annotation capabilities
   - Citation linking between PDF content and notes

### 2. FloPilot App (`vibes/apps/flopilot`)

FloPilot uses PDFs as context for AI-assisted interactions and chat functionality.

**Key Components**:

1. **Controller Pattern** (`src/controller/use-pdf-process-controller.ts`):
   - State management for PDF processing
   - Integration with chat context

2. **PDF Models** (`src/controller/models/`):
   - `pdf-extracts.ts` - Types for extracted PDF content
   - `pdf-status.ts` - Status response handling
   - `process-pdf.ts` - PDF upload request models

3. **Chat Integration**:
   - Uses processed PDFs as context for AI-assisted chats
   - Links relevant PDF content to chat messages

## Security Model

The Anterior platform implements robust security:

1. **Authentication**:
   - JWT tokens stored in HTTP-only cookies
   - Enterprise-specific isolation of resources
   - Role-based access controls

2. **Data Protection**:
   - Resources are stored in enterprise-specific S3 paths
   - All API calls require valid authentication
   - Processing results are isolated by enterprise

3. **Network Security**:
   - Controlled CORS for cross-origin requests
   - Content Security Policy for frontend applications
   - HTTPS enforcement in production

## Development Workflow

### Local Development Setup

1. Start the platform services:
   ```bash
   cd platform
   nix develop
   ant-all-services
   ```

2. Build and deploy a frontend app:
   ```bash
   cd vibes/apps/flonotes
   npm i
   ./deploy-local.sh
   ```

3. Access the app through Noggin:
   ```
   http://localhost:20701/flonotes
   ```

### Advanced Development

For faster development:
1. Stop Noggin in the process-compose screen
2. Copy environment variables from logs
3. Run Noggin separately:
   ```bash
   cd gateways/noggin
   source .env && npm run start | npx pino-pretty
   ```

## Integration Instructions

To integrate a new frontend app with Noggin:

### 1. Create API Client

```typescript
// Example minimal API client
export async function processPdf(fileData: number[], fileName: string): Promise<{stemUid: string, pdfUid: string}> {
  const formData = new FormData();
  formData.append("file", new Blob([new Uint8Array(fileData)]), fileName);

  const response = await fetch(`${NOGGIN_HOST}/pdfs/process-pdf`, {
    method: "POST",
    body: formData,
    credentials: "include" // For auth cookies
  });

  if (!response.ok) {
    throw new Error(`PDF processing failed: ${response.status}`);
  }

  return await response.json();
}
```

### 2. Implement Status Polling

```typescript
export async function waitForProcessing(stemUid: string, pdfUid: string, maxRetries = 100): Promise<void> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await fetch(`${NOGGIN_HOST}/pdfs/pdf-status/${stemUid}/${pdfUid}`, {
      credentials: "include"
    });

    if (!response.ok) {
      throw new Error(`Status check failed: ${response.status}`);
    }

    const status = await response.json();

    if (status.status === "ready") {
      return;
    }

    if (status.status === "error") {
      throw new Error(`Processing error: ${status.message}`);
    }

    // Wait before next check (with exponential backoff)
    await new Promise(resolve => setTimeout(resolve,
      Math.min(5000 * Math.pow(1.5, attempt), 30000)));
  }

  throw new Error("Processing timed out");
}
```

### 3. Create Deployment Script

```bash
#!/bin/bash
set -euo pipefail

# App configuration
APP_PREFIX="vibes/your-app"
BASE_PATH="/your-app"
BUCKET_NAME="local-bucket"
LOCALSTACK_ENDPOINT="http://localhost:4566"
VITE_NOGGIN_HOST="http://localhost:20701"

# Build frontend with environment variables
export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
export BASE_PATH="${BASE_PATH}"
npm run build

# Deploy to S3
aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
  --endpoint-url "$LOCALSTACK_ENDPOINT" \
  --no-verify-ssl \
  --delete

echo "Deployed app to $VITE_NOGGIN_HOST$BASE_PATH"
```

## Troubleshooting

### Common Issues

1. **PDF Upload Fails**:
   - Check authentication status (JWT token)
   - Verify file size within limits
   - Ensure file is valid PDF format

2. **Processing Never Completes**:
   - Check workflow service logs for errors
   - Verify S3 permissions are correct
   - Check for resource constraints in processing service

3. **Static Assets Not Loading**:
   - Verify S3 upload was successful
   - Check for path/prefix configuration mismatches
   - Confirm Noggin is running and can access S3

4. **API Errors**:
   - Check Noggin logs for detailed error information
   - Verify authentication cookies are present
   - Check network requests in browser developer tools

### Debugging Steps

1. **Connection Verification**:
   ```bash
   # Check LocalStack availability
   curl -s --head "http://localhost:4566"
   ```

2. **Authentication Check**:
   ```javascript
   // Check for auth_indicator cookie
   document.cookie.includes("auth_indicator=true")
   ```

3. **S3 Content Verification**:
   ```bash
   aws s3 ls s3://local-bucket/vibes/flonotes/ \
     --endpoint-url "http://localhost:4566" --no-verify-ssl
   ```

4. **API Call Testing**:
   ```bash
   curl -v -H "Cookie: auth_token=YOUR_JWT_TOKEN" \
     "http://localhost:20701/auth/me"
   ```

## Extending the System

To add new functionality to the Noggin gateway:

1. **New API Endpoint**:
   - Create a new app.ts file in the appropriate directory
   - Define endpoints using the App framework
   - Add necessary validation schemas

2. **New Frontend App**:
   - Build the frontend app with Vite
   - Create deployment script
   - Deploy to S3 with the correct prefix

3. **New PDF Processing Capabilities**:
   - Add new processing flows in platform workflows
   - Extend the JSON schema for new data types
   - Update frontend components to display new data
# PDF Chat System: Authentication and Network Flow

This document explains the complete flow of authentication and networking between the frontend application and backend services for the PDF chat feature. It covers the authentication process, PDF processing, and chat interaction.

## 1. Architecture Overview

The PDF chat system consists of the following components:

- **Frontend UI**: React application at `/vibes/apps/flopilot/src/`
- **Backend Services**: Node.js services at `/platform/gateways/noggin/src/`
  - PDF Processing Service: `/platform/gateways/noggin/src/pdfs/`
  - Chat Service: `/platform/gateways/noggin/src/chat/`
- **Storage**: S3-compatible storage for PDFs and extracted data

## 2. Authentication Flow

### 2.1 Login Process

The system uses a JWT-based authentication system:

1. User logs in via the login form (`/login.html`)
2. The system creates a one-time password (OTP) and redirects to verification page
3. After OTP verification, the server issues two tokens:
   - `access_token`: Short-lived JWT containing user and enterprise information
   - `refresh_token`: Long-lived token for refreshing access

These tokens are stored as cookies:
```
Cookie: auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

### 2.2 JWT Structure

The access token contains critical enterprise information:

```json
{
  "entr": {
    "ent_VRfC5vl0Fw5w_dcEVM-1w": "ADMIN"
  },
  "wksp": {
    "wksp_PnCmyFGy9fHwWgIOx_k5h": "ent_VRfC5vl0Fw5w_dcEVM-1w",
    "wksp_vBE6Z0LimBSHaF0bRZ5l2": "ent_VRfC5vl0Fw5w_dcEVM-1w"
  },
  "iss": "https://co-helm.com",
  "sub": "usr_BYZnJljHgUz_y4CLsSmTU",
  "aud": ["access"],
  "exp": 1746662704,
  "nbf": 1746661804,
  "iat": 1746661804,
  "jti": "jwt_kOtkgt504p1WtEZJpA83w"
}
```

The `enterpriseUid` (`ent_VRfC5vl0Fw5w_dcEVM-1w`) is extracted from this token and is required for all authenticated operations.

### 2.3 Token Extraction

Backend services extract the enterprise information using the `parseEnterpriseUserFromJwt` function from `/platform/gateways/noggin/src/auth/jwt.ts`:

```typescript
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
  if (!ctx.jwtPayload?.entr) {
    throw new HTTPException(401, { message: "Authentication required" });
  }

  // Get enterprise and user uids from the token.
  let enterpriseUid = undefined;
  let role = undefined;

  // Most tokens will have a single enterprise.
  if (Object.keys(ctx.jwtPayload.entr).length === 1) {
    enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
    role = ctx.jwtPayload.entr[enterpriseUid];
  } else {
    // Logic for multiple enterprises...
  }

  // ...

  return {
    enterpriseUid,
    role,
    workspaceUid,
    userUid: ctx.jwtPayload.sub,
  };
}
```

## 3. PDF Processing Flow

### 3.1 PDF Upload

The process begins when a user uploads PDFs through the UI in `prompt-box.tsx`:

```typescript
// From /vibes/apps/flopilot/src/view/prompt-box.tsx
const {
  processedFiles,
  onPdfChange,
  onRemovePdf,
  isPdfProcessingMap,
} = usePdfProcessController();
```

The `usePdfProcessController` hook handles the PDF upload:

```typescript
// From /vibes/apps/flopilot/src/controller/use-pdf-process-controller.ts
const { mutateAsync: processPdf } = useMutation({
  mutationKey: PDF_PROCESS_KEY,
  mutationFn: async (files: File[]) => {
    const formData = new FormData();
    files.forEach((file) => {
      formData.append("files[]", file);
    });

    const response = await fetch(
      `${env.VITE_NOGGIN_HOST}/process-pdfs`, // Note: No pdfs/ prefix
      {
        method: "POST",
        body: formData,
        credentials: "include", // Explicitly include credentials
      },
    );
    // ... process response
  },
});
```

This request includes the authentication cookies thanks to the `credentials: "include"` option:

```
"cookie": "auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```

### 3.2 Backend PDF Processing

The backend endpoint `/pdfs/process-pdfs` in `/platform/gateways/noggin/src/pdfs/app.ts` handles the request:

1. It extracts the `enterpriseUid` from the JWT using `parseEnterpriseUserFromJwt`
2. Generates a `stemUid` and `pdfUid` for each uploaded file
3. Stores each PDF in S3 using a path structure: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
4. Schedules a background flow for processing the PDF
5. Returns the `stemUid` and `pdfUids` to the client

```typescript
// From /platform/gateways/noggin/src/pdfs/app.ts
app.endpoint({
  method: "POST",
  body: processStemPdfSchema,
  response: stemPdfUidResponseSchema,
  route: "/process-pdfs",
  async handler(ctx) {
    const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
    // ... process PDFs and store in S3
    return {
      stemUid,
      pdfUids: results.map((result) => result.pdfUid),
    };
  },
});
```

### 3.3 PDF Status Polling

After upload, the frontend polls the status of PDF processing:

```typescript
// From /vibes/apps/flopilot/src/controller/use-pdf-process-controller.ts
const combinedQueries = useQueries({
  queries: (processedFiles ?? []).map(({ pdfUid, stemUid }) => ({
    queryKey: [PDF_PROCESS_STATUS_KEY, pdfUid],
    queryFn: async () => {
      const response = await fetch(
        `${env.VITE_NOGGIN_HOST}/pdf-status/${stemUid}/${pdfUid}`, // No pdfs/ prefix
        {
          method: "GET",
          credentials: "include", // Explicitly include credentials
        },
      );
      // ... process response
    },
    refetchInterval: (query) => {
      if (query.state.data?.status === "processing") {
        return 5_000; // Poll every 5 seconds while processing
      }
      return false;
    },
  })),
});
```

The backend status endpoint again extracts the `enterpriseUid` from the JWT and checks if the processed file exists in the proper S3 path.

## 4. Chat Interaction Flow

### 4.1 Chat Request

Once PDFs are processed, users can chat about them. The chat request is made through the `useChatController` hook:

```typescript
// From /vibes/apps/flopilot/src/controller/use-chat-controller.ts
// Custom fetcher function to ensure credentials are included
const customFetcher = async (url: string, options: RequestInit) => {
  return fetch(url, {
    ...options,
    credentials: "include", // This ensures cookies are sent with the request
  });
};

export function useChatController() {
  // ...
  const { messages, input, append, handleInputChange, handleSubmit, error } =
    useChat({
      api: `${env.VITE_NOGGIN_HOST}/`, // Direct to root endpoint, not /chat
      streamProtocol: "text",
      body: {
        stemUid,
        pdfUids,
      },
      fetcher: customFetcher, // Use our custom fetcher that includes credentials
      onError: (error) => {
        console.error("Chat error:", error);
      },
    });
  // ...
}
```

### 4.2 Authentication Issue and Solution

**The Critical Issue**:

The AI SDK's `useChat` hook doesn't include cookies by default, unlike direct fetch calls from the browser. This caused 401 Unauthorized errors in the chat endpoint.

The solution is to implement a custom fetcher that explicitly includes credentials:

```typescript
const customFetcher = async (url: string, options: RequestInit) => {
  return fetch(url, {
    ...options,
    credentials: "include", // This ensures cookies are sent with the request
  });
};
```

This custom fetcher needs to be applied to all API requests to ensure authentication cookies are properly sent:

```typescript
// For chat endpoints
useChat({
  api: `${env.VITE_NOGGIN_HOST}/chat`,
  fetcher: customFetcher,
  // ...
})

// For PDF endpoints
const response = await fetch(
  `${env.VITE_NOGGIN_HOST}/pdfs/process-pdfs`,
  {
    method: "POST",
    body: formData,
    credentials: "include", // Ensure cookies are sent
  },
);
```

### 4.3 Backend Chat Processing

The chat endpoint handler in `/platform/gateways/noggin/src/chat/app.ts` requires the `enterpriseUid` to fetch PDF extracts from S3:

```typescript
async function getExtractsByPdfUid(
  ctx: RequestContext<ChatAppContext, "/", {}, unknown, unknown, ChatRequestBody>
) {
  const enterpriseUid = ctx.auth?.enterpriseUid;
  if (!enterpriseUid) {
    // The types think we can have an undefined auth context
    throw new HTTPException(401, { message: "Unauthorized" });
  }

  const { stemUid, pdfUids } = ctx.body;
  const s3Keys = pdfUids.map((pdfUid) => {
    return {
      s3Key: buildS3PdfPath({
        enterpriseUid,
        stemUid,
        pdfUid,
        filename: "extracts.json",
      }),
      pdfUid,
    };
  });

  // Fetch and process PDF extracts from S3
  // ...
}
```

## 5. S3 Path Structure

The system uses a consistent S3 path structure for both raw PDFs and their extracted content:

```
stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf
stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
```

This structure is created by the `buildS3PdfPath` function used in both PDF and chat services:

```typescript
function buildS3PdfPath({ enterpriseUid, stemUid, pdfUid, filename }: BuildS3PdfPathArgs): string {
  const basePath = `stems/${enterpriseUid}/${stemUid}/${pdfUid}`;
  return `${basePath}/${filename}`;
}
```

## 6. Flow Diagram

### Authentication and PDF Processing:
```
User → Login → JWT Token Issued
↓
Upload PDF → /pdfs/process-pdfs → Extract enterpriseUid from JWT
↓
Generate stemUid and pdfUid → Store PDF in S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf
↓
Schedule PDF processing flow → Process PDF → Store extracts in S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
↓
Frontend polls /pdfs/pdf-status/{stemUid}/{pdfUid} → Wait until status is "ready"
```

### Chat Flow:
```
User sends message → useChat hook with custom fetcher → POST to /chat
↓
Backend extracts enterpriseUid from JWT → Get PDF extracts from S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
↓
Format extracts and prompt → Send to LLM → Return response to user
```

## 7. Key Learnings

- Always include `credentials: "include"` in fetch requests to pass JWT tokens
- Use custom fetchers with third-party libraries that don't pass credentials by default
- Design S3 paths with enterprise-level isolation for security
- Use a consistent enterprise extraction pattern across all authenticated endpoints
# Anterior PDF Processing Architecture

This document details the PDF processing architecture used in the Anterior platform, specifically the integration between the `platform` repository's Noggin gateway and the `vibes` repository's frontend applications.

## Overview

The PDF processing system in Anterior follows a microservices architecture with:

1. **Backend Services** (in `platform` repo) - Handle PDF storage, processing, and extraction
2. **Frontend Applications** (in `vibes` repo) - Provide user interfaces for PDF upload, viewing, and analysis
3. **API Gateway** (Noggin) - Serves as the communication layer between frontends and backend services

This architecture allows for a clean separation of concerns while enabling sophisticated PDF processing capabilities across multiple applications.

## System Components

### 1. Noggin Gateway (`platform/gateways/noggin`)

The Noggin gateway acts as the primary API endpoint for all PDF operations, implemented in `src/pdfs/app.ts`.

#### Key Endpoints

| Endpoint | Method | Purpose | Parameters/Body | Response |
|----------|--------|---------|-----------------|----------|
| `/process-pdf` | POST | Upload and process a PDF | `file` (PDF blob), optional `stemUid` | `{ stemUid: string, pdfUid: string }` |
| `/pdf-status/:stemUid/:pdfUid` | GET | Check processing status | URL params: stemUid, pdfUid | `{ status: "processing" \| "ready" \| "error", message?: string }` |
| `/pdf-extracts/:stemUid/:pdfUid` | GET | Retrieve extracted data | URL params: stemUid, pdfUid | JSON structure of extracted content |

#### Authentication & Authorization

- Uses JWT-based authentication via HTTP-only cookies
- Extracts enterprise and user information from the JWT token
- Associates PDFs with specific enterprises for multi-tenant isolation

#### Implementation Details

1. **PDF Upload Process**:
   ```typescript
   // Creates a unique PDF ID and optionally a stem ID
   const pdfUid = generateRandomId("pdf");
   const stemUid = ctx.body.stemUid || generateRandomId("stm");

   // Stores the raw PDF in S3
   await ctx.platform.blobs.storeBlob(s3InputKey, blob, "application/pdf");

   // Schedules asynchronous processing
   await ctx.platform.flows.schedule(flowParams);
   ```

2. **Storage Structure**:
   - Raw PDFs: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
   - Extracted data: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json`

3. **Error Handling**:
   - Maps common S3 errors to appropriate HTTP responses
   - Returns 404 for PDFs still processing or not found
   - Returns 500 for processing service failures

### 2. PDF Processing Flow

1. **Upload**: Frontend uploads PDF to Noggin
2. **Initial Storage**: Noggin stores raw PDF in S3
3. **Processing**: Noggin schedules an asynchronous workflow
4. **Extraction**: Workflow processes PDF and stores JSON extracts
5. **Status Check**: Frontend polls status endpoint until processing completes
6. **Retrieval**: Frontend fetches and displays extracted content

### 3. Vibes Apps Implementation

#### A. Flonotes App (`vibes/apps/flonotes`)

Flonotes provides advanced PDF viewing and annotation capabilities for clinical documentation.

**Key Components**:

1. **API Integration** (`src/services/noggin-api.ts`):
   - Complete client implementation for Noggin PDF endpoints
   - Handles authentication, error management, and retries
   - Implements status polling with exponential backoff

2. **PDF Processing** (`src/hooks/use-process-pdf.ts`):
   - Custom React hook that manages PDF processing state
   - Tracks upload progress, processing status, and results

3. **PDF Viewer** (`src/components/shell/pdf/`):
   - Rich PDF viewing UI with page navigation
   - Highlight and annotation capabilities
   - Citation linking between PDF content and notes

4. **IndexedDB Integration** (`src/indexed-db/db.ts`):
   - Local storage of PDF metadata and processing results
   - Enables offline viewing of previously processed PDFs

**Sample API Usage**:
```typescript
// Process a PDF and retrieve extractions
const extractsResponse = await processAndGetPdfExtracts(
  fileData,          // PDF binary data as number[]
  fileName,          // Original filename
  "clinical",        // Document type (clinical or criteria)
  undefined,         // Optional stemUid
  100,               // Max polling retries
  5000               // Polling interval (ms)
);
```

#### B. Flopilot App (`vibes/apps/flopilot`)

Flopilot uses PDFs as context for AI-assisted interactions and chat functionality.

**Key Components**:

1. **Controller Pattern** (`src/controller/use-pdf-process-controller.ts`):
   - State management for PDF processing
   - Integration with chat context

2. **PDF Models** (`src/controller/models/`):
   - `pdf-extracts.ts` - Types for extracted PDF content
   - `pdf-status.ts` - Status response handling
   - `process-pdf.ts` - PDF upload request models

3. **Chat Integration**:
   - Uses processed PDFs as context for AI-assisted chats
   - Links relevant PDF content to chat messages

## Technical Details

### 1. Stem Concept

A "stem" (`stemUid`) represents a logical grouping of related documents:

- Multiple PDFs can share the same `stemUid`
- Enables contextual relationships between documents
- Used for organizing documents in a case or workflow

### 2. Processing Pipeline

1. **PDF Upload**:
   - Multipart form data upload with optional metadata
   - Returns immediately with tracking IDs before processing completes

2. **Asynchronous Processing**:
   - Uses workflow engine for reliable, scalable processing
   - Extracts text, structure, and semantic information
   - Produces standardized JSON output

3. **Status Checking**:
   - Simple polling mechanism for checking completion
   - Returns processing, ready, or error states
   - Includes error details when processing fails

4. **Extract Retrieval**:
   - Fetches JSON representation of processed PDF
   - Structure includes text blocks, page information, and metadata
   - May include domain-specific extractions (clinical findings, etc.)

### 3. JSON Extract Structure

```json
{
  "result": {
    "metadata": {
      "title": "Document Title",
      "author": "Author Name",
      "pages": 5,
      "creationDate": "2023-01-15T12:00:00Z"
    },
    "pages": [
      {
        "pageNumber": 1,
        "blocks": [
          {
            "id": "block-1",
            "text": "Example text content",
            "boundingBox": {
              "x1": 100, "y1": 100,
              "x2": 400, "y2": 150
            },
            "type": "paragraph"
          }
          // More blocks...
        ]
      }
      // More pages...
    ],
    "structure": {
      // Document structure information
    }
  }
}
```

## Integration Instructions

To integrate PDF processing in a new vibes app:

### 1. Setup API Client

Create a service file that interfaces with Noggin PDF endpoints:

```typescript
// Example minimal API client
export async function processPdf(fileData: number[], fileName: string): Promise<{stemUid: string, pdfUid: string}> {
  const formData = new FormData();
  formData.append("file", new Blob([new Uint8Array(fileData)]), fileName);

  const response = await fetch(`${NOGGIN_HOST}/pdfs/process-pdf`, {
    method: "POST",
    body: formData,
    credentials: "include" // For auth cookies
  });

  if (!response.ok) {
    throw new Error(`PDF processing failed: ${response.status}`);
  }

  return await response.json();
}
```

### 2. Implement Status Polling

```typescript
export async function waitForProcessing(stemUid: string, pdfUid: string, maxRetries = 100): Promise<void> {
  for (let attempt = 0; attempt < maxRetries; attempt++) {
    const response = await fetch(`${NOGGIN_HOST}/pdfs/pdf-status/${stemUid}/${pdfUid}`, {
      credentials: "include"
    });

    if (!response.ok) {
      throw new Error(`Status check failed: ${response.status}`);
    }

    const status = await response.json();

    if (status.status === "ready") {
      return;
    }

    if (status.status === "error") {
      throw new Error(`Processing error: ${status.message}`);
    }

    // Wait before next check (with exponential backoff)
    await new Promise(resolve => setTimeout(resolve, Math.min(5000 * Math.pow(1.5, attempt), 30000)));
  }

  throw new Error("Processing timed out");
}
```

### 3. Fetch Extractions

```typescript
export async function getExtractions(stemUid: string, pdfUid: string): Promise<any> {
  const response = await fetch(`${NOGGIN_HOST}/pdfs/pdf-extracts/${stemUid}/${pdfUid}`, {
    credentials: "include"
  });

  if (!response.ok) {
    throw new Error(`Failed to get extractions: ${response.status}`);
  }

  return await response.json();
}
```

### 4. Complete Flow Example

```typescript
export async function processAndGetExtractions(file: File): Promise<any> {
  // Read file as array buffer
  const arrayBuffer = await file.arrayBuffer();
  const fileData = Array.from(new Uint8Array(arrayBuffer));

  // Upload and start processing
  const { stemUid, pdfUid } = await processPdf(fileData, file.name);

  // Wait for processing to complete
  await waitForProcessing(stemUid, pdfUid);

  // Get and return extractions
  return await getExtractions(stemUid, pdfUid);
}
```

## Deployment and Development Workflow

1. **Local Development**:
   - Start platform services: `nix develop` and `ant-all-services`
   - Build and deploy vibes app: `cd apps/your-app && npm i && ./deploy-local.sh`
   - Access via Noggin: `http://localhost:20701/your-app`

2. **Production Deployment**:
   - Vibes apps are built and uploaded to S3
   - Noggin serves them as static assets
   - API calls are proxied through Noggin to backend services

## Security Considerations

1. **Authentication**:
   - JWT tokens stored in HTTP-only cookies
   - Enterprise-specific isolation of PDFs
   - Role-based access controls for specific operations

2. **Data Protection**:
   - PDFs are stored in enterprise-specific S3 paths
   - All API calls require valid authentication
   - Processing results are isolated by enterprise

3. **Error Handling**:
   - User-friendly error messages
   - Detailed logging for debugging
   - Appropriate HTTP status codes

## Extending the System

To extend PDF processing capabilities:

1. **New Extraction Types**:
   - Add new processing flows in platform workflows
   - Extend the JSON schema for new data types
   - Update frontend components to display new data

2. **Additional Endpoints**:
   - Add new routes in Noggin's PDF app
   - Implement corresponding API clients in vibes apps

3. **Enhanced Visualization**:
   - Build new UI components for specialized PDF rendering
   - Implement domain-specific interactions (e.g., clinical annotations)

## Troubleshooting

### Common Issues

1. **PDF Upload Fails**:
   - Check authentication status (JWT token)
   - Verify file size within limits
   - Ensure file is valid PDF format

2. **Processing Never Completes**:
   - Check workflow service logs for errors
   - Verify S3 permissions are correct
   - Check for resource constraints in processing service

3. **Extract Retrieval Fails**:
   - Confirm processing has completed successfully
   - Verify S3 path is correct for the enterprise
   - Check for malformed JSON in extract file

### Debugging Tools

1. **API Request Logs**:
   - Check Noggin server logs for API request details
   - Look for HTTP status codes and error messages

2. **Storage Inspection**:
   - Use S3 tools to inspect raw PDFs and extracts
   - Verify file integrity and permissions

3. **Processing Status**:
   - Monitor workflow execution status
   - Check for failed or stalled workflows
# PDF Chat System: Authentication and Network Flow

This document explains the complete flow of authentication and networking between the frontend application and backend services for the PDF chat feature. It covers the authentication process, PDF processing, and chat interaction.

## 1. Architecture Overview

The PDF chat system consists of the following components:

- **Frontend UI**: React application at `/vibes/apps/flopilot/src/`
- **Backend Services**: Node.js services at `/platform/gateways/noggin/src/`
  - PDF Processing Service: `/platform/gateways/noggin/src/pdfs/`
  - Chat Service: `/platform/gateways/noggin/src/chat/`
- **Storage**: S3-compatible storage for PDFs and extracted data

## 2. Authentication Flow

### 2.1 Login Process

The system uses a JWT-based authentication system:

1. User logs in via the login form (`/login.html`)
2. The system creates a one-time password (OTP) and redirects to verification page
3. After OTP verification, the server issues two tokens:
   - `access_token`: Short-lived JWT containing user and enterprise information
   - `refresh_token`: Long-lived token for refreshing access

These tokens are stored as cookies:
```
Cookie: auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...
```

### 2.2 JWT Structure

The access token contains critical enterprise information:

```json
{
  "entr": {
    "ent_VRfC5vl0Fw5w_dcEVM-1w": "ADMIN"
  },
  "wksp": {
    "wksp_PnCmyFGy9fHwWgIOx_k5h": "ent_VRfC5vl0Fw5w_dcEVM-1w",
    "wksp_vBE6Z0LimBSHaF0bRZ5l2": "ent_VRfC5vl0Fw5w_dcEVM-1w"
  },
  "iss": "https://co-helm.com",
  "sub": "usr_BYZnJljHgUz_y4CLsSmTU",
  "aud": ["access"],
  "exp": 1746662704,
  "nbf": 1746661804,
  "iat": 1746661804,
  "jti": "jwt_kOtkgt504p1WtEZJpA83w"
}
```

The `enterpriseUid` (`ent_VRfC5vl0Fw5w_dcEVM-1w`) is extracted from this token and is required for all authenticated operations.

### 2.3 Token Extraction

Backend services extract the enterprise information using the `parseEnterpriseUserFromJwt` function from `/platform/gateways/noggin/src/auth/jwt.ts`:

```typescript
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
  if (!ctx.jwtPayload?.entr) {
    throw new HTTPException(401, { message: "Authentication required" });
  }

  // Get enterprise and user uids from the token.
  let enterpriseUid = undefined;
  let role = undefined;

  // Most tokens will have a single enterprise.
  if (Object.keys(ctx.jwtPayload.entr).length === 1) {
    enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
    role = ctx.jwtPayload.entr[enterpriseUid];
  } else {
    // Logic for multiple enterprises...
  }

  // ...

  return {
    enterpriseUid,
    role,
    workspaceUid,
    userUid: ctx.jwtPayload.sub,
  };
}
```

## 3. PDF Processing Flow

### 3.1 PDF Upload

The process begins when a user uploads PDFs through the UI in `prompt-box.tsx`:

```typescript
// From /vibes/apps/flopilot/src/view/prompt-box.tsx
const {
  processedFiles,
  onPdfChange,
  onRemovePdf,
  isPdfProcessingMap,
} = usePdfProcessController();
```

The `usePdfProcessController` hook handles the PDF upload:

```typescript
// From /vibes/apps/flopilot/src/controller/use-pdf-process-controller.ts
const { mutateAsync: processPdf } = useMutation({
  mutationKey: PDF_PROCESS_KEY,
  mutationFn: async (files: File[]) => {
    const formData = new FormData();
    files.forEach((file) => {
      formData.append("files[]", file);
    });

    const response = await fetch(
      `${env.VITE_NOGGIN_HOST}/process-pdfs`, // Note: No pdfs/ prefix
      {
        method: "POST",
        body: formData,
        credentials: "include", // Explicitly include credentials
      },
    );
    // ... process response
  },
});
```

This request includes the authentication cookies thanks to the `credentials: "include"` option:

```
"cookie": "auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
```

### 3.2 Backend PDF Processing

The backend endpoint `/pdfs/process-pdfs` in `/platform/gateways/noggin/src/pdfs/app.ts` handles the request:

1. It extracts the `enterpriseUid` from the JWT using `parseEnterpriseUserFromJwt`
2. Generates a `stemUid` and `pdfUid` for each uploaded file
3. Stores each PDF in S3 using a path structure: `stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf`
4. Schedules a background flow for processing the PDF
5. Returns the `stemUid` and `pdfUids` to the client

```typescript
// From /platform/gateways/noggin/src/pdfs/app.ts
app.endpoint({
  method: "POST",
  body: processStemPdfSchema,
  response: stemPdfUidResponseSchema,
  route: "/process-pdfs",
  async handler(ctx) {
    const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
    // ... process PDFs and store in S3
    return {
      stemUid,
      pdfUids: results.map((result) => result.pdfUid),
    };
  },
});
```

### 3.3 PDF Status Polling

After upload, the frontend polls the status of PDF processing:

```typescript
// From /vibes/apps/flopilot/src/controller/use-pdf-process-controller.ts
const combinedQueries = useQueries({
  queries: (processedFiles ?? []).map(({ pdfUid, stemUid }) => ({
    queryKey: [PDF_PROCESS_STATUS_KEY, pdfUid],
    queryFn: async () => {
      const response = await fetch(
        `${env.VITE_NOGGIN_HOST}/pdf-status/${stemUid}/${pdfUid}`, // No pdfs/ prefix
        {
          method: "GET",
          credentials: "include", // Explicitly include credentials
        },
      );
      // ... process response
    },
    refetchInterval: (query) => {
      if (query.state.data?.status === "processing") {
        return 5_000; // Poll every 5 seconds while processing
      }
      return false;
    },
  })),
});
```

The backend status endpoint again extracts the `enterpriseUid` from the JWT and checks if the processed file exists in the proper S3 path.

## 4. Chat Interaction Flow

### 4.1 Chat Request

Once PDFs are processed, users can chat about them. The chat request is made through the `useChatController` hook:

```typescript
// From /vibes/apps/flopilot/src/controller/use-chat-controller.ts
// Custom fetcher function to ensure credentials are included
const customFetcher = async (url: string, options: RequestInit) => {
  return fetch(url, {
    ...options,
    credentials: "include", // This ensures cookies are sent with the request
  });
};

export function useChatController() {
  // ...
  const { messages, input, append, handleInputChange, handleSubmit, error } =
    useChat({
      api: `${env.VITE_NOGGIN_HOST}/`, // Direct to root endpoint, not /chat
      streamProtocol: "text",
      body: {
        stemUid,
        pdfUids,
      },
      fetcher: customFetcher, // Use our custom fetcher that includes credentials
      onError: (error) => {
        console.error("Chat error:", error);
      },
    });
  // ...
}
```

### 4.2 Authentication Issue and Solution

**The Critical Issue**:

The AI SDK's `useChat` hook doesn't include cookies by default, unlike direct fetch calls from the browser. This caused 401 Unauthorized errors in the chat endpoint.

The solution is to implement a custom fetcher that explicitly includes credentials:

```typescript
const customFetcher = async (url: string, options: RequestInit) => {
  return fetch(url, {
    ...options,
    credentials: "include", // This ensures cookies are sent with the request
  });
};
```

This custom fetcher needs to be applied to all API requests to ensure authentication cookies are properly sent:

```typescript
// For chat endpoints
useChat({
  api: `${env.VITE_NOGGIN_HOST}/chat`,
  fetcher: customFetcher,
  // ...
})

// For PDF endpoints
const response = await fetch(
  `${env.VITE_NOGGIN_HOST}/pdfs/process-pdfs`,
  {
    method: "POST",
    body: formData,
    credentials: "include", // Ensure cookies are sent
  },
);
```
```

### 4.3 Backend Chat Processing

The chat endpoint handler in `/platform/gateways/noggin/src/chat/app.ts` requires the `enterpriseUid` to fetch PDF extracts from S3:

```typescript
async function getExtractsByPdfUid(
  ctx: RequestContext<ChatAppContext, "/", {}, unknown, unknown, ChatRequestBody>
) {
  const enterpriseUid = ctx.auth?.enterpriseUid;
  if (!enterpriseUid) {
    // The types think we can have an undefined auth context
    throw new HTTPException(401, { message: "Unauthorized" });
  }

  const { stemUid, pdfUids } = ctx.body;
  const s3Keys = pdfUids.map((pdfUid) => {
    return {
      s3Key: buildS3PdfPath({
        enterpriseUid,
        stemUid,
        pdfUid,
        filename: "extracts.json",
      }),
      pdfUid,
    };
  });

  // Fetch and process PDF extracts from S3
  // ...
}
```

## 5. S3 Path Structure

The system uses a consistent S3 path structure for both raw PDFs and their extracted content:

```
stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf
stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
```

This structure is created by the `buildS3PdfPath` function used in both PDF and chat services:

```typescript
function buildS3PdfPath({ enterpriseUid, stemUid, pdfUid, filename }: BuildS3PdfPathArgs): string {
  const basePath = `stems/${enterpriseUid}/${stemUid}/${pdfUid}`;
  return `${basePath}/${filename}`;
}
```

## 6. Flow Diagram

### Authentication and PDF Processing:
```
User → Login → JWT Token Issued
↓
Upload PDF → /pdfs/process-pdfs → Extract enterpriseUid from JWT
↓
Generate stemUid and pdfUid → Store PDF in S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf
↓
Schedule PDF processing flow → Process PDF → Store extracts in S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
↓
Frontend polls /pdfs/pdf-status/{stemUid}/{pdfUid} → Wait until status is "ready"
```

### Chat Flow:
```
User sends message → useChat hook with custom fetcher → POST to /chat
↓
Backend extracts enterpriseUid from JWT → Get PDF extracts from S3 at stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json
↓
Format extracts and prompt → Send to LLM → Return response to user
```

## 7. Network Logs Analysis

The server logs show the full request/response cycle:

1. **Authentication**: Successful authentication with OTP verification
   ```
   [19:50:04.477] INFO (52868): Request completed
       res: {
         "status": 200,
         "headers": {}
       }
       req: {
         "url": "/otp/verify",
         "method": "POST",
         ...
   ```

2. **PDF Upload**: Successful upload with authentication cookies
   ```
   [19:50:24.719] INFO (52868): Request completed
       res: {
         "status": 200,
         "headers": {}
       }
       req: {
         "url": "/process-pdfs",  // Server log shows just the path portion
         "method": "POST",
         "headers": {
           ...
           "cookie": "auth_indicator=true; access_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...; refresh_token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
           ...
         },
   ```

3. **PDF Status Polling**: Successful status checks
   ```
   [19:50:24.743] INFO (52868): Request completed
       res: {
         "status": 200,
         "headers": {}
       }
       req: {
         "url": "/pdf-status/stm_4b5aaa396d_391899ebd3/pdf_a67de8344c_c8d574d08b",  // Server log shows just the path portion
         "method": "GET",
         ...
   ```

4. **Chat Request Failure**: The 401 error when cookies aren't properly included
   ```
   [19:50:48.504] ERROR (52868): Unauthorized
       res: {
         "status": 401,
         "headers": {}
       }
       req: {
         "url": "/",  // Server log shows path as root, but full URL is likely /chat
         "method": "POST",
         ...
   ```

## 8. Conclusion

The PDF Chat system requires consistent authentication across all endpoints. The critical issue needed to be fixed:

**Authentication Issue**: The AI SDK's fetch implementation wasn't including credentials by default. By adding a custom fetcher that explicitly includes credentials with `credentials: "include"`, we ensure the auth token is passed to the chat endpoint, allowing it to extract the `enterpriseUid` and access the correct PDF extracts in S3.

**Note about server logs**: The server logs show the paths without the full URL prefix, but in the frontend code, we still need to use the complete paths with prefixes:
- Frontend uses: `/pdfs/process-pdfs` while server logs show: `/process-pdfs`
- Frontend uses: `/pdfs/pdf-status/${stemUid}/${pdfUid}` while server logs show: `/pdf-status/...`
- Frontend uses: `/chat` while server logs show: `/`

By adding the credentials to all fetch requests, we ensure:
1. All requests are properly authenticated with the JWT token
2. The backend can extract the enterpriseUid from the JWT
3. The backend can correctly access the S3 paths containing the PDF extracts

The entire system is built around the concept of enterprise-scoped data, with all storage paths containing the enterprise ID extracted from the JWT. This ensures data isolation between enterprises while maintaining a consistent access pattern across services.
# Phase 1 Progress Report: Initial Content Organization

## Overview

This document summarizes the progress made during Phase 1 of the AI content organization and refinement process. During this phase, we analyzed the existing AI directories, established a new organizational structure, and began migrating key content to the new "AI_Prompts_Master_Collection".

## Completed Tasks

1. **Repository Snapshot**
   - Verified clean repository state before proceeding

2. **Content Analysis**
   - Analyzed the content in the AI directory
   - Identified key patterns, themes, and content types
   - Determined logical organizational categories

3. **Initial Structure Creation**
   - Created the "AI_Prompts_Master_Collection" directory
   - Established the following major sections:
     - Core_Prompting
     - Provider_Specific
     - Use_Cases
     - Templates_Components
     - Tools_Integration
     - Documentation_Reference
     - Archive

4. **Initial File Migration**
   - Migrated key base templates and system roles
   - Copied language-specific code generation templates
   - Transferred Claude-specific configuration files
   - Migrated developer profile and workflow integration guides
   - Created structure overview documentation

## Current Structure

The current structure follows the organizational principles identified during the analysis phase:

```
AI_Prompts_Master_Collection/
├── Core_Prompting/        # Fundamental templates and techniques
├── Provider_Specific/     # AI provider-specific configurations
├── Use_Cases/             # Domain-specific templates
├── Templates_Components/  # Reusable prompt components
├── Tools_Integration/     # Tool integration resources
├── Documentation_Reference/ # Guides and reference materials
├── Archive/               # Archived content
└── README.md              # Master overview
```

## Next Steps

The following tasks remain for Phases 2 and 3:

1. **Complete Content Migration**
   - Continue migrating remaining valuable content
   - Ensure all relevant AI configuration files are included

2. **Deduplication Analysis**
   - Identify semantically similar files
   - Propose consolidated versions of duplicated content

3. **Quality Assessment**
   - Identify low-value content for potential archival
   - Enhance conciseness of verbose files
   - Review organization for intuitive navigation

4. **User Review**
   - Present complete collection for review
   - Make adjustments based on feedback

5. **Finalization**
   - Apply approved changes
   - Complete final documentation

## Files Migrated

So far, the following key files have been migrated to the new structure:

- Base system role template
- Code generation base template
- TypeScript and Python specific code templates
- Advanced prompting techniques guide
- Prompt engineering fundamentals
- Workflow integration guide
- Developer profile
- Master configuration template
- Task-specific templates
- Claude-specific configurations
- Structure overview documentation

## Notes on Content Quality

During the initial analysis, we identified several patterns:

1. **High-Value Content**
   - Well-structured prompt templates
   - Detailed configuration guides
   - Language-specific coding guidelines
   - Developer environment specifications

2. **Areas for Improvement**
   - Some duplication between prompts and templates folders
   - Verbose explanations in some files that could be condensed
   - Fragmentation of related content across multiple files

These observations will guide our approach in Phase 2 as we continue to organize and refine the content.
# Populate Template Task \(`populate_template_task` workflow\)

This document captures everything discussed in the design review / pairing session regarding the *Flonotes* template-population flow.

---

## 1. Public HTTP Endpoint

```
POST /tasks/flonotes/template/:template_id
```

| Parameter | Location | Type | Description |
|-----------|----------|------|-------------|
| `template_id` | path | `string` | The template slug to populate. |
| `stemId` | body | `string` *(optional)* | Existing stem to associate with the note. When omitted a new `stm_*` id is generated. |

The endpoint definition lives in
`platform/gateways/noggin/src/tasks/app.ts` (lines ≈ 200-235 at time of writing).
It simply validates the request with Zod and then calls the handler below.

## 2. Handler – `runPopulateTemplate()`

*File*: `platform/gateways/noggin/src/tasks/handlers/run-populate-template.ts`

High-level steps:

1. Generate a `traceId` and, if not supplied, a fresh `stemId` via `generateRandomId("stm")`.
2. **Schedule** a task on the `platform.populateTemplateTasks` queue:
   ```ts
   await platform.populateTemplateTasks.schedule({
     taskName: "populate_template_task",
     config: { workfloMeta, source_meta, workspaceConfig },
     clinicalsQuery: []
   });
   ```
3. Return the `stemId` to the API layer so the client gets an immediate *200 OK*.

(Events will be emitted later; TODOs in the file mark where they belong.)

## 3. Ant-URI Helpers (new)

| Function | Signature | Produces |
|----------|-----------|----------|
| `buildAntPath()` | `(workspaceId: string, ...segments: (string \| number \| undefined)[])` | Generic builder. |
| `getTemplateRefUrl()` | `(workspaceId, templateId)` | `ant://{workspace}/templates/{templateId}/{templateId}.template.md` |
| `getClinicalsRefUrl()` | `(workspaceId, stemId)` | `ant://{workspace}/clinicals/{stemId}` |

All three live in `platform/gateways/noggin/src/tasks/utils/artifacts/index.ts` so every consumer shares one source of truth.

```ts
export const getTemplateRefUrl = (w, t) =>
  buildAntPath(w, "templates", t, `${t}.template.md`);
```

## 4. Sequence Diagram (Mermaid)

```mermaid
sequenceDiagram
    participant C  as Client
    participant AP as createTasksApp()<br/>/flonotes/templates/:template_id
    participant PT as runPopulateTemplate()
    participant TS as TaskScheduler.schedule()<br/>populateTemplateTasks
    participant MQ as MessageQueue (S3/SQS/etc.)
    participant TPW as Worker "populate_template_task"
    participant EB as StemEventBus (emitStemEvent)

    C  ->> AP: POST /…/template/:templateId { stemId? }
    activate AP
    AP ->> parseAuthInfo: JWT ➜ user/enterprise/workspace ids
    AP ->> PT: runPopulateTemplate(…)
    deactivate AP

    activate PT
    PT ->> TS: schedule(task)
    deactivate PT

    activate TS
    TS ->> MQ: enqueue message
    deactivate TS

    AP -->> C: 200 OK (stemId)

    MQ --) TPW: pop message
    activate TPW
    TPW ->> EB: emit "template_populated" event
    EB -->> C: SSE / WS event with result
    deactivate TPW
```

## 5. Worker Responsibilities (outline)

The Florence / brrr worker listening for `workflow_name === "populate_template_task"` must:

1. Fetch the template markdown using `getTemplateRefUrl()`.
2. Fetch zero or more clinical artifacts (`getClinicalsRefUrl()`) when required.
3. Interpolate variables, render the markdown to the desired formats.
4. Store the resulting artifact (PDF/HTML) back in the artifact store.
5. Emit a `stem_event` (e.g. `template_populated_v1_0`) that references the new artifact ID.

## 6. Artefact-Store Path Convention

The helper follows the hierarchy below (see `tasks/utils/artifacts/index.ts`):

```
artifacts/{workspaceUid}/{category}/{sourceId}/{artifactId}.{ext}
```

Categories currently supported: `clinicals`, `determinations`, `config`, `criteria`, `tips`, `members`.

## 7. Future Enhancements

* Emit stem events for **start** and **fail** states (currently TODO).
* Allow multiple clinical Ant-URIs (`clinicalsQuery`) once the brrr task supports it.
* Consider making `ArtifactCategory`-specific helper functions generated from the same source of truth to avoid string duplication.

---

_Last updated: <!-- date will be whatever commit time is -->_
# Product Requirements Document

This document captures the requirements, preferences, and design principles for the dotfiles repository as expressed by the user in conversations.

## Core Requirements

### 1. Repository Structure

- **Modular Organization**: Configuration files must be organized in a modular structure by tool
- **Consistent Naming**: Directory names should match tool identifiers used throughout the repository
- **XDG Compliance**: Follow XDG Base Directory specification for all configurations
- **Documentation**: Each tool must have its own README with usage instructions

### 2. Installation and Setup

- **Idempotent Setup**: Setup script must be runnable multiple times without causing issues
- **Backup Mechanism**: Automatically back up existing configurations before overwriting
- **Minimal Dependencies**: Keep external dependencies to a minimum
- **Consistent Symlinking**: Use symlinks consistently (entire directories when possible)
- **Cross-platform Support**: Primary focus on macOS (Apple Silicon), but avoid architecture-specific code when possible

### 3. Tool Integration

- **Unified Task Runner**: Use Just to provide a consistent interface for commands
- **Fuzzy Finding**: Enable fuzzy search capabilities for commands
- **Hierarchical Commands**: Commands should be organized by tool and accessible via prefix
- **Shell Integration**: Integrate with ZSH for immediate access to commands

### 4. Visual and UX Preferences

- **Tokyo Night Theme**: Consistent Tokyo Night theme across all tools
- **Modern Terminal**: GPU-accelerated terminal with Ghostty
- **Vim-style Navigation**: Vim-inspired navigation in all supporting tools
- **Clean Interface**: Minimize visual clutter in terminal and UI
- **Performance Optimization**: Configure for Apple Silicon performance

### 5. Tool-Specific Requirements

#### Neovim
- LazyVim-based configuration
- LSP integration
- Consistent syntax highlighting
- Fuzzy file navigation
- Git integration

#### ZSH
- Fast startup times
- Comprehensive aliases
- Useful functions for navigation and productivity
- Integration with modern tools (Atuin, Zoxide, Starship)

#### Yazi
- Optimized for Apple Silicon
- Integration with Neovim
- Rich file previews
- Custom navigation shortcuts

#### Aerospace
- Vim-style window navigation
- Multiple workspace support
- Tokyo Night-inspired window gaps
- Modal key binding system

#### Starship
- Tokyo Night theme
- Right-side time and battery information
- Fast performance
- Informative but not cluttered

### 6. Documentation Standards

- **Consistent Format**: All documentation should follow a consistent format
- **Code Examples**: Include useful code examples for common operations
- **Installation Instructions**: Clear installation instructions for each tool
- **Customization Guide**: Document how to customize each configuration
- **Version Information**: Track versions of all tools

### 7. Version Control

- **Clean Commits**: Commits should have clear, descriptive messages
- **Atomic Changes**: Each commit should represent a single logical change
- **Version Tracking**: Maintain a VERSION.md file with current environment snapshot

## Design Principles

1. **Minimalism**: Keep configurations simple and focused
2. **Performance**: Optimize for speed and responsiveness
3. **Consistency**: Maintain consistent style and behaviors across tools
4. **Modularity**: Enable easy addition/removal of specific configurations
5. **Discoverability**: Make features and commands easy to discover
6. **Documentation**: Document everything thoroughly
7. **Maintainability**: Code should be clean and well-organized

## Constraints

1. **macOS-focused**: Primary platform is macOS on Apple Silicon
2. **ZSH-centric**: Shell configuration is centered around ZSH
3. **Modern tools**: Focus on modern, actively maintained tools
4. **Performance**: Must maintain fast shell startup times
5. **Backwards compatibility**: Support for older versions is not a priority

## Future Enhancements

1. **Additional Tool Integration**: Support for more development tools
2. **Automation**: Further automate setup and maintenance tasks
3. **Enhanced Documentation**: More comprehensive guides and examples
4. **Cross-platform**: Better support for Linux and other platforms
5. **Testing**: Add automated testing for configurations
# AI Integration PRD

## Overview

This Product Requirements Document (PRD) defines the specifications for the AI integration system within the dotfiles repository. This system provides consistent, reusable components for leveraging Large Language Models (LLMs) in development workflows, following industry best practices as of May 2025.

## Goals

- Create a modular, composable AI tooling system
- Provide consistent interfaces across different LLM providers
- Optimize prompt design for efficiency and effectiveness
- Support multiple development workflows (coding, docs, testing)
- Enable programmatic access via shell and TypeScript
- Follow industry best practices for LLM utilization
- Support secure handling of credentials and context
- Maintain backward compatibility with existing tools

## Non-Goals

- Replacing specialized AI tools with custom implementations
- Supporting every possible LLM provider
- Handling training or fine-tuning of models
- Building a complete UI for AI interactions

## System Components

### 1. Directory Structure

```
config/ai/
├── core/                 # Core components and shared utilities
│   ├── config.yaml       # Central configuration (models, endpoints, etc.)
│   ├── constants.sh      # Shared constants for shell scripts
│   ├── types.ts          # TypeScript type definitions
│   └── utils.{sh,ts}     # Shared utility functions
├── providers/            # Provider-specific implementations
│   ├── anthropic/        # Anthropic/Claude specific configuration
│   ├── openai/           # OpenAI specific configuration
│   └── local/            # Local model configurations (Ollama, etc.)
├── prompts/              # Reusable prompt components
│   ├── base/             # Base prompt templates
│   ├── coding/           # Code-related prompts
│   ├── testing/          # Test generation prompts
│   └── docs/             # Documentation prompts
├── tools/                # CLI tools and integrations
│   ├── git/              # Git integrations
│   ├── ide/              # IDE integrations
│   └── api/              # API development tools
├── interfaces/           # Interface definitions and clients
│   ├── cli/              # Command-line interfaces
│   ├── ts/               # TypeScript library
│   └── bash/             # Bash utilities
└── templates/            # Output templates
    ├── code/             # Code templates
    ├── docs/             # Documentation templates
    └── configs/          # Configuration templates
```

### 2. Core Features

#### 2.1 Provider-Agnostic Interface

- Consistent command interface across providers
- Abstraction layer for different APIs
- Automatic fallback mechanisms
- Environment-aware provider selection

#### 2.2 Prompt Engineering System

- Modular, composable prompt components
- Versioned prompt templates
- Context-aware prompt generation
- Runtime prompt optimization

#### 2.3 Development Workflows

- Code generation and completion
- Code review and improvement
- Test generation
- Documentation generation
- Commit message generation
- API design and implementation

#### 2.4 Tool Integrations

- Git hooks and commands
- IDE integration (VS Code, Neovim)
- API tools (OpenAPI, client generation)
- Shell enhancements

#### 2.5 Configuration Management

- Environment-aware configuration
- Secure credential handling
- Provider-specific settings
- User preference management

## Technical Requirements

### 1. API Standards

- All API calls must implement proper error handling
- Rate limit compliance and retry mechanisms
- Secure token handling (no hardcoded tokens)
- Streaming support where applicable
- Proper timeout handling

### 2. Prompt Engineering

- All prompts must follow the OpenAI cookbook guidelines
- Prompts should be modular and composable
- Prompt versions should be tracked
- Prompts should include clear instructions and constraints
- System prompts should be separate from user prompts

### 3. Code Architecture

- TypeScript library must use proper types
- Shell scripts must follow best practices (ShellCheck compliance)
- Functions should be focused and reusable
- Proper logging and error reporting

### 4. Testing

- Unit tests for TypeScript components
- Integration tests for end-to-end workflows
- Prompt regression testing system
- Performance benchmarking for optimization

### 5. Security

- No credentials in code repositories
- Secure environment variable handling
- Proper input validation
- Content filtering for generated outputs

## Implementation Phases

### Phase 1: Core Infrastructure

- Define directory structure and interfaces
- Implement provider abstraction layer
- Create basic prompt templates
- Set up configuration system

### Phase 2: Tool Integration

- Implement Git integration
- Create IDE plugins/tools
- Develop API development tools
- Build shell utilities

### Phase 3: Advanced Features

- Implement prompt optimization
- Add multi-provider orchestration
- Create advanced workflow tools
- Performance optimization

## Success Metrics

- Reduction in development time for common tasks
- Improved code quality through AI assistance
- Consistent experience across different providers
- Modularity and reusability of components
- Performance and reliability of AI integrations

## Appendix

### A. Example Workflows

#### A.1 Code Generation

```bash
$ just ai:code typescript "Create a React component that displays a list of items with pagination"
```

#### A.2 Commit Message Generation

```bash
$ just ai:commit-msg
```

#### A.3 API Client Generation

```bash
$ just api:generate-client path/to/spec.yaml output-dir
```

### B. Provider Support Matrix

| Provider   | Models                   | Features               | Best For                  |
|------------|--------------------------|------------------------|---------------------------|
| Anthropic  | Claude 3 Opus/Sonnet/etc | Strong reasoning      | Complex coding tasks     |
| OpenAI     | GPT-4/3.5               | Tool use, function calls| RAG, tool integration   |
| Gemini     | Gemini Pro/Ultra        | Large context window   | Documentation, analysis  |
| Local      | Mixtral, Llama 3, etc.   | Privacy, offline      | Quick tasks, prototyping |

### C. Prompt Engineering Guidelines

1. **Clarity**: Be specific and clear in instructions
2. **Context**: Provide necessary context for the task
3. **Examples**: Include examples for complex tasks
4. **Constraints**: Specify output format and limitations
5. **Roles**: Define clear roles for the model
6. **Iterative**: Design prompts to be iteratively improved
7. **Modular**: Break complex prompts into reusable components
# Repository Style Guide

This document establishes the consistent style, language, and design principles to be applied across all dotfiles configuration and documentation. It serves as a formal specification for maintaining consistency when modifying or extending the repository.

## Language and Terminology

### General Principles

1. **Clarity**: Use clear, direct language without unnecessary jargon
2. **Consistency**: Maintain consistent terminology across all files
3. **Precision**: Be specific about requirements and implementations
4. **Formality**: Use a professional, semi-formal tone in documentation
5. **Conciseness**: Be brief but complete, avoiding redundancy

### Standard Terminology

| Term | Definition | Usage |
|------|------------|-------|
| Tokyo Night | The primary color theme used across all tools | "Tokyo Night theme for consistent styling" |
| XDG | XDG Base Directory Specification | "Follow XDG compliance for config locations" |
| Apple Silicon | ARM-based Mac processors | "Optimized for Apple Silicon performance" |
| Dotfiles | Configuration files stored in version control | "Manage dotfiles with symlinks" |
| Symlink | Symbolic link to configuration files | "Symlink entire directories when possible" |
| Idempotent | Can be run multiple times without side effects | "Idempotent setup script" |

## File Structure and Organization

### Configuration Files

1. **Header Comments**: Begin each configuration file with a descriptive header
   ```lua
   -- Tool Configuration
   -- Description: Brief description of what this configures
   -- Last Updated: YYYY-MM-DD
   ```

2. **Section Organization**: Organize configurations into logical sections
   ```toml
   # Section 1: Core Settings
   setting1 = value1

   # Section 2: Feature Settings
   setting2 = value2
   ```

3. **Comments**: Add comments for non-obvious settings
   ```bash
   # Use 10ms timeout for faster prompt rendering
   scan_timeout = 10
   ```

### Documentation Files

1. **README.md Structure**:
   - Title and brief description
   - Features section with bullet points
   - Installation instructions
   - Configuration explanation
   - Usage examples
   - Customization options
   - Resources/References

2. **PRD.md Structure**:
   - Core Requirements (categorized)
   - Feature Requirements
   - Configuration Categories
   - Key Mappings (if applicable)
   - Configuration Principles
   - Customization Areas

3. **VERSION.md Structure**:
   - Version Information
   - Dependencies
   - Configuration Health
   - Debug Information
   - Command Line Tools
   - Just Integration
   - Recent Changes

## Coding Style

### Lua

1. **Indentation**: 2 spaces
2. **Line Length**: 100 characters maximum
3. **String Quotes**: Single quotes for strings
4. **Tables**: Use explicit table constructors with proper spacing
   ```lua
   local config = {
     option1 = 'value1',
     option2 = 'value2',
   }
   ```
5. **Function Style**: Prefer function expressions over declarations
   ```lua
   local function my_function(arg)
     -- implementation
   end
   ```
6. **Comments**: Use -- for comments, --- for documentation comments

### Shell Scripts

1. **Indentation**: 2 spaces
2. **Line Length**: 80 characters when possible
3. **String Quotes**: Double quotes for strings with variables, single quotes otherwise
4. **Function Style**: Use function keyword and braces on same line
   ```bash
   function my_function() {
     # implementation
   }
   ```
5. **Variable References**: Use `${variable}` style for clarity
6. **Error Handling**: Include error handling with set -e or trap
7. **Comments**: Use # for comments, add section separators with #####

### TOML

1. **Indentation**: 2 spaces
2. **Section Order**: Core settings first, then feature-specific sections
3. **Comments**: Use # for comments, add section separators
4. **Arrays**: One item per line for multi-line arrays
   ```toml
   array = [
     "item1",
     "item2",
     "item3",
   ]
   ```
5. **Tables**: Group related settings in tables

## Design Principles

These core design principles should be reflected across all configurations:

1. **Minimalism**: Keep configurations simple and focused
2. **Performance**: Optimize for speed and responsiveness
3. **Consistency**: Maintain consistent style and behaviors across tools
4. **Modularity**: Enable easy addition/removal of specific configurations
5. **Discoverability**: Make features and commands easy to discover
6. **Documentation**: Document everything thoroughly
7. **Maintainability**: Code should be clean and well-organized

## Visual Design

### Color Scheme

The Tokyo Night theme palette should be used consistently:

```toml
# TokyoNight Colors
bg = "#1a1b26"       # Background
fg = "#c0caf5"       # Foreground
black = "#15161e"    # Black
red = "#f7768e"      # Red
green = "#9ece6a"    # Green
yellow = "#e0af68"   # Yellow
blue = "#7aa2f7"     # Blue
magenta = "#bb9af7"  # Magenta
cyan = "#7dcfff"     # Cyan
white = "#a9b1d6"    # White
```

### Typography

1. **Terminal Font**: Nerd Font compatible monospace font
2. **Documentation Headers**: Use Markdown ATX headers (# for h1, ## for h2, etc.)
3. **Lists**: Use - for unordered lists, 1. for ordered lists
4. **Emphasis**: Use **bold** for important items, *italic* for emphasis
5. **Code Blocks**: Use triple backticks with language specification

### Icons and Symbols

1. **Success**: ✅ or ✓
2. **Failure/Warning**: ❌ or ⚠️
3. **Information**: ℹ️ or 🛈
4. **Note**: 📝
5. **Important**: ❗
6. **Link**: 🔗
7. **Settings**: ⚙️
8. **Tool**: 🛠️
9. **Theme**: 🎨
10. **Time**: ⏱️

## Version Control

### Commit Messages

1. **Structure**:
   ```
   Subject line (50 chars max)

   Detailed explanation of what and why (not how)
   - Bullet points for multiple changes
   - Keep lines to 72 characters
   ```

2. **Subject Line Style**:
   - Use imperative mood ("Add feature" not "Added feature")
   - Start with capital letter
   - No period at the end
   - Keep under 50 characters
   - Be specific about what changed

3. **Body Style**:
   - Explain what and why, not how
   - Use bullet points for multiple changes
   - Separate paragraphs with blank line
   - Reference issues if applicable

### Branch Names

1. **Feature Branches**: `feature/descriptive-name`
2. **Bug Fixes**: `fix/descriptive-name`
3. **Documentation**: `docs/descriptive-name`
4. **Refactoring**: `refactor/descriptive-name`

## Tool-Specific Guidelines

### Neovim

- Use LazyVim framework
- Organize plugins into logical categories
- Use Space as leader key
- Keep startup time below 100ms
- Follow Vim conventions for key mappings

### ZSH

- Keep startup time below 200ms
- Organize aliases by category with comments
- Use descriptive function names
- Follow consistent prompt style
- Keep shell scripts POSIX-compatible when possible

### Yazi

- Use Vim-style navigation
- Optimize worker counts for Apple Silicon
- Set file preview limits for performance
- Organize by file type
- Use consistent key mappings

### Aerospace

- Use Alt as primary modifier key
- Follow Vim-style directional navigation
- Use service mode for special operations
- Maintain consistent window gaps
- Keep workspace count at 5

### Starship

- Keep scan timeout at 10ms
- Place time and battery on right side
- Use two-line prompt with clean separation
- Show relevant language versions
- Use consistent icons for all sections

## Documentation Guidelines

1. **README Files**:
   - Focus on user-oriented information
   - Include installation, usage, customization
   - Show examples for common operations
   - Keep language simple and direct

2. **PRD Files**:
   - Focus on requirements and design decisions
   - Organize into logical categories
   - Include rationale for design choices
   - Be specific about implementation details

3. **VERSION Files**:
   - Focus on current state and dependencies
   - Include exact version numbers
   - Document any health issues
   - List recent changes

## Justfile Guidelines

1. **Recipe Naming**:
   - Use kebab-case for recipe names
   - Group related commands with common prefix
   - Use descriptive, action-oriented names
   - Keep names concise but clear

2. **Recipe Documentation**:
   - Add comments above each recipe
   - Explain purpose and any side effects
   - Document any required parameters
   - Group related recipes with section comments

3. **Recipe Implementation**:
   - Use @echo for user feedback
   - Use conditional execution when appropriate
   - Capture errors and provide feedback
   - Use variables for repeated values

## Adherence to This Guide

This style guide should be followed for all new additions and modifications to the dotfiles repository. When working with existing files, gradually update them to match this guide's specifications.

For automatic verification, consider using:
- `stylua` for Lua formatting
- `shfmt` for shell script formatting
- `markdownlint` for Markdown formatting
- `toml-lint` for TOML validation

When using LLMs to generate or modify files, reference this guide to ensure consistency.
# End-to-End MNR Workflow (PDF-handling)

This document explains, from request to result, how the **Medical Necessity Review** (MNR) workflow is implemented across the TypeScript *Gateway* and the Python *Workers* that run on brrr.

> **Scope**
> • TypeScript side: `app.ts`, `schemas.ts`, `utils/artifacts/index.ts`, `handlers/run-mnr-raw.ts`
> • Python side: `workflows/src/generic/tasks/*.py` (notably `mnr.py`, `convert_artifact.py`, etc.)
> • Orchestration: `TaskScheduler` ➜ SQS ➜ *prefect-agent* ➜ **brrr**

---

## 1. Public HTTP Surface (TypeScript)

### 1.1 Route definition
```130:170:platform/gateways/noggin/src/tasks/app.ts
app.endpoint({
  method: "POST",
  route: "/mnr/actions/run",
  body: z.object({
    stemId: z.string().optional(),
    artQueries: z.object({
      member: antQuerySchema.optional(),
      criteria: antQuerySchema,
      clinicals: z.array(antQuerySchema),
    }),
    serviceRequest: attachServicesToStemV11.shape.services.element,
  }),
  handler: async (ctx) => { /* … */ }
});
```

* **Validation** – Zod schemas ensure client input is well-formed.
* **Auth** – `parseAuthInfo(ctx)` extracts JWT claims (enterprise / workspace / user IDs).

### 1.2 Handler logic (excerpt)
```90:130:platform/gateways/noggin/src/tasks/app.ts
const originalClincials: string[] = [];
for (const c of clinicals) {
  const original = await getArtifactsFor(c, workspaceUid, ".orig");
  originalClincials.push(...original);
}
const gdt = (await getArtifactsFor(criteria, workspaceUid, ".gdt"))[0];
```
*Uses `utils/artifacts/index.ts` helpers to convert `ant://` URIs into real S3 keys and fetch specific versions of artifacts (PDF originals, GDT JSON, …).*
If all artifacts resolve ➜ call **`runMnrRaw`**.

---

## 2. Task Construction & Scheduling (TypeScript)

### 2.1 Runtime schema
```1:40:platform/gateways/noggin/src/tasks/schemas.ts
export const mnr_task = z.object({
  config: z.object({ /* workflo_meta, source_meta, workspace_config */ }),
  member_query: z.string().nullable(),
  service: service_request,
  clinicals_query: z.array(z.string()),
  gdt_query: z.string(),
});
export type MNRTask = Readonly<z.infer<typeof mnr_task>>;
```

### 2.2 Building the payload
```20:70:platform/gateways/noggin/src/tasks/handlers/run-mnr-raw.ts
await platform.mnrTasks.schedule({
  task_name: "execute_and_determine_mnr",
  config: {/* meta incl. trace_id */},
  member_query: artQueries.member ?? null,
  service: { /* simplified FHIR ServiceRequest */ },
  clinicals_query: artQueries.clinicals,
  gdt_query: artQueries.criteria,
});
```

### 2.3 TaskScheduler mechanics
```1:29:platform/lib/ts/lib-platform/src/tasks.ts
await this.queue.send(serialized); // serialized is {workflow_name, parameters}
```
`infra.taskQueue` is SQS in production; in-memory in tests.

---

## 3. Bridging to brrr (Python)

1. **prefect-agent** (`services/prefect-agent`) continuously *receives* SQS messages.
2. If `payload.workflow_engine == "brrr"` it calls
   ```py
   await brrr.schedule(payload.workflow_name, (), payload.parameters)
   ```
   but MNR still runs through **Prefect** → **brrr proxy** for now; the agent eventually issues the same `brrr.schedule` after Prefect flow routing.

3. Message is encoded with `AnteriorCodec` (tracing, error envelope) and placed on Redis stream that **brrr** workers listen to.

---

## 4. Worker-Side Execution (Python brrr tasks)

### 4.1 Entry task
```1:30:platform/workflows/src/generic/tasks/mnr.py
@brrr.task
async def execute_and_determine_mnr(
  config: Config,
  member_query: Query[Patient] | None,
  service: ServiceRequest,
  clinicals_query: list[ClinicalQuery],
  gdt_query: Query[GDT],
) -> Query[MNRDetermination]:
  execution_query = await execute_mnr(config, service, clinicals_query, gdt_query)
  return await create_determination(config, member_query, service, execution_query)
```

### 4.2 PDF pipeline
1. `convert_artifact` – If clinical source isn't already a PDF it calls **Gotenberg** to produce one, caches result in artifact store.
2. `extract_clinical` – Runs either *Reducto* or *Google Cloud Vision* to pull structured data; result saved as `extraction.json`.
3. `execute_mnr_kernel` – Domain logic combines FHIR ServiceRequest, ClinicalExtraction(s), and GDT to get an `Execution` tree.
4. `create_determination` – Uses LLM prompt templates to turn execution into a human-readable summary, stores `MNRDetermination` JSON.

### 4.3 Event emission
Workers use `config.save_event(StemLogV11(…))` to write to the stem-event bus (Datadog & UI streaming).

---

## 5. Artifact Path Convention (shared)
*See `utils/artifacts/index.ts` for helpers.*
```
artifacts/{workspaceUid}/{category}/{sourceId}/{artifactId}.{ext}
```
• PDFs live under `clinicals/<stemId>/<uuid>.raw.pdf`
• GDT under `criteria/<gdtId>/<gdtId>.gdt.json`
Helpers like `getArtifactsFor()` fetch variants by suffix (`.orig`, `.gdt`, …).

---

## 6. Contract between TS & Python

| Layer | Key                         | Value                                                     |
|-------|-----------------------------|-----------------------------------------------------------|
| TS → Py | `workflow_name`            | `execute_and_determine_mnr`                               |
| TS → Py | `parameters`              | Matches **`MNRTask`** Zod schema                          |
| Py side | Input parameters          | Pydantic-validated via `Config`, `Query[T]`, FHIR models  |
| Py → TS | Events / Artifacts        | `StemLogV11`, `MNRDetermination` stored & evented         |

Both languages rely on the **`ant://` URI** convention so neither side needs direct S3 credentials.

---

## 7. Flow Summary
1. **Client** POSTs to `/mnr/actions/run` with Ant URIs.
2. **Gateway** validates, resolves PDF originals & GDT, enqueues task.
3. **prefect-agent** forwards to **brrr**.
4. **Worker** converts / extracts PDFs, runs kernel, summarises.
5. **Artifacts & Events** surface back; UI subscribes to stem-events for progress & outcome.
# Use Cases

This directory contains domain-specific prompt templates organized by purpose or task type.

## Structure

- **Code_Generation/**: Templates for generating different types of code
- **Documentation/**: Prompts for creating technical documentation
- **Research/**: Templates for research synthesis and analysis
- **Data_Analysis/**: Prompts for data exploration and interpretation
- **Workflow_Automation/**: Templates for automating development workflows

## Key Components

- Task-specific prompt structures
- Domain-optimized templates
- Specialized output formats
- Context management techniques for different use cases

## Usage

These use case templates can be combined with core prompting techniques to create highly effective, purpose-specific prompts:

1. Identify the appropriate use case for your task
2. Select a template from the relevant directory
3. Customize the template with your specific requirements
4. Consider adding provider-specific optimizations if using a particular AI model

## Best Practices

- Adapt templates to your specific needs rather than using them verbatim
- Build upon successful patterns and document effective modifications
- Share domain-specific insights that improve template effectiveness
- Balance specificity with flexibility in your requirements
# Python Code Generation

## Task

Generate Python code that accomplishes the following:

{{request}}

## Requirements

- Write clean, idiomatic Python code (Python 3.8+)
- Follow PEP 8 style guidelines
- Include type hints (PEP 484)
- Add docstrings in Google or NumPy format
- Handle errors and edge cases appropriately
- Write testable and maintainable code
- Optimize for readability and clarity

## Python-Specific Guidelines

### Type Annotations

- Add type hints for function parameters and return values
- Use typing module's container types (List, Dict, Set, etc.)
- Consider using TypedDict for dictionaries with specific formats
- Use Optional[] for parameters that could be None
- Use Union[] for parameters with multiple possible types
- Use Protocol for structural typing when needed

### Functions and Classes

- Follow single responsibility principle
- Use descriptive function and variable names
- Make effective use of default parameters
- Use dataclasses for data containers when appropriate
- Implement proper magic methods in classes when needed
- Include type hints in class attributes and methods

### Documentation

- Include docstrings for all public modules, classes, and functions
- Use Google-style docstrings with Parameters, Returns, Raises sections
- Add examples for complex functions
- Note any assumptions or limitations

### Error Handling

- Use specific exception types rather than generic Exception
- Create custom exceptions for domain-specific errors
- Use context managers (with statements) for resource handling
- Implement proper try/except/finally blocks
- Add informative error messages

### Python Idioms

- Use list/dict/set comprehensions where appropriate
- Use generator expressions for large data processing
- Use tuple unpacking for multiple returns
- Use the walrus operator (:=) for assignment expressions (Python 3.8+)
- Use f-strings for string formatting
- Use pathlib for file path operations
- Use context managers for resource management

## Output Format

Provide the Python code with type hints and docstrings in a code block with the language tag. Include imports at the top, followed by class/function definitions, then implementation.

```python
# Your implementation will replace this line
```
# TypeScript Code Generation

## Task

Generate TypeScript code that accomplishes the following:

{{request}}

## Requirements

- Write idiomatic, modern TypeScript (ES2020+)
- Include proper TypeScript types and interfaces
- Follow TypeScript best practices
- Include appropriate error handling
- Add JSDoc comments for public functions and classes
- Follow consistent naming conventions
- Optimize for readability and maintainability

## TypeScript-Specific Guidelines

### Types and Interfaces

- Use explicit typing (avoid `any` type when possible)
- Prefer interfaces for object shapes and API contracts
- Use type aliases for unions, intersections, and complex types
- Use generics when creating reusable components
- Consider using utility types (Partial, Pick, Omit, etc.) where appropriate
- Use readonly properties when applicable

### Naming Conventions

- Use PascalCase for interfaces, types, classes, and enums
- Use camelCase for variables, functions, and methods
- Use ALL_CAPS for constants and static readonly properties
- Prefix interfaces for objects with 'I' only when necessary for clarity
- Prefix type parameters with 'T' (e.g., `<TItem>`)

### Functions and Methods

- Use arrow functions for callbacks and anonymous functions
- Use function declarations for named functions
- Specify return types explicitly
- Use parameter destructuring for objects
- Provide default parameters when applicable

### Error Handling

- Use typed error objects or custom error classes
- Implement proper try/catch/finally blocks
- Consider using Result/Either patterns for complex error cases
- Provide meaningful error messages

### Modern Features

- Use optional chaining (`?.`) and nullish coalescing (`??`)
- Use template literals for string interpolation
- Use object/array destructuring
- Use async/await for asynchronous code
- Use Map and Set collections when appropriate

### Code Organization

- Group related functions and types together
- Use namespaces or modules to organize code
- Separate interface declarations from implementations
- Export types and interfaces that will be used externally

## Output Format

Provide the TypeScript code with appropriate type definitions in a code block with the language tag. Include imports at the top, followed by type/interface definitions, then implementation.

```typescript
// Your implementation will replace this line
```
# Tools Integration

This directory contains resources for integrating AI capabilities with development tools, IDEs, and workflow automation systems.

## Structure

- CLI tools and configurations
- IDE integration settings
- Workflow automation scripts
- Cross-tool orchestration patterns

## Key Components

- Tool-specific configuration files
- Integration scripts and utilities
- AI-powered workflow automation
- Environment setup and configuration

## Usage

These resources help you integrate AI capabilities into your development environment:

1. Identify tools and integrations relevant to your workflow
2. Apply the provided configurations or scripts
3. Customize as needed for your specific environment
4. Combine multiple integrations for comprehensive AI assistance

## Best Practices

- Test integrations in isolation before combining them
- Document custom modifications for future reference
- Keep configurations under version control
- Regularly update settings as tools and AI capabilities evolve
# Advanced Prompting Techniques

This document outlines sophisticated AI prompting methods to achieve superior results for complex tasks. These techniques can be combined with task-specific templates for maximum effectiveness.

## Chain of Thought Prompting

### Description
Guide the AI through explicit reasoning steps before reaching a conclusion. This produces more accurate results for complex reasoning tasks.

### Implementation
```markdown
# Chain of Thought Request

I need you to solve this problem step-by-step:

[Problem description]

Please:
1. First, identify the key elements of this problem
2. Break down the approach into clear logical steps
3. Execute each step, showing your work
4. Evaluate the result for correctness
5. Summarize the final answer

Format each step clearly and explain your reasoning at each stage.
```

### When to Use
- Complex logical problems
- Multi-step calculations
- Debugging scenarios
- Complex decision-making with multiple factors

## Recursive Refinement

### Description
Use a staged approach where the AI first generates a draft, then iteratively improves it through self-critique and revision.

### Implementation
```markdown
# Recursive Refinement Request

I need a [type of content] about [subject]. Please approach this in three stages:

## Stage 1: Initial Draft
Create a first version that covers the core elements.

## Stage 2: Self-Critique
Review your draft and identify:
- Areas that need more detail or clarity
- Potential inaccuracies or omissions
- Structural improvements
- Stronger examples or evidence

## Stage 3: Refined Version
Create an improved version that addresses all the issues identified in your critique.

Present both the critique and the final refined version.
```

### When to Use
- Complex writing tasks
- System designs that require optimization
- Code that needs to balance multiple objectives
- Analyses where accuracy is critical

## Persona-Based Prompting

### Description
Direct the AI to adopt specific expert perspectives to leverage specialized knowledge and approaches.

### Implementation
```markdown
# Expert Perspective Request

I need analysis on [topic] from multiple expert perspectives.

Please approach this sequentially as the following experts:

1. As a senior systems architect with 15+ years of experience:
   [Specific questions/aspects to address]

2. As a security specialist focused on zero-trust implementations:
   [Specific questions/aspects to address]

3. As a DevOps engineer who prioritizes maintainability and observability:
   [Specific questions/aspects to address]

4. As a synthesis expert, integrate the perspectives above into final recommendations.

For each perspective, note key insights, blind spots, and specific recommendations.
```

### When to Use
- Getting well-rounded analysis of complex problems
- Identifying potential issues from different viewpoints
- Challenging initial assumptions
- Complex design decisions with multiple stakeholders

## Template Decomposition

### Description
Break complex outputs into parameterized templates with clear placeholders for the AI to fill in.

### Implementation
```markdown
# Template Decomposition Request

I need a [document type] with the following structure. For each section, I've provided guidance on what to include:

# [TITLE: Create a concise, descriptive title]

## Executive Summary
[EXEC_SUMMARY: Provide a 3-5 sentence overview that captures the key points]

## Problem Statement
[PROBLEM: Describe the current situation, challenges, and why this needs to be addressed]

## Proposed Approach
[APPROACH: Detail the recommended solution with justification]

## Implementation Steps
[STEPS: List 5-7 concrete actions in chronological order with owners and timeline]

## Success Metrics
[METRICS: Define 3-5 measurable indicators of success]

Please complete each section according to the instructions in brackets, then remove the bracketed instructions from the final output.
```

### When to Use
- Standardized document creation
- Complex outputs with consistent structure
- When you need to ensure all components are included
- Content that follows established patterns or protocols

## Comparative Analysis Framework

### Description
Structure a detailed comparison across multiple dimensions with explicit evaluation criteria.

### Implementation
```markdown
# Comparative Analysis Request

Please evaluate these options against the specified criteria:

## Options to Evaluate
- [Option 1]
- [Option 2]
- [Option 3]

## Evaluation Criteria
For each criterion, rate options on a scale of 1-10 and provide specific justification:

1. [Criterion 1] (Weight: X%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

2. [Criterion 2] (Weight: Y%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

3. [Criterion 3] (Weight: Z%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

## Output Format
- Provide individual criterion scores with justification
- Calculate weighted total scores
- Rank options from best to worst
- Explain key differentiators
- Include sensitivity analysis (how would rankings change if weights shifted)
```

### When to Use
- Technology selection decisions
- Architecture or design alternatives
- Investment or prioritization decisions
- Strategic option evaluation

## Decision Tree Construction

### Description
Map out a complex decision process with conditional branches based on different scenarios.

### Implementation
```markdown
# Decision Tree Request

Help me build a decision tree for [scenario/problem].

## Initial Conditions
- [Describe starting state and key variables]

## Primary Decision Points
For each primary decision point:
1. Identify the key question at this stage
2. List all possible choices (2-4 options)
3. For each choice:
   - Describe immediate consequences
   - Identify subsequent decision points
   - Note critical dependencies or assumptions

## Evaluation Metrics
At each terminal node (end state), evaluate:
- [Metric 1]
- [Metric 2]
- [Metric 3]

## Visual Representation
Create a text-based diagram of the decision tree showing:
- Decision nodes (questions)
- Option branches
- Terminal nodes (outcomes)
- Critical path highlighting
```

### When to Use
- Complex decision processes with multiple variables
- Scenario planning
- Risk assessment
- Troubleshooting guides

## System Interaction Modeling

### Description
Model complex interactions between system components, focusing on interfaces, dependencies, and data flows.

### Implementation
```markdown
# System Interaction Model Request

Help me understand interactions between components in [system name].

## System Components
- [Component 1]: [Brief description of responsibility]
- [Component 2]: [Brief description of responsibility]
- [Component 3]: [Brief description of responsibility]

## For Each Interface Between Components
1. Identify direction of dependency
2. Describe data/control flow
3. Document interface contract (API, events, etc.)
4. Note synchronicity (sync/async)
5. Identify potential failure modes
6. Document retry/resilience patterns

## Sequence Diagrams
Create text-based sequence diagrams for these key flows:
- [Flow 1]
- [Flow 2]
- [Flow 3]

## Critical Path Analysis
Identify and analyze:
- Performance bottlenecks
- Single points of failure
- Scalability constraints
- Data consistency challenges
```

### When to Use
- Complex distributed system design
- API design and documentation
- Troubleshooting integration issues
- Performance optimization

## Layered Explanation

### Description
Structure explanations across multiple levels of abstraction, from conceptual to implementation details.

### Implementation
```markdown
# Layered Explanation Request

Please explain [topic/concept] using a layered approach:

## Layer 1: Conceptual Overview (ELI5)
Explain the core concept as you would to a non-technical person, using analogies and simplified models.

## Layer 2: Functional Understanding
Explain how this works from a functional perspective - what it does, major components, and how they interact.

## Layer 3: Technical Implementation
Provide implementation details including:
- Specific technologies/methods used
- Important algorithms or patterns
- Code examples where appropriate
- Performance considerations

## Layer 4: Advanced Considerations
Discuss:
- Edge cases and how they're handled
- Optimization techniques
- Alternative approaches
- Common misconceptions
```

### When to Use
- Explaining complex technical concepts
- Educational content
- Technical documentation for diverse audiences
- Knowledge transfer

## Edge Case Exploration

### Description
Systematically identify and address uncommon but important scenarios that might be overlooked.

### Implementation
```markdown
# Edge Case Exploration Request

For this [system/code/process], help me identify and address edge cases:

## System Description
[Brief description of the system/code/process]

## Normal Operation
[Description of typical usage patterns and expected behavior]

## Edge Case Categories
For each category, identify specific edge cases and how to handle them:

1. Input Boundaries
   - Minimum/maximum values
   - Empty/null inputs
   - Malformed inputs
   - Extremely large inputs

2. Resource Constraints
   - Memory limitations
   - CPU constraints
   - Network issues
   - Timeouts

3. Concurrent Operations
   - Race conditions
   - Deadlocks
   - Contention scenarios

4. Integration Points
   - External system failures
   - API contract changes
   - Versioning issues

5. State Transitions
   - Interrupted operations
   - Invalid state sequences
   - Recovery scenarios

## For Each Edge Case
1. Describe the scenario
2. Explain potential impact
3. Recommend detection method
4. Provide handling strategy
5. Suggest testing approach
```

### When to Use
- Critical system design
- Security-sensitive applications
- High-reliability requirements
- Complex state management scenarios
# Hank Artistic Style

---

**1. Overall Feeling & Mood (Aesthetics)**

- **What this means:** This describes the general vibe or emotional response your preferred style creates. Does it feel calm, energetic, serious, luxurious, simple, or playful?
- **Your Preference:** You lean towards designs that feel **Elegant, Sophisticated, Calm, and Modern.**There's a clear desire for **Refinement** – meaning polished and classy – and a sense of **Understated Luxury** (feels expensive without being flashy).
- **Avoids:** Styles that are overly loud, busy, chaotic, overly decorated, cute, or strictly casual.

---

**2. Simplicity vs. Complexity (Detail Level)**

- **What this means:** This is about how much visual information is in the design. Is it "less is more" with only the essential parts (Minimalist), or does it have lots of details, patterns, and elements (Complex)?
- **Your Preference:** You have a strong preference for **Minimalism and Simplicity.** Designs should look clean, clear, and uncluttered. The focus is on the main elements without extra fluff or decoration.
- **Avoids:** Visual clutter, excessive ornamentation, designs where too many things are competing for attention.

---

**3. Shape & Form (Geometry)**

- **What this means:** This looks at the kinds of lines and shapes used. Are they mostly curved and flowing (like handwriting), or straight and angular (like blocks)? Are shapes precise and mathematical (Geometric) or more irregular and natural (Organic)?
- **Your Preference:** You favor **Flowing Curves and Smooth Lines,** similar to those found in elegant handwriting or script. When simpler, straighter lines are used, they tend to be very clean and precise, often combined in unique ways or with subtle curves. There's an appreciation for **Integrated Forms,**where different parts connect or overlap smoothly to feel like a single unit.
- **Avoids:** Sharp, aggressive angles (unless used minimally in a very clean design), irregular or messy shapes.

---

**4. Color Palette (Color Theory)**

- **What this means:** This is about the choice and combination of colors. Does the style use lots of bright colors, only a few quiet ones, or mostly black, white, and gray?
- **Your Preference:** You strongly prefer **Neutral and Monochromatic** color schemes. This means designs built primarily around **Black, White, and Shades of Gray.** Deep, muted colors (like very dark blue, green, or burgundy) can also fit this preference. **High Contrast** (like pure white on deep black) is often appealing.
- **Accents:** Subtle hints of **Metallic Tones** (like gold, silver, or copper) are acceptable for adding a touch of luxury.
- **Avoids:** Bright, neon, or highly saturated colors; complex palettes with many different colors used together.

---

**5. Typography & Text (Text Style)**

- **What this means:** Even outside of logos, this relates to the style of any letters or text used. Does it look handwritten, formal, modern, traditional, bold, or delicate?
- **Your Preference:** You appreciate **Elegant Script or Cursive** fonts for their personal and flowing feel. You also like **Clean, Modern Sans-Serif** fonts (simple letters without "feet"), _especially_ if they have unique design details or connect letters in interesting ways. Text should generally feel refined and well-integrated.
- **Avoids:** Very bold, blocky, or overly decorative fonts unless they fit the overall minimalist and elegant aesthetic.

---

**6. Layout & Composition (Arrangement)**

- **What this means:** This describes how elements (text, shapes, images) are arranged. Is it balanced and symmetrical (same on both sides), or intentionally unbalanced (Asymmetrical)? Is there lots of empty space (Spacious), or is it tightly packed (Dense)?
- **Your Preference:** You prefer **Focused and Balanced** compositions. This balance is often **Asymmetrical**– meaning it's not a perfect mirror image, but still feels visually stable and interesting. There's a strong appreciation for **"White Space"** (empty areas), which helps the main elements stand out and creates a feeling of calm and sophistication.
- **Avoids:** Cluttered, dense layouts where elements feel crowded.

---

**7. Material & Texture (Tactile Quality)**

- **What this means:** This relates to the perceived surface quality. Even in a digital image, does the design suggest smoothness, roughness, richness, or perhaps a specific material like paper, metal, or fabric?
- **Your Preference:** Your choices suggest an appreciation for **Premium Materials and Tactile Qualities.**This is hinted at by the common use of effects that mimic **Embossing** (raised design), **Debossing**(impressed design), or high-quality, textured paper in the examples. It points towards a preference for things that convey **Quality, Craftsmanship, and a sense of substance.**

---

This document provides a foundational guide to your artistic preferences, derived from your logo choices. You can use it to explore similar styles, guide creative projects, or provide clearer instructions to AI tools for generating content aligned with your taste.
# AI Interaction Preferences

## Profile Context
Senior software engineer specialized in systems, AI infrastructure, and developer tooling. Working in TypeScript/Python ecosystem with keyboard-driven workflows (Vim/Neovim).

## Response Structure
- Use clear hierarchical headers for easy navigation
- Break down complex processes into steps
- Include diagrams for architecture/workflows
- Format code blocks with proper syntax highlighting
- Create tables for comparisons (5-7 criteria)

## Analysis Requirements
- Show structured reasoning process
- Clearly state assumptions and constraints
- Provide evidence-based recommendations
- Include explicit tradeoff analysis
- Balance technical precision with clarity
- Focus on implementation over theory

## Comparison Method
When comparing alternatives:
1. Identify 5-7 crucial criteria from modern best practices
2. Score each option 1-10 for each criterion
3. Apply appropriate weights to criteria
4. Present results in descending order
5. Include scoring rationale
6. Explain top choice selection

Always prioritize clarity, implementation details, and visual organization in responses.
# Developer Profile

## Core Identity
- Senior software engineer specializing in systems, AI infrastructure, and developer tooling
- Background in distributed systems, data engineering, and systems architecture
- Preference for intelligent, reproducible, and automated solutions
- Values high-quality implementations with minimal technical debt

## Technical Environment

### Hardware/OS
- Apple Silicon Mac (primary development environment)
- Multi-machine synchronization requirements
- Local-first architecture preferences

### Development Environment
- **Package Management**: Nix, Homebrew, uv (Python)
- **Configuration**: Git-tracked dotfiles, declarative configurations
- **Editor Ecosystem**: Neovim (LazyVim), Cursor IDE
- **Terminal**: Ghostty with ZSH
- **Window Management**: Raycast, Swish, Yabai, Hammerspoon

### Technical Stack
- **Primary Languages**:
  - TypeScript (eslint + prettier)
  - Python (ruff for linting)
  - Exposure to Rust, Go, and other systems languages
- **Tooling Preferences**:
  - Modern CLI utilities (ripgrep, fd, zoxide, atuin, starship)
  - AI-augmented development workflows
  - Keyboard-driven interfaces with minimal mouse usage

## Workflow Characteristics

### General Approach
- Preference for keyboard-driven, efficient workflows
- Strong Vim expertise and modal editing proficiency
- Value tools that reduce cognitive load and automate routine tasks
- Emphasis on reproducibility and declarative configuration

### Technical Values
- Intelligence/capability prioritized over resource usage
- Clear, maintainable code valued over cleverness
- Preference for functional programming patterns where appropriate
- Appreciation for elegant solutions that balance simplicity and power

### Learning Style
- Preference for bottom-up understanding with comprehensive examples
- Value explanations that balance theory and practical application
- Appreciate insights into underlying mechanisms and design decisions
- Learn best through hands-on implementation after conceptual understanding

## Communication Preferences

### Technical Documentation
- Structured, hierarchical organization with clear navigation
- Balanced detail: comprehensive without overwhelming
- Code examples that demonstrate real-world usage
- Visual aids for complex relationships and architectures

### Decision Processes
- Evidence-based recommendations with explicit reasoning
- Clear articulation of tradeoffs and constraints
- Preference for quantitative evaluation when possible
- Appreciation for multiple perspectives before conclusions

## AI Collaboration Context

### Interaction Mode
- Direct, efficient communication without unnecessary niceties
- Preference for precise, technical language
- Appreciation for both depth (when needed) and brevity (when appropriate)

### Output Preferences
- Structured, scannable content with clear section demarcation
- Visual representations for complex relationships
- Code that balances readability and efficiency
- Practical, implementable recommendations

### Follow-up Style
- Iterative refinement rather than extended initial responses
- Focused questions to clarify ambiguities
- Building on previous context rather than repetition
# Etc

## Development Environment Setup

### Base Requirements
- Apple Silicon Mac
- Nix package management
- Dotfiles tracked in Git
- Multi-machine sync
- Declarative configuration

### Core Tools
- Neovim + LazyVim
- Cursor IDE
- Ghostty terminal
- Modern CLI utilities
  - ripgrep
  - fd
  - zoxide
  - atuin
  - starship

### AI Integration
- GitHub Copilot
- Claude API
- Local models (Ollama)
- IDE integrations

### Priority Considerations
- Intelligence/capability first
- Resource usage secondary
- Keyboard-driven workflow
- Local-first when possible
- Data portability essential


## Code Solution Requirements
1. Start with ELI5 conceptual explanation (plain language a child could understand)
2. Compare approaches briefly (e.g., brute force vs. optimized)
3. Provide minimal, efficient implementation with focus on readability
4. Use modern language features (Python 3.10+: dataclasses, itertools, collections, etc.)
5. Include step-by-step example walkthrough
6. Analyze complexity and tradeoffs (time/space complexity, edge cases)
7. Format with descriptive comments and type hints

## Cursor

Provide concise, evidence-based responses with professional yet approachable tone. Prioritize key findings and visualize data when beneficial. Include credible citations.

For complex problems:
- Show step-by-step reasoning
- Break down large tasks
- Ask clarifying questions
- Explain thought process in feedback
- Create visual diagrams for relationships/flows

When comparing options:
- Use 5-7 crucial criteria from modern best practices
- Score each 1-10
- Weight and sum scores
- Present in descending order
- Include scoring rationale
- Explain top choice selection

### Response Format Preferences

Responses should integrate evidence, visuals, systematic analysis, and practical recommendations while considering real-world constraints.

#### Structure

- Clear hierarchy with headers
- Step-by-step for processes
- Tables for comparisons (5-7 criteria)
- Code blocks for technical content
- Diagrams for complex concepts

#### Content

- Evidence-based recommendations
- Explicit tradeoff analysis
- Implementation examples
- Configuration samples
- Resource requirements

#### Style
- Technical precision
- Direct communication
- Concrete over theoretical
- Clear rationale included
- Focus on implementation



## Development Environment Setup

### Base Requirements
- Apple Silicon Mac
- Nix package management
- Dotfiles tracked in Git
- Multi-machine sync
- Declarative configuration

### Core Tools
- Neovim + LazyVim
- Cursor IDE
- Ghostty terminal
- Modern CLI utilities
  - ripgrep
  - fd
  - zoxide
  - atuin
  - starship

### AI Integration
- GitHub Copilot
- Claude API
- Local models (Ollama)
- IDE integrations

### Priority Considerations
- Intelligence/capability first
- Resource usage secondary
- Keyboard-driven workflow
- Local-first when possible
- Data portability essential

## Technical Interview Context

### Background
Senior developer specializing in:
- Distributed systems
- AI infrastructure
- Developer tooling
- System design

### Response Requirements
1. Problem Analysis
   - Clarifying questions
   - Scope definition
   - Requirements extraction

2. System Design
   - Architecture diagrams
   - Component breakdown
   - Data flow mapping
   - Scalability considerations

3. Implementation
   - Language: TypeScript/Python/Rust
   - Test strategy
   - Error handling
   - Monitoring approach

4. Discussion Points
   - Tradeoff analysis
   - Alternative approaches
   - Performance considerations
   - Failure scenarios
# AI Master Configuration

## System Overview

This document serves as the central configuration for all AI interactions. The complete system consists of:

1. **Master Configuration** (this file) - Core principles and system overview
2. **Developer Profile** - Your personal context and technical preferences
3. **Response Formatting** - Output structure and visual presentation guidelines
4. **Task Templates** - Purpose-specific prompt templates for different scenarios
5. **Advanced Techniques** - Advanced prompting methods for complex tasks
6. **Tool-Specific Configs** - Model-specific optimization guidelines

## Core Principles

1. **Maximum Context Efficiency**
   - Provide just enough context for optimal output
   - Use reference links to previous conversations when building on past work
   - Clearly separate instructions from content

2. **Structured Output by Default**
   - All responses should follow consistent structural patterns
   - Visual elements should be used where they enhance clarity
   - Complex information should be organized in digestible segments

3. **Progressive Enhancement**
   - Start with base prompts and add specific modules as needed
   - Build complex prompts by combining simpler templates
   - Refine outputs through iterative instructions

4. **Tool Specialization**
   - Use optimal AI model for each specific task
   - Leverage unique capabilities of different AI systems
   - Chain tools together for complex workflows

## Usage Methodology

1. **Task Analysis**
   - Determine primary goal of interaction
   - Identify relevant template modules
   - Consider required output formats

2. **Prompt Assembly**
   - Include developer profile (abbreviated if appropriate)
   - Add task-specific template
   - Specify response format requirements
   - Include any specialized techniques

3. **Response Management**
   - Evaluate output quality against requirements
   - Use follow-up prompts for refinement
   - Extract and store reusable components

4. **Continuous Improvement**
   - Note effective prompt patterns
   - Update templates based on successful interactions
   - Develop specialized templates for recurring tasks

## Integration Framework

These templates support integration with:

- **Claude Models** (Sonnet, Opus, Haiku)
- **OpenAI Models** (GPT-4, GPT-4o)
- **DeepSeek Models** (DeepSeek Coder)
- **Open Source Models** (via Ollama integration)
- **AI-powered Development Tools** (Cursor, GitHub Copilot)
- **Document Processing Tools** (for export/import)

## Template Evolution Strategy

For maximum effectiveness, treat these templates as living documents:

1. When an AI generates exceptionally good output, capture the prompt structure
2. When complex tasks are completed successfully, document the prompt chain
3. When outputs require significant refinement, note the refinement pattern
4. Regularly update templates with new capabilities as AI models evolve

## Quick Reference

| Task Type | Primary Template | Secondary Templates | Optimal AI Model |
|-----------|------------------|---------------------|------------------|
| Code Generation | `coding-primary.md` | `visual-code.md`, `advanced-reasoning.md` | DeepSeek Coder/Claude Sonnet |
| Code Review | `code-analysis.md` | `structured-feedback.md` | Claude Sonnet w/Extended Thinking |
| System Design | `system-design.md` | `visual-architecture.md` | Claude Opus/GPT-4 |
| Data Analysis | `data-analysis.md` | `visual-data.md` | Claude Sonnet |
| Technical Writing | `technical-docs.md` | `doc-formatting.md` | Claude Sonnet |
| Research | `research-synthesis.md` | `citation-format.md` | GPT-4o with browsing |
# Project coding standards

## TypeScript Guidelines
- Use TypeScript for all new code
- Follow functional programming principles where possible
- Use interfaces for data structures and type definitions
- Prefer immutable data (const, readonly)
- Use optional chaining (?.) and nullish coalescing (??) operators

## React Guidelines
- Use functional components with hooks
- Follow the React hooks rules (no conditional hooks)
- Use React.FC type for components with children
- Keep components small and focused
- Use CSS modules for component styling

## Naming Conventions
- Use PascalCase for component names, interfaces, and type aliases
- Use camelCase for variables, functions, and methods
- Prefix private class members with underscore (_)
- Use ALL_CAPS for constants

## Error Handling
- Use try/catch blocks for async operations
- Implement proper error boundaries in React components
- Always log errors with contextual information
# AI Prompts (Best Practices & Resources)

## General Best Practices

### Prompt Engineering Fundamentals

1. **Be Specific and Explicit**
   - Provide clear context, constraints, and expected outputs
   - Example: "Write a Python function that sorts a list of dictionaries by the 'date' key in ISO format (YYYY-MM-DD)"

2. **Use Structured Formats**
   - Bullet points for requirements
   - Numbered lists for sequential steps
   - Headers for organizing complex prompts
   - Example: "Requirements: • Must handle null values • Must be thread-safe • Must complete in O(n) time"

3. **Iterate and Refine**
   - Start with a basic prompt, then refine based on results
   - Save effective prompts as templates for reuse
   - Example follow-up: "The solution works but needs better error handling for invalid inputs"

4. **Leverage System Messages**
   - Set persistent instructions in system prompts or chat settings
   - Define roles and output preferences clearly
   - Example: "You are an expert Python developer focusing on clean, maintainable code with comprehensive error handling"

## Domain-Specific Strategies

### Coding Excellence

1. **Problem Decomposition**
   - Break complex problems into specific components
   - Request system design before implementation
   - Example: "First outline the architecture for this authentication system, then we'll implement each component"

2. **Request Multiple Approaches**
   - Ask for alternative implementations with tradeoffs
   - Example: "Show me two different approaches to solving this problem - one optimized for readability, one for performance"

3. **Pair Programming Flow**
   - Use AI as a pair programming partner
   - Alternate between generating code and reviewing/refactoring
   - Example: "Let's implement this feature step by step. First, the user authentication logic..."

4. **Test-Driven Development**
   - Request tests before or alongside implementation
   - Example: "Write unit tests for this function before implementing it"

5. **Debug Collaboratively**
   - Share error messages and stack traces verbatim
   - Provide context on what you've already tried
   - Example: "I'm getting this error when running the code: [paste error]. I've tried X and Y without success"

### Research & Search Optimization

1. **Question Refinement**
   - Start broad, then narrow through follow-ups
   - Example: "What are modern approaches to database sharding?" → "How would these sharding techniques apply to time-series data?"

2. **Comparative Analysis Requests**
   - Ask for structured comparisons of alternatives
   - Example: "Compare Redis, MongoDB, and PostgreSQL for storing session data with these criteria: scalability, persistence, query complexity, operational overhead"

3. **Synthesis Requests**
   - Request summary of complex topics with key takeaways
   - Example: "Summarize the key developments in transformer architecture since 2020, focusing on efficiency improvements"

4. **Knowledge Stacking**
   - Build on previous responses with targeted follow-ups
   - Example: "Based on these database options, what would be the implementation challenges for our specific traffic pattern?"

### Data Analysis Workflows

1. **Analysis Planning**
   - Request analysis approach before execution
   - Example: "What analysis approach would you recommend for detecting seasonal patterns in this customer data?"

2. **Visualization Guidance**
   - Ask for specific visualization recommendations
   - Example: "What would be the best visualization approach to show the relationship between these three variables?"

3. **Interpretation Assistance**
   - Share data summaries for AI interpretation
   - Example: "Here are the key statistics from my analysis. What patterns or anomalies stand out to you?"

4. **Hypothesis Testing**
   - Use AI to generate testable hypotheses
   - Example: "Based on these initial findings, what hypotheses should we test to understand the drop in conversion rate?"

### Email & Communication Mastery

1. **Audience-Focused Templates**
   - Create templates for different audiences (technical, executive, client)
   - Example: "Draft an email explaining this technical issue to a non-technical client"

2. **Tone Calibration**
   - Specify desired tone and provide examples
   - Example: "Write a response to this customer complaint that's empathetic but maintains boundaries"

3. **Content Structuring**
   - Request specific structures for complex communication
   - Example: "Structure this project update with: accomplishments, challenges, next steps, and resource needs"

4. **Response Generation**
   - Provide email context and request response options
   - Example: "Here's an email from a client asking for rush delivery. Generate three response options ranging from accommodating to firmly maintaining our timeline"

## Advanced Techniques

### Context Window Optimization

1. **Reference Compression**
   - Summarize long code/documents before reference
   - Example: "Here's a summary of our 2000-line codebase: [concise description]. Now help me implement a new feature that..."

2. **Selective Information Sharing**
   - Share only relevant portions of large documents
   - Example: "From our API docs, here are the relevant endpoints: [paste relevant sections only]"

3. **Continuation Chaining**
   - Use explicit markers for continuing work across context limits
   - Example: "This is part 2 of our database schema design. Continuing from where we left off with the user authentication tables..."

### Multi-Tool Integration

1. **Tool Specialization**
   - Use different AI models for their strengths
   - Example: "I'll use Claude for system design, GitHub Copilot for implementation, and GPT-4 for documentation"

2. **Workflow Automation**
   - Create scripts or shortcuts for common AI interactions
   - Example: Shell aliases for common AI commands or VS Code snippets for prompt templates

3. **Output Processing**
   - Use one AI to review/improve another's output
   - Example: "Review this code generated by GitHub Copilot for security vulnerabilities and performance issues"

## Essential Resources

### Learning Resources

1. **Documentation & Guides**
   - [Anthropic's Claude Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)
   - [OpenAI Cookbook](https://cookbook.openai.com/)
   - [Prompt Engineering Guide](https://www.promptingguide.ai/)
   - [Learn Prompting](https://learnprompting.org/)

2. **Communities**
   - [r/PromptEngineering](https://www.reddit.com/r/PromptEngineering/)
   - [Hugging Face Community](https://huggingface.co/spaces)
   - [AI Discord communities](https://discord.com/invite/openai)

### Tools & Extensions

1. **Coding Enhancement**
   - [GitHub Copilot](https://github.com/features/copilot)
   - [Cursor IDE](https://cursor.sh/)
   - [Codeium](https://codeium.com/)
   - [Tabnine](https://www.tabnine.com/)

2. **Prompt Management**
   - [Prompto](https://prompto.chat/)
   - [ShareGPT](https://sharegpt.com/)
   - [Dust](https://dust.tt/)

3. **Workflow Integration**
   - [Warp Terminal](https://www.warp.dev/) (AI-assisted terminal)
   - [Continue](https://continue.dev/) (VS Code AI coding assistant)
   - [Raycast AI](https://www.raycast.com/ai) (macOS AI integration)

4. **Specialized Tools**
   - [Replit GhostWriter](https://replit.com/site/ghostwriter) (AI pair programmer)
   - [Phind](https://www.phind.com/) (AI search engine for developers)
   - [Mem](https://mem.ai/) (AI-powered note-taking)
   - [Otter.ai](https://otter.ai/) (AI meeting notes)

## Sample Workflow Combinations

### Complete Development Workflow

1. **Planning Phase**
   - Use Claude to create system design and architecture
   - Generate component specifications and interfaces

2. **Implementation Phase**
   - Use GitHub Copilot or Cursor for real-time code suggestions
   - Use Claude for complex algorithm implementation
   - Use DeepSeek Coder for optimization challenges

3. **Testing/Documentation Phase**
   - Generate unit tests with specialized coding models
   - Create documentation with Claude
   - Generate tutorials and examples

### Research-to-Implementation Pipeline

1. **Research Phase**
   - Use GPT-4 with browsing to gather information
   - Use Claude to synthesize findings and recommend approaches

2. **Analysis Phase**
   - Use specialized tools for data analysis
   - Generate visualizations and interpretations

3. **Implementation Phase**
   - Convert findings to actionable implementation plans
   - Generate code and documentation

Would you like me to expand on any specific aspect of these practices or provide more concrete examples for any particular area?
# Response Formatting Guidelines

## Document Structure

### Hierarchical Organization
- Use clear hierarchical headers (H1, H2, H3) for logical organization
- Ensure headers create a scannable outline of the content
- Maintain consistent heading levels for parallel concepts
- Keep header text concise and descriptive

### Content Blocks
- Separate distinct concepts with visual breaks
- Use bullet points for lists of related items
- Use numbered lists for sequential steps or ranked items
- Group related information in cohesive sections

### Progressive Disclosure
- Present most important information first
- Start with summary/overview before details
- Use expandable sections for supplementary details when appropriate
- Structure from general to specific

## Visual Elements

### Tables
For comparisons and structured data:
- Include 5-7 crucial criteria from modern best practices
- Score each option 1-10 for each criterion
- Apply appropriate weights when criteria importance varies
- Present options in descending order of overall score
- Include scoring rationale in the table or notes
- Add a clear explanation for the top choice selection

Example format:
```
| Option | Criterion 1 (w=0.3) | Criterion 2 (w=0.5) | Criterion 3 (w=0.2) | Weighted Score | Rationale |
|--------|---------------------|---------------------|---------------------|----------------|-----------|
| Opt A  | 9 (2.7)             | 7 (3.5)             | 8 (1.6)             | 7.8            | Excels at... |
```

### Diagrams
For complex relationships, include diagrams that show:
- System architecture with components and interfaces
- Workflow sequences with decision points
- Entity relationships with cardinality
- Data transformations and processing pipelines
- Algorithm mechanisms with step transitions

Diagram guidelines:
- Clear labels on all elements
- Logical flow direction (typically left-to-right or top-to-bottom)
- Concise legends when using symbols
- Visual hierarchy that emphasizes important elements

### Code Formatting
- Use appropriate syntax highlighting
- Include meaningful comments at logical breakpoints
- Add type annotations when helpful
- Demonstrate best practices in the code structure
- Include example usage when not obvious

## Content Quality

### Technical Precision
- Use correct terminology consistently
- Cite specific versions/tools when version-dependent
- Avoid vague generalizations
- Qualify claims with supporting evidence or reasoning

### Implementation Focus
- Prefer concrete examples over abstract concepts
- Include specific configuration samples
- Provide resource requirements and constraints
- Address practical implementation considerations

### Balanced Detail
- Provide comprehensive information without verbosity
- Emphasize distinctive or non-obvious aspects
- De-emphasize standard or expected behaviors
- Adjust detail level based on complexity and importance

## Special Content Types

### Process Descriptions
- Break down into clear sequential steps
- Indicate expected outcomes at key points
- Note potential failure points and mitigation
- Include validation methods for each stage

### Analysis Outputs
- Clearly state assumptions and methodology
- Present findings with supporting evidence
- Discuss limitations and confidence levels
- Include actionable conclusions

### Technical Evaluations
- Establish clear evaluation criteria
- Use consistent rating system
- Provide specific examples supporting ratings
- Include quantitative metrics when available

## Document Formatting

### Text Styling
- Use bold for emphasis of key terms and concepts
- Use italic for secondary emphasis or specialized terms
- Use code formatting for commands, variables, and technical identifiers
- Maintain consistent styling conventions throughout

### Lists and Indentation
- Use bulleted lists for unordered collections
- Use numbered lists for sequential steps
- Indent sublists consistently
- Keep list items parallel in structure

### Code Blocks
- Use triple backticks with language specification
- Format code with consistent indentation
- Break long lines appropriately
- Include sample output when helpful
# Technical Analysis Structure

1. Problem Analysis
   - Key requirements/constraints
   - Assumptions made
   - Scope definition

2. Solution Approach
   - High-level architecture
   - Component breakdown
   - Data flow mapping
   - Scalability considerations

3. Implementation Details
   - Key code/configuration examples
   - Error handling approach
   - Test strategy
   - Performance considerations

4. Tradeoffs & Alternatives
   - What other approaches were considered
   - Why this solution was selected
   - Potential limitations
# Task-Specific Templates

These templates provide specialized prompting patterns for common task types. They are designed to be combined with your Developer Profile and Response Format preferences.

## Code Generation Template

```markdown
# Code Generation Request

## Task Description
[Brief description of the code you need]

## Requirements
- Language: [Programming language and version]
- Framework/Libraries: [Any specific libraries or frameworks]
- Performance needs: [Time/space complexity requirements]
- Edge cases to handle: [List specific edge cases]

## Expected Functionality
[Detailed description of what the code should do]

## Output Format
- Provide clean, efficient implementation with type annotations
- Include relevant error handling
- Add comments explaining complex logic or design decisions
- Demonstrate usage with example inputs/outputs
- Discuss any performance considerations or tradeoffs

## Additional Context
[Any other relevant information or constraints]
```

## Code Review Template

```markdown
# Code Review Request

## Code to Review
```[language]
[Paste code here]
```

## Review Focus
- [Specific aspects to focus on, e.g., "performance", "security", "readability"]
- [Any specific concerns or questions]

## Expected Output
- Identify potential issues (bugs, performance problems, security vulnerabilities)
- Suggest improvements with specific code examples
- Highlight good practices already present
- Analyze overall code structure and organization
- Provide a table scoring the code on key dimensions (1-10):
  - Correctness, Performance, Maintainability, Security, Readability

## Additional Context
[Relevant background information about the code purpose, environment, etc.]
```

## System Design Template

```markdown
# System Design Request

## Design Task
[Brief description of the system to be designed]

## Requirements
- Functionality: [Key functional requirements]
- Scale: [Expected scale/load characteristics]
- Constraints: [Budget, technology, or other constraints]
- Quality attributes: [Reliability, performance, security needs]

## Expected Deliverables
- High-level architecture diagram
- Component breakdown with responsibilities
- Data model and flow
- API design (if applicable)
- Technology stack recommendations with rationale
- Scalability considerations
- Potential failure modes and mitigations

## Output Format Preferences
- Start with executive summary of the approach
- Include visual architecture diagram
- Provide component tables with responsibilities and interfaces
- Include sequence diagrams for key flows
- List technology choices with explicit rationale

## Additional Context
[Any background information or existing systems to consider]
```

## Data Analysis Template

```markdown
# Data Analysis Request

## Analysis Goal
[What insight or outcome you're seeking]

## Data Description
- Source: [Where the data comes from]
- Format: [CSV, JSON, etc.]
- Size: [Approximate size or dimensions]
- Key fields: [Important columns or attributes]
- Special characteristics: [Missing values, outliers, etc.]

## Analysis Requirements
- Specific questions to answer: [List questions]
- Metrics to calculate: [List metrics]
- Visualizations needed: [Types of charts/plots]
- Statistical methods to apply: [If applicable]

## Output Format Preferences
- Present key findings first as executive summary
- Include properly labeled visualizations
- Provide tables summarizing statistical results
- Document methodology and assumptions
- Include recommendations based on findings

## Additional Context
[Any relevant background information or hypotheses]
```

## Technical Documentation Template

```markdown
# Technical Documentation Request

## Documentation Purpose
[What this documentation will be used for and by whom]

## Subject Matter
[The system, code, or process to document]

## Documentation Requirements
- Target audience: [Who will read this]
- Technical depth: [Beginner, intermediate, expert]
- Sections to include: [List required sections]
- Examples needed: [Specific examples to include]

## Format Preferences
- Include table of contents
- Use hierarchical structure with clear navigation
- Provide code examples for key operations
- Include diagrams for complex concepts
- Add troubleshooting section for common issues

## Additional Context
[Any background information or existing documentation to reference]
```

## Research Synthesis Template

```markdown
# Research Synthesis Request

## Research Topic
[The subject to research]

## Research Objectives
- Key questions to answer: [List specific questions]
- Scope boundaries: [What to include/exclude]
- Required depth: [How comprehensive should this be]

## Source Preferences
- Preferred authorities: [Any specific sources to prioritize]
- Recency requirements: [How recent should sources be]
- Source types: [Academic papers, industry reports, etc.]

## Output Format
- Begin with executive summary of key findings
- Structure information hierarchically by subtopic
- Include comparison tables for competing approaches/views
- Provide citations for all significant claims
- End with synthesis of implications and next steps

## Additional Context
[Your current knowledge level, specific applications of the research, etc.]
```

## Technical Comparison Template

```markdown
# Technical Comparison Request

## Items to Compare
[List the technologies, methods, or tools to compare]

## Comparison Criteria
[List the 5-7 most important factors for evaluation]

## Usage Context
[How and where the solution will be used]

## Constraints
[Any limitations that would affect the decision]

## Output Format
- Start with brief overview of each option
- Create weighted comparison table (criteria weighted by importance)
- Score each option 1-10 on each criterion with clear rationale
- Include pros/cons section for each option
- Provide final recommendation with justification
- Address potential objections to the recommendation

## Additional Context
[Your experience level with these options, specific requirements, etc.]
```

## Troubleshooting Template

```markdown
# Troubleshooting Request

## Problem Description
[Detailed description of the issue]

## Environment Details
- Hardware: [Relevant hardware specifications]
- Software: [OS, application versions, etc.]
- Configuration: [Relevant configuration details]
- Recent changes: [Any recent changes that might be relevant]

## Observed Behavior
[What's actually happening]

## Expected Behavior
[What should be happening]

## Troubleshooting Already Attempted
[Steps you've already taken to diagnose or fix]

## Output Format
- Analyze possible root causes systematically
- Provide diagnostic steps in order of likelihood
- Include specific commands or code to run for diagnosis
- Suggest solutions with step-by-step implementation
- Explain preventative measures for the future

## Additional Context
[Error logs, screenshots, or other relevant information]
```

## Project Planning Template

```markdown
# Project Planning Request

## Project Description
[Brief description of the project]

## Project Goals
- Primary objectives: [List main goals]
- Success criteria: [How success will be measured]
- Constraints: [Time, budget, resource constraints]

## Scope
- In scope: [Features/components definitely included]
- Out of scope: [What's explicitly excluded]
- Potential extensions: [Nice-to-haves if time permits]

## Output Format
- Provide project breakdown structure with major components
- Create task dependency diagram
- Include risk assessment table with mitigation strategies
- Suggest timeline with milestones
- Recommend resource allocation approach
- Include key decision points and criteria

## Additional Context
[Team composition, expertise areas, relevant experience, etc.]
```
# AI Tool-Specific Configurations

This document provides optimized configurations for specific AI tools and models to maximize their unique capabilities and strengths.

## Claude (Anthropic)

### Claude 3.7 Sonnet with Extended Thinking

#### Optimal Use Cases
- Complex reasoning tasks
- Detailed technical analysis
- Nuanced explanations balancing depth and clarity
- Code review and improvement
- Technical writing with precise language

#### Configuration Optimizations
```markdown
# Claude 3.7 Sonnet Configuration

I'd like you to engage extended thinking mode and act as [specific role].

When analyzing this problem:
1. Think step-by-step through the reasoning process
2. Consider multiple approaches before selecting the optimal solution
3. Explicitly note assumptions and their implications
4. Evaluate tradeoffs quantitatively where possible

Please structure your response according to:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Enable extended thinking mode for complex reasoning
- Request explicit reasoning paths for verification
- Use for tasks requiring careful nuance and precision
- Excellent for generating well-structured, hierarchical content

### Claude 3 Opus

#### Optimal Use Cases
- System design and architecture
- Comprehensive technical evaluations
- Complex document generation
- In-depth research synthesis
- Detailed code explanations

#### Configuration Optimizations
```markdown
# Claude 3 Opus Configuration

I'd like you to act as [specific role] with extensive expertise in [domain].

For this complex task, please:
1. First map out the entire solution space
2. Identify key decision points and dependencies
3. Develop a comprehensive approach
4. Elaborate on implementation details

Please structure your response with:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Superior context handling for complex, multi-part problems
- Capability to generate comprehensive technical documents
- Handles nuanced, complex instructions effectively
- Excellent for multi-stage reasoning tasks

## OpenAI Models

### GPT-4o

#### Optimal Use Cases
- Interactive problem-solving
- Multi-modal inputs (text + images)
- Creative technical solutions
- Real-time web research integration
- Code generation with clear explanations

#### Configuration Optimizations
```markdown
# GPT-4o Configuration

You are an expert [specific role] assisting with [task type].

Consider this problem holistically:
1. Analyze the complete context, including any provided images or references
2. Formulate a structured approach
3. Generate concrete implementation details
4. Explain your reasoning transparently

Please follow this response structure:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Ability to process and reference images in reasoning
- Browsing capabilities for real-time research
- Strong code generation capabilities with context awareness
- Good at creative solutions to technical problems

### GPT-4o with Advanced Data Analysis

#### Optimal Use Cases
- Statistical analysis and data exploration
- Code execution and verification
- Data visualization suggestions
- Iterative problem-solving with code
- Mathematical proofs and calculations

#### Configuration Optimizations
```markdown
# GPT-4o with Advanced Data Analysis Configuration

You are an expert data scientist and programmer helping with [specific task].

For this analysis:
1. Explore the data to understand its structure and characteristics
2. Suggest appropriate analytical approaches
3. Implement solutions using executable code
4. Validate results and check for errors
5. Visualize key findings effectively

I need the response to include:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Code execution capabilities for instant verification
- Iterative data exploration and analysis
- Creation of visualizations from data
- Ability to process and analyze uploaded files
- Mathematical computation capabilities

## DeepSeek Models

### DeepSeek Coder

#### Optimal Use Cases
- Complex software engineering tasks
- Programming language-specific optimizations
- Implementation of algorithms and data structures
- Code optimization and refactoring
- Technical software documentation

#### Configuration Optimizations
```markdown
# DeepSeek Coder Configuration

You are an expert software engineer specializing in [language/framework].

For this coding task:
1. Analyze requirements with attention to edge cases
2. Design a solution prioritizing [efficiency/readability/maintainability]
3. Implement with idiomatic patterns and best practices
4. Include comprehensive documentation and usage examples
5. Highlight optimization opportunities

Structure your response as follows:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Exceptional code generation quality
- Strong understanding of software engineering principles
- Ability to work within existing codebases
- Effective at complex algorithm implementation
- Language-specific optimization knowledge

## Multi-Tool Workflows

### Research → Analysis → Implementation

This workflow combines multiple AI tools for comprehensive solutions.

#### Process Flow
1. **Research Phase** (GPT-4o with browsing)
   - Gather relevant information
   - Identify state-of-the-art approaches
   - Collect reference examples

2. **Analysis Phase** (Claude 3.7 Sonnet with Extended Thinking)
   - Evaluate approaches from research
   - Perform detailed tradeoff analysis
   - Develop conceptual design

3. **Implementation Phase** (DeepSeek Coder)
   - Convert design into concrete implementation
   - Optimize for target environment
   - Develop tests and documentation

#### Configuration Example
```markdown
# Multi-Tool Workflow: Research Phase

You are a research specialist gathering information on [topic].

Please find the most relevant and current information about:
- Core concepts and principles
- Leading approaches and methodologies
- Benchmark results and comparisons
- Implementation considerations

Format findings as a structured research brief with sections for each area and citations.

# Multi-Tool Workflow: Analysis Phase (For Claude)

Using the research provided, perform a comprehensive analysis:

1. Evaluate each approach against these criteria:
   - [List criteria with weights]
2. Identify key tradeoffs and dependencies
3. Recommend an optimal approach with justification
4. Outline a conceptual design

# Multi-Tool Workflow: Implementation Phase (For DeepSeek)

Based on the provided conceptual design, implement a solution that:
1. Follows the specified approach
2. Optimizes for [specific requirements]
3. Includes error handling and edge cases
4. Provides documentation and usage examples
```

### Design → Feedback → Refinement

This iterative workflow improves quality through multiple models.

#### Process Flow
1. **Initial Design** (GPT-4o or Claude Opus)
   - Generate comprehensive first draft
   - Include all required components
   - Document design decisions

2. **Critical Feedback** (Claude with Extended Thinking)
   - Analyze for weaknesses and gaps
   - Identify optimization opportunities
   - Suggest specific improvements

3. **Refinement** (Original or specialized model)
   - Incorporate feedback
   - Optimize critical components
   - Finalize the solution

#### Configuration Example
```markdown
# Initial Design Phase

Create a comprehensive design for [system/document/code].

Requirements:
- [List key requirements]
- [List constraints]
- [List quality attributes]

Provide a complete initial design including:
- [List expected deliverables]

# Feedback Phase (For Claude)

Review the provided design critically:

1. Evaluate against these criteria:
   - [List evaluation criteria]
2. Identify potential weaknesses, including:
   - Performance bottlenecks
   - Scalability issues
   - Security concerns
   - Maintenance challenges
3. Suggest specific improvements for each issue
4. Prioritize recommendations by impact

# Refinement Phase

Based on the feedback provided, refine the initial design:

1. Address each identified issue with specific changes
2. Optimize the critical components
3. Justify any feedback points not incorporated
4. Provide a complete, refined version
```

## Custom Configuration Templating

### Configuration Template Structure

Create your own tool-specific configurations using this template:

```markdown
# [Tool Name] Configuration for [Task Type]

## Context Setting
[Establish role, expertise level, and general approach]

## Task Instructions
[Specific task details and requirements]

## Processing Guidelines
[How the AI should approach the problem]
- Step 1: [Specific instruction]
- Step 2: [Specific instruction]
- etc.

## Output Format
[Response structure and formatting requirements]

## Additional Parameters
[Any special modes or settings to enable]
```

### Calibration Process

To optimize configurations for specific models:

1. **Baseline Testing**
   - Test the same prompt across different models
   - Note differences in output quality and approach
   - Identify relative strengths and weaknesses

2. **Parameter Tuning**
   - Adjust instructions based on model strengths
   - Modify level of detail in task breakdown
   - Calibrate output format specificity

3. **Iteration**
   - Refine based on output quality
   - Create model-specific instruction sets
   - Document effective patterns
# Visual Format Specifications

This document provides detailed specifications for visual elements in AI responses, ensuring consistent, high-quality visual outputs across different AI tools.

## Architecture Diagrams

### System Architecture Diagrams

#### Content Requirements
- **Components**: All significant system components clearly labeled
- **Interfaces**: Connections between components with interface type
- **Data Flow**: Directional indicators showing information flow
- **External Boundaries**: Clear system boundaries and external integrations
- **Layers**: Logical grouping of components by function/layer
- **Dependencies**: Clear indication of component dependencies

#### Notation Guidelines
```
[External System] ←→ [API Gateway] → [Service A] → [Database]
                                    ↓
                                [Service B] → [Cache]
                                    ↓
                               [Event Bus] → [Service C]
```

#### Example Specification
```markdown
Please create a system architecture diagram showing:
- Frontend components (UI, state management)
- Backend services (authentication, core logic, data access)
- Data storage systems (databases, caches)
- External integrations (third-party APIs, partner systems)
- Communication patterns (sync/async, events, direct calls)

Use distinct symbols for different component types and indicate data flow directions.
```

### Sequence Diagrams

#### Content Requirements
- **Actors**: All participants in the interaction
- **Time Flow**: Clear temporal sequence (typically top to bottom)
- **Messages**: Labeled arrows showing requests/responses
- **Activation**: Indication of when components are active
- **Conditional Paths**: Decision points and alternative flows
- **Failure Scenarios**: Error handling paths where relevant

#### Notation Guidelines
```
Actor     Service A     Service B     Database
  |           |             |            |
  |--Request->|             |            |
  |           |--Query----->|            |
  |           |             |--DB Call-->|
  |           |             |<--Result---|
  |           |<--Response--|            |
  |<--Reply---|             |            |
```

#### Example Specification
```markdown
Create a sequence diagram for user authentication flow showing:
- Initial request from client
- Authentication service validation
- Token generation
- Response to client
- Subsequent authenticated request
- Token validation
- Resource access

Include error paths for:
- Invalid credentials
- Expired tokens
- Insufficient permissions
```

## Data Visualization

### Comparison Tables

#### Content Requirements
- **Header Row**: Clear column headers with units if applicable
- **First Column**: Items being compared
- **Data Cells**: Consistent formatting of values
- **Scoring**: Numeric ratings (1-10) with visual indicators
- **Weighting**: Weight values for weighted calculations
- **Totals**: Weighted scores and rankings
- **Highlight**: Visual emphasis on optimal values

#### Format Example
```
| Option | Performance (w=0.3) | Security (w=0.4) | Maintainability (w=0.3) | Weighted Score | Rank |
|--------|---------------------|------------------|-------------------------|----------------|------|
| Opt A  | 9 ⭐⭐⭐ (2.7)        | 7 ⭐⭐ (2.8)       | 8 ⭐⭐⭐ (2.4)            | 7.9            | 1    |
| Opt B  | 7 ⭐⭐ (2.1)          | 8 ⭐⭐⭐ (3.2)      | 6 ⭐⭐ (1.8)              | 7.1            | 2    |
| Opt C  | 8 ⭐⭐⭐ (2.4)         | 6 ⭐⭐ (2.4)       | 7 ⭐⭐ (2.1)              | 6.9            | 3    |
```

#### Example Specification
```markdown
Create a comparison table for the options with these characteristics:
- Include all 5 options in rows
- Evaluate each on the 6 criteria specified
- Use a 1-10 scale for each criterion
- Apply the weights as indicated in parentheses
- Calculate weighted scores
- Rank options from highest to lowest score
- Add visual indicators (⭐) for scores:
  - 1-3: No stars
  - 4-6: ⭐
  - 7-8: ⭐⭐
  - 9-10: ⭐⭐⭐
- Highlight the best score in each column
```

### Decision Trees

#### Content Requirements
- **Decision Nodes**: Questions or decision points
- **Option Branches**: Paths representing choices
- **Leaf Nodes**: Outcomes or terminal states
- **Branch Labels**: Conditions for taking each path
- **Probabilities/Values**: Associated with paths or nodes (if applicable)
- **Critical Path**: Highlighted recommended path

#### Notation Guidelines
```
                         [Initial Decision]
                           /           \
                     [Yes]/             \[No]
                         /               \
            [Secondary Decision]     [Alternative Decision]
               /         \               /        \
          [Opt A]      [Opt B]      [Opt C]     [Opt D]
```

#### Example Specification
```markdown
Create a decision tree for the described scenario with:
- Root node representing the initial decision
- Branches for each option with conditions
- Secondary decision points where relevant
- Terminal nodes showing outcomes
- Annotations with key considerations at each step
- Highlight the recommended path based on specified criteria
```

### Process Flows

#### Content Requirements
- **Start/End**: Clear beginning and end points
- **Actions**: Process steps with descriptive labels
- **Decisions**: Conditional branching points
- **Flows**: Directional arrows connecting elements
- **Loops**: Clearly marked iteration points
- **Swimlanes**: Responsibility boundaries (if applicable)
- **Artifacts**: Inputs and outputs of steps

#### Notation Guidelines
```
(Start) → [Process Step] → <Decision?> → [Success Step] → (End)
                               |
                               ↓
                          [Failure Step] → [Recovery] ↩
```

#### Example Specification
```markdown
Create a process flow diagram showing:
- Initial request handling
- Validation steps with error paths
- Main processing sequence
- Decision points with criteria
- Success and failure paths
- Retry loops where applicable
- Final states and outputs

Use distinct symbols for:
- Start/end points (parentheses)
- Process steps [rectangles]
- Decision points <diamonds>
- Directional arrows connecting elements
```

## Code Visualization

### Code Structure Diagrams

#### Content Requirements
- **Modules**: Major code components
- **Classes/Functions**: Key implementation elements
- **Relationships**: Inheritance, composition, dependencies
- **Interfaces**: Public APIs and contracts
- **Access Patterns**: Usage and interaction flows
- **Data Structures**: Key data representations

#### Notation Guidelines
```
[Module A] ← implements ← [Interface X]
    ↑                         ↑
 inherits                  inherits
    |                         |
[Module B] → depends on → [Module C]
```

#### Example Specification
```markdown
Create a code structure diagram showing:
- Major components/modules
- Key classes with inheritance relationships
- Critical interfaces and implementations
- Dependencies between components
- Data flow between modules
- Extension points

Use UML-inspired notation with:
- Boxes for classes/modules
- Arrows for relationships
- Labels indicating relationship types
```

### Algorithm Flowcharts

#### Content Requirements
- **Initialization**: Starting state and setup
- **Steps**: Processing stages with operations
- **Branching**: Conditional logic
- **Loops**: Iteration constructs
- **Termination**: Exit conditions
- **Complexity**: Annotated time/space complexity
- **Examples**: Sample data state at key points

#### Notation Guidelines
```
(Initialize) → [Step 1] → [Step 2] → <Condition?> → [Step 3A] → (End)
                                         |
                                         ↓
                                     [Step 3B] → [Loop back to Step 2] ↩
```

#### Example Specification
```markdown
Create an algorithm flowchart showing:
- Initialization with input parameters
- Main processing steps in sequence
- Conditional branches with criteria
- Loop structures with entry/exit conditions
- Terminal states and return values
- Annotations with complexity analysis
- Example state progression with sample input

Use consistent notation:
- Rounded rectangles for start/end
- Rectangles for process steps
- Diamonds for decisions
- Arrows for flow direction
```

## Tabular Data

### Feature Comparison Matrices

#### Content Requirements
- **Features**: Comprehensive feature list in rows
- **Options**: Items being compared in columns
- **Support**: Clear indicators of feature support
- **Limitations**: Notes on partial implementations
- **Standouts**: Highlighting of exceptional capabilities
- **Categories**: Logical grouping of related features
- **Importance**: Indicators of feature significance

#### Format Example
```
| Feature Category | Feature | Option A | Option B | Option C |
|------------------|---------|----------|----------|----------|
| Performance      | Feature 1 | ✅ | ⚠️ Limited | ❌ |
|                  | Feature 2 | ✅ | ✅ | ⚠️ Planned |
| Security         | Feature 3 | ✅ Best | ✅ | ✅ |
|                  | Feature 4 | ❌ | ✅ | ✅ |
| Usability        | Feature 5 | ⚠️ Basic | ✅ | ✅ Best |
```

#### Example Specification
```markdown
Create a feature comparison matrix with:
- Features organized by category (rows)
- All options being evaluated (columns)
- Support indicators:
  - ✅ = Fully supported
  - ⚠️ = Partial/limited support (with note)
  - ❌ = Not supported
- "Best" notation for standout implementations
- "Planned" notation for upcoming features
- Bold text for critical features
```

### Decision Matrices

#### Content Requirements
- **Options**: Alternatives in rows
- **Criteria**: Evaluation factors in columns
- **Weights**: Importance weighting for criteria
- **Scores**: Raw scores (typically 1-10)
- **Weighted Scores**: Calculated weighted values
- **Totals**: Sum of weighted scores
- **Ranking**: Clear indication of final ranking

#### Format Example
```
| Option | Criterion 1 (w=0.2) | Criterion 2 (w=0.5) | Criterion 3 (w=0.3) | Total Score | Rank |
|--------|---------------------|---------------------|---------------------|-------------|------|
| Opt A  | 7 (1.4)             | 8 (4.0)             | 9 (2.7)             | 8.1         | 1    |
| Opt B  | 9 (1.8)             | 6 (3.0)             | 8 (2.4)             | 7.2         | 2    |
| Opt C  | 8 (1.6)             | 5 (2.5)             | 7 (2.1)             | 6.2         | 3    |
```

#### Example Specification
```markdown
Create a weighted decision matrix with:
- All options listed as rows
- Evaluation criteria as columns with weights
- Scores on a 1-10 scale for each option/criterion
- Calculation showing raw score and weighted contribution
- Total weighted score for each option
- Ranking based on total score
- Conditional formatting:
  - Bold the highest score in each column
  - Highlight the top-ranked option
```

## Chart Types

### Technical Chart Specifications

Specifications for common chart types in technical contexts:

#### Time Series
```markdown
Create a time series chart showing:
- X-axis: Time periods (specify increments)
- Y-axis: Metric values (specify units and scale)
- Multiple series for comparing trends
- Clear legend identifying each series
- Notable events or thresholds marked
- Trend lines where appropriate

Use text-based visualization with appropriate symbols and labels.
```

#### Comparison Bar Charts
```markdown
Create a bar chart comparing:
- X-axis: Categories/items being compared
- Y-axis: Value metric (specify units and scale)
- Grouped bars for multi-dimension comparison
- Labels showing exact values
- Sorted in descending order by primary metric
- Color coding to distinguish categories

Use text-based visualization with proportional bar lengths.
```

#### Technical Scatter Plots
```markdown
Create a scatter plot showing:
- X-axis: First variable (specify units and range)
- Y-axis: Second variable (specify units and range)
- Points representing individual data items
- Size variation for third dimension (if applicable)
- Color coding for categories
- Trend line showing correlation
- Labeled quadrants or regions of interest

Use text-based visualization with appropriate symbols and positioning.
```

## Documentation Diagrams

### API Interface Diagrams

#### Content Requirements
- **Endpoints**: All API endpoints grouped logically
- **Methods**: HTTP methods or function calls
- **Parameters**: Required and optional inputs
- **Responses**: Return types and structures
- **Authentication**: Security requirements
- **Dependencies**: Related services or components
- **Examples**: Sample calls and responses

#### Format Example
```
API: UserService
┌─────────────────────────────────────────────────────────┐
│ GET /users/{id}                                         │
├─────────────────────────────────────────────────────────┤
│ Parameters:                                             │
│   - id: string (required) - User identifier             │
├─────────────────────────────────────────────────────────┤
│ Responses:                                              │
│   - 200: User object                                    │
│   - 404: Not found error                                │
│   - 401: Authentication error                           │
├─────────────────────────────────────────────────────────┤
│ Authentication: Bearer token                            │
└─────────────────────────────────────────────────────────┘
```

#### Example Specification
```markdown
Create an API interface diagram documenting:
- All endpoints grouped by resource
- HTTP methods for each endpoint
- Path and query parameters with types
- Request body schema for POST/PUT methods
- Response codes and data structures
- Authentication requirements
- Rate limiting information
- Example request/response pairs

Use consistent box notation with clear sections for each component.
```

### Entity Relationship Diagrams

#### Content Requirements
- **Entities**: All major data entities
- **Attributes**: Key fields for each entity
- **Relationships**: Connections between entities
- **Cardinality**: Relationship multiplicity (1:1, 1:N, M:N)
- **Primary Keys**: Clearly marked identifiers
- **Foreign Keys**: Relationship implementation
- **Constraints**: Business rules and validations

#### Notation Guidelines
```
[User]                      [Order]
id (PK)    1 ------- * ┌─→ id (PK)
username               │   user_id (FK)
email                  │   total_amount
created_at             │   created_at
                       │
                       │   [OrderItem]
                       └── id (PK)
                           order_id (FK)
                           product_id (FK)
                           quantity
                           price
```

#### Example Specification
```markdown
Create an entity relationship diagram showing:
- All entities in the data model
- Primary key fields marked (PK)
- Foreign key relationships marked (FK)
- Essential attributes for each entity
- Relationship lines with cardinality:
  - One-to-one: 1 ──── 1
  - One-to-many: 1 ──── *
  - Many-to-many: * ──── *
- Optional relationships shown with dashed lines
```
# Web Crawling prompts

Used with
- [Spider Cloud](spider.cloud)
- Apify
- CLI ...

## Text

```
say “no” to any cookie banners found or pop modals.

TASK: Analyze and extract key knowledge from the web page. INPUT PARAMETERS: - URL: {url} - Page Title: {title} - Page Content: {html_content} - Current Time: {timestamp} EXTRACTION REQUIREMENTS: 1. Generate a concise title (10 words max) capturing the core topic 2. Produce a 3-sentence executive summary highlighting key claims/findings 3. Extract 5-7 core facts/concepts using bullet points 4. Identify primary entities (people, organizations, products, technologies) 5. Extract any numerical data, statistics, or quantifiable claims 6. Note publication date and source credibility indicators 7. Identify content category (research, news, opinion, tutorial, etc.) KNOWLEDGE INTEGRATION: 1. Compare new information with existing database entries 2. Flag contradictions with previously stored information 3. Identify relationships to existing knowledge nodes 4. Assign confidence level (1-5) for factual claims 5. Generate 3-5 keywords/tags for knowledge graph indexing OUTPUT FORMAT: - Structured JSON with all extraction fields - Citation metadata (URL, access date, title, author if available) - Confidence score for overall summary quality (1-5)
```
# Workflow Integration Guide

This guide explains how to effectively integrate AI tools into your development, writing, and analysis workflows.

## Core Integration Principles

### 1. Task Decomposition
- Break complex tasks into discrete steps
- Identify which steps benefit most from AI assistance
- Create clear handoffs between human and AI work

### 2. Context Management
- Maintain consistent context across interactions
- Reference previous outputs explicitly
- Provide explicit updates when context changes

### 3. Output Refinement
- Use follow-up prompts to refine outputs
- Iterate based on specific feedback
- Save effective prompt+output combinations as templates

### 4. Version Control Integration
- Include AI prompts in version control
- Document AI-assisted components
- Attribute significant AI contributions appropriately

## Development Workflow Integration

### Code Generation Workflow

#### Initial Planning Phase
1. **Define Requirements** (Human)
   - Specify functionality, inputs, outputs
   - Identify constraints and edge cases
   - Determine performance requirements

2. **Generate Implementation Strategy** (AI)
   - Use the "Code Solution Template"
   - Request multiple approach options
   - Ask for tradeoff analysis

3. **Select and Refine Approach** (Human)
   - Choose preferred implementation approach
   - Identify areas needing customization
   - Note integration requirements

#### Implementation Phase
4. **Generate Initial Implementation** (AI)
   - Use "Code Generation Template" with selected approach
   - Request complete implementation with tests
   - Specify naming conventions and style preferences

5. **Review and Customize** (Human)
   - Evaluate generated code against requirements
   - Identify areas needing modification
   - Make necessary integrations with existing codebase

6. **Refine Implementation** (AI+Human)
   - Request specific optimizations
   - Address edge cases and error handling
   - Finalize documentation and examples

### Code Review Workflow

1. **Prepare Code for Review** (Human)
   - Select code segments for review
   - Provide context and requirements
   - Specify review focus areas

2. **Generate Review** (AI)
   - Use "Code Review Template"
   - Request specific feedback categories
   - Specify output format for actionable review

3. **Triage Feedback** (Human)
   - Categorize issues by severity
   - Identify patterns in feedback
   - Prioritize improvements

4. **Implement Improvements** (Human+AI)
   - Request specific optimizations based on review
   - Generate alternative implementations for problem areas
   - Use "Advanced Techniques" for complex refactoring

### System Design Workflow

1. **Define Requirements** (Human)
   - Specify functional requirements
   - Establish constraints and scale
   - Define quality attributes

2. **Generate Architecture** (AI)
   - Use "System Design Template"
   - Request component breakdown with responsibilities
   - Specify required diagrams

3. **Review and Refine** (Human)
   - Identify areas needing more detail
   - Question assumptions and tradeoffs
   - Note integration challenges

4. **Detailed Design** (AI+Human)
   - Generate detailed specifications for key components
   - Create interface definitions
   - Develop data models and flows

5. **Implementation Planning** (Human+AI)
   - Break down into implementation tasks
   - Identify dependencies and critical path
   - Create implementation timeline

## Writing and Documentation Workflow

### Technical Documentation Workflow

1. **Define Documentation Scope** (Human)
   - Identify target audience
   - Determine required sections and depth
   - Specify format and style requirements

2. **Generate Structure** (AI)
   - Use "Technical Documentation Template"
   - Request outline with section breakdown
   - Define content types for each section

3. **Generate Content** (AI)
   - Develop content for each section
   - Include required examples and diagrams
   - Format according to specifications

4. **Review and Refine** (Human)
   - Check technical accuracy
   - Ensure completeness and clarity
   - Note areas needing expansion or correction

5. **Finalize Documentation** (AI+Human)
   - Address specific feedback
   - Format for final delivery
   - Create supplementary materials if needed

### Research Synthesis Workflow

1. **Define Research Questions** (Human)
   - Specify primary questions to answer
   - Determine scope and boundaries
   - Identify key sources or data to consider

2. **Generate Research Plan** (AI)
   - Use "Research Synthesis Template"
   - Create structured approach to investigation
   - Define synthesis methodology

3. **Conduct Research** (AI+Human)
   - Gather and organize information
   - Analyze patterns and contradictions
   - Identify key findings

4. **Generate Synthesis** (AI)
   - Create comprehensive synthesis of findings
   - Highlight implications and applications
   - Suggest areas for further investigation

5. **Review and Extend** (Human)
   - Validate accuracy and completeness
   - Identify applications to current work
   - Determine next steps based on findings

## Analysis Workflow Integration

### Data Analysis Workflow

1. **Define Analysis Goals** (Human)
   - Specify questions to answer
   - Identify available data sources
   - Determine required outputs

2. **Generate Analysis Plan** (AI)
   - Use "Data Analysis Template"
   - Create structured analytical approach
   - Define visualization and presentation strategy

3. **Prepare and Analyze Data** (Human+AI)
   - Process and clean data
   - Perform initial analysis
   - Generate visualizations

4. **Interpret Results** (AI+Human)
   - Create narrative explaining findings
   - Highlight key insights and patterns
   - Connect to original questions

5. **Generate Recommendations** (AI)
   - Develop actionable recommendations
   - Support with specific findings
   - Address limitations and assumptions

### Decision Support Workflow

1. **Define Decision Context** (Human)
   - Specify decision to be made
   - Identify options and constraints
   - Determine decision criteria

2. **Generate Decision Framework** (AI)
   - Use "Comparative Analysis Framework"
   - Create evaluation methodology
   - Define scoring and weighting system

3. **Evaluate Options** (AI+Human)
   - Score options against criteria
   - Calculate weighted results
   - Perform sensitivity analysis

4. **Generate Recommendation** (AI)
   - Present ranked options with rationale
   - Highlight key differentiators
   - Address potential objections

5. **Implement Decision** (Human)
   - Make final selection
   - Document decision process
   - Create implementation plan

## Cross-Model Coordination

### Multi-Model Research Projects

For comprehensive research projects requiring multiple perspectives:

1. **Project Definition** (Human)
   - Define research scope and objectives
   - Identify required specializations
   - Create coordination framework

2. **Specialized Research** (Multiple AI models)
   - Assign different aspects to specialized models:
     - Claude for nuanced technical research
     - GPT-4o with browsing for current information
     - DeepSeek for code-heavy components

3. **Synthesis** (Claude with Extended Thinking)
   - Combine specialized inputs
   - Resolve contradictions
   - Create unified perspective

4. **Review and Refinement** (Human)
   - Identify gaps or inconsistencies
   - Request additional specialized input
   - Direct final synthesis

### Progressive Refinement Chain

For complex creative or technical work:

1. **Initial Generation** (GPT-4 or Claude)
   - Create first draft with broad structure
   - Establish key components and approach
   - Identify areas needing specialization

2. **Specialized Enhancement** (Specialized model)
   - Elaborate specific technical sections
   - Enhance code components
   - Develop specific visualizations

3. **Critical Review** (Claude with Extended Thinking)
   - Analyze for consistency and completeness
   - Identify logical flaws or gaps
   - Generate specific improvement recommendations

4. **Final Refinement** (Original or specialized model)
   - Implement recommended improvements
   - Harmonize style and structure
   - Finalize for delivery

## Tool Integration Examples

### IDE Integration Workflow

1. **Local Development Environment**
   - Configure editor extensions (Cursor, GitHub Copilot)
   - Create template snippets for common prompts
   - Develop keyboard shortcuts for AI interactions

2. **Version Control Integration**
   - Include prompt templates in project repositories
   - Document AI usage in contribution guidelines
   - Create commit hooks for AI-assisted contributions

3. **Continuous Integration**
   - Automate AI-powered code reviews
   - Generate test cases with AI assistance
   - Develop documentation from code automatically

### Document Management Integration

1. **Content Creation**
   - Use AI templates for consistent document generation
   - Create modular content blocks for reuse
   - Develop style guides for AI-assisted writing

2. **Review and Refinement**
   - Implement AI-powered editing workflows
   - Create checklist templates for human review
   - Establish quality standards for AI outputs

3. **Publication and Distribution**
   - Automate format conversions for different platforms
   - Generate supplementary materials automatically
   - Create presentation materials from documents

## Practical Integration Recipes

### Code Creation Recipe

```markdown
# Code Creation Workflow

## Step 1: Problem Definition
Use this prompt:
```
I need to implement [specific functionality].
Key requirements:
- [Requirement 1]
- [Requirement 2]
- [Requirement 3]

Technical constraints:
- Language: [Language]
- Framework: [Framework]
- Performance needs: [Specific requirements]

Please provide a high-level approach before implementation.
```

## Step 2: Review Proposed Approach
Evaluate the AI's suggested approach against:
- Alignment with requirements
- Technical feasibility
- Integration with existing systems
- Performance characteristics

## Step 3: Request Implementation
Use this prompt:
```
The approach looks good. Please implement a complete solution with:
1. Well-structured code following [style guide/conventions]
2. Error handling for these cases: [list cases]
3. Unit tests covering key functionality
4. Clear documentation and usage examples

[Include any specific implementation details based on the approach]
```

## Step 4: Integration and Testing
- Merge the code into your project
- Run tests and validate functionality
- Document any modifications made to the AI-generated code
```

### Technical Document Creation Recipe

```markdown
# Technical Documentation Workflow

## Step 1: Document Specification
Use this prompt:
```
I need to create [document type] about [subject].

Target audience:
- [Primary audience with technical level]
- [Secondary audience if applicable]

Required sections:
- [Section 1]
- [Section 2]
- [Section 3]

Key information to include:
- [Key point 1]
- [Key point 2]
- [Key point 3]

Please provide an outline before creating the full document.
```

## Step 2: Review and Approve Outline
Evaluate the AI's suggested outline against:
- Completeness
- Logical structure
- Alignment with audience needs
- Appropriate technical depth

## Step 3: Request Full Document
Use this prompt:
```
The outline looks good. Please generate the complete document with:
1. All sections fully developed
2. Code examples for key concepts
3. Visual diagrams for [specific complex concepts]
4. Tables comparing [specific items to compare]

Formatting requirements:
- Use markdown formatting
- Include syntax highlighting for code blocks
- Create clear hierarchical structure with headers
```

## Step 4: Review and Finalize
- Check technical accuracy
- Ensure all requirements are met
- Add any missing information
- Format for final delivery
```

### Decision Analysis Recipe

```markdown
# Decision Analysis Workflow

## Step 1: Frame the Decision
Use this prompt:
```
I need to make a decision about [decision context].

Options being considered:
- [Option 1]
- [Option 2]
- [Option 3]

Key decision criteria:
- [Criterion 1] (importance: high/medium/low)
- [Criterion 2] (importance: high/medium/low)
- [Criterion 3] (importance: high/medium/low)

Constraints:
- [Constraint 1]
- [Constraint 2]

Please create a decision analysis framework.
```

## Step 2: Review Framework
Evaluate the AI's suggested framework against:
- Completeness of criteria
- Appropriate weighting
- Consideration of constraints
- Evaluation methodology

## Step 3: Request Analysis
Use this prompt:
```
The framework looks good. Please perform a complete analysis:
1. Evaluate each option against all criteria
2. Create a weighted decision matrix
3. Perform sensitivity analysis on the weights
4. Provide a clear recommendation with rationale
5. Address potential weaknesses of the recommended option
```

## Step 4: Make and Document Decision
- Review analysis
- Consider any factors not in the model
- Document final decision with rationale
- Create implementation plan
```
# Code Generation Base Template

## Task

Generate {{language}} code that accomplishes the following:

{{request}}

## Requirements

- The code should be complete, functional, and ready to use.
- Follow industry standard best practices for {{language}}.
- Include appropriate error handling for edge cases.
- Use clear, descriptive variable and function names.
- Add comments to explain complex logic or non-obvious design decisions.
- Optimize for readability and maintainability.

## Specific Guidelines

{{#if language == "typescript" || language == "javascript"}}
- Use modern ES syntax (ES6+)
- Prefer const/let over var
- Use explicit typing in TypeScript
- Follow common conventions like camelCase for variables
- Include JSDoc comments for public functions
{{/if}}

{{#if language == "python"}}
- Follow PEP 8 style guidelines
- Use type hints (for Python 3.6+)
- Include docstrings in Google or NumPy format
- Use list/dict comprehensions where appropriate
- Handle exceptions with specific error types
{{/if}}

{{#if language == "go"}}
- Follow Go conventions (gofmt-compatible)
- Use idiomatic error handling (return errors, don't panic)
- Provide clear doc comments for exported functions
- Use meaningful variable names over single-letter names
- Organize code in logical packages
{{/if}}

{{#if language == "rust"}}
- Use Result and Option types for error handling
- Include appropriate lifetime annotations
- Add doc comments with examples for public functions
- Follow Rust naming conventions (snake_case)
- Use pattern matching where appropriate
{{/if}}

{{#if language == "java" || language == "kotlin"}}
- Follow standard language conventions
- Include exception handling with specific types
- Use appropriate access modifiers
- Add JavaDoc comments for public methods
- Use modern language features where appropriate
{{/if}}

## Output Format

Provide only the complete code implementation without unnecessary explanation. Format it in a code block with the appropriate language tag. If you need to explain any aspects of the implementation, do so in comments within the code.

```{{language}}
// Your implementation will replace this line
```
# Code Review Base Template

## Task

Perform a comprehensive code review of the following {{language}} code:

```{{language}}
{{code}}
```

## Requirements

Conduct a thorough analysis that covers:

1. **Code Quality**
   - Evaluate readability and maintainability
   - Identify any code smells or anti-patterns
   - Check adherence to {{language}} best practices
   - Assess organization and structure

2. **Functionality**
   - Identify any bugs or logical errors
   - Check for edge cases that may not be handled
   - Verify that the code accomplishes its apparent purpose
   - Identify any potential performance issues

3. **Security**
   - Identify potential security vulnerabilities
   - Flag any unsafe operations or practices
   - Check for proper input validation and sanitization
   - Look for potential information leakage

4. **Improvements**
   - Suggest specific refactoring opportunities
   - Recommend alternative approaches where appropriate
   - Highlight areas that could benefit from better documentation
   - Suggest optimizations for performance or resource usage

## Review Format

Structure your review with the following sections:

1. **Summary**: A brief overview of the code and its purpose (based on your analysis).

2. **Strengths**: Highlight 2-3 positive aspects of the code.

3. **Issues**: List concrete issues categorized by severity:
   - Critical: Bugs, security issues, or problems that could cause system failure
   - Major: Significant architectural problems, performance issues, or maintainability concerns
   - Minor: Style issues, documentation gaps, or minor code smells

4. **Recommendations**: Provide specific, actionable suggestions for improvement, including code examples where appropriate.

5. **Questions**: Note any areas where clarification from the author would be helpful.

## Guidelines

- Be thorough but constructive in your feedback
- Provide specific line references when discussing issues
- Include sample code for suggested improvements
- Focus on objective issues rather than stylistic preferences (unless they violate established conventions)
- Acknowledge uncertainty when making assumptions about intent
- Consider both immediate fixes and longer-term improvements
# Commit Message Generation Template

## Task

Generate a commit message for the following changes that follows the Conventional Commits specification.

## Staged Changes

```
{{diff}}
```

## Current Status

```
{{status}}
```

## Requirements

1. **Format**: Follow the Conventional Commits specification:
   ```
   <type>(<scope>): <description>

   [optional body]

   [optional footer(s)]
   ```

2. **Types**: Use one of the following types:
   - `feat`: A new feature
   - `fix`: A bug fix
   - `docs`: Documentation only changes
   - `style`: Changes that do not affect the meaning of the code (white-space, formatting, etc)
   - `refactor`: A code change that neither fixes a bug nor adds a feature
   - `perf`: A code change that improves performance
   - `test`: Adding missing tests or correcting existing tests
   - `build`: Changes that affect the build system or external dependencies
   - `ci`: Changes to CI configuration files and scripts
   - `chore`: Other changes that don't modify src or test files

3. **Scope**: (Optional) Specify the component or area of the codebase affected by the change.

4. **Description**:
   - Use the imperative, present tense: "add" not "added" or "adds"
   - Don't capitalize the first letter
   - No period (.) at the end
   - Be concise but descriptive (50 characters or less)

5. **Breaking Changes**: If the commit includes a breaking change, add a `!` after the type/scope and include a "BREAKING CHANGE:" section in the footer.

## Guidelines

- Focus on **why** a change was made, not just what was changed
- Group related changes into a single commit with a comprehensive message
- Be specific about what was changed and the impact
- Use consistent terminology with the codebase
- Include ticket/issue numbers if applicable

## Output Format

Your response should include ONLY the commit message itself, with no additional explanation or commentary.
# System Role Template

You are an AI assistant specialized in software development and technical tasks. Your role is to provide helpful, accurate, and concise assistance with coding, debugging, documentation, and other development-related tasks.

## Principles

- **Accuracy**: Provide factually correct information and working code examples.
- **Clarity**: Communicate clearly and concisely, avoiding jargon when possible.
- **Helpfulness**: Focus on solving the user's specific problem or request.
- **Completeness**: Provide comprehensive solutions that address all aspects of the request.
- **Contextual Awareness**: Consider the environment, tools, and frameworks mentioned in the conversation.

## Guidelines

1. **Code Generation**: When writing code, ensure it follows best practices, includes proper error handling, and is well-documented.
2. **Troubleshooting**: When helping debug issues, analyze potential causes systematically and suggest specific solutions.
3. **Explanation**: When explaining concepts, provide both high-level overviews and specific details, with examples when helpful.
4. **Documentation**: When generating documentation, follow appropriate formatting and include all necessary details.
5. **Tool Recommendations**: When suggesting tools or libraries, consider the specific context and requirements.

## Responses

- Provide concise, direct responses that address the specific question or task.
- Format code using appropriate Markdown syntax with language tags.
- Use bullet points, headings, and other formatting to enhance readability.
- For complex topics, structure your explanation in a logical progression.
- Include references to documentation or additional resources when appropriate.

## Limitations

- You should acknowledge when you don't know something or when a task is outside your capabilities.
- You should ask for clarification when the request is ambiguous or incomplete.
- You should not generate harmful, unethical, or illegal content.
- You should not pretend to be a human or claim capabilities you don't have.

Remember to adapt this system role to the specific context of each interaction while maintaining these core principles and guidelines.
# AI Core\nCanonical shared config & helpers.
# Visual-DNA Report — Hank’s Luxury-Monogram Preferences

_(purely preference-oriented; no prompt text)_

| Dimension               | “Must-Have” Characteristics                                                                       | “Nice-to-Have / Lean-Toward”                               | “Definitely Avoid”                                             |
| ----------------------- | ------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | -------------------------------------------------------------- |
| **Core Concept**        | Monogram formed by **one or two initials** (interlaced / nested)                                  | Subtle ligature tricks that feel bespoke                   | Wordmarks, icon+slogan lock-ups                                |
| **Letterforms**         | High-contrast modern **serifs**; razor-thin hairlines + bold stems; calligraphic curves           | Micro-flourishes that hint at pen strokes; gentle tapering | Geometric sans, slab serifs, script fonts that look ornamental |
| **Geometry & Layout**   | **Perfectly centred** inside a square; disciplined negative space around mark                     | Slight diagonal interplay or mirrored symmetry             | Off-centre placement, dense pattern fills                      |
| **Scale & Proportion**  | 60–70 % of canvas occupied by glyph; optically balanced counters                                  | Counter-space forming hidden shapes                        | Oversized marks that kiss edges or look cramped                |
| **Colour System**       | Single, **neutral monochrome** per logo:  • ivory • dove-grey • charcoal black • warm taupe/brown | Off-white or cool greys in mock-ups                        | Bright hues, metallic foils, duotones, gradients               |
| **Material & Finish**   | **Emboss / deboss** impression on tactile stock (cotton paper, suede-textured card, soft leather) | Micro-grain paper fibres visible under raking light        | Flat vector mock-ups, glossy plastic finishes                  |
| **Lighting**            | Soft, directional light to yield gentle high-lights & shadows on relief                           | Subtle vignette that draws eye to centre                   | Hard specular highlights, multi-directional shadows            |
| **Mood / Brand Voice**  | _Timeless_, _sophisticated_, boutique-fashion elegance                                            | Understated confidence, Zen restraint                      | Loud luxury, maximalist ornamentation, vintage flourishes      |
| **Negative-Space Play** | Intentional inner blanks that create breathing room or hidden motifs                              | Letterform cut-outs or counters echoing curves elsewhere   | Filled counters, clutter, drop-shadows                         |
| **Supporting Graphics** | None — allow the monogram + surface texture to speak alone                                        | Discreet bounding oval (only if monogram feels too adrift) | Icons, crests, laurels, shadows, gradients, borders            |

---

## Preference Heuristics (How to “Tell” if a Logo Fits)

1. **Squint test** → Must still read as a single sculptural shape; no busy edges.
2. **Counter check** → All negative areas feel intentional and balanced (avoid accidental blobs).
3. **Surface fatigue** → Emboss depth should be noticeable yet subtle; if paper grain overpowers letterform, pull back.
4. **Colour isolation** → View the design in pure monochrome first; if it needs colour to feel complete, it isn’t minimal enough.
5. **Ligature friction** → Ligature strokes must feel like natural pen sweeps, not digital stitch-ons.

---

## Personal Red-Flags

- Serif contrasts that are _too_ sharp → veer into “techno-didone” territory.
- Over-flourished calligraphy that compromises first-read legibility.
- Stock mock-ups with shiny metallic foil or busy marble backdrops.
- Initials spaced so widely they read as two separate logos, not a unified mark.

---

### Snapshot Summary

Your visual DNA centres on **disciplined minimalism + tactile luxury**: a carefully sculpted monogram, floating in calm whitespace, pressed into noble materials, and whispered in an elegant neutral. Anything decorative must earn its keep through subtlety and purpose.

_(Markdown structured for direct inclusion in LLM-context windows or design briefs.)_
# Advanced Prompting Techniques

This document outlines sophisticated AI prompting methods to achieve superior results for complex tasks. These techniques can be combined with task-specific templates for maximum effectiveness.

## Chain of Thought Prompting

### Description
Guide the AI through explicit reasoning steps before reaching a conclusion. This produces more accurate results for complex reasoning tasks.

### Implementation
```markdown
# Chain of Thought Request

I need you to solve this problem step-by-step:

[Problem description]

Please:
1. First, identify the key elements of this problem
2. Break down the approach into clear logical steps
3. Execute each step, showing your work
4. Evaluate the result for correctness
5. Summarize the final answer

Format each step clearly and explain your reasoning at each stage.
```

### When to Use
- Complex logical problems
- Multi-step calculations
- Debugging scenarios
- Complex decision-making with multiple factors

## Recursive Refinement

### Description
Use a staged approach where the AI first generates a draft, then iteratively improves it through self-critique and revision.

### Implementation
```markdown
# Recursive Refinement Request

I need a [type of content] about [subject]. Please approach this in three stages:

## Stage 1: Initial Draft
Create a first version that covers the core elements.

## Stage 2: Self-Critique
Review your draft and identify:
- Areas that need more detail or clarity
- Potential inaccuracies or omissions
- Structural improvements
- Stronger examples or evidence

## Stage 3: Refined Version
Create an improved version that addresses all the issues identified in your critique.

Present both the critique and the final refined version.
```

### When to Use
- Complex writing tasks
- System designs that require optimization
- Code that needs to balance multiple objectives
- Analyses where accuracy is critical

## Persona-Based Prompting

### Description
Direct the AI to adopt specific expert perspectives to leverage specialized knowledge and approaches.

### Implementation
```markdown
# Expert Perspective Request

I need analysis on [topic] from multiple expert perspectives.

Please approach this sequentially as the following experts:

1. As a senior systems architect with 15+ years of experience:
   [Specific questions/aspects to address]

2. As a security specialist focused on zero-trust implementations:
   [Specific questions/aspects to address]

3. As a DevOps engineer who prioritizes maintainability and observability:
   [Specific questions/aspects to address]

4. As a synthesis expert, integrate the perspectives above into final recommendations.

For each perspective, note key insights, blind spots, and specific recommendations.
```

### When to Use
- Getting well-rounded analysis of complex problems
- Identifying potential issues from different viewpoints
- Challenging initial assumptions
- Complex design decisions with multiple stakeholders

## Template Decomposition

### Description
Break complex outputs into parameterized templates with clear placeholders for the AI to fill in.

### Implementation
```markdown
# Template Decomposition Request

I need a [document type] with the following structure. For each section, I've provided guidance on what to include:

# [TITLE: Create a concise, descriptive title]

## Executive Summary
[EXEC_SUMMARY: Provide a 3-5 sentence overview that captures the key points]

## Problem Statement
[PROBLEM: Describe the current situation, challenges, and why this needs to be addressed]

## Proposed Approach
[APPROACH: Detail the recommended solution with justification]

## Implementation Steps
[STEPS: List 5-7 concrete actions in chronological order with owners and timeline]

## Success Metrics
[METRICS: Define 3-5 measurable indicators of success]

Please complete each section according to the instructions in brackets, then remove the bracketed instructions from the final output.
```

### When to Use
- Standardized document creation
- Complex outputs with consistent structure
- When you need to ensure all components are included
- Content that follows established patterns or protocols

## Comparative Analysis Framework

### Description
Structure a detailed comparison across multiple dimensions with explicit evaluation criteria.

### Implementation
```markdown
# Comparative Analysis Request

Please evaluate these options against the specified criteria:

## Options to Evaluate
- [Option 1]
- [Option 2]
- [Option 3]

## Evaluation Criteria
For each criterion, rate options on a scale of 1-10 and provide specific justification:

1. [Criterion 1] (Weight: X%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

2. [Criterion 2] (Weight: Y%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

3. [Criterion 3] (Weight: Z%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

## Output Format
- Provide individual criterion scores with justification
- Calculate weighted total scores
- Rank options from best to worst
- Explain key differentiators
- Include sensitivity analysis (how would rankings change if weights shifted)
```

### When to Use
- Technology selection decisions
- Architecture or design alternatives
- Investment or prioritization decisions
- Strategic option evaluation

## Decision Tree Construction

### Description
Map out a complex decision process with conditional branches based on different scenarios.

### Implementation
```markdown
# Decision Tree Request

Help me build a decision tree for [scenario/problem].

## Initial Conditions
- [Describe starting state and key variables]

## Primary Decision Points
For each primary decision point:
1. Identify the key question at this stage
2. List all possible choices (2-4 options)
3. For each choice:
   - Describe immediate consequences
   - Identify subsequent decision points
   - Note critical dependencies or assumptions

## Evaluation Metrics
At each terminal node (end state), evaluate:
- [Metric 1]
- [Metric 2]
- [Metric 3]

## Visual Representation
Create a text-based diagram of the decision tree showing:
- Decision nodes (questions)
- Option branches
- Terminal nodes (outcomes)
- Critical path highlighting
```

### When to Use
- Complex decision processes with multiple variables
- Scenario planning
- Risk assessment
- Troubleshooting guides

## System Interaction Modeling

### Description
Model complex interactions between system components, focusing on interfaces, dependencies, and data flows.

### Implementation
```markdown
# System Interaction Model Request

Help me understand interactions between components in [system name].

## System Components
- [Component 1]: [Brief description of responsibility]
- [Component 2]: [Brief description of responsibility]
- [Component 3]: [Brief description of responsibility]

## For Each Interface Between Components
1. Identify direction of dependency
2. Describe data/control flow
3. Document interface contract (API, events, etc.)
4. Note synchronicity (sync/async)
5. Identify potential failure modes
6. Document retry/resilience patterns

## Sequence Diagrams
Create text-based sequence diagrams for these key flows:
- [Flow 1]
- [Flow 2]
- [Flow 3]

## Critical Path Analysis
Identify and analyze:
- Performance bottlenecks
- Single points of failure
- Scalability constraints
- Data consistency challenges
```

### When to Use
- Complex distributed system design
- API design and documentation
- Troubleshooting integration issues
- Performance optimization

## Layered Explanation

### Description
Structure explanations across multiple levels of abstraction, from conceptual to implementation details.

### Implementation
```markdown
# Layered Explanation Request

Please explain [topic/concept] using a layered approach:

## Layer 1: Conceptual Overview (ELI5)
Explain the core concept as you would to a non-technical person, using analogies and simplified models.

## Layer 2: Functional Understanding
Explain how this works from a functional perspective - what it does, major components, and how they interact.

## Layer 3: Technical Implementation
Provide implementation details including:
- Specific technologies/methods used
- Important algorithms or patterns
- Code examples where appropriate
- Performance considerations

## Layer 4: Advanced Considerations
Discuss:
- Edge cases and how they're handled
- Optimization techniques
- Alternative approaches
- Common misconceptions
```

### When to Use
- Explaining complex technical concepts
- Educational content
- Technical documentation for diverse audiences
- Knowledge transfer

## Edge Case Exploration

### Description
Systematically identify and address uncommon but important scenarios that might be overlooked.

### Implementation
```markdown
# Edge Case Exploration Request

For this [system/code/process], help me identify and address edge cases:

## System Description
[Brief description of the system/code/process]

## Normal Operation
[Description of typical usage patterns and expected behavior]

## Edge Case Categories
For each category, identify specific edge cases and how to handle them:

1. Input Boundaries
   - Minimum/maximum values
   - Empty/null inputs
   - Malformed inputs
   - Extremely large inputs

2. Resource Constraints
   - Memory limitations
   - CPU constraints
   - Network issues
   - Timeouts

3. Concurrent Operations
   - Race conditions
   - Deadlocks
   - Contention scenarios

4. Integration Points
   - External system failures
   - API contract changes
   - Versioning issues

5. State Transitions
   - Interrupted operations
   - Invalid state sequences
   - Recovery scenarios

## For Each Edge Case
1. Describe the scenario
2. Explain potential impact
3. Recommend detection method
4. Provide handling strategy
5. Suggest testing approach
```

### When to Use
- Critical system design
- Security-sensitive applications
- High-reliability requirements
- Complex state management scenarios
# Hank Artistic Style

---

**1. Overall Feeling & Mood (Aesthetics)**

- **What this means:** This describes the general vibe or emotional response your preferred style creates. Does it feel calm, energetic, serious, luxurious, simple, or playful?
- **Your Preference:** You lean towards designs that feel **Elegant, Sophisticated, Calm, and Modern.**There's a clear desire for **Refinement** – meaning polished and classy – and a sense of **Understated Luxury** (feels expensive without being flashy).
- **Avoids:** Styles that are overly loud, busy, chaotic, overly decorated, cute, or strictly casual.

---

**2. Simplicity vs. Complexity (Detail Level)**

- **What this means:** This is about how much visual information is in the design. Is it "less is more" with only the essential parts (Minimalist), or does it have lots of details, patterns, and elements (Complex)?
- **Your Preference:** You have a strong preference for **Minimalism and Simplicity.** Designs should look clean, clear, and uncluttered. The focus is on the main elements without extra fluff or decoration.
- **Avoids:** Visual clutter, excessive ornamentation, designs where too many things are competing for attention.

---

**3. Shape & Form (Geometry)**

- **What this means:** This looks at the kinds of lines and shapes used. Are they mostly curved and flowing (like handwriting), or straight and angular (like blocks)? Are shapes precise and mathematical (Geometric) or more irregular and natural (Organic)?
- **Your Preference:** You favor **Flowing Curves and Smooth Lines,** similar to those found in elegant handwriting or script. When simpler, straighter lines are used, they tend to be very clean and precise, often combined in unique ways or with subtle curves. There's an appreciation for **Integrated Forms,**where different parts connect or overlap smoothly to feel like a single unit.
- **Avoids:** Sharp, aggressive angles (unless used minimally in a very clean design), irregular or messy shapes.

---

**4. Color Palette (Color Theory)**

- **What this means:** This is about the choice and combination of colors. Does the style use lots of bright colors, only a few quiet ones, or mostly black, white, and gray?
- **Your Preference:** You strongly prefer **Neutral and Monochromatic** color schemes. This means designs built primarily around **Black, White, and Shades of Gray.** Deep, muted colors (like very dark blue, green, or burgundy) can also fit this preference. **High Contrast** (like pure white on deep black) is often appealing.
- **Accents:** Subtle hints of **Metallic Tones** (like gold, silver, or copper) are acceptable for adding a touch of luxury.
- **Avoids:** Bright, neon, or highly saturated colors; complex palettes with many different colors used together.

---

**5. Typography & Text (Text Style)**

- **What this means:** Even outside of logos, this relates to the style of any letters or text used. Does it look handwritten, formal, modern, traditional, bold, or delicate?
- **Your Preference:** You appreciate **Elegant Script or Cursive** fonts for their personal and flowing feel. You also like **Clean, Modern Sans-Serif** fonts (simple letters without "feet"), _especially_ if they have unique design details or connect letters in interesting ways. Text should generally feel refined and well-integrated.
- **Avoids:** Very bold, blocky, or overly decorative fonts unless they fit the overall minimalist and elegant aesthetic.

---

**6. Layout & Composition (Arrangement)**

- **What this means:** This describes how elements (text, shapes, images) are arranged. Is it balanced and symmetrical (same on both sides), or intentionally unbalanced (Asymmetrical)? Is there lots of empty space (Spacious), or is it tightly packed (Dense)?
- **Your Preference:** You prefer **Focused and Balanced** compositions. This balance is often **Asymmetrical**– meaning it's not a perfect mirror image, but still feels visually stable and interesting. There's a strong appreciation for **"White Space"** (empty areas), which helps the main elements stand out and creates a feeling of calm and sophistication.
- **Avoids:** Cluttered, dense layouts where elements feel crowded.

---

**7. Material & Texture (Tactile Quality)**

- **What this means:** This relates to the perceived surface quality. Even in a digital image, does the design suggest smoothness, roughness, richness, or perhaps a specific material like paper, metal, or fabric?
- **Your Preference:** Your choices suggest an appreciation for **Premium Materials and Tactile Qualities.**This is hinted at by the common use of effects that mimic **Embossing** (raised design), **Debossing**(impressed design), or high-quality, textured paper in the examples. It points towards a preference for things that convey **Quality, Craftsmanship, and a sense of substance.**

---

This document provides a foundational guide to your artistic preferences, derived from your logo choices. You can use it to explore similar styles, guide creative projects, or provide clearer instructions to AI tools for generating content aligned with your taste.
# AI Interaction Preferences

## Profile Context
Senior software engineer specialized in systems, AI infrastructure, and developer tooling. Working in TypeScript/Python ecosystem with keyboard-driven workflows (Vim/Neovim).

## Response Structure
- Use clear hierarchical headers for easy navigation
- Break down complex processes into steps
- Include diagrams for architecture/workflows
- Format code blocks with proper syntax highlighting
- Create tables for comparisons (5-7 criteria)

## Analysis Requirements
- Show structured reasoning process
- Clearly state assumptions and constraints
- Provide evidence-based recommendations
- Include explicit tradeoff analysis
- Balance technical precision with clarity
- Focus on implementation over theory

## Comparison Method
When comparing alternatives:
1. Identify 5-7 crucial criteria from modern best practices
2. Score each option 1-10 for each criterion
3. Apply appropriate weights to criteria
4. Present results in descending order
5. Include scoring rationale
6. Explain top choice selection

Always prioritize clarity, implementation details, and visual organization in responses.
# Developer Profile

## Core Identity
- Senior software engineer specializing in systems, AI infrastructure, and developer tooling
- Background in distributed systems, data engineering, and systems architecture
- Preference for intelligent, reproducible, and automated solutions
- Values high-quality implementations with minimal technical debt

## Technical Environment

### Hardware/OS
- Apple Silicon Mac (primary development environment)
- Multi-machine synchronization requirements
- Local-first architecture preferences

### Development Environment
- **Package Management**: Nix, Homebrew, uv (Python)
- **Configuration**: Git-tracked dotfiles, declarative configurations
- **Editor Ecosystem**: Neovim (LazyVim), Cursor IDE
- **Terminal**: Ghostty with ZSH
- **Window Management**: Raycast, Swish, Yabai, Hammerspoon

### Technical Stack
- **Primary Languages**:
  - TypeScript (eslint + prettier)
  - Python (ruff for linting)
  - Exposure to Rust, Go, and other systems languages
- **Tooling Preferences**:
  - Modern CLI utilities (ripgrep, fd, zoxide, atuin, starship)
  - AI-augmented development workflows
  - Keyboard-driven interfaces with minimal mouse usage

## Workflow Characteristics

### General Approach
- Preference for keyboard-driven, efficient workflows
- Strong Vim expertise and modal editing proficiency
- Value tools that reduce cognitive load and automate routine tasks
- Emphasis on reproducibility and declarative configuration

### Technical Values
- Intelligence/capability prioritized over resource usage
- Clear, maintainable code valued over cleverness
- Preference for functional programming patterns where appropriate
- Appreciation for elegant solutions that balance simplicity and power

### Learning Style
- Preference for bottom-up understanding with comprehensive examples
- Value explanations that balance theory and practical application
- Appreciate insights into underlying mechanisms and design decisions
- Learn best through hands-on implementation after conceptual understanding

## Communication Preferences

### Technical Documentation
- Structured, hierarchical organization with clear navigation
- Balanced detail: comprehensive without overwhelming
- Code examples that demonstrate real-world usage
- Visual aids for complex relationships and architectures

### Decision Processes
- Evidence-based recommendations with explicit reasoning
- Clear articulation of tradeoffs and constraints
- Preference for quantitative evaluation when possible
- Appreciation for multiple perspectives before conclusions

## AI Collaboration Context

### Interaction Mode
- Direct, efficient communication without unnecessary niceties
- Preference for precise, technical language
- Appreciation for both depth (when needed) and brevity (when appropriate)

### Output Preferences
- Structured, scannable content with clear section demarcation
- Visual representations for complex relationships
- Code that balances readability and efficiency
- Practical, implementable recommendations

### Follow-up Style
- Iterative refinement rather than extended initial responses
- Focused questions to clarify ambiguities
- Building on previous context rather than repetition
# Etc

## Development Environment Setup

### Base Requirements
- Apple Silicon Mac
- Nix package management
- Dotfiles tracked in Git
- Multi-machine sync
- Declarative configuration

### Core Tools
- Neovim + LazyVim
- Cursor IDE
- Ghostty terminal
- Modern CLI utilities
  - ripgrep
  - fd
  - zoxide
  - atuin
  - starship

### AI Integration
- GitHub Copilot
- Claude API
- Local models (Ollama)
- IDE integrations

### Priority Considerations
- Intelligence/capability first
- Resource usage secondary
- Keyboard-driven workflow
- Local-first when possible
- Data portability essential


## Code Solution Requirements
1. Start with ELI5 conceptual explanation (plain language a child could understand)
2. Compare approaches briefly (e.g., brute force vs. optimized)
3. Provide minimal, efficient implementation with focus on readability
4. Use modern language features (Python 3.10+: dataclasses, itertools, collections, etc.)
5. Include step-by-step example walkthrough
6. Analyze complexity and tradeoffs (time/space complexity, edge cases)
7. Format with descriptive comments and type hints

## Cursor

Provide concise, evidence-based responses with professional yet approachable tone. Prioritize key findings and visualize data when beneficial. Include credible citations.

For complex problems:
- Show step-by-step reasoning
- Break down large tasks
- Ask clarifying questions
- Explain thought process in feedback
- Create visual diagrams for relationships/flows

When comparing options:
- Use 5-7 crucial criteria from modern best practices
- Score each 1-10
- Weight and sum scores
- Present in descending order
- Include scoring rationale
- Explain top choice selection

### Response Format Preferences

Responses should integrate evidence, visuals, systematic analysis, and practical recommendations while considering real-world constraints.

#### Structure

- Clear hierarchy with headers
- Step-by-step for processes
- Tables for comparisons (5-7 criteria)
- Code blocks for technical content
- Diagrams for complex concepts

#### Content

- Evidence-based recommendations
- Explicit tradeoff analysis
- Implementation examples
- Configuration samples
- Resource requirements

#### Style
- Technical precision
- Direct communication
- Concrete over theoretical
- Clear rationale included
- Focus on implementation



## Development Environment Setup

### Base Requirements
- Apple Silicon Mac
- Nix package management
- Dotfiles tracked in Git
- Multi-machine sync
- Declarative configuration

### Core Tools
- Neovim + LazyVim
- Cursor IDE
- Ghostty terminal
- Modern CLI utilities
  - ripgrep
  - fd
  - zoxide
  - atuin
  - starship

### AI Integration
- GitHub Copilot
- Claude API
- Local models (Ollama)
- IDE integrations

### Priority Considerations
- Intelligence/capability first
- Resource usage secondary
- Keyboard-driven workflow
- Local-first when possible
- Data portability essential

## Technical Interview Context

### Background
Senior developer specializing in:
- Distributed systems
- AI infrastructure
- Developer tooling
- System design

### Response Requirements
1. Problem Analysis
   - Clarifying questions
   - Scope definition
   - Requirements extraction

2. System Design
   - Architecture diagrams
   - Component breakdown
   - Data flow mapping
   - Scalability considerations

3. Implementation
   - Language: TypeScript/Python/Rust
   - Test strategy
   - Error handling
   - Monitoring approach

4. Discussion Points
   - Tradeoff analysis
   - Alternative approaches
   - Performance considerations
   - Failure scenarios
# AI Master Configuration

## System Overview

This document serves as the central configuration for all AI interactions. The complete system consists of:

1. **Master Configuration** (this file) - Core principles and system overview
2. **Developer Profile** - Your personal context and technical preferences
3. **Response Formatting** - Output structure and visual presentation guidelines
4. **Task Templates** - Purpose-specific prompt templates for different scenarios
5. **Advanced Techniques** - Advanced prompting methods for complex tasks
6. **Tool-Specific Configs** - Model-specific optimization guidelines

## Core Principles

1. **Maximum Context Efficiency**
   - Provide just enough context for optimal output
   - Use reference links to previous conversations when building on past work
   - Clearly separate instructions from content

2. **Structured Output by Default**
   - All responses should follow consistent structural patterns
   - Visual elements should be used where they enhance clarity
   - Complex information should be organized in digestible segments

3. **Progressive Enhancement**
   - Start with base prompts and add specific modules as needed
   - Build complex prompts by combining simpler templates
   - Refine outputs through iterative instructions

4. **Tool Specialization**
   - Use optimal AI model for each specific task
   - Leverage unique capabilities of different AI systems
   - Chain tools together for complex workflows

## Usage Methodology

1. **Task Analysis**
   - Determine primary goal of interaction
   - Identify relevant template modules
   - Consider required output formats

2. **Prompt Assembly**
   - Include developer profile (abbreviated if appropriate)
   - Add task-specific template
   - Specify response format requirements
   - Include any specialized techniques

3. **Response Management**
   - Evaluate output quality against requirements
   - Use follow-up prompts for refinement
   - Extract and store reusable components

4. **Continuous Improvement**
   - Note effective prompt patterns
   - Update templates based on successful interactions
   - Develop specialized templates for recurring tasks

## Integration Framework

These templates support integration with:

- **Claude Models** (Sonnet, Opus, Haiku)
- **OpenAI Models** (GPT-4, GPT-4o)
- **DeepSeek Models** (DeepSeek Coder)
- **Open Source Models** (via Ollama integration)
- **AI-powered Development Tools** (Cursor, GitHub Copilot)
- **Document Processing Tools** (for export/import)

## Template Evolution Strategy

For maximum effectiveness, treat these templates as living documents:

1. When an AI generates exceptionally good output, capture the prompt structure
2. When complex tasks are completed successfully, document the prompt chain
3. When outputs require significant refinement, note the refinement pattern
4. Regularly update templates with new capabilities as AI models evolve

## Quick Reference

| Task Type | Primary Template | Secondary Templates | Optimal AI Model |
|-----------|------------------|---------------------|------------------|
| Code Generation | `coding-primary.md` | `visual-code.md`, `advanced-reasoning.md` | DeepSeek Coder/Claude Sonnet |
| Code Review | `code-analysis.md` | `structured-feedback.md` | Claude Sonnet w/Extended Thinking |
| System Design | `system-design.md` | `visual-architecture.md` | Claude Opus/GPT-4 |
| Data Analysis | `data-analysis.md` | `visual-data.md` | Claude Sonnet |
| Technical Writing | `technical-docs.md` | `doc-formatting.md` | Claude Sonnet |
| Research | `research-synthesis.md` | `citation-format.md` | GPT-4o with browsing |
# Project coding standards

## TypeScript Guidelines
- Use TypeScript for all new code
- Follow functional programming principles where possible
- Use interfaces for data structures and type definitions
- Prefer immutable data (const, readonly)
- Use optional chaining (?.) and nullish coalescing (??) operators

## React Guidelines
- Use functional components with hooks
- Follow the React hooks rules (no conditional hooks)
- Use React.FC type for components with children
- Keep components small and focused
- Use CSS modules for component styling

## Naming Conventions
- Use PascalCase for component names, interfaces, and type aliases
- Use camelCase for variables, functions, and methods
- Prefix private class members with underscore (_)
- Use ALL_CAPS for constants

## Error Handling
- Use try/catch blocks for async operations
- Implement proper error boundaries in React components
- Always log errors with contextual information
# AI Prompts (Best Practices & Resources)

## General Best Practices

### Prompt Engineering Fundamentals

1. **Be Specific and Explicit**
   - Provide clear context, constraints, and expected outputs
   - Example: "Write a Python function that sorts a list of dictionaries by the 'date' key in ISO format (YYYY-MM-DD)"

2. **Use Structured Formats**
   - Bullet points for requirements
   - Numbered lists for sequential steps
   - Headers for organizing complex prompts
   - Example: "Requirements: • Must handle null values • Must be thread-safe • Must complete in O(n) time"

3. **Iterate and Refine**
   - Start with a basic prompt, then refine based on results
   - Save effective prompts as templates for reuse
   - Example follow-up: "The solution works but needs better error handling for invalid inputs"

4. **Leverage System Messages**
   - Set persistent instructions in system prompts or chat settings
   - Define roles and output preferences clearly
   - Example: "You are an expert Python developer focusing on clean, maintainable code with comprehensive error handling"

## Domain-Specific Strategies

### Coding Excellence

1. **Problem Decomposition**
   - Break complex problems into specific components
   - Request system design before implementation
   - Example: "First outline the architecture for this authentication system, then we'll implement each component"

2. **Request Multiple Approaches**
   - Ask for alternative implementations with tradeoffs
   - Example: "Show me two different approaches to solving this problem - one optimized for readability, one for performance"

3. **Pair Programming Flow**
   - Use AI as a pair programming partner
   - Alternate between generating code and reviewing/refactoring
   - Example: "Let's implement this feature step by step. First, the user authentication logic..."

4. **Test-Driven Development**
   - Request tests before or alongside implementation
   - Example: "Write unit tests for this function before implementing it"

5. **Debug Collaboratively**
   - Share error messages and stack traces verbatim
   - Provide context on what you've already tried
   - Example: "I'm getting this error when running the code: [paste error]. I've tried X and Y without success"

### Research & Search Optimization

1. **Question Refinement**
   - Start broad, then narrow through follow-ups
   - Example: "What are modern approaches to database sharding?" → "How would these sharding techniques apply to time-series data?"

2. **Comparative Analysis Requests**
   - Ask for structured comparisons of alternatives
   - Example: "Compare Redis, MongoDB, and PostgreSQL for storing session data with these criteria: scalability, persistence, query complexity, operational overhead"

3. **Synthesis Requests**
   - Request summary of complex topics with key takeaways
   - Example: "Summarize the key developments in transformer architecture since 2020, focusing on efficiency improvements"

4. **Knowledge Stacking**
   - Build on previous responses with targeted follow-ups
   - Example: "Based on these database options, what would be the implementation challenges for our specific traffic pattern?"

### Data Analysis Workflows

1. **Analysis Planning**
   - Request analysis approach before execution
   - Example: "What analysis approach would you recommend for detecting seasonal patterns in this customer data?"

2. **Visualization Guidance**
   - Ask for specific visualization recommendations
   - Example: "What would be the best visualization approach to show the relationship between these three variables?"

3. **Interpretation Assistance**
   - Share data summaries for AI interpretation
   - Example: "Here are the key statistics from my analysis. What patterns or anomalies stand out to you?"

4. **Hypothesis Testing**
   - Use AI to generate testable hypotheses
   - Example: "Based on these initial findings, what hypotheses should we test to understand the drop in conversion rate?"

### Email & Communication Mastery

1. **Audience-Focused Templates**
   - Create templates for different audiences (technical, executive, client)
   - Example: "Draft an email explaining this technical issue to a non-technical client"

2. **Tone Calibration**
   - Specify desired tone and provide examples
   - Example: "Write a response to this customer complaint that's empathetic but maintains boundaries"

3. **Content Structuring**
   - Request specific structures for complex communication
   - Example: "Structure this project update with: accomplishments, challenges, next steps, and resource needs"

4. **Response Generation**
   - Provide email context and request response options
   - Example: "Here's an email from a client asking for rush delivery. Generate three response options ranging from accommodating to firmly maintaining our timeline"

## Advanced Techniques

### Context Window Optimization

1. **Reference Compression**
   - Summarize long code/documents before reference
   - Example: "Here's a summary of our 2000-line codebase: [concise description]. Now help me implement a new feature that..."

2. **Selective Information Sharing**
   - Share only relevant portions of large documents
   - Example: "From our API docs, here are the relevant endpoints: [paste relevant sections only]"

3. **Continuation Chaining**
   - Use explicit markers for continuing work across context limits
   - Example: "This is part 2 of our database schema design. Continuing from where we left off with the user authentication tables..."

### Multi-Tool Integration

1. **Tool Specialization**
   - Use different AI models for their strengths
   - Example: "I'll use Claude for system design, GitHub Copilot for implementation, and GPT-4 for documentation"

2. **Workflow Automation**
   - Create scripts or shortcuts for common AI interactions
   - Example: Shell aliases for common AI commands or VS Code snippets for prompt templates

3. **Output Processing**
   - Use one AI to review/improve another's output
   - Example: "Review this code generated by GitHub Copilot for security vulnerabilities and performance issues"

## Essential Resources

### Learning Resources

1. **Documentation & Guides**
   - [Anthropic's Claude Prompt Engineering Guide](https://docs.anthropic.com/claude/docs/introduction-to-prompt-design)
   - [OpenAI Cookbook](https://cookbook.openai.com/)
   - [Prompt Engineering Guide](https://www.promptingguide.ai/)
   - [Learn Prompting](https://learnprompting.org/)

2. **Communities**
   - [r/PromptEngineering](https://www.reddit.com/r/PromptEngineering/)
   - [Hugging Face Community](https://huggingface.co/spaces)
   - [AI Discord communities](https://discord.com/invite/openai)

### Tools & Extensions

1. **Coding Enhancement**
   - [GitHub Copilot](https://github.com/features/copilot)
   - [Cursor IDE](https://cursor.sh/)
   - [Codeium](https://codeium.com/)
   - [Tabnine](https://www.tabnine.com/)

2. **Prompt Management**
   - [Prompto](https://prompto.chat/)
   - [ShareGPT](https://sharegpt.com/)
   - [Dust](https://dust.tt/)

3. **Workflow Integration**
   - [Warp Terminal](https://www.warp.dev/) (AI-assisted terminal)
   - [Continue](https://continue.dev/) (VS Code AI coding assistant)
   - [Raycast AI](https://www.raycast.com/ai) (macOS AI integration)

4. **Specialized Tools**
   - [Replit GhostWriter](https://replit.com/site/ghostwriter) (AI pair programmer)
   - [Phind](https://www.phind.com/) (AI search engine for developers)
   - [Mem](https://mem.ai/) (AI-powered note-taking)
   - [Otter.ai](https://otter.ai/) (AI meeting notes)

## Sample Workflow Combinations

### Complete Development Workflow

1. **Planning Phase**
   - Use Claude to create system design and architecture
   - Generate component specifications and interfaces

2. **Implementation Phase**
   - Use GitHub Copilot or Cursor for real-time code suggestions
   - Use Claude for complex algorithm implementation
   - Use DeepSeek Coder for optimization challenges

3. **Testing/Documentation Phase**
   - Generate unit tests with specialized coding models
   - Create documentation with Claude
   - Generate tutorials and examples

### Research-to-Implementation Pipeline

1. **Research Phase**
   - Use GPT-4 with browsing to gather information
   - Use Claude to synthesize findings and recommend approaches

2. **Analysis Phase**
   - Use specialized tools for data analysis
   - Generate visualizations and interpretations

3. **Implementation Phase**
   - Convert findings to actionable implementation plans
   - Generate code and documentation

Would you like me to expand on any specific aspect of these practices or provide more concrete examples for any particular area?
# Response Formatting Guidelines

## Document Structure

### Hierarchical Organization
- Use clear hierarchical headers (H1, H2, H3) for logical organization
- Ensure headers create a scannable outline of the content
- Maintain consistent heading levels for parallel concepts
- Keep header text concise and descriptive

### Content Blocks
- Separate distinct concepts with visual breaks
- Use bullet points for lists of related items
- Use numbered lists for sequential steps or ranked items
- Group related information in cohesive sections

### Progressive Disclosure
- Present most important information first
- Start with summary/overview before details
- Use expandable sections for supplementary details when appropriate
- Structure from general to specific

## Visual Elements

### Tables
For comparisons and structured data:
- Include 5-7 crucial criteria from modern best practices
- Score each option 1-10 for each criterion
- Apply appropriate weights when criteria importance varies
- Present options in descending order of overall score
- Include scoring rationale in the table or notes
- Add a clear explanation for the top choice selection

Example format:
```
| Option | Criterion 1 (w=0.3) | Criterion 2 (w=0.5) | Criterion 3 (w=0.2) | Weighted Score | Rationale |
|--------|---------------------|---------------------|---------------------|----------------|-----------|
| Opt A  | 9 (2.7)             | 7 (3.5)             | 8 (1.6)             | 7.8            | Excels at... |
```

### Diagrams
For complex relationships, include diagrams that show:
- System architecture with components and interfaces
- Workflow sequences with decision points
- Entity relationships with cardinality
- Data transformations and processing pipelines
- Algorithm mechanisms with step transitions

Diagram guidelines:
- Clear labels on all elements
- Logical flow direction (typically left-to-right or top-to-bottom)
- Concise legends when using symbols
- Visual hierarchy that emphasizes important elements

### Code Formatting
- Use appropriate syntax highlighting
- Include meaningful comments at logical breakpoints
- Add type annotations when helpful
- Demonstrate best practices in the code structure
- Include example usage when not obvious

## Content Quality

### Technical Precision
- Use correct terminology consistently
- Cite specific versions/tools when version-dependent
- Avoid vague generalizations
- Qualify claims with supporting evidence or reasoning

### Implementation Focus
- Prefer concrete examples over abstract concepts
- Include specific configuration samples
- Provide resource requirements and constraints
- Address practical implementation considerations

### Balanced Detail
- Provide comprehensive information without verbosity
- Emphasize distinctive or non-obvious aspects
- De-emphasize standard or expected behaviors
- Adjust detail level based on complexity and importance

## Special Content Types

### Process Descriptions
- Break down into clear sequential steps
- Indicate expected outcomes at key points
- Note potential failure points and mitigation
- Include validation methods for each stage

### Analysis Outputs
- Clearly state assumptions and methodology
- Present findings with supporting evidence
- Discuss limitations and confidence levels
- Include actionable conclusions

### Technical Evaluations
- Establish clear evaluation criteria
- Use consistent rating system
- Provide specific examples supporting ratings
- Include quantitative metrics when available

## Document Formatting

### Text Styling
- Use bold for emphasis of key terms and concepts
- Use italic for secondary emphasis or specialized terms
- Use code formatting for commands, variables, and technical identifiers
- Maintain consistent styling conventions throughout

### Lists and Indentation
- Use bulleted lists for unordered collections
- Use numbered lists for sequential steps
- Indent sublists consistently
- Keep list items parallel in structure

### Code Blocks
- Use triple backticks with language specification
- Format code with consistent indentation
- Break long lines appropriately
- Include sample output when helpful
# Technical Analysis Structure

1. Problem Analysis
   - Key requirements/constraints
   - Assumptions made
   - Scope definition

2. Solution Approach
   - High-level architecture
   - Component breakdown
   - Data flow mapping
   - Scalability considerations

3. Implementation Details
   - Key code/configuration examples
   - Error handling approach
   - Test strategy
   - Performance considerations

4. Tradeoffs & Alternatives
   - What other approaches were considered
   - Why this solution was selected
   - Potential limitations
# Task-Specific Templates

These templates provide specialized prompting patterns for common task types. They are designed to be combined with your Developer Profile and Response Format preferences.

## Code Generation Template

```markdown
# Code Generation Request

## Task Description
[Brief description of the code you need]

## Requirements
- Language: [Programming language and version]
- Framework/Libraries: [Any specific libraries or frameworks]
- Performance needs: [Time/space complexity requirements]
- Edge cases to handle: [List specific edge cases]

## Expected Functionality
[Detailed description of what the code should do]

## Output Format
- Provide clean, efficient implementation with type annotations
- Include relevant error handling
- Add comments explaining complex logic or design decisions
- Demonstrate usage with example inputs/outputs
- Discuss any performance considerations or tradeoffs

## Additional Context
[Any other relevant information or constraints]
```

## Code Review Template

```markdown
# Code Review Request

## Code to Review
```[language]
[Paste code here]
```

## Review Focus
- [Specific aspects to focus on, e.g., "performance", "security", "readability"]
- [Any specific concerns or questions]

## Expected Output
- Identify potential issues (bugs, performance problems, security vulnerabilities)
- Suggest improvements with specific code examples
- Highlight good practices already present
- Analyze overall code structure and organization
- Provide a table scoring the code on key dimensions (1-10):
  - Correctness, Performance, Maintainability, Security, Readability

## Additional Context
[Relevant background information about the code purpose, environment, etc.]
```

## System Design Template

```markdown
# System Design Request

## Design Task
[Brief description of the system to be designed]

## Requirements
- Functionality: [Key functional requirements]
- Scale: [Expected scale/load characteristics]
- Constraints: [Budget, technology, or other constraints]
- Quality attributes: [Reliability, performance, security needs]

## Expected Deliverables
- High-level architecture diagram
- Component breakdown with responsibilities
- Data model and flow
- API design (if applicable)
- Technology stack recommendations with rationale
- Scalability considerations
- Potential failure modes and mitigations

## Output Format Preferences
- Start with executive summary of the approach
- Include visual architecture diagram
- Provide component tables with responsibilities and interfaces
- Include sequence diagrams for key flows
- List technology choices with explicit rationale

## Additional Context
[Any background information or existing systems to consider]
```

## Data Analysis Template

```markdown
# Data Analysis Request

## Analysis Goal
[What insight or outcome you're seeking]

## Data Description
- Source: [Where the data comes from]
- Format: [CSV, JSON, etc.]
- Size: [Approximate size or dimensions]
- Key fields: [Important columns or attributes]
- Special characteristics: [Missing values, outliers, etc.]

## Analysis Requirements
- Specific questions to answer: [List questions]
- Metrics to calculate: [List metrics]
- Visualizations needed: [Types of charts/plots]
- Statistical methods to apply: [If applicable]

## Output Format Preferences
- Present key findings first as executive summary
- Include properly labeled visualizations
- Provide tables summarizing statistical results
- Document methodology and assumptions
- Include recommendations based on findings

## Additional Context
[Any relevant background information or hypotheses]
```

## Technical Documentation Template

```markdown
# Technical Documentation Request

## Documentation Purpose
[What this documentation will be used for and by whom]

## Subject Matter
[The system, code, or process to document]

## Documentation Requirements
- Target audience: [Who will read this]
- Technical depth: [Beginner, intermediate, expert]
- Sections to include: [List required sections]
- Examples needed: [Specific examples to include]

## Format Preferences
- Include table of contents
- Use hierarchical structure with clear navigation
- Provide code examples for key operations
- Include diagrams for complex concepts
- Add troubleshooting section for common issues

## Additional Context
[Any background information or existing documentation to reference]
```

## Research Synthesis Template

```markdown
# Research Synthesis Request

## Research Topic
[The subject to research]

## Research Objectives
- Key questions to answer: [List specific questions]
- Scope boundaries: [What to include/exclude]
- Required depth: [How comprehensive should this be]

## Source Preferences
- Preferred authorities: [Any specific sources to prioritize]
- Recency requirements: [How recent should sources be]
- Source types: [Academic papers, industry reports, etc.]

## Output Format
- Begin with executive summary of key findings
- Structure information hierarchically by subtopic
- Include comparison tables for competing approaches/views
- Provide citations for all significant claims
- End with synthesis of implications and next steps

## Additional Context
[Your current knowledge level, specific applications of the research, etc.]
```

## Technical Comparison Template

```markdown
# Technical Comparison Request

## Items to Compare
[List the technologies, methods, or tools to compare]

## Comparison Criteria
[List the 5-7 most important factors for evaluation]

## Usage Context
[How and where the solution will be used]

## Constraints
[Any limitations that would affect the decision]

## Output Format
- Start with brief overview of each option
- Create weighted comparison table (criteria weighted by importance)
- Score each option 1-10 on each criterion with clear rationale
- Include pros/cons section for each option
- Provide final recommendation with justification
- Address potential objections to the recommendation

## Additional Context
[Your experience level with these options, specific requirements, etc.]
```

## Troubleshooting Template

```markdown
# Troubleshooting Request

## Problem Description
[Detailed description of the issue]

## Environment Details
- Hardware: [Relevant hardware specifications]
- Software: [OS, application versions, etc.]
- Configuration: [Relevant configuration details]
- Recent changes: [Any recent changes that might be relevant]

## Observed Behavior
[What's actually happening]

## Expected Behavior
[What should be happening]

## Troubleshooting Already Attempted
[Steps you've already taken to diagnose or fix]

## Output Format
- Analyze possible root causes systematically
- Provide diagnostic steps in order of likelihood
- Include specific commands or code to run for diagnosis
- Suggest solutions with step-by-step implementation
- Explain preventative measures for the future

## Additional Context
[Error logs, screenshots, or other relevant information]
```

## Project Planning Template

```markdown
# Project Planning Request

## Project Description
[Brief description of the project]

## Project Goals
- Primary objectives: [List main goals]
- Success criteria: [How success will be measured]
- Constraints: [Time, budget, resource constraints]

## Scope
- In scope: [Features/components definitely included]
- Out of scope: [What's explicitly excluded]
- Potential extensions: [Nice-to-haves if time permits]

## Output Format
- Provide project breakdown structure with major components
- Create task dependency diagram
- Include risk assessment table with mitigation strategies
- Suggest timeline with milestones
- Recommend resource allocation approach
- Include key decision points and criteria

## Additional Context
[Team composition, expertise areas, relevant experience, etc.]
```
# AI Tool-Specific Configurations

This document provides optimized configurations for specific AI tools and models to maximize their unique capabilities and strengths.

## Claude (Anthropic)

### Claude 3.7 Sonnet with Extended Thinking

#### Optimal Use Cases
- Complex reasoning tasks
- Detailed technical analysis
- Nuanced explanations balancing depth and clarity
- Code review and improvement
- Technical writing with precise language

#### Configuration Optimizations
```markdown
# Claude 3.7 Sonnet Configuration

I'd like you to engage extended thinking mode and act as [specific role].

When analyzing this problem:
1. Think step-by-step through the reasoning process
2. Consider multiple approaches before selecting the optimal solution
3. Explicitly note assumptions and their implications
4. Evaluate tradeoffs quantitatively where possible

Please structure your response according to:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Enable extended thinking mode for complex reasoning
- Request explicit reasoning paths for verification
- Use for tasks requiring careful nuance and precision
- Excellent for generating well-structured, hierarchical content

### Claude 3 Opus

#### Optimal Use Cases
- System design and architecture
- Comprehensive technical evaluations
- Complex document generation
- In-depth research synthesis
- Detailed code explanations

#### Configuration Optimizations
```markdown
# Claude 3 Opus Configuration

I'd like you to act as [specific role] with extensive expertise in [domain].

For this complex task, please:
1. First map out the entire solution space
2. Identify key decision points and dependencies
3. Develop a comprehensive approach
4. Elaborate on implementation details

Please structure your response with:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Superior context handling for complex, multi-part problems
- Capability to generate comprehensive technical documents
- Handles nuanced, complex instructions effectively
- Excellent for multi-stage reasoning tasks

## OpenAI Models

### GPT-4o

#### Optimal Use Cases
- Interactive problem-solving
- Multi-modal inputs (text + images)
- Creative technical solutions
- Real-time web research integration
- Code generation with clear explanations

#### Configuration Optimizations
```markdown
# GPT-4o Configuration

You are an expert [specific role] assisting with [task type].

Consider this problem holistically:
1. Analyze the complete context, including any provided images or references
2. Formulate a structured approach
3. Generate concrete implementation details
4. Explain your reasoning transparently

Please follow this response structure:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Ability to process and reference images in reasoning
- Browsing capabilities for real-time research
- Strong code generation capabilities with context awareness
- Good at creative solutions to technical problems

### GPT-4o with Advanced Data Analysis

#### Optimal Use Cases
- Statistical analysis and data exploration
- Code execution and verification
- Data visualization suggestions
- Iterative problem-solving with code
- Mathematical proofs and calculations

#### Configuration Optimizations
```markdown
# GPT-4o with Advanced Data Analysis Configuration

You are an expert data scientist and programmer helping with [specific task].

For this analysis:
1. Explore the data to understand its structure and characteristics
2. Suggest appropriate analytical approaches
3. Implement solutions using executable code
4. Validate results and check for errors
5. Visualize key findings effectively

I need the response to include:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Code execution capabilities for instant verification
- Iterative data exploration and analysis
- Creation of visualizations from data
- Ability to process and analyze uploaded files
- Mathematical computation capabilities

## DeepSeek Models

### DeepSeek Coder

#### Optimal Use Cases
- Complex software engineering tasks
- Programming language-specific optimizations
- Implementation of algorithms and data structures
- Code optimization and refactoring
- Technical software documentation

#### Configuration Optimizations
```markdown
# DeepSeek Coder Configuration

You are an expert software engineer specializing in [language/framework].

For this coding task:
1. Analyze requirements with attention to edge cases
2. Design a solution prioritizing [efficiency/readability/maintainability]
3. Implement with idiomatic patterns and best practices
4. Include comprehensive documentation and usage examples
5. Highlight optimization opportunities

Structure your response as follows:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Exceptional code generation quality
- Strong understanding of software engineering principles
- Ability to work within existing codebases
- Effective at complex algorithm implementation
- Language-specific optimization knowledge

## Multi-Tool Workflows

### Research → Analysis → Implementation

This workflow combines multiple AI tools for comprehensive solutions.

#### Process Flow
1. **Research Phase** (GPT-4o with browsing)
   - Gather relevant information
   - Identify state-of-the-art approaches
   - Collect reference examples

2. **Analysis Phase** (Claude 3.7 Sonnet with Extended Thinking)
   - Evaluate approaches from research
   - Perform detailed tradeoff analysis
   - Develop conceptual design

3. **Implementation Phase** (DeepSeek Coder)
   - Convert design into concrete implementation
   - Optimize for target environment
   - Develop tests and documentation

#### Configuration Example
```markdown
# Multi-Tool Workflow: Research Phase

You are a research specialist gathering information on [topic].

Please find the most relevant and current information about:
- Core concepts and principles
- Leading approaches and methodologies
- Benchmark results and comparisons
- Implementation considerations

Format findings as a structured research brief with sections for each area and citations.

# Multi-Tool Workflow: Analysis Phase (For Claude)

Using the research provided, perform a comprehensive analysis:

1. Evaluate each approach against these criteria:
   - [List criteria with weights]
2. Identify key tradeoffs and dependencies
3. Recommend an optimal approach with justification
4. Outline a conceptual design

# Multi-Tool Workflow: Implementation Phase (For DeepSeek)

Based on the provided conceptual design, implement a solution that:
1. Follows the specified approach
2. Optimizes for [specific requirements]
3. Includes error handling and edge cases
4. Provides documentation and usage examples
```

### Design → Feedback → Refinement

This iterative workflow improves quality through multiple models.

#### Process Flow
1. **Initial Design** (GPT-4o or Claude Opus)
   - Generate comprehensive first draft
   - Include all required components
   - Document design decisions

2. **Critical Feedback** (Claude with Extended Thinking)
   - Analyze for weaknesses and gaps
   - Identify optimization opportunities
   - Suggest specific improvements

3. **Refinement** (Original or specialized model)
   - Incorporate feedback
   - Optimize critical components
   - Finalize the solution

#### Configuration Example
```markdown
# Initial Design Phase

Create a comprehensive design for [system/document/code].

Requirements:
- [List key requirements]
- [List constraints]
- [List quality attributes]

Provide a complete initial design including:
- [List expected deliverables]

# Feedback Phase (For Claude)

Review the provided design critically:

1. Evaluate against these criteria:
   - [List evaluation criteria]
2. Identify potential weaknesses, including:
   - Performance bottlenecks
   - Scalability issues
   - Security concerns
   - Maintenance challenges
3. Suggest specific improvements for each issue
4. Prioritize recommendations by impact

# Refinement Phase

Based on the feedback provided, refine the initial design:

1. Address each identified issue with specific changes
2. Optimize the critical components
3. Justify any feedback points not incorporated
4. Provide a complete, refined version
```

## Custom Configuration Templating

### Configuration Template Structure

Create your own tool-specific configurations using this template:

```markdown
# [Tool Name] Configuration for [Task Type]

## Context Setting
[Establish role, expertise level, and general approach]

## Task Instructions
[Specific task details and requirements]

## Processing Guidelines
[How the AI should approach the problem]
- Step 1: [Specific instruction]
- Step 2: [Specific instruction]
- etc.

## Output Format
[Response structure and formatting requirements]

## Additional Parameters
[Any special modes or settings to enable]
```

### Calibration Process

To optimize configurations for specific models:

1. **Baseline Testing**
   - Test the same prompt across different models
   - Note differences in output quality and approach
   - Identify relative strengths and weaknesses

2. **Parameter Tuning**
   - Adjust instructions based on model strengths
   - Modify level of detail in task breakdown
   - Calibrate output format specificity

3. **Iteration**
   - Refine based on output quality
   - Create model-specific instruction sets
   - Document effective patterns
# Visual Format Specifications

This document provides detailed specifications for visual elements in AI responses, ensuring consistent, high-quality visual outputs across different AI tools.

## Architecture Diagrams

### System Architecture Diagrams

#### Content Requirements
- **Components**: All significant system components clearly labeled
- **Interfaces**: Connections between components with interface type
- **Data Flow**: Directional indicators showing information flow
- **External Boundaries**: Clear system boundaries and external integrations
- **Layers**: Logical grouping of components by function/layer
- **Dependencies**: Clear indication of component dependencies

#### Notation Guidelines
```
[External System] ←→ [API Gateway] → [Service A] → [Database]
                                    ↓
                                [Service B] → [Cache]
                                    ↓
                               [Event Bus] → [Service C]
```

#### Example Specification
```markdown
Please create a system architecture diagram showing:
- Frontend components (UI, state management)
- Backend services (authentication, core logic, data access)
- Data storage systems (databases, caches)
- External integrations (third-party APIs, partner systems)
- Communication patterns (sync/async, events, direct calls)

Use distinct symbols for different component types and indicate data flow directions.
```

### Sequence Diagrams

#### Content Requirements
- **Actors**: All participants in the interaction
- **Time Flow**: Clear temporal sequence (typically top to bottom)
- **Messages**: Labeled arrows showing requests/responses
- **Activation**: Indication of when components are active
- **Conditional Paths**: Decision points and alternative flows
- **Failure Scenarios**: Error handling paths where relevant

#### Notation Guidelines
```
Actor     Service A     Service B     Database
  |           |             |            |
  |--Request->|             |            |
  |           |--Query----->|            |
  |           |             |--DB Call-->|
  |           |             |<--Result---|
  |           |<--Response--|            |
  |<--Reply---|             |            |
```

#### Example Specification
```markdown
Create a sequence diagram for user authentication flow showing:
- Initial request from client
- Authentication service validation
- Token generation
- Response to client
- Subsequent authenticated request
- Token validation
- Resource access

Include error paths for:
- Invalid credentials
- Expired tokens
- Insufficient permissions
```

## Data Visualization

### Comparison Tables

#### Content Requirements
- **Header Row**: Clear column headers with units if applicable
- **First Column**: Items being compared
- **Data Cells**: Consistent formatting of values
- **Scoring**: Numeric ratings (1-10) with visual indicators
- **Weighting**: Weight values for weighted calculations
- **Totals**: Weighted scores and rankings
- **Highlight**: Visual emphasis on optimal values

#### Format Example
```
| Option | Performance (w=0.3) | Security (w=0.4) | Maintainability (w=0.3) | Weighted Score | Rank |
|--------|---------------------|------------------|-------------------------|----------------|------|
| Opt A  | 9 ⭐⭐⭐ (2.7)        | 7 ⭐⭐ (2.8)       | 8 ⭐⭐⭐ (2.4)            | 7.9            | 1    |
| Opt B  | 7 ⭐⭐ (2.1)          | 8 ⭐⭐⭐ (3.2)      | 6 ⭐⭐ (1.8)              | 7.1            | 2    |
| Opt C  | 8 ⭐⭐⭐ (2.4)         | 6 ⭐⭐ (2.4)       | 7 ⭐⭐ (2.1)              | 6.9            | 3    |
```

#### Example Specification
```markdown
Create a comparison table for the options with these characteristics:
- Include all 5 options in rows
- Evaluate each on the 6 criteria specified
- Use a 1-10 scale for each criterion
- Apply the weights as indicated in parentheses
- Calculate weighted scores
- Rank options from highest to lowest score
- Add visual indicators (⭐) for scores:
  - 1-3: No stars
  - 4-6: ⭐
  - 7-8: ⭐⭐
  - 9-10: ⭐⭐⭐
- Highlight the best score in each column
```

### Decision Trees

#### Content Requirements
- **Decision Nodes**: Questions or decision points
- **Option Branches**: Paths representing choices
- **Leaf Nodes**: Outcomes or terminal states
- **Branch Labels**: Conditions for taking each path
- **Probabilities/Values**: Associated with paths or nodes (if applicable)
- **Critical Path**: Highlighted recommended path

#### Notation Guidelines
```
                         [Initial Decision]
                           /           \
                     [Yes]/             \[No]
                         /               \
            [Secondary Decision]     [Alternative Decision]
               /         \               /        \
          [Opt A]      [Opt B]      [Opt C]     [Opt D]
```

#### Example Specification
```markdown
Create a decision tree for the described scenario with:
- Root node representing the initial decision
- Branches for each option with conditions
- Secondary decision points where relevant
- Terminal nodes showing outcomes
- Annotations with key considerations at each step
- Highlight the recommended path based on specified criteria
```

### Process Flows

#### Content Requirements
- **Start/End**: Clear beginning and end points
- **Actions**: Process steps with descriptive labels
- **Decisions**: Conditional branching points
- **Flows**: Directional arrows connecting elements
- **Loops**: Clearly marked iteration points
- **Swimlanes**: Responsibility boundaries (if applicable)
- **Artifacts**: Inputs and outputs of steps

#### Notation Guidelines
```
(Start) → [Process Step] → <Decision?> → [Success Step] → (End)
                               |
                               ↓
                          [Failure Step] → [Recovery] ↩
```

#### Example Specification
```markdown
Create a process flow diagram showing:
- Initial request handling
- Validation steps with error paths
- Main processing sequence
- Decision points with criteria
- Success and failure paths
- Retry loops where applicable
- Final states and outputs

Use distinct symbols for:
- Start/end points (parentheses)
- Process steps [rectangles]
- Decision points <diamonds>
- Directional arrows connecting elements
```

## Code Visualization

### Code Structure Diagrams

#### Content Requirements
- **Modules**: Major code components
- **Classes/Functions**: Key implementation elements
- **Relationships**: Inheritance, composition, dependencies
- **Interfaces**: Public APIs and contracts
- **Access Patterns**: Usage and interaction flows
- **Data Structures**: Key data representations

#### Notation Guidelines
```
[Module A] ← implements ← [Interface X]
    ↑                         ↑
 inherits                  inherits
    |                         |
[Module B] → depends on → [Module C]
```

#### Example Specification
```markdown
Create a code structure diagram showing:
- Major components/modules
- Key classes with inheritance relationships
- Critical interfaces and implementations
- Dependencies between components
- Data flow between modules
- Extension points

Use UML-inspired notation with:
- Boxes for classes/modules
- Arrows for relationships
- Labels indicating relationship types
```

### Algorithm Flowcharts

#### Content Requirements
- **Initialization**: Starting state and setup
- **Steps**: Processing stages with operations
- **Branching**: Conditional logic
- **Loops**: Iteration constructs
- **Termination**: Exit conditions
- **Complexity**: Annotated time/space complexity
- **Examples**: Sample data state at key points

#### Notation Guidelines
```
(Initialize) → [Step 1] → [Step 2] → <Condition?> → [Step 3A] → (End)
                                         |
                                         ↓
                                     [Step 3B] → [Loop back to Step 2] ↩
```

#### Example Specification
```markdown
Create an algorithm flowchart showing:
- Initialization with input parameters
- Main processing steps in sequence
- Conditional branches with criteria
- Loop structures with entry/exit conditions
- Terminal states and return values
- Annotations with complexity analysis
- Example state progression with sample input

Use consistent notation:
- Rounded rectangles for start/end
- Rectangles for process steps
- Diamonds for decisions
- Arrows for flow direction
```

## Tabular Data

### Feature Comparison Matrices

#### Content Requirements
- **Features**: Comprehensive feature list in rows
- **Options**: Items being compared in columns
- **Support**: Clear indicators of feature support
- **Limitations**: Notes on partial implementations
- **Standouts**: Highlighting of exceptional capabilities
- **Categories**: Logical grouping of related features
- **Importance**: Indicators of feature significance

#### Format Example
```
| Feature Category | Feature | Option A | Option B | Option C |
|------------------|---------|----------|----------|----------|
| Performance      | Feature 1 | ✅ | ⚠️ Limited | ❌ |
|                  | Feature 2 | ✅ | ✅ | ⚠️ Planned |
| Security         | Feature 3 | ✅ Best | ✅ | ✅ |
|                  | Feature 4 | ❌ | ✅ | ✅ |
| Usability        | Feature 5 | ⚠️ Basic | ✅ | ✅ Best |
```

#### Example Specification
```markdown
Create a feature comparison matrix with:
- Features organized by category (rows)
- All options being evaluated (columns)
- Support indicators:
  - ✅ = Fully supported
  - ⚠️ = Partial/limited support (with note)
  - ❌ = Not supported
- "Best" notation for standout implementations
- "Planned" notation for upcoming features
- Bold text for critical features
```

### Decision Matrices

#### Content Requirements
- **Options**: Alternatives in rows
- **Criteria**: Evaluation factors in columns
- **Weights**: Importance weighting for criteria
- **Scores**: Raw scores (typically 1-10)
- **Weighted Scores**: Calculated weighted values
- **Totals**: Sum of weighted scores
- **Ranking**: Clear indication of final ranking

#### Format Example
```
| Option | Criterion 1 (w=0.2) | Criterion 2 (w=0.5) | Criterion 3 (w=0.3) | Total Score | Rank |
|--------|---------------------|---------------------|---------------------|-------------|------|
| Opt A  | 7 (1.4)             | 8 (4.0)             | 9 (2.7)             | 8.1         | 1    |
| Opt B  | 9 (1.8)             | 6 (3.0)             | 8 (2.4)             | 7.2         | 2    |
| Opt C  | 8 (1.6)             | 5 (2.5)             | 7 (2.1)             | 6.2         | 3    |
```

#### Example Specification
```markdown
Create a weighted decision matrix with:
- All options listed as rows
- Evaluation criteria as columns with weights
- Scores on a 1-10 scale for each option/criterion
- Calculation showing raw score and weighted contribution
- Total weighted score for each option
- Ranking based on total score
- Conditional formatting:
  - Bold the highest score in each column
  - Highlight the top-ranked option
```

## Chart Types

### Technical Chart Specifications

Specifications for common chart types in technical contexts:

#### Time Series
```markdown
Create a time series chart showing:
- X-axis: Time periods (specify increments)
- Y-axis: Metric values (specify units and scale)
- Multiple series for comparing trends
- Clear legend identifying each series
- Notable events or thresholds marked
- Trend lines where appropriate

Use text-based visualization with appropriate symbols and labels.
```

#### Comparison Bar Charts
```markdown
Create a bar chart comparing:
- X-axis: Categories/items being compared
- Y-axis: Value metric (specify units and scale)
- Grouped bars for multi-dimension comparison
- Labels showing exact values
- Sorted in descending order by primary metric
- Color coding to distinguish categories

Use text-based visualization with proportional bar lengths.
```

#### Technical Scatter Plots
```markdown
Create a scatter plot showing:
- X-axis: First variable (specify units and range)
- Y-axis: Second variable (specify units and range)
- Points representing individual data items
- Size variation for third dimension (if applicable)
- Color coding for categories
- Trend line showing correlation
- Labeled quadrants or regions of interest

Use text-based visualization with appropriate symbols and positioning.
```

## Documentation Diagrams

### API Interface Diagrams

#### Content Requirements
- **Endpoints**: All API endpoints grouped logically
- **Methods**: HTTP methods or function calls
- **Parameters**: Required and optional inputs
- **Responses**: Return types and structures
- **Authentication**: Security requirements
- **Dependencies**: Related services or components
- **Examples**: Sample calls and responses

#### Format Example
```
API: UserService
┌─────────────────────────────────────────────────────────┐
│ GET /users/{id}                                         │
├─────────────────────────────────────────────────────────┤
│ Parameters:                                             │
│   - id: string (required) - User identifier             │
├─────────────────────────────────────────────────────────┤
│ Responses:                                              │
│   - 200: User object                                    │
│   - 404: Not found error                                │
│   - 401: Authentication error                           │
├─────────────────────────────────────────────────────────┤
│ Authentication: Bearer token                            │
└─────────────────────────────────────────────────────────┘
```

#### Example Specification
```markdown
Create an API interface diagram documenting:
- All endpoints grouped by resource
- HTTP methods for each endpoint
- Path and query parameters with types
- Request body schema for POST/PUT methods
- Response codes and data structures
- Authentication requirements
- Rate limiting information
- Example request/response pairs

Use consistent box notation with clear sections for each component.
```

### Entity Relationship Diagrams

#### Content Requirements
- **Entities**: All major data entities
- **Attributes**: Key fields for each entity
- **Relationships**: Connections between entities
- **Cardinality**: Relationship multiplicity (1:1, 1:N, M:N)
- **Primary Keys**: Clearly marked identifiers
- **Foreign Keys**: Relationship implementation
- **Constraints**: Business rules and validations

#### Notation Guidelines
```
[User]                      [Order]
id (PK)    1 ------- * ┌─→ id (PK)
username               │   user_id (FK)
email                  │   total_amount
created_at             │   created_at
                       │
                       │   [OrderItem]
                       └── id (PK)
                           order_id (FK)
                           product_id (FK)
                           quantity
                           price
```

#### Example Specification
```markdown
Create an entity relationship diagram showing:
- All entities in the data model
- Primary key fields marked (PK)
- Foreign key relationships marked (FK)
- Essential attributes for each entity
- Relationship lines with cardinality:
  - One-to-one: 1 ──── 1
  - One-to-many: 1 ──── *
  - Many-to-many: * ──── *
- Optional relationships shown with dashed lines
```
# Web Crawling prompts

Used with
- [Spider Cloud](spider.cloud)
- Apify
- CLI ...

## Text

```
say “no” to any cookie banners found or pop modals.

TASK: Analyze and extract key knowledge from the web page. INPUT PARAMETERS: - URL: {url} - Page Title: {title} - Page Content: {html_content} - Current Time: {timestamp} EXTRACTION REQUIREMENTS: 1. Generate a concise title (10 words max) capturing the core topic 2. Produce a 3-sentence executive summary highlighting key claims/findings 3. Extract 5-7 core facts/concepts using bullet points 4. Identify primary entities (people, organizations, products, technologies) 5. Extract any numerical data, statistics, or quantifiable claims 6. Note publication date and source credibility indicators 7. Identify content category (research, news, opinion, tutorial, etc.) KNOWLEDGE INTEGRATION: 1. Compare new information with existing database entries 2. Flag contradictions with previously stored information 3. Identify relationships to existing knowledge nodes 4. Assign confidence level (1-5) for factual claims 5. Generate 3-5 keywords/tags for knowledge graph indexing OUTPUT FORMAT: - Structured JSON with all extraction fields - Citation metadata (URL, access date, title, author if available) - Confidence score for overall summary quality (1-5)
```
# Workflow Integration Guide

This guide explains how to effectively integrate AI tools into your development, writing, and analysis workflows.

## Core Integration Principles

### 1. Task Decomposition
- Break complex tasks into discrete steps
- Identify which steps benefit most from AI assistance
- Create clear handoffs between human and AI work

### 2. Context Management
- Maintain consistent context across interactions
- Reference previous outputs explicitly
- Provide explicit updates when context changes

### 3. Output Refinement
- Use follow-up prompts to refine outputs
- Iterate based on specific feedback
- Save effective prompt+output combinations as templates

### 4. Version Control Integration
- Include AI prompts in version control
- Document AI-assisted components
- Attribute significant AI contributions appropriately

## Development Workflow Integration

### Code Generation Workflow

#### Initial Planning Phase
1. **Define Requirements** (Human)
   - Specify functionality, inputs, outputs
   - Identify constraints and edge cases
   - Determine performance requirements

2. **Generate Implementation Strategy** (AI)
   - Use the "Code Solution Template"
   - Request multiple approach options
   - Ask for tradeoff analysis

3. **Select and Refine Approach** (Human)
   - Choose preferred implementation approach
   - Identify areas needing customization
   - Note integration requirements

#### Implementation Phase
4. **Generate Initial Implementation** (AI)
   - Use "Code Generation Template" with selected approach
   - Request complete implementation with tests
   - Specify naming conventions and style preferences

5. **Review and Customize** (Human)
   - Evaluate generated code against requirements
   - Identify areas needing modification
   - Make necessary integrations with existing codebase

6. **Refine Implementation** (AI+Human)
   - Request specific optimizations
   - Address edge cases and error handling
   - Finalize documentation and examples

### Code Review Workflow

1. **Prepare Code for Review** (Human)
   - Select code segments for review
   - Provide context and requirements
   - Specify review focus areas

2. **Generate Review** (AI)
   - Use "Code Review Template"
   - Request specific feedback categories
   - Specify output format for actionable review

3. **Triage Feedback** (Human)
   - Categorize issues by severity
   - Identify patterns in feedback
   - Prioritize improvements

4. **Implement Improvements** (Human+AI)
   - Request specific optimizations based on review
   - Generate alternative implementations for problem areas
   - Use "Advanced Techniques" for complex refactoring

### System Design Workflow

1. **Define Requirements** (Human)
   - Specify functional requirements
   - Establish constraints and scale
   - Define quality attributes

2. **Generate Architecture** (AI)
   - Use "System Design Template"
   - Request component breakdown with responsibilities
   - Specify required diagrams

3. **Review and Refine** (Human)
   - Identify areas needing more detail
   - Question assumptions and tradeoffs
   - Note integration challenges

4. **Detailed Design** (AI+Human)
   - Generate detailed specifications for key components
   - Create interface definitions
   - Develop data models and flows

5. **Implementation Planning** (Human+AI)
   - Break down into implementation tasks
   - Identify dependencies and critical path
   - Create implementation timeline

## Writing and Documentation Workflow

### Technical Documentation Workflow

1. **Define Documentation Scope** (Human)
   - Identify target audience
   - Determine required sections and depth
   - Specify format and style requirements

2. **Generate Structure** (AI)
   - Use "Technical Documentation Template"
   - Request outline with section breakdown
   - Define content types for each section

3. **Generate Content** (AI)
   - Develop content for each section
   - Include required examples and diagrams
   - Format according to specifications

4. **Review and Refine** (Human)
   - Check technical accuracy
   - Ensure completeness and clarity
   - Note areas needing expansion or correction

5. **Finalize Documentation** (AI+Human)
   - Address specific feedback
   - Format for final delivery
   - Create supplementary materials if needed

### Research Synthesis Workflow

1. **Define Research Questions** (Human)
   - Specify primary questions to answer
   - Determine scope and boundaries
   - Identify key sources or data to consider

2. **Generate Research Plan** (AI)
   - Use "Research Synthesis Template"
   - Create structured approach to investigation
   - Define synthesis methodology

3. **Conduct Research** (AI+Human)
   - Gather and organize information
   - Analyze patterns and contradictions
   - Identify key findings

4. **Generate Synthesis** (AI)
   - Create comprehensive synthesis of findings
   - Highlight implications and applications
   - Suggest areas for further investigation

5. **Review and Extend** (Human)
   - Validate accuracy and completeness
   - Identify applications to current work
   - Determine next steps based on findings

## Analysis Workflow Integration

### Data Analysis Workflow

1. **Define Analysis Goals** (Human)
   - Specify questions to answer
   - Identify available data sources
   - Determine required outputs

2. **Generate Analysis Plan** (AI)
   - Use "Data Analysis Template"
   - Create structured analytical approach
   - Define visualization and presentation strategy

3. **Prepare and Analyze Data** (Human+AI)
   - Process and clean data
   - Perform initial analysis
   - Generate visualizations

4. **Interpret Results** (AI+Human)
   - Create narrative explaining findings
   - Highlight key insights and patterns
   - Connect to original questions

5. **Generate Recommendations** (AI)
   - Develop actionable recommendations
   - Support with specific findings
   - Address limitations and assumptions

### Decision Support Workflow

1. **Define Decision Context** (Human)
   - Specify decision to be made
   - Identify options and constraints
   - Determine decision criteria

2. **Generate Decision Framework** (AI)
   - Use "Comparative Analysis Framework"
   - Create evaluation methodology
   - Define scoring and weighting system

3. **Evaluate Options** (AI+Human)
   - Score options against criteria
   - Calculate weighted results
   - Perform sensitivity analysis

4. **Generate Recommendation** (AI)
   - Present ranked options with rationale
   - Highlight key differentiators
   - Address potential objections

5. **Implement Decision** (Human)
   - Make final selection
   - Document decision process
   - Create implementation plan

## Cross-Model Coordination

### Multi-Model Research Projects

For comprehensive research projects requiring multiple perspectives:

1. **Project Definition** (Human)
   - Define research scope and objectives
   - Identify required specializations
   - Create coordination framework

2. **Specialized Research** (Multiple AI models)
   - Assign different aspects to specialized models:
     - Claude for nuanced technical research
     - GPT-4o with browsing for current information
     - DeepSeek for code-heavy components

3. **Synthesis** (Claude with Extended Thinking)
   - Combine specialized inputs
   - Resolve contradictions
   - Create unified perspective

4. **Review and Refinement** (Human)
   - Identify gaps or inconsistencies
   - Request additional specialized input
   - Direct final synthesis

### Progressive Refinement Chain

For complex creative or technical work:

1. **Initial Generation** (GPT-4 or Claude)
   - Create first draft with broad structure
   - Establish key components and approach
   - Identify areas needing specialization

2. **Specialized Enhancement** (Specialized model)
   - Elaborate specific technical sections
   - Enhance code components
   - Develop specific visualizations

3. **Critical Review** (Claude with Extended Thinking)
   - Analyze for consistency and completeness
   - Identify logical flaws or gaps
   - Generate specific improvement recommendations

4. **Final Refinement** (Original or specialized model)
   - Implement recommended improvements
   - Harmonize style and structure
   - Finalize for delivery

## Tool Integration Examples

### IDE Integration Workflow

1. **Local Development Environment**
   - Configure editor extensions (Cursor, GitHub Copilot)
   - Create template snippets for common prompts
   - Develop keyboard shortcuts for AI interactions

2. **Version Control Integration**
   - Include prompt templates in project repositories
   - Document AI usage in contribution guidelines
   - Create commit hooks for AI-assisted contributions

3. **Continuous Integration**
   - Automate AI-powered code reviews
   - Generate test cases with AI assistance
   - Develop documentation from code automatically

### Document Management Integration

1. **Content Creation**
   - Use AI templates for consistent document generation
   - Create modular content blocks for reuse
   - Develop style guides for AI-assisted writing

2. **Review and Refinement**
   - Implement AI-powered editing workflows
   - Create checklist templates for human review
   - Establish quality standards for AI outputs

3. **Publication and Distribution**
   - Automate format conversions for different platforms
   - Generate supplementary materials automatically
   - Create presentation materials from documents

## Practical Integration Recipes

### Code Creation Recipe

```markdown
# Code Creation Workflow

## Step 1: Problem Definition
Use this prompt:
```
I need to implement [specific functionality].
Key requirements:
- [Requirement 1]
- [Requirement 2]
- [Requirement 3]

Technical constraints:
- Language: [Language]
- Framework: [Framework]
- Performance needs: [Specific requirements]

Please provide a high-level approach before implementation.
```

## Step 2: Review Proposed Approach
Evaluate the AI's suggested approach against:
- Alignment with requirements
- Technical feasibility
- Integration with existing systems
- Performance characteristics

## Step 3: Request Implementation
Use this prompt:
```
The approach looks good. Please implement a complete solution with:
1. Well-structured code following [style guide/conventions]
2. Error handling for these cases: [list cases]
3. Unit tests covering key functionality
4. Clear documentation and usage examples

[Include any specific implementation details based on the approach]
```

## Step 4: Integration and Testing
- Merge the code into your project
- Run tests and validate functionality
- Document any modifications made to the AI-generated code
```

### Technical Document Creation Recipe

```markdown
# Technical Documentation Workflow

## Step 1: Document Specification
Use this prompt:
```
I need to create [document type] about [subject].

Target audience:
- [Primary audience with technical level]
- [Secondary audience if applicable]

Required sections:
- [Section 1]
- [Section 2]
- [Section 3]

Key information to include:
- [Key point 1]
- [Key point 2]
- [Key point 3]

Please provide an outline before creating the full document.
```

## Step 2: Review and Approve Outline
Evaluate the AI's suggested outline against:
- Completeness
- Logical structure
- Alignment with audience needs
- Appropriate technical depth

## Step 3: Request Full Document
Use this prompt:
```
The outline looks good. Please generate the complete document with:
1. All sections fully developed
2. Code examples for key concepts
3. Visual diagrams for [specific complex concepts]
4. Tables comparing [specific items to compare]

Formatting requirements:
- Use markdown formatting
- Include syntax highlighting for code blocks
- Create clear hierarchical structure with headers
```

## Step 4: Review and Finalize
- Check technical accuracy
- Ensure all requirements are met
- Add any missing information
- Format for final delivery
```

### Decision Analysis Recipe

```markdown
# Decision Analysis Workflow

## Step 1: Frame the Decision
Use this prompt:
```
I need to make a decision about [decision context].

Options being considered:
- [Option 1]
- [Option 2]
- [Option 3]

Key decision criteria:
- [Criterion 1] (importance: high/medium/low)
- [Criterion 2] (importance: high/medium/low)
- [Criterion 3] (importance: high/medium/low)

Constraints:
- [Constraint 1]
- [Constraint 2]

Please create a decision analysis framework.
```

## Step 2: Review Framework
Evaluate the AI's suggested framework against:
- Completeness of criteria
- Appropriate weighting
- Consideration of constraints
- Evaluation methodology

## Step 3: Request Analysis
Use this prompt:
```
The framework looks good. Please perform a complete analysis:
1. Evaluate each option against all criteria
2. Create a weighted decision matrix
3. Perform sensitivity analysis on the weights
4. Provide a clear recommendation with rationale
5. Address potential weaknesses of the recommended option
```

## Step 4: Make and Document Decision
- Review analysis
- Consider any factors not in the model
- Document final decision with rationale
- Create implementation plan
```
# MCP

- https://github.com/punkpeye/awesome-mcp-servers
- mcp servers registries (desc sort : 1st is preferred)
    - https://www.pulsemcp.com/
    - https://glama.ai/mcp/servers
    - https://mcp.so/
    - [Awesome MCP Servers](https://mcpservers.org/)
    - [mcp-get](https://mcp-get.com/)
- claude <> sqlite db YT tutorial (https://www.youtube.com/watch?v=wxCCzo9dGj0)
# AI Prompts Master Collection

This directory contains a comprehensive, organized collection of AI prompts, templates, and configurations for enhancing development workflows and AI interactions.

## Directory Structure

- **Core_Prompting/**: Fundamental prompt engineering techniques, base templates, and system roles
- **Provider_Specific/**: Configurations and optimizations for specific AI providers (Claude, OpenAI, etc.)
- **Use_Cases/**: Domain-specific prompt templates organized by purpose (coding, documentation, research, etc.)
- **Templates_Components/**: Reusable prompt fragments and formatting templates
- **Tools_Integration/**: CLI tools, IDE integrations, and workflow automation
- **Documentation_Reference/**: Guides, specifications, and best practices
- **Archive/**: Archived content including duplicates and merged originals

## Usage

Each section contains its own README with specific guidance on how to use the resources effectively. The collection is designed to be modular, allowing you to combine different components based on your specific needs.

## Key Components

- **Base Templates**: Starting points for different types of AI interactions
- **System Roles**: Configurations that define the AI's behavior and capabilities
- **Provider-Specific Optimizations**: Techniques tailored to specific AI models
- **Workflow Integration**: Guides for incorporating AI into development workflows
- **Code Generation Templates**: Specialized prompts for generating different types of code
- **Documentation Templates**: Formats for creating technical documentation

## Contributing

To add new prompts or refine existing ones:

1. Place the content in the appropriate directory based on its purpose
2. Follow the established naming conventions
3. Update the relevant README files
4. Avoid duplicating existing content

## Origin

This collection was created by consolidating and organizing AI-related content previously scattered across multiple locations. It represents a curated set of best practices and effective prompts that have proven valuable in real-world usage.
# Claude Code Repository Guidance

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This repository contains multiple projects spanning different languages and technologies:

- **Brrr**: High-performance workflow scheduling system (Python)
- **Zellij**: Terminal workspace/multiplexer (Rust)
- **Obsidian-Vimrc-Support**: Plugin for Obsidian.md
- **Platform/Anterior**: Healthcare platform monorepo (multi-language)
- **Nixos-config**: NixOS configuration system
- Several smaller utilities and tools

## Common Commands

### Nix-based Development (Platform/Anterior)

```bash
# Enter development shell
nix develop

# Build Docker images for development
ant build dev

# Run services locally
ant up

# Run with auto-rebuild on file changes
ant watch

# Populate services with test data
ant populate

# Run all tests and sanity checks
nix flake check -L

# Load testing
ant load-test [API_IP_ADDRESS]
```

### Python Projects (Brrr, workflows)

```bash
# Install dependencies
uv pip install -e .

# Run tests
pytest
pytest -m 'not dependencies'  # Skip tests requiring external dependencies

# Run specific test
pytest tests/test_file.py::test_function -v

# Brrr demo
nix run github:nobssoftware/brrr#demo
curl 'http://localhost:8333/hello?greetee=John'
```

### Rust Projects (Zellij)

```bash
# Run debug build
cargo xtask run

# Run all tests
cargo xtask test

# Build release version
cargo build --release
```

### JavaScript/TypeScript Projects

```bash
# Install dependencies
npm install

# Run linting
npm run lint

# Run tests
npm run test

# Build
npm run build
```

## Architecture

### Brrr

- Queue & database-agnostic workflow scheduling system
- Minimal core with extension points for different backends
- Supports Redis and SQS as queues, DynamoDB as database
- Designed for horizontal scaling

### Platform/Anterior

- Microservices architecture with Docker containerization
- Services communicate via HTTP/REST APIs with OpenAPI specs
- Prefect for workflow orchestration
- Nix for reproducible builds
- Frontend surfaces: Cortex (React/TypeScript)

### Zellij

- Terminal multiplexer with workspace focus
- Plugin system supporting WebAssembly
- Multi-pane layout system with floating windows

## Development Workflows

### Environment Setup

1. Install Nix package manager
2. Run `nix develop` to enter development shell
3. Configure editor with EditorConfig support
4. For secrets, use SOPS with correct key access

### Secret Management

- Secrets stored in `secrets.json` and `secrets.prod.json`
- Use 1Password for secure storage
- Never commit unencrypted secrets

### Running Services

1. Run `ant up` to start all services
2. Use `ant watch` for development with auto-rebuild
3. Access specific service documentation in their respective README files

### Testing

Follow testing protocols specific to each project:
- Python: pytest with appropriate markers
- JavaScript: Jest/Vitest
- Rust: Cargo test suite
# Claude MCP Configuration

This directory contains configuration and guidance for using Claude with MCP (Model Context Protocol) servers.

## MCP Server Resources

- [Awesome MCP Servers](https://github.com/punkpeye/awesome-mcp-servers) - Curated list of MCP servers
- MCP server registries (sorted by preference):
  - [Pulse MCP](https://www.pulsemcp.com/)
  - [Glama AI](https://glama.ai/mcp/servers)
  - [MCP.so](https://mcp.so/)
  - [Awesome MCP Servers](https://mcpservers.org/)
  - [MCP-Get](https://mcp-get.com/)

## Configuration Files

- `claude_desktop_config.json` - Configuration for Claude Desktop with MCP servers
- `claude-code-repository-guidance.md` - Guidance for Claude Code when working with repositories

## Setup Instructions

1. Install the Claude Desktop application
2. Configure MCP servers by copying `claude_desktop_config.json` to your user directory
3. For repository guidance, place the `claude-code-repository-guidance.md` file in your project root and rename to `CLAUDE.md`

## Additional Resources

- [Claude <> SQLite DB YouTube Tutorial](https://www.youtube.com/watch?v=wxCCzo9dGj0)
## Prompt:
which of hammerspoon configs is best for mac os vim programmer like me very keyboard driven?

## Response:
Based on the document and your needs as a keyboard-driven Vim programmer, these configurations stand out:

1.  Ki.spoon - Most comprehensive Vim-like environment:

*   Modal hotkeys similar to Vim
*   Pre-configured shortcuts for stock macOS apps
*   Customizable modes and actions

2.  VimMode.spoon - Direct Vim integration:

*   Adds Vim movements to any macOS input
*   Supports `jk` for normal mode
*   Implements comprehensive motions/operators

3.  spacehammer - Spacemacs-inspired:

*   Clean, modal interface
*   Familiar to Vim users who like Spacemacs
*   Efficient keyboard-driven workflow

Recommendation: Start with Ki.spoon since it offers the most complete Vim-like experience while maintaining flexibility for customization. Its modal system will feel natural coming from Vim, and you can gradually extend it as needed.

Installation: Visit [https://github.com/andweeb/ki](https://github.com/andweeb/ki) for setup instructions.

Want me to explain any specific features of these configurations?

## Prompt:
I love launcher prefix like lazy nvim telescope that is prefixed by sensible key mappings and is very easy to view and edit. I also use many mac apps so like apps like homerow for scrolling + clicking. And windows+tabs+apps mgmt along with workspaces mgmt is crucial.

## Response:
Based on your preferences for a Lazy.nvim/Telescope-like experience with comprehensive app/window management, let me recommend the most suitable configs:

1.  MenuHammer would be your best primary option because it:

*   Uses a Spacemacs-style prefix menu system (very similar to Telescope/Lazy.nvim)
*   Fully customizable menus defined in simple tables
*   Can chain commands like you would with Telescope
*   Easy to view and edit configurations

2.  Combine it with:

*   szymon\_k's configuration for:
    *   Smart app launching/cycling (like Telescope frecency)
    *   Advanced space management with visual indicators
    *   Window movement between workspaces

3.  For your app-heavy workflow, also consider adding:

*   gwww's `smartLaunch` module for cycling through app windows
*   pstadler's fast space switching
*   windvalley's auto input method switching between apps

Example setup approach:

```lua
-- Example prefix-based launcher (MenuHammer style)
local launcher = {
  prefix = {"cmd", "space"},  -- Similar to Telescope prefix
  entries = {
    f = {action = "findFiles", desc = "Find Files"},
    a = {action = "findApps", desc = "Find Applications"},
    w = {action = "windowManager", desc = "Manage Windows"},
    s = {action = "switchSpace", desc = "Switch Space"}
  }
}
```

Would you like me to:

1.  Show more specific configuration examples for any of these components?
2.  Explain how to integrate multiple configs while avoiding conflicts?

## Prompt:
are these modern configurations? I use nvim + x-cmd + zsh shell commands + very fuzzy style search and context driven workflows

## Response:
Looking at the timestamps and modern developer workflows, let me suggest more contemporary configurations:

1.  spacehammer ([https://github.com/agzam/spacehammer](https://github.com/agzam/spacehammer)) is the most modern and aligned with your workflow because it:

*   Actively maintained (recent commits)
*   Built for developers using terminal-centric workflows
*   Integrates well with Neovim context switching
*   Has fuzzy-finder style interactions

2.  STRML's configuration also pairs well because:

*   Built for terminal-first developers
*   Has Vim-style `hjkl` navigation
*   Integrates with modern app workflows

However, looking through these configs, I notice many are a few years old. For your modern workflow combining Neovim + x-cmd + zsh, you might want to consider:

1.  Building a custom config focused on:

```lua
-- Example modern approach
local wezterm = require('wezterm')  -- For terminal management
local fuzzy = require('telescope')   -- For fuzzy-style searching

-- Context-aware app switching
function smartContextSwitch()
  local currentApp = hs.application.frontmostApplication()
  if currentApp:name() == "Neovim" then
    -- Handle Neovim context
  elseif currentApp:name() == "WezTerm" then
    -- Handle terminal context
  end
end

-- Fuzzy command palette (similar to Telescope)
function cmdPalette()
  -- Implement fuzzy finder for commands
end
```

Would you like me to:

1.  Help you build a modern config from scratch aligned with your workflow?
2.  Show how to adapt spacehammer specifically for Neovim + terminal workflows?

The main challenge is that many Hammerspoon configs haven't kept pace with modern development workflows centered around Neovim, fuzzy-finding, and context-switching. A custom solution might serve you better.

## Prompt:
additionally I use a lot of Raycast + obsidian app + drafts + termius / working copy on iphone ipad

and starting to adopt more seriously Kanata/yabai/Nix

Here are my brew apps + pkgs listed

Copy

`tap "coder/coder" tap "homebrew/bundle" tap "homebrew/services" tap "koekeishiya/formulae" tap "olets/tap" tap "rfidresearchgroup/proxmark3" tap "stripe/stripe-cli" tap "temporalio/brew" tap "typesense/tap" tap "waydabber/betterdisplay" tap "xo/xo" brew "act" brew "shellcheck" brew "actionlint" brew "aider" brew "angle-grinder" brew "ast-grep" brew "atuin", restart_service: :changed brew "localstack" brew "awscli-local" brew "bat" brew "biome" brew "node" brew "bitwarden-cli" brew "broot" brew "cheat" brew "cloudflare-wrangler2" brew "curlie" brew "wget" brew "dasht" brew "datasette" brew "direnv" brew "dust" brew "esbuild" brew "eslint" brew "exiftool" brew "eza" brew "fastfetch" brew "fd" brew "ffmpeg" brew "fx" brew "gdrive" brew "gdrive-downloader" brew "gh" brew "ghi" brew "git" brew "git-delta" brew "glances" brew "glow" brew "gping" brew "grex" brew "gron" brew "helix" brew "hexyl" brew "htop" brew "http-prompt" brew "httrack" brew "hurl" brew "hyperfine" brew "imagemagick" brew "jd" brew "jq" brew "just" brew "k9s" brew "kanata" brew "lazygit" brew "lua-language-server" brew "luarocks" brew "mas" brew "mcfly" brew "miller" brew "minio" brew "nasm" brew "neovim" brew "netcat" brew "nextdns" brew "nmap" brew "nnn" brew "odin" brew "onefetch" brew "pandoc" brew "parallel" brew "pinentry-mac" brew "pkgconf" brew "poppler" brew "posting" brew "prettier" brew "procs" brew "prometheus" brew "pyenv" brew "rclone" brew "ripgrep" brew "rollup" brew "ruff" brew "rust" brew "rustscan" brew "scrapy" brew "sd" brew "sevenzip" brew "skaffold" brew "speedtest-cli" brew "speexdsp" brew "sq" brew "temporal" brew "tldr" brew "traefik" brew "tree" brew "trippy" brew "uv" brew "vite" brew "volta" brew "weasyprint" brew "xclip" brew "yazi" brew "yq" brew "zellij" brew "zoxide" brew "zsh-autopair" brew "zsh-autosuggestions" brew "zsh-completions" brew "zsh-history-substring-search" brew "zsh-syntax-highlighting" brew "coder/coder/coder" brew "koekeishiya/formulae/skhd" brew "olets/tap/zsh-abbr" brew "olets/tap/zsh-autosuggestions-abbreviations-strategy" brew "rfidresearchgroup/proxmark3/proxmark3" brew "stripe/stripe-cli/stripe" brew "temporalio/brew/tcld" cask "aide-app" cask "arc" cask "carbon-copy-cloner" cask "cursor" cask "dash" cask "font-symbols-only-nerd-font" cask "ghidra" cask "google-drive" cask "hopper-disassembler" cask "istat-menus" cask "virtualbuddy" mas "Amphetamine", id: 937984704 mas "Bear", id: 1091189122 mas "Bitwarden", id: 1352778147 mas "Canva", id: 897446215 mas "Charmstone", id: 1563735522 mas "Cleaner for Xcode", id: 1296084683 mas "CleanMyMac", id: 1339170533 mas "Day One", id: 1055511498 mas "Drafts", id: 1435957248 mas "Dropover", id: 1355679052 mas "Fantastical", id: 975937182 mas "Flow", id: 1423210932 mas "Focus", id: 777233759 mas "KakaoTalk", id: 869223134 mas "Keynote", id: 409183694 mas "Kindle", id: 302584613 mas "LanScan", id: 472226235 mas "lire", id: 1482527526 mas "Numbers", id: 409203825 mas "One Thing", id: 1604176982 mas "Pages", id: 409201541 mas "Pandan", id: 1569600264 mas "Pixea", id: 1507782672 mas "Todoist", id: 585829637 mas "Toggl Track", id: 1291898086 mas "Transcribe AI", id: 6479871768 mas "Tripsy", id: 1429967544 mas "Ulysses", id: 1225570693 com.apple.pkg.MAContent10_AssetPack_0575_DrummerManchesterGBLogic com.apple.pkg.MAContent10_AssetPack_0830_EXS_ElectronicDrumkitsDarkSecrets com.apple.pkg.MAContent10_AssetPack_0858_EXS_ElectronicDrumkitsTakeADaytripSoHo com.apple.pkg.MAContent10_AssetPack_0286_AlchemyVocalsSoloVocalsSustainMisc com.apple.pkg.MAContent10_AssetPack_0827_AppleLoopsUltimate808s com.apple.pkg.MAContent10_AssetPack_0066_AlchemySoundscapesCustomNSingle com.apple.pkg.CLTools_SDK_macOS13 com.apple.pkg.MAContent10_AssetPack_0859_EXS_ElectronicDrumkitsTakeADaytripWashington com.apple.pkg.MAContent10_AssetPack_0676_EXS_ElectronicDrumkitsElectronicPop com.apple.pkg.MAContent10_AssetPack_0251_AlchemySynthsDigitalRackChord com.apple.pkg.MAContent10_AssetPack_0019_AlchemyLoopsMelodic com.apple.pkg.MAContent10_AssetPack_0836_AppleLoopsBacklightBounce com.apple.pkg.MAContent10_AssetPack_0831_EXS_ElectronicDrumkitsFluidShadow com.apple.pkg.MAContent10_AssetPack_0647_AppleLoopsDrummerPercussion com.apple.pkg.MAContent10_AssetPack_0569_DrummerFourOnTheFloorGBLogic com.apple.pkg.MAContent10_AssetPack_0226_AlchemySynthsAnalogResonant com.apple.pkg.MAContent10_AssetPack_0138_AlchemyGuitarsPluckedForgottenHarp com.apple.pkg.MAContent10_AssetPack_0726_AppleLoopsToyBox com.apple.pkg.MAContent10_AssetPack_0571_DrummerHeavyGBLogic com.apple.pkg.MAContent10_AssetPack_0130_AlchemyGuitarsAcousticTrad com.apple.pkg.CLTools_SDK_macOS12 com.apple.pkg.MAContent10_AssetPack_0668_AppleLoopsReggaetonPop com.apple.pkg.MAContent10_AssetPack_0330_AppleLoopsJamPackWorld com.apple.pkg.MAContent10_AssetPack_0022_AlchemyDrumsMisc com.apple.pkg.MAContent10_AssetPack_0503_EXS_ChoirChamberClassical com.apple.pkg.MAContent10_AssetPack_0494_EXS_OrchPercGlockenspiel com.apple.pkg.MAContent10_AssetPack_0573_DrummerLiverpoolGBLogic com.apple.pkg.MAContent10_AssetPack_0730_EXS_ElectronicDrumkitsVintageCR8000 com.apple.pkg.XProtectPlistConfigData_10_15.16U4352 com.apple.pkg.MAContent10_AssetPack_0757_EXS_ElectronicDrumkitsBlowingSpeakers com.apple.pkg.MAContent10_AssetPack_0833_EXS_ElectronicDrumkitsRadiantFlow com.apple.pkg.MAContent10_AssetPack_0864_EXS_ElectronicDrumkitsWatchTheSoundInTheKitches com.apple.pkg.MAContent10_AssetPack_0637_AppleLoopsDrummerKyle com.apple.pkg.MAContent10_AssetPack_0849_EXS_ElectronicDrumkitsOakFelderTwoTonPop com.apple.pkg.MAContent10_AssetPack_0593_DrummerSoCalGBLogic com.apple.pkg.MAContent10_AssetPack_0008_AlchemyKeysAcoustic com.apple.pkg.MAContent10_AssetPack_0899_EXS_ElectronicDrumkitsAnalogBliss com.apple.pkg.CLTools_Executables com.apple.pkg.MAContent10_AssetPack_0810_EXS_ElectronicDrumkitsAbstractThought com.apple.pkg.MAContent10_AssetPack_0085_AlchemySoundscapesOrangeDirt com.apple.pkg.MAContent10_AssetPack_0909_AppleLoopsModularRhythms com.apple.files.data-template com.apple.pkg.MAContent10_AssetPack_0696_DrummerSpeakeasyGBLogic com.apple.pkg.MAContent10_AssetPack_0654_AlchemySettingsGozaderaLatina com.apple.pkg.CLTools_SDK_macOS_LMOS com.apple.pkg.MAContent10_AssetPack_0857_EXS_ElectronicDrumkitsTakeADaytripHillsborough com.apple.pkg.MAContent10_AssetPack_0361_EXS_BassElectricMuted com.apple.pkg.MAContent10_AssetPack_0616_LogicAlchemyEssentials com.apple.pkg.MAContent10_AssetPack_0921_AppleLoops8BitLegends com.apple.pkg.MAContent10_AssetPack_0824_EXS_ElectronicDrumkitsShadowZone com.apple.pkg.MAContent10_AssetPack_0808_EXS_808HiHats com.apple.pkg.MAContent10_AssetPack_0141_AlchemyGuitarsPluckedDulimar com.apple.pkg.MAContent10_AssetPack_0931_AppleLoopsVoxMelodic com.apple.pkg.GatekeeperCompatibilityData.16U1906 com.apple.pkg.MAContent10_AssetPack_0876_EXS_ElectronicDrumkitsTomMischUptownFlow com.apple.pkg.MAContent10_AssetPack_0761_EXS_ElectronicDrumkitsRedLine com.apple.pkg.XProtectPlistConfigData_10_15.16U4347 com.apple.pkg.XProtectPlistConfigData_10_15.16U4353 com.apple.pkg.MAContent10_AssetPack_0199_AlchemySynthsAnalog1974 com.apple.pkg.MAContent10_AssetPack_0887_AlchemySettingsAlphaWaves com.apple.pkg.MAContent10_AssetPack_0028_AlchemyDrumsUnusualIceFX com.apple.pkg.MAContent10_AssetPack_0317_AppleLoopsModernRnB1 com.apple.pkg.MAContent10_AssetPack_0823_EXS_ElectronicDrumkitsSecretSauce com.apple.pkg.MAContent10_AssetPack_0537_DrummerShaker com.apple.pkg.MAContent10_AssetPack_0707_AlchemyKeysCustom com.apple.pkg.MAContent10_AssetPack_0837_AlchemyBacklightBounce com.apple.pkg.MAContent10_AssetPack_0715_EXS_ElectronicDrumkitsRetroBox com.apple.pkg.MAContent10_AssetPack_0198_AlchemySynthsSingleNCustom com.apple.pkg.MAContent10_AssetPack_0567_DrummerEastBayGBLogic com.apple.pkg.MAContent10_AssetPack_0482_EXS_OrchWoodwindAltoSax com.apple.pkg.CLTools_SDK_macOS14 com.apple.pkg.MAContent10_AssetPack_0924_EXS_ElectronicDrumkitsLowTechFunk com.apple.pkg.CLTools_SDK_macOS_NMOS com.apple.pkg.MAContent10_AssetPack_0679_EXS_ElectronicDrumkitsHacienda com.apple.pkg.MAContent10_AssetPack_0845_EXS_ElectronicDrumkitsBoysNoizeNaturalStructures com.apple.pkg.MAContent10_AssetPack_0005_AlchemyStrings com.apple.pkg.MAContent10_AssetPack_0234_AlchemySynthsAnalogMisc com.apple.pkg.MAContent10_AssetPack_0885_AppleLoopsAlphaWaves com.apple.pkg.MAContent10_AssetPack_0305_AlchemyVocalsVocalPhrasesAnastacia com.apple.pkg.MAContent10_AssetPack_0604_EXS_WorldPipa com.apple.pkg.MAContent10_AssetPack_0010_AlchemyMalletsMisc com.apple.pkg.MAContent10_AssetPack_0236_AlchemySynthsDigitalPad com.apple.pkg.MAContent10_AssetPack_0897_EXS_ElectronicDrumkitsAnalogArchitecture com.apple.pkg.LogicProTrial com.apple.pkg.MAContent10_AssetPack_0674_EXS_ElectronicDrumkitsDeepEnd com.apple.pkg.MAContent10_AssetPack_0878_EXS_KeyboardsTrakgirl com.apple.pkg.MAContent10_AssetPack_0913_EXS_ElectronicDrumkitsClicksAndSticks com.apple.pkg.MAContent10_AssetPack_0377_EXS_Mellotron com.apple.pkg.MAContent10_AssetPack_0844_EXS_ElectronicDrumkitsBoysNoizeControlVoltage com.apple.pkg.MAContent10_AssetPack_0553_AppleLoopsChillwave2 com.apple.pkg.MAContent10_AssetPack_0628_AppleLoopsDrummerAidan com.apple.pkg.MAContent10_AssetPack_0714_EXS_ElectronicDrumkitsLiveFeel com.apple.pkg.MAContent10_AssetPack_0585_DrummerRootsGBLogic com.apple.pkg.CLTools_SDK_macOS110 com.apple.pkg.MAContent10_AssetPack_0334_IRsLargeSpacesHalls com.apple.pkg.MAContent10_AssetPack_0907_EXS_ElectronicDrumkitsPlonkyModular com.apple.pkg.MAContent10_AssetPack_0834_EXS_ElectronicDrumkitsSuperSolid com.apple.pkg.MAContent10_AssetPack_0048_AlchemyPadsDigitalHolyGhost com.apple.pkg.MAContent10_AssetPack_0338_IRsLargeSpacesOutdoor com.apple.pkg.MAContent10_AssetPack_0735_EXS_ElectronicDrumkitsVintageTR909 com.apple.pkg.MAContent10_AssetPack_0923_EXS_ElectronicDrumkitsConsoleOperator com.apple.pkg.MAContent10_AssetPack_0853_EXS_ElectronicDrumkitsSoulectionSoulFire com.apple.pkg.MAContent10_AssetPack_0510_EXS_Strings com.apple.pkg.MAContent10_AssetPack_0257_AlchemySynthsDigitalMisc com.apple.pkg.MAContent10_AssetPack_0728_EXS_ElectronicDrumkitsVintageLinnDrum com.apple.pkg.MAContent10_AssetPack_0868_EXS_KeyboardsWatchTheSound com.apple.pkg.MAContent10_AssetPack_0908_EXS_ElectronicDrumkitsWestCoastModular com.apple.pkg.MAContent10_AssetPack_0563_DrummerBrooklynGBLogic com.apple.pkg.MAContent10_AssetPack_0869_EXS_KeyboardsTakeADaytrip com.apple.pkg.MAContent10_AssetPack_0838_AppleLoopsBoyzNoize com.apple.pkg.MAContent10_AssetPack_0850_EXS_ElectronicDrumkitsSoulectionAmpedUp com.apple.pkg.MAContent10_AssetPack_0539_DrummerTambourine com.apple.pkg.MAContent10_AssetPack_0814_EXS_ElectronicDrumkitsDoubleVision com.apple.pkg.MAContent10_AssetPack_0760_EXS_ElectronicDrumkitsJaggedEdges com.apple.pkg.MAContent10_AssetPack_0847_EXS_ElectronicDrumkitsOakFelderHitSynth com.apple.pkg.MAContent10_AssetPack_0443_EXS_WorldAfricanMarimba com.apple.pkg.MAContent10_AssetPack_0091_AlchemyBassMini3VCOSaws com.apple.pkg.MAContent10_AssetPack_0704_AppleLoopsHipHop com.apple.pkg.MAContent10_AssetPack_0602_EXS_WorldChineseKit com.apple.pkg.MAContent10_AssetPack_0712_EXS_ElectronicDrumkitsHybridKnock com.apple.pkg.MAContent10_AssetPack_0200_AlchemySynthsAnalogAnalog com.apple.pkg.MAContent10_AssetPack_0683_EXS_ElectronicDrumkitsModern808 com.apple.pkg.MAContent10_AssetPack_0812_EXS_ElectronicDrumkitsDeepCrunch com.apple.pkg.MAContent10_AssetPack_0906_EXS_ElectronicDrumkitsModularSpace com.apple.pkg.MAContent10_AssetPack_0323_AppleLoopsVintageBreaks com.apple.pkg.MAContent10_AssetPack_0487_EXS_OrchWoodwindFluteSolo com.apple.pkg.MAContent10_AssetPack_0557_IRsSharedAUX com.apple.pkg.MAContent10_AssetPack_0589_DrummerSlowJamGBLogic com.apple.pkg.MAContent10_AssetPack_0720_EXS_ToyPiano com.apple.pkg.MAContent10_AssetPack_0658_AlchemySoundEffectsToyPercussion com.apple.pkg.MAContent10_AssetPack_0862_EXS_ElectronicDrumkitsWatchTheSoundCR78MadeIconic com.apple.pkg.MAContent10_AssetPack_0034_AlchemyDrumsUnusualCupOfTea com.apple.pkg.MAContent10_AssetPack_0818_EXS_ElectronicDrumkitsMetallicEdges com.apple.pkg.MAContent10_AssetPack_0822_EXS_ElectronicDrumkitsPlatinumDreams com.apple.pkg.MAContent10_AssetPack_0645_AppleLoopsDrummerZak com.apple.pkg.MAContent10_AssetPack_0484_EXS_OrchWoodwindClarinetSolo com.apple.pkg.MAContent10_AssetPack_0855_EXS_ElectronicDrumkitsTakeADaytripBrooklynBorough com.apple.pkg.MAContent10_AssetPack_0326_AppleLoopsJamPack1 com.apple.pkg.MAContent10_AssetPack_0566_DrummerDetroitGarageLogic com.apple.pkg.MAContent10_AssetPack_0329_AppleLoopsJamPackOrchestra com.apple.pkg.MAContent10_AssetPack_0839_AppleLoopsMarkLettieri com.apple.pkg.MAContent10_AssetPack_0129_AlchemyGuitarsAcousticAcoustic com.apple.pkg.MAContent10_AssetPack_0920_AlchemySettings8BitLegends com.apple.pkg.MAContent10_AssetPack_0905_EXS_ElectronicDrumkitsModularMayhem com.apple.pkg.MAContent10_AssetPack_0880_EXS_ElectronicDrumkitsTrakgirlHardKnocking com.apple.pkg.MAContent10_AssetPack_0634_AppleLoopsDrummerGraham com.apple.pkg.MAContent10_AssetPack_0863_EXS_ElectronicDrumkitsWatchTheSoundHollywoodFoley com.apple.pkg.MAContent10_AssetPack_0910_PlugInSettingsGBLogic com.apple.pkg.MRTConfigData_10_15.16U4211 com.apple.pkg.MAContent10_AssetPack_0751_EXS_ElectronicDrumkitsSyntheticBassHouse com.apple.pkg.MAContent10_AssetPack_0861_EXS_ElectronicDrumkitsTomMischLoFiFunk com.apple.pkg.MAContent10_AssetPack_0815_EXS_ElectronicDrumkitsFutureFlex com.apple.pkg.MAContent10_AssetPack_0708_AlchemyOrgansCustom com.apple.pkg.MAContent10_AssetPack_0310_UB_DrumMachineDesignerGB com.apple.pkg.MAContent10_AssetPack_0273_AlchemyVocalsFXNNoisesMisc com.apple.pkg.MAContent10_AssetPack_0316_AppleLoopsDubstep1 com.apple.pkg.MAContent10_AssetPack_0803_EXS_KeyboardCollection com.apple.pkg.MAContent10_AssetPack_0745_AlchemyBassHouse com.apple.pkg.MAContent10_AssetPack_0491_EXS_OrchBrass com.apple.pkg.MAContent10_AssetPack_0865_EXS_ElectronicDrumkitsWatchTheSoundLinnDrumTunedDown com.apple.pkg.MAContent10_AssetPack_0879_EXS_ElectronicDrumkitsTrakgirlGrimeyFunk com.apple.pkg.MAContent10_AssetPack_0809_EXS_ElectronicDrumkitsAbsoluteZero com.apple.pkg.MAContent10_AssetPack_0555_AppleLoopsGarageBand2 com.apple.pkg.MAContent10_AssetPack_0373_EXS_GuitarsAcousticClassical com.apple.pkg.MAContent10_AssetPack_0898_EXS_ElectronicDrumkitsAnalogBlaze com.apple.pkg.MAContent10_AssetPack_0659_AlchemySynthCustom com.apple.pkg.MAContent10_AssetPack_0919_AlchemySamples8BitLegends com.apple.pkg.MAContent10_AssetPack_0564_DrummerBrooklynLogic com.apple.pkg.MAContent10_AssetPack_0900_EXS_ElectronicDrumkitsAnalogDream com.apple.pkg.MAContent10_AssetPack_0560_LTPBasicPiano1 com.apple.pkg.MAContent10_AssetPack_0721_EXS_XyloKeyboard com.apple.pkg.MAContent10_AssetPack_0657_AlchemyPadsCustom com.apple.pkg.MAContent10_AssetPack_0841_AppleLoopsTomMisch com.apple.pkg.MAContent10_AssetPack_0605_EXS_ClassicalGrand com.apple.pkg.MAContent10_AssetPack_0710_AlchemyRhythmicCustomSynthCustom com.apple.pkg.MAContent10_AssetPack_0315_AppleLoopsElectroHouse1 com.apple.pkg.MAContent10_AssetPack_0319_AppleLoopsDeepHouse com.apple.pkg.MAContent10_AssetPack_0152_AlchemyGuitarsElectric12StringPluck com.apple.pkg.MAContent10_AssetPack_0322_AppleLoopsDiscoFunk1 com.apple.pkg.MAContent10_AssetPack_0597_LTPChordTrainer com.apple.pkg.MAContent10_AssetPack_0685_EXS_ElectronicDrumkitsPlatinum com.apple.pkg.MAContent10_AssetPack_0807_EXS_ElectronicDrumkitsAboveAndBeyond com.apple.pkg.MAContent10_AssetPack_0577_DrummerMotownRevisitedGBLogic com.apple.pkg.MAContent10_AssetPack_0873_IRsProducerPacks01 com.apple.pkg.MAContent10_AssetPack_0883_AppleLoopsOakFelder com.apple.pkg.MAContent10_AssetPack_0492_EXS_OrchKit com.apple.pkg.MAContent10_AssetPack_0773_AlchemySamplesPrismatica com.apple.pkg.MAContent10_AssetPack_0753_ElectronicDrumkitsPrismatica com.apple.pkg.MAContent10_AssetPack_0801_AppleLoopsPrismaticaB com.apple.pkg.MAContent10_AssetPack_0770_AppleLoopsVisionAndVerse com.apple.pkg.MAContent10_AssetPack_0587_DrummerScientificMethodGBLogic com.apple.pkg.MAContent10_AssetPack_0758_EXS_ElectronicDrumkitsHardBounce com.apple.pkg.MAContent10_AssetPack_0912_EXS_ElectronicDrumkitsChoppedUpKit com.apple.pkg.MAContent10_AssetPack_0341_IRsWarpedMovingSpaces com.apple.pkg.MAContent10_AssetPack_0742_AppleLoopsTransitionEffects com.apple.pkg.MAContent10_AssetPack_0364_EXS_BassElectricStinger com.apple.pkg.MAContent10_AssetPack_0819_EXS_ElectronicDrumkitsModernKnock com.apple.pkg.MAContent10_AssetPack_0544_EXS_GuitarsGB com.apple.pkg.MAContent10_AssetPack_0776_EXS_ElectronicDrumkitsVisionAndVerse com.apple.pkg.MAContent10_AssetPack_0846_EXS_ElectronicDrumkitsOakFelderAcousticBump com.apple.pkg.MAContent10_AssetPack_0552_AppleLoopsModernRnB2 com.apple.pkg.MAContent10_AssetPack_0705_AlchemySettingsFlexAndFlow com.apple.pkg.MAContent10_AssetPack_0540_PlugInSettingsGB com.apple.pkg.MAContent10_AssetPack_0911_AppleLoopsBeatTape com.apple.pkg.MAContent10_AssetPack_0638_AppleLoopsDrummerLevi com.apple.pkg.MAContent10_AssetPack_0375_EXS_GuitarsVintageStrat com.apple.pkg.MAContent10_AssetPack_0501_EXS_OrchOrgan com.apple.pkg.MAContent10_AssetPack_0835_EXS_ElectronicDrumkitsWarmEmbers com.apple.pkg.MAContent10_AssetPack_0852_EXS_ElectronicDrumkitsSoulectionCreamSoda com.apple.pkg.MAContent10_AssetPack_0732_EXS_ElectronicDrumkitsVintageTR707 com.apple.pkg.MAContent10_AssetPack_0737_EXS_ElectronicDrumkitsVintageSDSV com.apple.pkg.MAContent10_AssetPack_0633_AppleLoopsDrummerGavin com.apple.pkg.InstallAssistant.macOSSequoia com.apple.pkg.MAContent10_AssetPack_0643_AppleLoopsDrummerParker com.apple.pkg.MAContent10_AssetPack_0925_EXS_ElectronicDrumkitsModernChiptune com.apple.pkg.MAContent10_AssetPack_0860_EXS_ElectronicDrumkitsTomMischAgogoFunk com.apple.pkg.MAContent10_AssetPack_0238_AlchemySynthsDigitalPluck com.apple.pkg.MAContent10_AssetPack_0804_IRsLargeSpacesSprings com.apple.pkg.MAContent10_AssetPack_0332_IRsMediumSpaces com.apple.pkg.MAContent10_AssetPack_0656_AlchemyMallets com.apple.pkg.MAContent10_AssetPack_0630_AppleLoopsDrummerBenny com.apple.pkg.MAContent10_AssetPack_0724_EXS_ElectronicDrumkitsKnickKnack com.apple.pkg.MAContent10_AssetPack_0904_EXS_ElectronicDrumkitsModularDrop com.apple.pkg.MAContent10_AssetPack_0826_EXS_ElectronicDrumkitsTitansOfBass com.apple.pkg.MAContent10_AssetPack_0636_AppleLoopsDrummerJesse com.apple.pkg.MAContent10_AssetPack_0615_GBLogicAlchemyEssentials com.apple.pkg.MAContent10_AssetPack_0328_AppleLoopsJamRhythmSection com.apple.pkg.MAContent10_AssetPack_0551_AppleLoopsDubstep2 com.apple.pkg.MAContent10_AssetPack_0691_IRsInstrumentProfiles com.apple.pkg.MAContent10_AssetPack_0509_EXS_StringsEnsemble com.apple.pkg.MAContent10_AssetPack_0903_EXS_ElectronicDrumkitsMicroModular com.apple.pkg.MAContent10_AssetPack_0629_AppleLoopsDrummerAnders com.apple.pkg.MAContent10_AssetPack_0729_EXS_ElectronicDrumkitsVintageCR78 com.apple.pkg.MAContent10_AssetPack_0131_AlchemyGuitarsAcousticMisc com.apple.pkg.MAContent10_AssetPack_0324_AppleLoopsBluesGarage com.apple.pkg.MAContent10_AssetPack_0314_AppleLoopsHipHop1 com.apple.pkg.MAContent10_AssetPack_0927_EXS_ElectronicDrumkitsPoppingBits com.apple.pkg.MAContent10_AssetPack_0635_AppleLoopsDrummerIan com.apple.pkg.MAContent10_AssetPack_0333_IRsLargeSpacesRooms com.apple.pkg.RosettaUpdateAuto com.apple.pkg.MAContent10_AssetPack_0756_AppleLoopsSkylineHeat com.apple.pkg.MAContent10_AssetPack_0641_AppleLoopsDrummerMax com.apple.pkg.MAContent10_AssetPack_0680_EXS_ElectronicDrumkitsInfinity com.apple.pkg.MAContent10_AssetPack_0867_EXS_ElectronicDrumkitsWatchTheSoundPawnShop808 com.apple.pkg.MAContent10_AssetPack_0670_EXS_ElectronicDrumkitsAdvancedMachines com.apple.pkg.MAContent10_AssetPack_0105_AlchemyBassAnalogAnalogMisc com.apple.pkg.MAContent10_AssetPack_0886_AlchemySamplesAlphaWaves com.apple.pkg.MAContent10_AssetPack_0296_AlchemySoundEffectsSynths com.apple.pkg.MAContent10_AssetPack_0662_AppleLoopsDrummerAustin com.apple.pkg.MAContent10_AssetPack_0922_EXS_ElectronicDrumkitsBattleBlocks com.apple.pkg.MAContent10_AssetPack_0483_EXS_OrchWoodwindBasson com.apple.pkg.MAContent10_AssetPack_0325_AppleLoopsGarageBand1 com.apple.pkg.MAContent10_AssetPack_0376_EXS_GuitarsWarmElectric com.apple.pkg.MAContent10_AssetPack_0825_EXS_ElectronicDrumkitsSuperNatural com.apple.pkg.MAContent10_AssetPack_0697_DrummerSpeakeasyLogic com.apple.pkg.MAContent10_AssetPack_0917_EXS_ElectronicDrumkitsStudioCuts com.apple.pkg.MAContent10_AssetPack_0681_EXS_ElectronicDrumkitsLaserBattle com.apple.pkg.MAContent10_AssetPack_0723_EXS_ToyMelodica com.apple.pkg.MAContent10_AssetPack_0337_IRsLargeSpacesIndoor com.apple.pkg.MAContent10_AssetPack_0579_DrummerNeoSoulGBLogic com.apple.pkg.MAContent10_AssetPack_0740_AppleLoopsHipHop3 com.apple.pkg.MAContent10_AssetPack_0339_IRsLargeSpacesWarped com.apple.pkg.MAContent10_AssetPack_0371_EXS_GuitarsAcoustic com.apple.pkg.MAContent10_AssetPack_0901_EXS_ElectronicDrumkitsAnalogHaze com.apple.pkg.MAContent10_AssetPack_0355_EXS_PianoGrand com.apple.pkg.MAContent10_AssetPack_0312_UB_UltrabeatKitsGBLogic com.apple.pkg.MAContent10_AssetPack_0762_EXS_ElectronicDrumkitsTightTrap com.apple.pkg.MAContent10_AssetPack_0561_DrummerBluebirdGBLogic com.apple.pkg.MAContent10_AssetPack_0646_AppleLoopsDrummerElectronic com.apple.pkg.MAContent10_AssetPack_0037_AlchemyPadsCustomNSingle com.apple.pkg.MAContent10_AssetPack_0875_ProducerPackSettingsOakFelder com.apple.pkg.MAContent10_AssetPack_0550_AppleLoopsElectroHouse2 com.apple.pkg.MAContent10_AssetPack_0749_EXS_ElectronicDrumkitsHeavyBassHouse com.apple.pkg.MAContent10_AssetPack_0205_AlchemySynthsAnalogDream com.apple.pkg.MAContent10_AssetPack_0598_LTPBasicGuitar1 com.apple.pkg.MAContent10_AssetPack_0843_EXS_ElectronicDrumkitsBoysNoizeBigBang com.apple.pkg.MAContent10_AssetPack_0733_EXS_ElectronicDrumkitsVintageTR727 com.apple.pkg.MAContent10_AssetPack_0682_EXS_ElectronicDrumkitsMinimalist com.apple.pkg.MAContent10_AssetPack_0007_AlchemyKeysElectric com.apple.pkg.MAContent10_AssetPack_0652_AlchemySettingsEdgesAndAngles com.apple.pkg.MAContent10_AssetPack_0688_EXS_WorldTaikoDrums com.apple.pkg.MAContent10_AssetPack_0866_EXS_ElectronicDrumkitsWatchTheSoundMixedBoomBap com.apple.pkg.MAContent10_AssetPack_0747_EXS_ElectronicDrumkitsDeepBassHouse com.apple.pkg.MAContent10_AssetPack_0345_IRsWarpedAnalogCircuits com.apple.pkg.MAContent10_AssetPack_0902_EXS_ElectronicDrumkitsAnalogTraces com.apple.pkg.MAContent10_AssetPack_0744_AppleLoopsChromiumFray com.apple.pkg.MAContent10_AssetPack_0811_EXS_ElectronicDrumkitsDarkArts com.apple.pkg.MAContent10_AssetPack_0884_EXS_KeyboardsSoulection com.apple.pkg.MAContent10_AssetPack_0854_EXS_ElectronicDrumkitsTakeADaytripAlphabetCity com.apple.pkg.MAContent10_AssetPack_0926_EXS_ElectronicDrumkitsOldSchool8Bit com.apple.pkg.MAContent10_AssetPack_0851_EXS_ElectronicDrumkitsSoulectionBigBump com.apple.pkg.MAContent10_AssetPack_0568_DrummerEastBayLogic com.apple.pkg.MAContent10_AssetPack_0583_DrummerRetroRockGBLogic com.apple.pkg.MAContent10_AssetPack_0661_DrummerBlueRidgeLogic com.apple.pkg.MAContent10_AssetPack_0667_AppleLoopsFutureBass com.apple.pkg.MAContent10_AssetPack_0820_EXS_ElectronicDrumkitsModernThrowback com.apple.pkg.MAContent10_AssetPack_0718_EXS_MusicBox com.apple.pkg.MAContent10_AssetPack_0354_EXS_PianoSteinway com.apple.pkg.MAContent10_AssetPack_0639_AppleLoopsDrummerLogan com.apple.pkg.MAContent10_AssetPack_0881_EXS_ElectronicDrumkitsTrakgirlLofiFeels com.apple.pkg.MAContent10_AssetPack_0370_EXS_MalletsVibraphone com.apple.pkg.MAContent10_AssetPack_0653_AlchemyBassCustom com.apple.pkg.MAContent10_AssetPack_0671_EXS_ElectronicDrumkitsArcadeFrenzy com.apple.pkg.MAContent10_AssetPack_0832_EXS_ElectronicDrumkitsPureHouse com.apple.pkg.MAContent10_AssetPack_0840_AppleLoopsTakeADaytrip com.apple.pkg.MAContent10_AssetPack_0321_AppleLoopsIndieDisco com.apple.pkg.MAContent10_AssetPack_0632_AppleLoopsDrummerDarcy com.apple.pkg.MAContent10_AssetPack_0145_AlchemyGuitarsPluckedHarp com.apple.pkg.MAContent10_AssetPack_0877_EXS_ElectronicDrumkitsTrakgirlMysticGroove com.apple.pkg.MAContent10_AssetPack_0603_EXS_WorldErhu com.apple.pkg.MAContent10_AssetPack_0299_AlchemyMalletsMetal com.apple.pkg.MAContent10_AssetPack_0660_DrummerBlueRidgeGBLogic com.apple.pkg.MAContent10_AssetPack_0362_EXS_BassElectricPicked com.apple.pkg.MAContent10_AssetPack_0601_AppleLoopsChineseTraditional com.apple.pkg.MAContent10_AssetPack_0842_EXS_ElectronicDrumkitsBoysNoizeAnalogDrive com.apple.pkg.MAContent10_AssetPack_0489_EXS_OrchWoodwindOboeSolo com.apple.pkg.MAContent10_AssetPack_0821_EXS_ElectronicDrumkitsOverdriveBump com.apple.pkg.MAContent10_AssetPack_0686_EXS_WorldGuzheng com.apple.pkg.MAContent10_AssetPack_0663_AppleLoopsDrummerTyrell com.apple.pkg.MAContent10_AssetPack_0748_EXS_ElectronicDrumkitsHardBassHouse com.apple.pkg.XProtectPayloads_10_15.16U4351 com.apple.pkg.MAContent10_AssetPack_0538_DrummerSticks com.apple.pkg.MAContent10_AssetPack_0731_EXS_ElectronicDrumkitsVintageTR606 com.apple.pkg.MAContent10_AssetPack_0581_DrummerPortlandGBLogic com.apple.pkg.MAContent10_AssetPack_0358_EXS_BassElectricFingerStyle com.apple.pkg.MAContent10_AssetPack_0677_EXS_ElectronicDrumkitsFoundSound com.apple.pkg.MAContent10_AssetPack_0882_AppleLoopsSoulection com.apple.pkg.MobileDeviceDevelopment com.apple.pkg.MAContent10_AssetPack_0713_EXS_ElectronicDrumkitsInterchange com.apple.pkg.MAContent10_AssetPack_0672_EXS_ElectronicDrumkitsBumper com.apple.pkg.MAContent10_AssetPack_0548_EXS_iOSInstruments com.apple.pkg.CLTools_SwiftBackDeploy com.apple.pkg.MAContent10_AssetPack_0292_AlchemySoundEffectsInstruments com.apple.pkg.MAContent10_AssetPack_0289_AlchemySoundEffectsNatural com.apple.pkg.MAContent10_AssetPack_0675_EXS_ElectronicDrumkitsDembow com.apple.pkg.MAContent10_AssetPack_0914_EXS_ElectronicDrumkitsDustBowl com.apple.pkg.MAContent10_AssetPack_0545_EXS_DrumsGB com.apple.pkg.MAContent10_AssetPack_0092_AlchemyBassMini3VCOSquares com.apple.pkg.MAContent10_AssetPack_0344_IRsWarpedSpeakers com.apple.pkg.MAContent10_AssetPack_0813_EXS_ElectronicDrumkitsDoingScience com.apple.pkg.MAContent10_AssetPack_0027_AlchemyDrumsElectronicMisc com.apple.pkg.MAContent10_AssetPack_0736_EXS_ElectronicDrumkitsVintageDrumTraks com.apple.pkg.MAContent10_AssetPack_0722_EXS_ElectronicDrumkits70sRhythmBox com.apple.pkg.MAContent10_AssetPack_0259_AlchemyVocalsChoirFemale com.apple.pkg.MAContent10_AssetPack_0562_DrummerBluebirdLogic com.apple.pkg.MAContent10_AssetPack_0915_EXS_ElectronicDrumkitsPreciousGems com.apple.pkg.MAContent10_AssetPack_0774_AlchemySettingsPrismatica com.apple.pkg.MAContent10_AssetPack_0816_EXS_ElectronicDrumkitsHybridGrime com.apple.pkg.MAContent10_AssetPack_0870_EXS_KeyboardsTomMisch com.apple.pkg.MAContent10_AssetPack_0684_EXS_ElectronicDrumkitsModern909 com.apple.pkg.MAContent10_AssetPack_0711_EXS_ElectronicDrumkits808Flex com.apple.pkg.MAContent10_AssetPack_0500_EXS_OrchHarp com.apple.pkg.MAContent10_AssetPack_0759_EXS_ElectronicDrumkitsHardKnock com.apple.pkg.MAContent10_AssetPack_0554_AppleLoopsDiscoFunk2 com.apple.pkg.MAContent10_AssetPack_0717_EXS_ToyGlockenspiel com.apple.pkg.CLTools_macOS_SDK com.apple.pkg.MAContent10_AssetPack_0817_EXS_ElectronicDrumkitsLoFiHype com.apple.pkg.MAContent10_AssetPack_0754_EXS_ElectronicDrumkitsSolaris com.apple.pkg.MAContent10_AssetPack_0283_AlchemyVocalsSoloVocalsSustainAnastacia com.apple.pkg.MAContent10_AssetPack_0293_AlchemySoundEffectsTechnology com.apple.pkg.MAContent10_AssetPack_0706_AlchemyBassCustom com.apple.pkg.MAContent10_AssetPack_0928_EXS_ElectronicDrumkitsVintageArcade com.apple.pkg.XcodeSystemResources com.apple.pkg.MAContent10_AssetPack_0595_DrummerSunsetGBLogic com.apple.pkg.MAContent10_AssetPack_0746_EXS_ElectronicDrumkitsBrightBassHouse com.apple.pkg.MAContent10_AssetPack_0591_DrummerSmashGBLogic com.apple.pkg.MAContent10_AssetPack_0743_SoundEffectsCustom2 com.apple.pkg.MAContent10_AssetPack_0644_AppleLoopsDrummerRose com.apple.pkg.MAContent10_AssetPack_0829_EXS_ElectronicDrumkitsHTownFire com.apple.pkg.MAContent10_AssetPack_0009_AlchemyMalletsSynthetic com.apple.pkg.MAContent10_AssetPack_0752_EXS_ElectronicDrumkitsModernDisco com.apple.pkg.MAContent10_AssetPack_0318_AppleLoopsTechHouse com.apple.pkg.MAContent10_AssetPack_0642_AppleLoopsDrummerNikki com.apple.pkg.MAContent10_AssetPack_0570_DrummerFourOnTheFloorLogic com.apple.pkg.MAContent10_AssetPack_0549_AppleLoopsHipHop2 com.apple.pkg.MAContent10_AssetPack_0565_DrummerDetroitGarageGBLogic com.apple.pkg.MAContent10_AssetPack_0655_AlchemyKeysCustom com.apple.pkg.MAContent10_AssetPack_0856_EXS_ElectronicDrumkitsTakeADaytripBurlington com.apple.pkg.MAContent10_AssetPack_0872_AppleLoopsWatchTheSound com.apple.pkg.MAContent10_AssetPack_0327_AppleLoopsJamPackRemixTools com.apple.pkg.MAContent10_AssetPack_0536_DrummerClapsCowbell com.apple.pkg.MAContent10_AssetPack_0802_AppleLoopsKeyboardCollection com.apple.pkg.MAContent10_AssetPack_0640_AppleLoopsDrummerMason com.apple.pkg.MAContent10_AssetPack_0874_AlchemyProducerPacks01 com.apple.pkg.MAContent10_AssetPack_0547_EXS_StringsGB com.apple.pkg.MAContent10_AssetPack_0871_AppleLoopsTrakgirl com.apple.pkg.MAContent10_AssetPack_0828_AlchemySamplesUltimate808s com.apple.pkg.MAContent10_AssetPack_0673_EXS_ElectronicDrumkitsCompact com.apple.pkg.MAContent10_AssetPack_0716_EXS_MiniMarimba com.apple.pkg.MAContent10_AssetPack_0848_EXS_ElectronicDrumkitsOakFelderMixedUp com.apple.pkg.MAContent10_AssetPack_0295_AlchemySoundEffectsAmbienceNature com.apple.pkg.MAContent10_AssetPack_0687_EXS_WorldKoto com.apple.pkg.MAContent10_AssetPack_0734_EXS_ElectronicDrumkitsVintageTR808 com.apple.pkg.XProtectPlistConfigData_10_15.16U4349 com.apple.pkg.MAContent10_AssetPack_0631_AppleLoopsDrummerCurtis com.apple.pkg.MAContent10_AssetPack_0678_EXS_ElectronicDrumkitsFutureBreaks com.apple.pkg.MAContent10_AssetPack_0331_IRsSmallSpaces com.apple.pkg.MAContent10_AssetPack_0360_EXS_BassElectricLiverpool com.apple.pkg.MAContent10_AssetPack_0750_EXS_ElectronicDrumkitsNaturalBassHouse com.apple.pkg.MAContent10_AssetPack_0648_Percussionist com.apple.pkg.MAContent10_AssetPack_0357_EXS_BassAcousticUprightJazz com.apple.pkg.MAContent10_AssetPack_0719_EXS_ToyCelesta com.apple.pkg.MAContent10_AssetPack_0709_AlchemyPadsCustom com.apple.pkg.MAContent10_AssetPack_0916_EXS_ElectronicDrumkitsSpacedOut com.apple.pkg.MAContent10_AssetPack_0320_AppleLoopsChillwave1 com.apple.pkg.MAContent10_AssetPack_0335_IRsLargeSpacesPlates com.apple.pkg.XProtectPlistConfigData_10_15.16U4348 dev.alexhay.MenuBox com.renfei.SnippetsLab com.apple.pkg.Pages14 com.toggl.daneel com.wiheads.paste com.sindresorhus.Pandan com.yubico.ykman org.erb.nutshellblack com.malteundjan.focus-osx com.lowtechguys.rcmd com.p5sys.jump.mac.viewer com.agiletortoise.Drafts-OSX app.pastenow.PasteNow design.yugen.Flow net.temurin.21.jdk com.if.Amphetamine com.amazon.Lassen com.canva.canvaeditor com.apple.pkg.iMovie_AppStore org.wireshark.ChmodBPF.pkg com.ua.transcribe.ai maccatalyst.com.lireapp.macOS com.yubico.yubioath com.hegenberg.BetterSnapTool com.flexibits.fantastical2.mac dk.simonbs.DataJar com.ulyssesapp.mac pl.wojciechkulik.Snippety com.focusrite.FocusriteControl com.logi.installer.pluginservice.media.package com.janandre.HomeKitTimers com.flexibits.cardhop.mac net.shinyfrog.bear com.amazon.aiv.AIVApp com.iwaxx.LanScan com.apple.pkg.GarageBand_AppStore com.google.Keystone com.marcoarment.instapaperpro recipes.mela.appkit org.pqrs.Karabiner-Elements com.sindresorhus.One-Thing com.bitwarden.desktop com.adobe.acrobat.AcrobatDCUpd2400420272 maccatalyst.com.overdesigned.Cheatsheet net.whatsapp.WhatsApp imagetasks.Pixea org.wireshark.path_helper.pkg com.tailscale.ipn.macsys com.journeytech.navigation com.termius.mac org.pqrs.Karabiner-DriverKit-VirtualHIDDevice com.apple.pkg.LogicPro_AppStore me.damir.dropover-mac com.bloombuilt.dayone-mac com.apple.pkg.Xcode com.alexhay.Console com.apple.pkg.Keynote14 app.tripsy.ios com.kakao.KakaoTalkMac cc.anybox.Anybox us.zoom.pkg.videomeeting ai.perplexity.mac io.tailscale.ipn.macos com.hyperdev.joinzoom.macos com.thomasricouard.IceCubesApp com.redhat.podman com.okatbest.boop io.hyperapp.XcodeCleaner com.mr-brightside.ParcelOSX com.todoist.mac.Todoist io.osquery.agent com.brettterpstra.marked2 com.apple.pkg.Numbers14 com.apple.pkg.ConfigurationUtility com.andydev.alpenglow com.logi.installer.pluginservice.package org.p0deje.Maccy com.knollsoft.Charmstone com.macpaw.CleanMyMac-mas com.henrikruscon.Klack tv.parsec.www com.apple.pkg.WWDC_Catalyst com.branchseer.rapidmg md.obsidian.Obsidian-Web-Clipper`

## Response:
Based on your extensive toolset and workflow preferences, let me suggest a modern Hammerspoon configuration approach that integrates well with your existing tools:

1.  Primary Configuration Strategy:

*   Use spacehammer as base but modernize it with:
    *   Raycast-like fuzzy finding
    *   Integration with Obsidian/Drafts for quick capture
    *   Context-aware window management that works with yabai
    *   Terminal-centric workflow support

Here's a suggested modern configuration approach:

```lua
-- Example modern Hammerspoon configuration structure
local wm = require('window-management')
local fuzzy = require('fuzzy')
local apps = require('apps')
local keys = require('keys')

-- Context-aware modal system (similar to your Neovim setup)
local function setupModes()
  local modes = {
    normal = {}, -- Default mode
    window = {}, -- Window management mode
    app = {},    -- App switching/launching mode
    nav = {},    -- Navigation mode
    capture = {} -- Quick capture to Obsidian/Drafts
  }
end

-- Fuzzy finder integration (Telescope-like)
local function setupFuzzy()
  -- Integrate with zoxide for smart directory jumping
  -- Add Raycast-like app switching
end

-- Terminal integration
local function setupTerminal()
  -- Integrate with zellij/wezterm
  -- Handle terminal-specific window rules
end
```

Would you like me to:

1.  Show how to integrate this with yabai for better window management?
2.  Create a Raycast-like fuzzy finder for your specific apps?
3.  Set up context-aware keybindings that adapt based on current app (Neovim/terminal/Obsidian)?

The key is building a configuration that complements your existing tools rather than replacing them, while maintaining the speed and flexibility you're used to from your terminal-centric workflow.

## Prompt:
modern CLi or easy online tool to clone entire website domain recursively?

## Response:
Based on your command-line expertise and tools list, here are the most modern and effective options:

1.  wget (already in your brew list):

```bash
wget --mirror --convert-links --adjust-extension --page-requisites --no-parent https://example.com
```

2.  httrack (already in your brew list):

```bash
httrack "https://example.com" -O "./website-clone" "+*.example.com/*" -v
```

However, for a more modern approach, I'd recommend:

3.  ArchiveBox

```bash
# Install
brew install archivebox

# Archive
archivebox add https://example.com
```

Key benefits:

*   Modern CLI interface
*   Better JavaScript handling
*   Multiple archive formats
*   API access
*   Built-in index/search

4.  For API-first sites, you might want to try:

```bash
# Using curl with rate limiting and retry
curl -H "Accept: application/json" \
     --retry 3 \
     --retry-delay 1 \
     -o "page-#1.json" \
     "https://api.example.com/pages/{1..100}"
```

5.  For dynamic sites specifically:

```javascript
# Using Browser-based Crawling
playwright-cli pdf https://example.com example.pdf
```

Recommendation: Start with wget since it's already installed and handles most cases well. If you need better JavaScript support or hit issues with dynamic content, move to ArchiveBox.

Want me to explain the specific flags for any of these tools?
## Prompt:
best mac os apple silicon modern setup with nvim + cli tools + etc in 2025? candidates now are e.g. ([https://github.com/ryan4yin/nix-darwin-kickstarter/](https://github.com/ryan4yin/nix-darwin-kickstarter/)) + ([https://github.com/juspay/nixos-unified-template](https://github.com/juspay/nixos-unified-template))

## Response:
I notice you've shared some GitHub links. While I can't directly access them, I can help you design an optimal MacOS development setup for Apple Silicon in 2025. Let me analyze this systematically.

Here's a recommended modern setup for MacOS on Apple Silicon in 2025:

1.  Package/System Management:

*   Nix package manager with home-manager
*   Homebrew (as fallback for MacOS-specific tools)
*   Terminal: WezTerm (Rust-based, great performance on ARM)
*   Shell: Fish + Starship prompt
*   Git + gh CLI

2.  Core Development Setup:

*   Neovim latest stable with Lua config
    *   Package manager: lazy.nvim
    *   LSP: native LSP + mason.nvim
    *   Treesitter for syntax
    *   Telescope for fuzzy finding
    *   Which-key for keybindings
    *   nvim-cmp for completion
*   tmux for session management
*   ripgrep + fd for searching
*   zoxide for smart directory navigation

3.  Development Tools:

*   asdf for runtime version management
*   Docker Desktop for ARM
*   lazygit for TUI git operations
*   delta for git diffs
*   bat for file preview
*   eza (formerly exa) for ls replacement
*   bottom for system monitoring

Key advantages of this setup:

1.  Reproducibility: Nix provides declarative configuration
2.  Performance: All tools are ARM-native
3.  Modern features: Full LSP support, fuzzy finding, git integration
4.  Maintainability: Modular config structure
5.  Resource efficiency: Terminal-based workflow

For implementation, I recommend this approach:

1.  Install Nix first:

```bash
sh <(curl -L https://nixos.org/nix/install)
```

2.  Set up home-manager:

```bash
nix-channel --add https://github.com/nix-home-manager/home-manager/archive/master.tar.gz home-manager
nix-channel --update
```

3.  Create a minimal configuration:

```nix
{ config, pkgs, ... }:
{
  home.packages = with pkgs; [
    ripgrep
    fd
    eza
    bat
    bottom
    delta
    lazygit
    wezterm
    zoxide
  ];

  programs.fish.enable = true;
  programs.starship.enable = true;
  programs.tmux.enable = true;
}
```

4.  For Neovim, I recommend maintaining a separate config structure:

```
~/.config/nvim/
├── init.lua
├── lua/
│   ├── plugins/
│   ├── configs/
│   └── keymaps.lua
└── after/
    └── plugin/
```

Would you like me to provide more specific details about any part of this setup? For example, I could share:

1.  A detailed Neovim configuration
2.  Specific Nix configuration snippets
3.  Performance optimization tips for Apple Silicon

## Prompt:
reference collection of awesome nix resources below

Copy

## Response:
I'll help analyze the awesome-nix resources to create an optimized MacOS Apple Silicon development environment setup. Let me break this down systematically.

Here's an optimized setup structured by key components:

1.  Core System Foundation:

```nix
# flake.nix
{
  inputs = {
    nixpkgs.url = "github:nixos/nixpkgs/nixos-unstable";
    home-manager = {
      url = "github:nix-community/home-manager";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    darwin = {
      url = "github:lnl7/nix-darwin";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    flake-parts.url = "github:hercules-ci/flake-parts";
  };
}
```

2.  Development Environment Setup:

*   Use nix-darwin for macOS system configuration
*   home-manager for user environment
*   devenv for reproducible development environments
*   direnv + nix-direnv for automatic environment loading

3.  Modern Terminal Stack:

*   WezTerm (Rust-based terminal, great for ARM)
*   Fish shell + starship prompt
*   zoxide for smart directory navigation
*   Modern CLI replacements:
    *   bat for cat
    *   eza for ls
    *   fd for find
    *   ripgrep for grep

4.  Development Tools:

*   Neovim with:
    *   nixd (Official Nix LSP)
    *   lazy.nvim for plugin management
    *   Treesitter for syntax
    *   mason.nvim for LSP management
*   Git tools:
    *   lazygit for TUI
    *   delta for diffs
    *   gh CLI

5.  Performance Optimizations:

```nix
{
  nix = {
    settings = {
      auto-optimise-store = true;
      experimental-features = [ "nix-command" "flakes" ];
      extra-platforms = [ "aarch64-darwin" ];
      substituters = [
        "https://cache.nixos.org"
        "https://nix-community.cachix.org"
      ];
    };
  };
}
```

6.  Recommended CLI Tools (all ARM-native):

```nix
environment.systemPackages = with pkgs; [
  # Core
  alejandra  # Nix formatter
  nil # Nix LSP
  nix-index # File lookup
  cached-nix-shell # Faster nix-shell

  # Development
  devenv # Dev environments
  direnv # Auto env loading
  comma # Quick package execution

  # Monitoring
  bottom # System monitor

  # Modern replacements
  bat ripgrep fd eza
  zoxide starship
];
```

Key Improvements for 2025:

1.  Full Apple Silicon optimization
2.  Flake-based configuration
3.  Modular structure using flake-parts
4.  Modern LSP support (nixd)
5.  Cachix integration for faster builds
6.  Devenv for standardized environments

Would you like me to:

1.  Provide a complete flake configuration?
2.  Detail the Neovim setup optimized for Nix development?
3.  Share specific performance tuning for Apple Silicon?

## Prompt:
how about nvim config folders + cursor IDE + other files/folders that I want to git track remotely for declarative nix based (+flake) mac os dev env management (devenv.sh will be used)? Show me dir hierarchy + conventions with rationale on reason. Below are the apps that I have for reference

Copy

`# core tap "homebrew/bundle" brew "atuin" brew "bat" brew "delta" brew "fd" brew "fzf" brew "gh" brew "lazygit" brew "nvim" brew "procs" brew "rg" brew "sd" brew "tldr" brew "tldr" brew "trippy" brew "uv" brew "volta" brew "yazi" brew "zsh-autopair" brew "zsh-autosuggestions" brew "zsh-completions" brew "zsh-history-substring-search" brew "zsh-syntax-highlighting" brew "zoxide" brew "starship" brew "bitwarden-cli" brew "lsd" brew "eza" cask "cursor" cask "royal-tsx" cask "google-chrome" cask "zen-browser" cask "ghostty" cask "obsidian" cask "arq" # cask "parsec" cask "raycast" # cask "tailscale" cask "termius" cask "virtualbuddy" cask "bitwarden" # cask "jump-desktop-connect"`

## Response:
I'll help design a clean directory structure for a declarative Nix-based macOS development environment. Let me organize this systematically.

Here's a recommended directory structure with rationale:

```
~/nixos-config/  # Root directory for all Nix configurations
├── README.md
├── flake.nix    # Main flake configuration
├── flake.lock   # Lock file
├── .gitignore   # Ignore temporary files and secrets
├── Makefile     # Common commands for rebuilding system
│
├── modules/     # Reusable NixOS/nix-darwin modules
│   ├── darwin/  # macOS-specific configurations
│   │   ├── default.nix
│   │   ├── system.nix    # System-level settings
│   │   └── apps.nix      # GUI apps configuration
│   │
│   └── home/    # home-manager configurations
│       ├── default.nix
│       ├── cli.nix       # CLI tools configuration
│       ├── git.nix       # Git configuration
│       └── shell.nix     # Shell configuration
│
├── configs/     # Actual config files for tools
│   ├── nvim/    # Neovim configuration
│   │   ├── init.lua
│   │   └── lua/
│   │       ├── plugins/   # Plugin configurations
│   │       ├── core/      # Core settings
│   │       └── utils/     # Utility functions
│   │
│   ├── cursor/   # Cursor IDE settings
│   │   ├── settings.json
│   │   ├── keybindings.json
│   │   └── snippets/
│   │
│   ├── ghostty/  # Terminal configuration
│   │   └── config
│   │
│   └── raycast/  # Raycast scripts and config
│       └── commands/
│
├── pkgs/       # Custom package definitions
│   └── default.nix
│
├── users/      # User-specific configurations
│   └── your-username/
│       ├── default.nix
│       └── secrets.nix    # (encrypted) API keys, tokens, etc.
│
└── lib/        # Shared Nix functions and utilities
    └── default.nix
```

Rationale for key design decisions:

1.  **Top-level organization**:
    *   `modules/` - Separates system (darwin) and user (home) configurations
    *   `configs/` - Stores actual config files that get symlinked
    *   `users/` - User-specific settings and secrets
    *   `pkgs/` - Custom package definitions if needed
2.  **Neovim structure** (`configs/nvim/`):
    *   Modern Lua-based config
    *   Separated into plugins, core, and utilities
    *   Easy to maintain and extend
3.  **Cursor IDE settings** (`configs/cursor/`):
    *   Settings and keybindings separate from VSCode
    *   Maintains portability across machines
4.  **CLI tools configurations**:
    *   All CLI tool configs (starship, zoxide, etc.) managed via home-manager
    *   Declarative configuration in `modules/home/cli.nix`
5.  **Development environments**: For devenv.sh integration, add:

```
├── devenv/
│   ├── default.nix        # Shared development settings
│   ├── python.nix         # Python development environment
│   ├── rust.nix          # Rust development environment
│   └── node.nix          # Node.js development environment
```

6.  **Homebrew integration**: Rather than using Brewfile, manage GUI apps through nix-darwin:

```nix
# modules/darwin/apps.nix
{
  homebrew = {
    enable = true;
    casks = [
      "cursor"
      "royal-tsx"
      "ghostty"
      "obsidian"
      # ... other casks
    ];
    brews = [
      "bitwarden-cli"
      # ... other formulae
    ];
  };
}
```

Would you like me to:

1.  Provide a sample flake.nix that ties this structure together?
2.  Show detailed configurations for specific tools like Neovim or Cursor?
3.  Explain the devenv.sh integration in more detail?
## Prompt:
analyze attached awesome mac apps list for best setup for my mac os.

## Response:
I'll help analyze the extensive list of Mac applications to recommend an optimal setup. Let me use the analysis tool to process this data and then create a visualization of recommended apps by category.

```javascript
const content = await window.fs.readFile('paste.txt', { encoding: 'utf8' });

// Parse the content into categories and apps
const categories = {};
let currentCategory = '';

content.split('\n').forEach(line => {
  if (line.includes('## ') || line.includes('# ')) {
    currentCategory = line.replace(/#/g, '').trim();
    categories[currentCategory] = [];
  } else if (line.startsWith('- ') || line.startsWith('* ')) {
    const appMatch = line.match(/^[-*]\s+(.*?)\s+-\s+(.+)/);
    if (appMatch) {
      const [_, name, description] = appMatch;
      const isOpenSource = line.includes('Open-Source Software');
      const isFreeware = line.includes('Freeware');

      categories[currentCategory].push({
        name: name.trim(),
        description: description.split('.')[0].trim(),
        isOpenSource,
        isFreeware
      });
    }
  }
});

// Analyze essential categories and top apps
const essentialCategories = [
  'Reading and Writing Tools',
  'Developer Tools',
  'Productivity',
  'Menu Bar Tools',
  'Security Tools',
  'Window Management'
];

const recommendations = {};

essentialCategories.forEach(category => {
  if (categories[category]) {
    // Prioritize apps that are both open source and free
    const apps = categories[category]
      .filter(app => app.isOpenSource || app.isFreeware)
      .slice(0, 3);
    recommendations[category] = apps;
  }
});

console.log('Recommended apps by category:', JSON.stringify(recommendations, null, 2));
```

Based on the analysis, I'll create a visual recommendation dashboard for an optimal Mac setup. Let me create a React component to display this information.

Recommended Mac Setup

Click to open component

Based on the analysis of the Mac applications list, here are my key recommendations for an optimal setup:

1.  Essential Productivity Tools:

*   Rectangle for window management (free, open-source alternative to paid options)
*   Alfred as a Spotlight replacement (significantly enhances productivity)
*   MonitorControl for external display management

2.  Development Environment:

*   VS Code as the primary editor (excellent plugin ecosystem)
*   iTerm2 for terminal needs (more powerful than default Terminal)
*   Docker Desktop for containerization

3.  Security & Privacy:

*   Bitwarden for password management (free, open-source)
*   LuLu as a firewall (good for monitoring outbound connections)
*   BlockBlock for monitoring system modifications

4.  Writing & Documentation:

*   Obsidian for note-taking (powerful knowledge management)
*   VS Code with Markdown plugins
*   LibreOffice for general office needs

5.  Menu Bar Utilities:

*   Itsycal for calendar quick-access
*   Stats for system monitoring
*   Hidden for managing menu bar clutter

Key Considerations:

*   All recommended tools are either free or open-source
*   Focus on active maintenance and community support
*   Emphasis on privacy and security
*   Preference for native apps over Electron-based ones where possible

Would you like me to provide more specific details about any of these categories or recommendations?

## Prompt:
update recommendations for me given my preferences below,

*   heavily AI tooling driven
*   heavily keyboard driven
*   vim-lover
*   heavily prefer declarative driven tools for perfect reproductivility
*   love to automate thiings (e.g. code snippets)
*   love keyboard shortcuts or autocompletion
*   ideally want offline support

and I need to handle tasks such as

*   fullstack programming (e.g. js/ts, python, lua, rust, etc) +
*   dotfiles maangement (nix ideally)
*   documents management on Dropbox + GDrive, iCloud Drive
*   general writing, reading, summarizing, editing, comparing, reviewing, etc
*   voice transcrioption, dictation, translation
*   time management
*   project management
*   communication (email, message)
*   notes + files + storage org

I purely care about utility (cost doesn't matter) to get me as effective as possible + I like modern , fast , long-term minded tools that provide advanced functionality even if slow beginning, love customizability, love data not being tied to tool for later down the line.

## Response:
I'll help revise the recommendations based on your specific preferences and needs, focusing on power tools that align with keyboard-driven, AI-enhanced, and automation-focused workflows.

Power User Mac Setup Recommendations

Click to open component

Let me explain the key rationale behind these updated recommendations:

1.  Core Development Environment:

*   Neovim + LazyVim as your primary editor (fully keyboard-driven, Lua configurable, excellent for all your languages)
*   Wezterm over iTerm2 (Lua configurable, better performance, built-in multiplexer)
*   Nix + Home Manager for declarative system management (perfect for your dotfiles and reproducibility needs)

2.  AI & Automation Tools:

*   Raycast + AI Command (more programmable than Alfred, better AI integration)
*   Claude Desktop for deep work (better context handling than ChatGPT)
*   Obsidian with Copilot integration (local-first, extensible)

3.  Document & File Management:

*   Obsidian as your knowledge base (local-first, git-friendly)
*   Marta file manager (vim keybindings, Lua extensible)
*   rclone for cloud storage management (CLI-first, scriptable)

4.  Productivity & Organization:

*   Yabai + skhd (tiling window management with keyboard control)
*   Notion for project management (API-first approach, automation friendly)
*   Linear for task management (keyboard-driven, developer-focused)

5.  Communication & Audio:

*   Whisper + Aiko for offline transcription
*   MailMate (powerful email client with vim keybindings)
*   Drafts (quick capture with powerful automation)

Additional Tool Recommendations:

1.  Development Extensions:

*   lazygit for git operations
*   ripgrep + fd for fast searching
*   gh CLI for GitHub integration

2.  Data Management:

*   DBeaver for database management
*   DataGrip for more advanced DB work
*   VSCode as a secondary editor for pair programming

3.  Automation Tools:

*   Keyboard Maestro for system-wide automation
*   Hammerspoon for custom automation (Lua-based)
*   BetterTouchTool for advanced input customization

Key Benefits of This Setup:

1.  Everything is keyboard-driven with vim-style bindings where possible
2.  Tools are either local-first or have excellent data export capabilities
3.  Most configuration is declarative and can be version controlled
4.  Strong focus on automation and extensibility
5.  Preference for tools with active development and good documentation

Would you like me to elaborate on any particular category or provide specific configuration recommendations for any of these tools?

## Prompt:
go over list again and update to give me at least 1 suggestion per category

## Response:
I'll provide additional high-quality alternatives for each category that match your preferences for keyboard-driven, automation-friendly, and data-portable tools.

Expanded Power User Mac Setup with Alternatives

Click to open component

Key updates and rationale for the alternatives:

1.  Development Environment:

*   Added Helix as a simpler alternative to Neovim (built-in LSP, less configuration)
*   Added Emacs + Doom for those wanting the org-mode ecosystem
*   Added Kitty + tmux as a more established terminal setup

2.  System Management:

*   Added Homebrew + mackup as a transitional solution
*   Added GNU Stow for those wanting more manual control
*   All options support declarative configuration

3.  AI & Automation:

*   Added LaunchBar as a mature alternative
*   Added Codeium + Continue as alternative AI coding assistants
*   All support extensive keyboard shortcuts

4.  Knowledge Management:

*   Added Logseq for outline-based thinking
*   Added org-mode for maximum power (if willing to invest in Emacs)
*   All support local-first storage and version control

5.  File Management:

*   Added Forklift for a more traditional approach
*   Added nnn + syncthing for a terminal-based solution
*   All support keyboard-driven operations

6.  Window Management:

*   Added Amethyst for simpler tiling
*   Added Rectangle Pro for traditional management
*   All support extensive keyboard shortcuts

7.  Project Management:

*   Added Height as a Linear alternative
*   Added Taskwarrior + TimeWarrior for CLI lovers
*   All support keyboard-first operation

8.  Communication:

*   Added mu4e for Emacs integration
*   Added Fastmail + aerc for a mixed GUI/TUI approach
*   All support powerful filtering and automation

Would you like me to elaborate on any specific tool or provide configuration examples for any of these options?

## Prompt:
now provide a comprehensive list of ALL items in original list where you just pick any useful, modern tool (at least 1 from each section) with why it would be generatlly useful

## Response:
I'll analyze the original list and select modern, useful tools from each section, explaining their general utility regardless of workflow preferences.

Essential Modern Mac Applications

Click to open component

These recommendations focus on modern, well-maintained tools that solve real problems regardless of specific workflow preferences. Each tool was selected for being:

*   Actively maintained and likely to remain relevant
*   Well-designed for macOS
*   Solving a genuine need
*   Having good support and documentation

A few highlights worth elaborating on:

1.  DevUtils deserves special mention because it replaces many one-off web tools with offline alternatives, which is great for both privacy and reliability.
2.  Raycast is highlighted because it's modernizing the launcher/automation space with regular updates and a great plugin ecosystem.
3.  Arc Browser is an interesting new take on browsers that's particularly good at handling multiple contexts (work/personal/projects).
4.  IINA shows how a modern, open-source app can provide a better experience than commercial alternatives by focusing on native macOS integration.
5.  Dropover is a great example of improving core OS functionality in a meaningful way. It makes file management much more fluid.

Would you like more details about any particular category or specific tool configurations?

## Prompt:
how about email clients?

## Response:
Let me analyze the email clients from the list and expand on the best modern options, focusing on their key features and use cases.

Mac Email Client Comparison

Click to open component

Some additional insights about email clients that aren't immediately apparent from the comparison:

1.  Protocol Support:

*   MailMate has the best IMAP implementation, handling edge cases other clients miss
*   Mimestream is unique in implementing Gmail's API directly rather than IMAP
*   Apple Mail has good support for modern authentication methods

2.  Performance Considerations:

*   Mimestream is notably fast on Apple Silicon
*   MailMate can handle very large mailboxes well due to its architecture
*   Spark can be resource-intensive with its cloud features

3.  Privacy Aspects:

*   MailMate and Apple Mail are the most privacy-focused, keeping everything local
*   Canary offers end-to-end encryption but some features require cloud services
*   Spark and Airmail route data through their servers for some features

4.  Recent Developments:

*   Mimestream is actively developing and regularly adding features
*   MailMate development is slower but stable
*   Apple Mail is getting more features with each macOS update

5.  Unique Features Worth Noting:

*   MailMate's smart mailboxes are significantly more powerful than other clients
*   Canary's AI features are well-implemented for email composition
*   Mimestream's Gmail label handling is better than any other native client

Would you like me to elaborate on any specific aspect or compare particular features in more detail?

## Prompt:
your choices are a bit suboptimal for me or out of date - for example, I like Marta but you choice dropover. Also, I prefer Cursor IDE but you wrote Visual Studio Code. I like Ghostty terminal but you suggested ... none actually. I like Hammerspoon or yabai, but you suggested Rectangle.

Provide full best cutting edge, powerful, customizable apps again with my feedback

## Response:
You're absolutely right - let me revise with more cutting-edge and power-user focused tools, incorporating your feedback.

Cutting-Edge Power User Mac Setup 2024

Click to open component

Key changes and rationale from previous recommendations:

1.  Development:

*   Cursor over VSCode for native AI integration and better performance
*   Ghostty over iTerm2/Alacritty for modern GPU acceleration and cleaner design
*   Kept Neovim but emphasized LazyVim distribution for modern defaults

2.  File Management:

*   Marta over ForkLift/Commander One for superior customization
*   Added rclone for serious cloud management needs

3.  System Control:

*   Hammerspoon over simple automation tools for deeper system control
*   yabai over Rectangle for true tiling capabilities

4.  Modern Features:

*   Continue over GitHub Copilot for local-first AI
*   Mimestream over traditional email clients for modern Gmail integration

Some notable tools I haven't included but worth mentioning:

*   Kitty terminal: Good alternative to Ghostty if you need more mature features
*   Helix editor: Modern alternative to Neovim if you want simpler configuration
*   Amethyst: Lighter alternative to yabai if you need fewer permissions
*   mu4e: Power user email alternative if you're in the Emacs ecosystem

Would you like me to elaborate on any particular category or provide specific configuration examples?

## Prompt:
give me more detailed list on AI tooling (IDE, CLI command line, mac app, etc). And similarly for advanced utility tools, and PKM apps. Lastly, I want detailed list of apps that could be read it later & web link tools for me to migrate my saved resources from (Anybox, arc browser, chrome, todoist)

## Response:
I'll provide a comprehensive analysis of tools in these categories, focusing on modern, power-user focused options.

Advanced Mac Tooling Analysis

Click to open component

For migrating your saved resources, here's a recommended approach:

1.  Export Process:

*   Anybox: Export to JSON, then use a custom script to parse into your desired format
*   Arc: Export spaces and bookmarks to HTML, then process with a script
*   Chrome: Export bookmarks to HTML
*   Todoist: Export to CSV

2.  Recommended Migration Path:

```python
# Pseudo-code for migration
import json
from pathlib import Path

def process_exports():
    # Process Anybox JSON
    anybox_data = json.load(Path('anybox_export.json'))

    # Process Arc HTML
    arc_bookmarks = parse_html('arc_bookmarks.html')

    # Process Chrome bookmarks
    chrome_bookmarks = parse_html('chrome_bookmarks.html')

    # Process Todoist CSV
    todoist_tasks = parse_csv('todoist_export.csv')

    # Combine all sources
    all_bookmarks = normalize_bookmarks([
        convert_anybox(anybox_data),
        convert_arc(arc_bookmarks),
        convert_chrome(chrome_bookmarks),
        extract_links(todoist_tasks)
    ])

    # Export to your chosen system
    export_to_system(all_bookmarks)
```

3.  Recommended Destinations:

*   For personal use: Raindrop.io (best features) or LinkDing (self-hosted)
*   For team use: Notion or Raindrop.io Team
*   For archival: LinkAce with automated backups

Would you like me to elaborate on any particular category or provide more specific migration details?

## Prompt:
how about Cline (prev Claude Dev) + LM studio + Aide + Ollama + BoltAI + PaletteBrain for AI code editors? And for neovim, explain and compare lazyvim, nvchad with mini.nvim

## Response:
Let me break down these AI code editors and Neovim distributions in detail.

AI Code Editors and Neovim Distributions Comparison

Click to open component

Some additional insights not captured in the visualization:

1.  AI Tool Integration Strategy:

*   Consider using LM Studio for model management and Ollama for integration
*   aider works well with both local models and OpenAI/Claude
*   PaletteBrain can be configured to use your local models via Ollama

2.  Recommended AI Setups:

```bash
# Local-first setup
ollama pull codellama:34b
ollama pull deepseek-coder:33b
aider --model codellama:34b

# Hybrid setup
export OPENAI_API_KEY="..."
export ANTHROPIC_API_KEY="..."
# Use local models for code completion
# Use Claude/GPT-4 for complex reasoning
```

3.  Neovim Distribution Selection:

*   LazyVim: Best for those wanting a complete, well-maintained setup
*   NvChad: Good for those prioritizing performance
*   mini.nvim: Excellent for learning and building your own config

4.  Custom Integration Example:

```lua
-- Neovim integration with local AI
-- Add to your init.lua
local function setup_ai_integration()
  -- Setup Ollama endpoint
  vim.g.ai_endpoint = "http://localhost:11434/api/generate"

  -- Custom keybinding for AI assistance
  vim.keymap.set('n', '<leader>aa', function()
    -- Get visual selection
    local text = vim.fn.getreg('v')
    -- Send to AI
    -- Process response
  end)
end
```

Would you like me to elaborate on any specific aspect or provide more detailed configuration examples?

## Prompt:
comparison table (score + rank with rationale) of all AI coding + general usage apps with at least 5 core important criteria

## Response:
Let me create a comprehensive comparison of AI apps focusing on key criteria for coding and general usage.

AI Tools Comparison Matrix

Click to open component

Additional considerations for specific use cases:

1.  For Large Codebases:

*   Continue (Local) with custom model is best for understanding large projects
*   Claude Developer handles complex multi-file context well
*   LM Studio with large models (34B+) can match cloud performance

2.  For Quick Iterations:

*   Cursor or BoltAI for rapid prototyping
*   PaletteBrain for quick system-wide access
*   aider for fast CLI-based changes

3.  For Team Integration:

*   Continue with shared model server
*   Claude Developer with team subscription
*   Cursor with shared settings

4.  For Privacy Requirements:

*   Local setup: Continue + Ollama
*   Hybrid: Local models for code, Claude for planning
*   Custom: LM Studio + private API

Would you like me to elaborate on any specific aspect or provide setup examples for any particular combination?

## Prompt:
is continue truly the best option?

## Response:
Let me provide a more critical and nuanced analysis of Continue and its competitors.

Key Considerations:

1.  Continue's Limitations:

*   Still relatively new and less mature compared to alternatives
*   Setup complexity for local models can be challenging
*   Community/ecosystem is smaller than alternatives
*   Documentation could be more comprehensive
*   Some features still feel experimental

2.  Strong Alternative Contenders:

Cursor:

*   More mature development
*   Better VSCode extension ecosystem compatibility
*   More polished UI/UX
*   Active development and updates
*   Large user base providing feedback
*   Built-in chat interface that works well

GitHub Copilot:

*   More mature and battle-tested
*   Better inline completions
*   Larger training dataset
*   Better IDE integrations across the board
*   More consistent performance
*   Microsoft/GitHub backing

Claude Developer (formerly Cline):

*   Superior reasoning capabilities
*   Better at complex architectural discussions
*   High-quality explanations
*   More reliable for complex tasks
*   Built-in context handling
*   Clean, focused interface

3.  Updated Recommendation:

For most developers, I would actually recommend:

1.  GitHub Copilot + Cursor as primary setup
2.  Claude Developer for complex reasoning/planning
3.  Local setup (Ollama + custom integration) as supplementary

The reasons:

*   More mature and reliable tooling
*   Better ecosystem integration
*   More consistent performance
*   Larger communities
*   Better documentation and support

Continue is promising but probably shouldn't be ranked #1 currently. It's worth watching but might be better as a supplementary tool rather than primary development environment.

Would you like me to provide a revised detailed comparison focusing on these more established tools?

## Prompt:
update recommendations to reflect that I do not care about cost or resource intensiveness, or difficulty of setup, my interest is only in achieving highest intelligence tool.

## Response:
Ah, this significantly changes the evaluation criteria. Let me provide an updated analysis focusing purely on intelligence/capability without resource or cost constraints.

Maximum Capability AI Development Setup

Click to open component

Published

Example integration configuration for this maximum capability setup:

```python
# Router configuration example
ROUTER_CONFIG = {
    "completion": {
        "primary": "github_copilot",
        "fallback": "codellama_34b",
        "threshold": 0.8  # confidence threshold
    },
    "architecture": {
        "primary": "claude",
        "fallback": "llama2_70b",
        "context_window": 100000
    },
    "analysis": {
        "primary": "deepseek_coder",
        "fallback": "claude",
        "depth": "maximum"
    },
    "explanation": {
        "primary": "claude",
        "fallback": "mixtral",
        "style": "detailed"
    }
}

# Model selection based on task characteristics
def route_query(query, context, task_type):
    if task_type == "architecture":
        return claude_client.generate(
            query,
            context=context,
            temperature=0.7,
            max_tokens=4000
        )
    elif task_type == "completion":
        return copilot_client.complete(
            query,
            context=context,
            max_alternatives=5
        )
    # etc.
```

Key differences in this recommendation versus my previous one:

1.  Multiple models running simultaneously rather than choosing one
2.  Heavier emphasis on Claude for reasoning tasks
3.  Local models as supplements rather than alternatives
4.  Custom routing layer for optimal tool selection
5.  Much more sophisticated integration strategy

Would you like me to elaborate on any part of this setup or provide more specific integration examples?
## Prompt:
analyze + compare + score + rank all dev tools below for best mac os powerful functionality with best AI tooling + keyboard driven + + modern UIUX + design. Show output in tabular format with 5-7 most important devtooling criteria scoring out of 10 and include total score

## Response:
I'll analyze and compare the development tools based on your criteria. First, let me establish the key criteria that matter most for modern developer tooling on macOS:

1.  AI Integration & Intelligence (AI features, smart suggestions, code completion)
2.  Keyboard-Driven Efficiency (shortcuts, command palette, minimal mouse usage)
3.  Modern UI/UX (design, performance, native feel)
4.  Extensibility & Integration (plugins, tool connections, workflow automation)
5.  Development Features (debugging, refactoring, analysis tools)

Developer Tools Comparison

Click to open component

Based on the analysis, here are the key findings:

1.  Visual Studio Code (45/50) emerges as the top choice, primarily due to:

*   Exceptional AI integration through GitHub Copilot and other AI extensions
*   Comprehensive keyboard-driven workflow with customizable shortcuts
*   Rich extension ecosystem with over 30,000 extensions
*   Regular updates with modern features while maintaining performance

2.  JetBrains IDEs (44/50) come in as a close second:

*   Built-in AI assistant with strong code analysis
*   Powerful refactoring capabilities and debugging tools
*   Native performance and deep language-specific features
*   Excellent keyboard control with IDE-wide search

3.  Nova, Zed, and Sublime Text (tied at 37/50) offer different strengths:

*   Nova excels in native macOS integration and design
*   Zed brings innovative multiplayer features and modern performance
*   Sublime Text offers unmatched speed and keyboard efficiency

The scoring particularly emphasizes the growing importance of AI integration and keyboard-driven workflows in modern development tools, while still valuing traditional aspects like extensibility and core development features.

Would you like me to elaborate on any specific aspect of the comparison or explain the reasoning behind particular scores?

## Prompt:
how can I capture all installed apps (cli+gui+pkgs) all on text file on mac os? installed from homebrew, dmg, pkgs, brew bundle, app store

## Response:
I'll help you create a comprehensive solution to capture all installed applications on your macOS system. Let me create a script that will aggregate information from different sources.

macOS Apps Export Script

Click to open code

This script creates a comprehensive export of all installed applications on your Mac. Here's how to use it:

1.  Save the script as `capture-mac-apps.sh`
2.  Make it executable: `chmod +x capture-mac-apps.sh`
3.  Run it: `./capture-mac-apps.sh`

The script will create a detailed export that includes:

1.  All applications in your /Applications directory
2.  Mac App Store applications (requires `mas` CLI tool)
3.  Homebrew formulae and casks
4.  A complete Brewfile for easy reinstallation
5.  System package receipts (installed .pkg files)
6.  Command line tools from common locations
7.  Node.js global packages
8.  Python pip packages
9.  Ruby gems
10.  VS Code extensions

The output will be saved on your Desktop in a folder called `mac-apps-export` with a timestamp, making it easy to track changes over time.

For the best results, ensure you have these tools installed:

```bash
brew install mas   # For Mac App Store apps
brew install terminal-notifier   # For notifications (optional)
```

The script is designed to gracefully handle cases where certain package managers aren't installed, so it will still work even if you don't have all package managers installed.

To make this script even more useful, you could add it to your regular backup routine or run it periodically using `cron`. Would you like me to explain how to set up automated running of this script?

## Prompt:
clean following zsh dotfiles files (e.g. zshrc) code up to be valid working state but keep all dependencies and configs possible and then optimize

## Response:
Let me help you optimize and clean up the zsh configuration files. I'll analyze and refactor the code while keeping the functionality and organizing it in a more maintainable way.

Optimized ZSH Configuration

Click to open code

I've created an optimized version of your zsh configuration with several improvements:

1.  Structural Improvements:

*   Organized into clear, logical sections with descriptive comments
*   Removed duplicate code and consolidated related settings
*   Ensured correct initialization order (environment -> paths -> tools -> completions)
*   Added error handling for critical dependencies

2.  Performance Optimizations:

*   Enabled early performance settings for faster shell startup
*   Prevented duplicate compinit calls
*   Used conditional loading for optional tools
*   Optimized Homebrew settings
*   Used null\_glob (N) flag for path checking

3.  Key Features Preserved:

*   Modern CLI tools integration (bat, ripgrep, fzf, etc.)
*   Development environment setup (Node.js, Python, etc.)
*   Comprehensive Git workflows
*   Dotfile management with chezmoi
*   Shell enhancements (zoxide, atuin, etc.)

4.  New Improvements:

*   Better error messages for missing dependencies
*   More consistent alias naming
*   Clearer documentation
*   Proper XDG base directory support
*   Better history management
*   Improved completion system initialization

To use this configuration:

1.  Save it as `~/.zshrc`
2.  Install required dependencies:

```bash
brew install bat ripgrep fd fzf eza zoxide atuin starship
brew install zsh-syntax-highlighting zsh-autosuggestions zsh-autopair
```

3.  For development tools:

```bash
brew install fnm uv
```

Would you like me to explain any specific section in more detail or make additional optimizations?

## Prompt:
include brew info also below

Copy

`~ 18s ❯ brew shellenv export HOMEBREW_PREFIX="/opt/homebrew"; export HOMEBREW_CELLAR="/opt/homebrew/Cellar"; export HOMEBREW_REPOSITORY="/opt/homebrew"; fpath[1,0]="/opt/homebrew/share/zsh/site-functions"; PATH="/opt/homebrew/bin:/opt/homebrew/sbin:/Users/hank/.local/state/fnm_multishells/65442_1736376213366/bin:/usr/local/bin:/Users/hank/.local/bin:/Users/hank/bin:/Users/hank/.bun/bin:/Users/hank/go/bin:/Users/hank/.cache/lm-studio/bin:/usr/bin:/usr/sbin:/bin:/sbin:/Users/hank/.local/state/fnm_multishells/65407_1736376213165/bin:/Users/hank/.local/state/fnm_multishells/65384_1736376212845/bin:/System/Cryptexes/App/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Applications/Wireshark.app/Contents/MacOS:/opt/podman/bin:/Applications/Ghostty.app/Contents/MacOS"; export PATH; [ -z "${MANPATH-}" ] || export MANPATH=":${MANPATH#:}"; export INFOPATH="/opt/homebrew/share/info:${INFOPATH:-}"; ~ ❯ brew config HOMEBREW_VERSION: 4.4.15 ORIGIN: https://github.com/Homebrew/brew HEAD: b6fafba4864d65acabf966415e14b2dd86d81e1a Last commit: 4 days ago Branch: stable Core tap HEAD: 7231f2e0cc657fde42a2cc69800ca4f4ca823287 Core tap last commit: 68 minutes ago Core tap JSON: 05 Jan 17:30 UTC Core cask tap JSON: 05 Jan 17:30 UTC HOMEBREW_PREFIX: /opt/homebrew HOMEBREW_BAT: set HOMEBREW_CASK_OPTS: [] HOMEBREW_CURL_RETRIES: 2 HOMEBREW_EDITOR: nvim HOMEBREW_MAKE_JOBS: 24 HOMEBREW_NO_ANALYTICS: set HOMEBREW_NO_AUTO_UPDATE: set HOMEBREW_NO_ENV_HINTS: set HOMEBREW_NO_INSTALLED_DEPENDENTS_CHECK: set HOMEBREW_NO_INSTALL_FROM_API: set Homebrew Ruby: 3.3.6 => /opt/homebrew/Library/Homebrew/vendor/portable-ruby/3.3.6/bin/ruby CPU: 24-core 64-bit arm_blizzard_avalanche Clang: 16.0.0 build 1600 Git: 2.47.1 => /opt/homebrew/bin/git Curl: 8.7.1 => /usr/bin/curl macOS: 15.2-arm64 CLT: 16.2.0.0.1.1733547573 Xcode: N/A Rosetta 2: false`

## Response:
I'll enhance the previous configuration with the additional Homebrew details. Let me update the artifact.

Optimized ZSH Configuration with Enhanced Homebrew Setup

Click to open code

Optimized ZSH Configuration

Click to open code

Optimized ZSH Configuration

Click to open code

I've enhanced the Homebrew configuration with several improvements:

1.  Complete Environment Setup:
    *   Added HOMEBREW\_CELLAR and HOMEBREW\_REPOSITORY exports
    *   Properly configured MANPATH and INFOPATH
    *   Added all relevant Homebrew environment variables
2.  Performance Optimizations:
    *   Added HOMEBREW\_MAKE\_JOBS for parallel builds
    *   Enhanced the performance settings with proper documentation
    *   Better PATH management through brew shellenv
3.  Better Initialization:
    *   Created a dedicated brew\_init function
    *   Improved completion system integration
    *   Better error handling
4.  Additional Features:
    *   Set HOMEBREW\_EDITOR to nvim
    *   Configured optimal curl retries
    *   Added proper documentation for each setting

Here's a quick guide to the Homebrew-related aliases that are already in the configuration:

Common Operations:

```bash
b         # Short for 'brew'
bupd      # Update Homebrew
bupg      # Upgrade all packages
bclean    # Cleanup old versions
bpull     # Update, upgrade, and cleanup in one go
bin       # Install a package
bi        # Get info about a package
bs        # Search for a package

# Cask operations
bcl       # List installed casks
bcin      # Install a cask
bcup      # Upgrade casks

# Bundle operations (for Brewfile)
bb        # Run brew bundle
bbls      # Dump current packages to Brewfile
bbsave    # Save and backup Brewfile via chezmoi
```

Would you like me to add any additional Homebrew-specific configurations or explain any part in more detail?

## Prompt:
is the path ordering correct? which shoudl be first?

## Response:
Let me analyze the path ordering based on the `brew shellenv` output and best practices.

From your `brew shellenv` output, we can see this order:

```bash
PATH="/opt/homebrew/bin:/opt/homebrew/sbin:                # Homebrew paths
/Users/hank/.local/state/fnm_multishells/65442_.../bin:   # FNM Node paths
/usr/local/bin:                                           # System local
/Users/hank/.local/bin:/Users/hank/bin:                   # User local
/Users/hank/.bun/bin:                                     # Bun
/Users/hank/go/bin:                                       # Go
/Users/hank/.cache/lm-studio/bin:                         # LM Studio
/usr/bin:/usr/sbin:/bin:/sbin:                           # System paths
..."                                                      # Other paths
```

Let me update the path configuration to ensure the correct priority order:

Optimized ZSH Configuration

Click to open code

The updated path ordering follows these principles:

1.  **Package Manager First**
    *   Homebrew paths come first to ensure package-managed binaries take precedence
2.  **Version Managers & Language Runtimes**
    *   FNM (Node.js version manager)
    *   Language-specific paths (Bun, Python, Rust, Deno, Go)
    *   These need to be high priority to ensure correct version management
3.  **User Binaries**
    *   Local user overrides come next
    *   Allows user-installed binaries to override system versions when needed
4.  **System Local**
    *   /usr/local paths for locally compiled software
5.  **App-Specific Paths**
    *   Application-specific binaries like LM Studio
6.  **System Paths**
    *   Standard system binary locations
    *   macOS-specific paths
    *   Lower priority ensures they don't override user/package manager choices
7.  **Application Paths**
    *   Specific application binary paths
    *   Lowest priority as they're usually accessed directly

This ordering ensures:

*   Package-managed software takes precedence
*   Version managers work correctly
*   User overrides are respected
*   System binaries don't interfere with custom installations

Would you like me to make any adjustments to this path ordering or explain any specific part in more detail?

## Prompt:
> The generated artifact uses libraries we don't support: `@/components/ui/table`

Can you fix this error in `dev-tools-comparison`?

## Response:
I apologize for using the unsupported shadcn/ui table component. Let me fix the React component to use only native HTML table elements with Tailwind's core utility classes.

Developer Tools Comparison

Click to open component

I've made the following changes:

1.  Removed the shadcn/ui table component dependency
2.  Replaced it with native HTML `<table>` elements
3.  Used only Tailwind's core utility classes for styling
4.  Added proper borders and spacing
5.  Included alternating row colors for better readability
6.  Added a score explanation footer
7.  Maintained all the original functionality and data

The component now uses only standard React and core Tailwind CSS classes, with no additional dependencies. The table is still fully responsive and maintains a clean, professional appearance.

Would you like me to make any adjustments to the styling or layout?

## Prompt:
include brew info also below

`~ 18s ❯ brew shellenv export HOMEBREW_PREFIX="/opt/homebrew"; export HOMEBREW_CELLAR="/opt/homebrew/Cellar"; export HOMEBREW_REPOSITORY="/opt/homebrew"; fpath[1,0]="/opt/homebrew/share/zsh/site-functions"; PATH="/opt/homebrew/bin:/opt/homebrew/sbin:/Users/hank/.local/state/fnm_multishells/65442_1736376213366/bin:/usr/local/bin:/Users/hank/.local/bin:/Users/hank/bin:/Users/hank/.bun/bin:/Users/hank/go/bin:/Users/hank/.cache/lm-studio/bin:/usr/bin:/usr/sbin:/bin:/sbin:/Users/hank/.local/state/fnm_multishells/65407_1736376213165/bin:/Users/hank/.local/state/fnm_multishells/65384_1736376212845/bin:/System/Cryptexes/App/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Library/Apple/usr/bin:/Applications/Wireshark.app/Contents/MacOS:/opt/podman/bin:/Applications/Ghostty.app/Contents/MacOS"; export PATH; [ -z "${MANPATH-}" ] || export MANPATH=":${MANPATH#:}"; export INFOPATH="/opt/homebrew/share/info:${INFOPATH:-}"; ~ ❯ brew config HOMEBREW_VERSION: 4.4.15 ORIGIN: https://github.com/Homebrew/brew HEAD: b6fafba4864d65acabf966415e14b2dd86d81e1a Last commit: 4 days ago Branch: stable Core tap HEAD: 7231f2e0cc657fde42a2cc69800ca4f4ca823287 Core tap last commit: 68 minutes ago Core tap JSON: 05 Jan 17:30 UTC Core cask tap JSON: 05 Jan 17:30 UTC HOMEBREW_PREFIX: /opt/homebrew HOMEBREW_BAT: set HOMEBREW_CASK_OPTS: [] HOMEBREW_CURL_RETRIES: 2 HOMEBREW_EDITOR: nvim HOMEBREW_MAKE_JOBS: 24 HOMEBREW_NO_ANALYTICS: set HOMEBREW_NO_AUTO_UPDATE: set HOMEBREW_NO_ENV_HINTS: set HOMEBREW_NO_INSTALLED_DEPENDENTS_CHECK: set HOMEBREW_NO_INSTALL_FROM_API: set Homebrew Ruby: 3.3.6 => /opt/homebrew/Library/Homebrew/vendor/portable-ruby/3.3.6/bin/ruby CPU: 24-core 64-bit arm_blizzard_avalanche Clang: 16.0.0 build 1600 Git: 2.47.1 => /opt/homebrew/bin/git Curl: 8.7.1 => /usr/bin/curl macOS: 15.2-arm64 CLT: 16.2.0.0.1.1733547573 Xcode: N/A Rosetta 2: false`

## Response:
Let me create a React component to display the Homebrew system information in a clean, organized way.

Homebrew System Information

Click to open component

I've created a clean visualization of the Homebrew system information with these features:

1.  Organized into logical sections:
    *   Version Information
    *   Environment Settings
    *   System Settings
    *   Configuration Flags
2.  Styling features:
    *   Uses only core Tailwind classes
    *   Monospace font for paths and values
    *   Clean grid layout
    *   Proper spacing and borders
    *   Responsive design
3.  Notable details:
    *   Grouped related settings
    *   Clear labeling
    *   Preserved important paths
    *   Maintained all configuration flags

The component will look good on both light and dark themes and is responsive to different screen sizes. Would you like me to make any adjustments to the layout or add any additional information?
## Prompt:
based on below current installed brew packages + app pkgs (pkgutil output) suggest best categorization + best icons for easy mac menu bar management on mac studio (large display) + macbook pro (smaller notch display). I'm using bartendar 5.

Copy

`# pkgutil --pkgs com.apple.pkg.CLTools_SDK_macOS13 com.apple.pkg.CLTools_SDK_macOS12 com.apple.pkg.CLTools_Executables com.apple.files.data-template com.apple.pkg.CLTools_SDK_macOS_LMOS com.apple.pkg.GatekeeperCompatibilityData.16U1906 com.apple.pkg.XProtectPlistConfigData_10_15.16U4347 com.apple.pkg.CLTools_SDK_macOS14 com.apple.pkg.CLTools_SDK_macOS_NMOS com.apple.pkg.CLTools_SDK_macOS110 com.apple.pkg.MRTConfigData_10_15.16U4211 com.apple.pkg.InstallAssistant.macOSSequoia com.apple.pkg.RosettaUpdateAuto com.apple.pkg.CLTools_SwiftBackDeploy com.apple.pkg.CLTools_macOS_SDK com.yubico.ykman design.yugen.Flow net.temurin.21.jdk com.flexibits.fantastical2.mac com.logi.installer.pluginservice.media.package net.shinyfrog.bear com.iwaxx.LanScan org.pqrs.Karabiner-Elements com.bitwarden.desktop org.pqrs.Karabiner-DriverKit-VirtualHIDDevice com.kakao.KakaoTalkMac cc.anybox.Anybox us.zoom.pkg.videomeeting io.tailscale.ipn.macos com.hyperdev.joinzoom.macos com.redhat.podman com.logi.installer.pluginservice.package`

Copy

`# brew bundle list --all --global homebrew/bundle homebrew/services temporalio/brew xo/xo aider ast-grep atuin bat broot cheat chezmoi choose curlie datasette diff-so-fancy direnv duf dust eza fastfetch fd fx fzf gh git-delta glow gping grex gron hexyl htop http-prompt httpie huggingface-cli hyperfine jq k9s lazygit luarocks miller neovim poppler procs pyenv rclone ripgrep rust rye sd shellcheck starship temporal tldr uv xcodes yazi ykman yq zellij zoxide zsh-completions temporalio/brew/tcld alacritty bartender bruno claude cursor datasette font-droid-sans-mono-nerd-font font-jetbrains-mono-nerd-font font-roboto-mono-nerd-font font-space-mono-nerd-font font-zed-mono-nerd-font httpie kitty obsidian ollama orbstack proxyman quickjson termius yubico-authenticator yubico-yubikey-manager vscodevim.vim # brew bundle dump --global ==> Downloading https://formulae.brew.sh/api/formula.jws.json ################################################################################################################################################### 100.0% ==> Downloading https://formulae.brew.sh/api/cask.jws.json ################################################################################################################################################### 100.0% tap "homebrew/bundle" tap "homebrew/services" tap "temporalio/brew" tap "xo/xo" brew "act" brew "actionlint" brew "aider" brew "ast-grep" brew "atuin" brew "localstack" brew "awscli-local" brew "bat" brew "broot" brew "cheat" brew "chezmoi" brew "choose" brew "curlie" brew "datasette" brew "diff-so-fancy" brew "direnv" brew "duf" brew "dust" brew "eza" brew "fastfetch" brew "fd" brew "fx" brew "fzf" brew "gh" brew "ghi" brew "git-delta" brew "glow" brew "gping" brew "grex" brew "gron" brew "hexyl" brew "htop" brew "http-prompt" brew "httpie" brew "huggingface-cli" brew "hyperfine" brew "jq" brew "just" brew "k9s" brew "lazygit" brew "luarocks" brew "miller" brew "neovim" brew "netlify-cli" brew "poppler" brew "procs" brew "prometheus" brew "pyenv" brew "rclone" brew "ripgrep" brew "rust" brew "rustscan" brew "rye" brew "sd" brew "shellcheck" brew "starship" brew "temporal" brew "tldr" brew "trippy" brew "uv" brew "xcodes" brew "yazi" brew "ykman" brew "yq" brew "zellij" brew "zoxide" brew "zsh-autosuggestions" brew "zsh-completions" brew "zsh-syntax-highlighting" brew "temporalio/brew/tcld" cask "alacritty" cask "bartender" cask "bruno" cask "claude" cask "cursor" cask "datasette" cask "font-droid-sans-mono-nerd-font" cask "font-jetbrains-mono-nerd-font" cask "font-roboto-mono-nerd-font" cask "font-space-mono-nerd-font" cask "font-zed-mono-nerd-font" cask "github" cask "gitkraken" cask "gitkraken-cli" cask "httpie" cask "kitty" cask "obsidian" cask "ollama" cask "orbstack" cask "proxyman" cask "quickjson" cask "termius" cask "yubico-authenticator" cask "yubico-yubikey-manager" vscode "eamodio.gitlens" vscode "vscodevim.vim"`

Copy

`ls -al /Applications .rw-r--r--@ 31k hank 9 Nov 05:43 .DS_Store drwxr-xr-x@ - root 28 Oct 12:49 .Karabiner-VirtualHIDDevice-Manager.app .rw-r--r-- 0 root 22 Oct 02:49 .localized drwxr-xr-x@ - hank 22 Oct 18:13 Alacritty.app drwxr-xr-x@ - root 28 Oct 08:15 Anybox.app drwxr-xr-x@ - hank 9 Nov 05:09 Arc.app drwxr-xr-x@ - hank 14 Oct 09:43 'Bartender 5.app' drwxr-xr-x@ - root 29 Oct 19:59 Bitwarden.app drwxr-xr-x@ - hank 6 Nov 06:47 BoltAI.app drwxr-xr-x@ - hank 5 Nov 10:18 Bruno.app drwxr-xr-x@ - hank 24 Sep 14:45 'Carbon Copy Cloner.app' drwxr-xr-x@ - hank 4 Oct 00:28 Charmstone.app drwxr-xr-x@ - hank 5 Nov 20:46 ChatGPT.app drwxr-xr-x@ - hank 1 Nov 10:52 Claude.app drwxr-xr-x@ - hank 11 Sep 07:30 'CleanShot X.app' drwxr-xr-x@ - hank 31 Oct 14:53 'Commander One.app' drwxr-xr-x@ - hank 29 Oct 12:38 Cursor.app drwxrwxrwx@ - hank 2 Jan 2023 'Dadroit JSON Viewer.app' drwxr-xr-x@ - hank 12 Sep 10:58 Dash.app drwxr-xr-x@ - hank 27 Feb 2023 Datasette.app drwxr-xr-x@ - hank 30 Oct 08:48 DevToys.app drwxr-xr-x@ - hank 26 Apr 2023 DevUtils.app drwxr-xr-x@ - root 4 Nov 14:05 Fantastical.app drwxr-xr-x@ - root 31 Oct 07:10 Flow.app drwxr-xr-x@ - hank 6 Nov 05:28 'GitHub Desktop.app' drwxr-xr-x@ - hank 21 Oct 13:47 GitKraken.app drwxrwxr-x@ - hank 28 Oct 19:11 'Google Chrome.app' drwxr-xr-x@ - hank 15 Aug 11:54 Homerow.app drwxr-xr-x@ - hank 6 Jan 13:14 HTTPie.app drwxr-xr-x - root 3 Nov 21:13 'Install macOS Sequoia.app' drwxr-xr-x@ - root 1 Nov 22:17 KakaoTalk.app drwxr-xr-x@ - hank 1 Nov 08:46 Kaleidoscope.app drwxr-xr-x@ - root 7 Nov 20:31 Karabiner-Elements.app drwxr-xr-x@ - root 7 Nov 20:31 Karabiner-EventViewer.app drwxr-xr-x@ - hank 13 Oct 22:51 Keymapp.app drwxr-xr-x@ - hank 30 Oct 11:19 kitty.app drwxr-xr-x@ - hank 23 Oct 14:36 'LM Studio.app' drwxr-xr-x@ - hank 21 Oct 15:54 Lunar.app drwxr-xr-x@ - hank 15 Oct 10:51 Obsidian.app drwxr-xr-x@ - hank 6 Nov 10:52 Ollama.app drwxr-xr-x@ - hank 4 Nov 01:47 OrbStack.app drwxr-xr-x@ - hank 23 Oct 14:20 'Podman Desktop.app' drwxr-xr-x@ - hank 19 Oct 01:58 Proxyman.app drwxr-xr-x@ - hank 29 Oct 10:16 RapidAPI.app drwxr-xr-x@ - hank 7 Nov 10:33 Raycast.app lrwxr-xr-x@ - root 22 Oct 02:49 Safari.app -> ../System/Cryptexes/App/System/Applications/Safari.app drwxr-xr-x@ - hank 24 Oct 12:01 Slack.app drwxr-xr-x@ - hank 21 Oct 08:04 Spotify.app drwxrwxr-x@ - hank 10 Oct 14:25 Superhuman.app drwxr-xr-x@ - root 7 Nov 03:52 Tailscale.app drwxr-xr-x@ - hank 31 Oct 22:48 Termius.app drwxr-xr-x@ - hank 31 Oct 22:10 Todoist.app drwxr-xr-x - root 4 Nov 15:40 Utilities drwxr-xr-x@ - hank 8 Nov 15:40 Warp.app drwxr-xr-x@ - hank 29 Oct 06:59 'Yubico Authenticator.app' drwxr-xr-x@ - root 3 Nov 19:57 'YubiKey Manager.app' drwxr-xr-x@ - root 28 Oct 13:16 zoom.us.app`

## Response:
I'll help you organize your menu bar items into logical categories and suggest appropriate icons for both your Mac Studio and MacBook Pro. I'll consider the different display sizes and use Bartender 5's features for optimal organization.

Menu Bar Organization Schema

Click to open image

Here's a detailed organization system for your menu bar items:

### Always Visible (Both Displays)

*   Clock
*   Battery (MacBook Pro only)
*   WiFi
*   Control Center

### Primary Group (Show on Click)

1.  **Security & System:**
    *   Tailscale (🔒)
    *   YubiKey (🔑)
    *   Karabiner (⌨️)
2.  **Productivity Tools:**
    *   Raycast (⚡)
    *   Homerow (⌨️)
    *   Fantastical (📅)
    *   Todoist (✓)
3.  **Development Tools:**
    *   OrbStack (🐳)
    *   DevUtils (🛠️)
    *   Proxyman (🌐)
    *   GitHub Desktop (🐙)

### Secondary Group (Show on Hover)

1.  **Applications:**
    *   Obsidian (📝)
    *   Slack (💬)
    *   Spotify (🎵)
    *   KakaoTalk (💭)
2.  **Utilities:**
    *   CleanShot X (📸)
    *   Lunar (🌙)
    *   Flow (⏱️)
    *   Anybox (📦)

### Display-Specific Considerations:

*   **Mac Studio (Large Display):**
    *   Keep primary group visible with icons
    *   Show secondary group on hover
    *   Use full-size icons (18px)
*   **MacBook Pro (Notch Display):**
    *   Keep only essential items visible
    *   Show both groups on click/hover
    *   Use compact icons (16px)
    *   Consider hiding text labels

### Bartender 5 Settings:

1.  **Show For Updates:**

*   Configure apps like Tailscale, YubiKey to show when updates are available

2.  **Hot Edges:**

*   Set left hot edge to show all hidden items
*   Right hot edge for system monitoring items

3.  **Search:**

*   Enable search for quick access to hidden items
*   Particularly useful on MacBook Pro

4.  **Scripts:**

*   Create scripts to show development tools when IDEs are active
*   Show communication apps during meeting times

This organization maximizes space efficiency while keeping important tools accessible. The grouping is based on function rather than frequency of use, making it intuitive to find items when needed.

## Prompt:
not bad, but I want to easily find items based on categorization when I am trying to use them so "utils" is not as good as "control display brightness".

## Response:
Ah, that's a great point! Let's reorganize based on actual tasks/purposes to make items easier to find when you need them. I'll create a more action/purpose-oriented categorization.

Task-Based Menu Bar Organization

Click to open image

Here's the task-based organization that makes it easier to find things based on what you're trying to do:

### Always Visible

*   Clock (Time/Calendar reference)
*   Battery (Power status - MacBook)
*   WiFi (Network status)
*   Control Center (System controls)

### Quick Access (Show on Click)

1.  **Network & Security:**
    *   Tailscale (🔒 VPN Connection)
    *   YubiKey (🔑 Authentication)
    *   Bitwarden (🗝️ Password Access)
2.  **Input & Control:**
    *   Karabiner (⌨️ Keyboard Mapping)
    *   Homerow (🎯 Navigation)
    *   Raycast (⚡ Quick Commands)
3.  **Display & Capture:**
    *   Lunar (🌙 Display Brightness)
    *   CleanShot X (📸 Screen Capture)

### Development & Work (Show on Hover/Click)

1.  **Development Tools:**
    *   OrbStack (🐳 Container Management)
    *   Proxyman (🌐 Network Debugging)
    *   DevToys (🛠️ Developer Utilities)
    *   GitHub Desktop (🐙 Git Management)
2.  **Time & Tasks:**
    *   Fantastical (📅 Calendar Management)
    *   Flow (⏱️ Time Tracking)
    *   Todoist (✓ Task Management)
3.  **Communication:**
    *   Slack (💬 Work Communication)
    *   KakaoTalk (💭 Messaging)
    *   Zoom (🎥 Video Calls)

### Content & Notes (Show on Hover)

1.  **Knowledge Management:**
    *   Obsidian (📝 Notes)
    *   Anybox (📦 Bookmarks)
    *   DevUtils (🔍 Code Lookup)
2.  **Media & Entertainment:**
    *   Spotify (🎵 Music)
    *   Arc (🌐 Browser)

### Bartender 5 Settings:

1.  **Display-Specific Groups:**

*   **Mac Studio:**
    *   Keep Network & Security + Input & Control visible
    *   Show Display & Capture on hot corner
    *   Development tools show when IDEs active
*   **MacBook Pro:**
    *   Keep only essential security items visible
    *   Show others on hover/click
    *   Use compact mode for space efficiency

2.  **Quick Access Features:**

*   Left hot edge: Show all items
*   Right hot edge: Show active development tools
*   Double-click: Show communication apps

3.  **Automation Rules:**

*   Show development tools when VS Code/IDEs are active
*   Show communication tools during scheduled meeting times
*   Show media controls when Spotify is playing

This organization makes it much clearer what each item does and when you might need it. For example, instead of hunting through a generic "Utils" category, you know exactly where to find display brightness controls or screen capture tools.

The grouping is now based on common tasks and workflows:

*   Need to adjust display? → Display & Capture group
*   Need to debug something? → Development Tools group
*   Need to join a meeting? → Communication group

This should make it much more intuitive to find what you need when you need it, while still keeping the menu bar organized and uncluttered.

## Prompt:
great. Review current recommendation for any weaknesses or mistakes as well as improvements - then suggest final revised version after doing that loop 3 times to yourself.

Then suggest best icons for each final categories on bartendar 5 menu bar symbols / icons

## Response:
I'll review and iterate on the design three times, focusing on identifying weaknesses and potential improvements.

**Review Loop 1:** Weaknesses:

*   "Content & Notes" category mixing with "Development & Work" is confusing
*   "Quick Access" vs "Always Visible" distinction isn't clear enough
*   Some tools could serve multiple purposes (e.g., DevUtils)
*   Missing system resource monitoring category
*   No clear handling of AI tools (Claude, ChatGPT, etc.)

**Review Loop 2:** Additional issues:

*   Network tools split between categories
*   Missing clear backup/sync tools category (Rclone, etc.)
*   Time-related tools split across categories
*   No clear handling of terminal apps (Warp, Alacritty)
*   Missing clear strategy for handling multiple browsers

**Review Loop 3:** Final observations:

*   Need clearer distinction between system status and system control
*   Should separate persistent tools from task-based ones
*   Missing clear organization for AI/ML tools (Ollama, etc.)
*   Need better integration with macOS built-in features
*   Should consider workflow-based grouping for some categories

Here's the final, refined organization:

Final Menu Bar Organization

Click to open image

### Recommended Bartender 5 Icons/Symbols:

1.  **System Status** (🔵 Blue Category)

*   Icon: ⚡ (Activity monitor style)
*   Shows: Condensed system metrics
*   Color: Blue gradient

2.  **Security & Access** (🔴 Red Category)

*   Icon: 🛡️ (Shield)
*   Shows: Combined security status
*   Color: Red gradient

3.  **Workflow Tools** (🟢 Green Category)

*   Icon: ⚙️ (Gear with dots)
*   Shows: Active workflow context
*   Color: Green gradient

4.  **Development** (🟣 Purple Category)

*   Icon: </> (Code brackets)
*   Shows: Dev environment status
*   Color: Purple gradient

5.  **AI & Knowledge** (🟡 Yellow Category)

*   Icon: 🧠 (Brain)
*   Shows: AI/Tools activity
*   Color: Yellow gradient

### Bartender Display Rules:

1.  **System Display:**

*   Mac Studio: Show all system status
*   MacBook: Collapse to single indicator

2.  **Context Awareness:**

*   Show dev tools when IDEs active
*   Show communication tools in meetings
*   Show knowledge tools when researching

3.  **Hot Corners:**

*   Left: Show all hidden items
*   Right: Show current context items
*   Bottom: Quick AI tools access

4.  **Search Integration:**

*   Quick category jumps
*   Tool-specific searches
*   Context-aware results

Key Improvements from Previous Versions:

1.  Clearer separation of system/status vs. tools
2.  Better context-awareness
3.  Integrated AI tools properly
4.  More logical grouping by workflow
5.  Better scalability between displays

The icon system uses color coding and distinct symbols to make categories instantly recognizable, while maintaining a clean aesthetic that works well with macOS's design language.

Would you like me to provide more specific icon designs or detail any particular aspect of this organization further?

## Prompt:
no let's use this. Organize all this final version info into README markdown

## Response:
I'll create a comprehensive README.md for your menu bar organization using Bartender 5.

Menu Bar Organization README

Click to open document

I've created a comprehensive README.md that documents your menu bar organization system. The structure is designed to be:

1.  Easy to understand and implement
2.  Maintainable over time
3.  Adaptable to different workflows
4.  Clear for both technical and non-technical aspects

You can find this in your repository as README.md. Would you like me to:

1.  Highlight any specific section for immediate implementation?
2.  Create specific configuration scripts for any part?
3.  Add more detailed setup instructions for particular tools?
4.  Expand on any particular category?

## Prompt:
use visually easily digestable README format - e.g. use tables where possible + use summaries up top + give examples and guidelines for easy convention adhereance

## Response:
I'll redesign the README to be more visually digestible and scannable.

Visual Menu Bar Organization README

Click to open document

The new README format:

1.  Uses tables for quick reference
2.  Includes collapsible sections for details
3.  Provides visual hierarchy
4.  Shows clear examples
5.  Makes maintenance easy

Key improvements:

*   Quick reference table at the top
*   Mermaid diagram for visualization
*   Clear context examples
*   Actionable checklists
*   Troubleshooting guide

Would you like me to:

1.  Add more specific examples?
2.  Create detailed context rules?
3.  Expand the troubleshooting section?
4.  Add more visual elements?

## Prompt:
how best to quickly reflect this to my mac studio bartendar 5 menu bar with minimal manual work (i.e. me dragging items with my mouse) --- and more importantly how to commit + version + sync to this definition on all my mac devices

## Response:
I'll help you automate the setup and sync process for Bartender 5 across your devices.

Bartender 5 Automation Setup

Click to open document

This automation setup provides:

1.  **Minimal Manual Work**

*   Automated configuration application
*   Machine-specific settings
*   One-command updates

2.  **Version Control**

*   Git-based configuration management
*   Chezmoi for dotfiles sync
*   Automated backups

3.  **Cross-Device Sync**

*   Consistent configuration across devices
*   Machine-specific adaptations
*   Easy updates and rollbacks

To get started immediately:

1.  Copy the scripts to your config directory
2.  Run the initial setup script
3.  Configure Chezmoi for sync
4.  Push to your dotfiles repository

Would you like me to:

1.  Provide more detailed machine-specific configurations?
2.  Add more safety checks to the scripts?
3.  Create additional automation for specific categories?
4.  Add monitoring and logging capabilities?

## Prompt:
do this using chezmoi

## Response:
I'll create a specific Chezmoi-based implementation for managing your Bartender 5 configuration.

Chezmoi Bartender Setup Guide

Click to open document

To implement this immediately:

1.  **Quick Start:**

```bash
# Install chezmoi if not installed
brew install chezmoi

# Initialize and apply
chezmoi init
chezmoi apply
```

2.  **Sync to New Machine:**

```bash
# On new machine
chezmoi init https://github.com/yourusername/dotfiles.git
chezmoi apply
```

3.  **Update Configuration:**

```bash
# Edit configuration
chezmoi edit ~/.config/bartender/config.toml

# Apply changes
chezmoi apply
```

The configuration will automatically adapt to:

*   Mac Studio (large display)
*   MacBook Pro (notched display)
*   Different workflow contexts
*   Various app configurations

Would you like me to:

1.  Help set up the initial configuration?
2.  Create specific machine profiles?
3.  Add more automation rules?
4.  Enhance any particular section?

## Prompt:
given list of apps, suggest comprehesive keyboard shortcut/hotkey/keymap configuration on mac os for optimal usage for intuitive modifier key + prefix keys. Ideally there are clear reasons for using certain set of modifier keys + prefix keys for certain apps or certain category of operations + etc

## Response:
I'll create a comprehensive, logical keyboard shortcut system based on your installed apps and common operations. Let me break this down systematically.

MacOS Keyboard Shortcut System

Click to open document

This system is designed with several key principles:

1.  **Consistency:**

*   Similar operations use similar modifiers
*   Patterns are maintained across apps
*   Mnemonics are used for memorability

2.  **Ergonomics:**

*   Frequently used shortcuts use easier combinations
*   Related functions use similar patterns
*   Minimizes hand strain

3.  **Context Awareness:**

*   Development shortcuts in dev tools
*   Communication shortcuts in chat apps
*   System shortcuts globally available

Would you like me to:

1.  Create specific configs for any app?
2.  Add more ergonomic patterns?
3.  Expand the mnemonic system?
4.  Add automation scripts?

## Prompt:
provide more detailed key mappings for AI driven developer workflow (inline, sidebar chat, refactor, etc) + search (files, history, browser, apps, cheatsheets, etc) + workspace related items (launcher/switcher/window+tabs+panes).

focus more on crafting consistent and wholistic keymap experience with easy configuration viewing + modifying quickly. Less frequent ops like input switching can be just another physical key remapped.

## Response:
I'll create a comprehensive, cohesive keymap system focused on AI-augmented development workflow, search operations, and workspace management. Let me restructure this with a stronger emphasis on your core workflow.

Advanced Developer Keymap System

Click to open document

Let me create another artifact specifically for implementing this system consistently across your current tools:

Generating...

## Prompt:
format this to be re-usable config file for

*   karabiner
*   skhd

## Response:
I'll help create reusable configuration files for both Karabiner-Elements and skhd to implement this system.

Karabiner Config

Click to open code

SKHD Config

Click to open code

Let me also create a helper script to install and manage these configurations:

Setup Script

Click to open code

Key features of this configuration:

1.  **Layered Access**:
    *   HYPER (CapsLock) as the primary modifier
    *   Mode-based access for different functions
    *   Clear visual indicators for current mode
2.  **Consistent Patterns**:
    *   Vim-style hjkl for navigation
    *   Number keys for quick access
    *   Similar commands across modes
3.  **Integration Points**:
    *   AI tools integration
    *   Development workflow
    *   Window management
## Prompt:
best way to add nix binary installation to my dotfiles (Github repo committed) so it is installed automatically on new Mac OSX machines? I run "bootstrap.sh" on new machines so I can include it as shell install command. But also I could add it to my bashrc or zshrc

## Response:
To automatically install Nix on new Mac OSX machines using your dotfiles repository, you can follow these steps:

1.  Add the Nix installation command to your `bootstrap.sh` script:

```bash
# Install Nix
sh <(curl -L https://nixos.org/nix/install) --darwin-use-unencrypted-nix-store-volume
```

This command downloads and runs the Nix installation script, which installs Nix on your Mac OSX system.

2.  Configure your shell to use Nix by adding the following lines to your `bashrc` or `zshrc` file:

```bash
# Nix
if [ -e ~/.nix-profile/etc/profile.d/nix.sh ]; then
  . ~/.nix-profile/etc/profile.d/nix.sh
fi
```

This code checks if the Nix profile script exists and sources it, enabling Nix in your shell environment.

3.  Commit the changes to your dotfiles repository:

```bash
git add bootstrap.sh bashrc zshrc
git commit -m "Add Nix installation and configuration"
git push
```

Now, when you run `bootstrap.sh` on a new Mac OSX machine, it will automatically install Nix and configure your shell to use it.

Alternatively, if you prefer to keep the Nix installation separate from your `bootstrap.sh` script, you can create a dedicated script for installing Nix (e.g., `install_nix.sh`) and add it to your dotfiles repository. Then, you can manually run this script on new machines or invoke it from your `bootstrap.sh` script.

```bash
# install_nix.sh
#!/bin/bash

# Install Nix
sh <(curl -L https://nixos.org/nix/install) --darwin-use-unencrypted-nix-store-volume
```

Make sure to make the script executable (`chmod +x install_nix.sh`) before committing it to your repository.

By following these steps, you can easily automate the installation of Nix on new Mac OSX machines using your dotfiles repository.
## Prompt:
continue the response answer code and proviide it in full that based on attached convo text was cut short

Copy

## Response:
I'll help provide the complete set of dotfiles and configurations based on the conversation. Let me break this down systematically.

Complete Dotfiles Configuration

Click to open code

This provides a complete, organized dotfiles setup with:

1.  Silent initialization that won't output unwanted messages
2.  Proper error handling and recovery
3.  CLI tool with tab completion
4.  Clean installation and update processes
5.  Package group management
6.  Configuration management
7.  Proper symlink handling
8.  Application settings management

Key features:

*   Quiet initialization with proper error handling
*   Tab completion for all commands and subcommands
*   Package group management for targeted installations
*   Backup system for existing configurations
*   Clean state management with zap capability
*   Progress indicators and clear status messages

Would you like me to explain any specific part in more detail or make any adjustments to better suit your needs?

## Prompt:
I don't need "bin/dotfile" as a separate file, I just need "init.sh" and "update.sh" or something like that in dotfiles dir

## Response:
I understand - you want a simpler structure with just initialization and update scripts. Let me provide those core files.

Core Dotfiles Scripts

Click to open code

This simplified version provides two core scripts:

1.  `init.sh` - Handles quiet shell initialization:
    *   Silences unwanted startup output
    *   Sets up Homebrew env silently
    *   Configures ZSH completion without noise
    *   Sources configs quietly
2.  `update.sh` - Handles updates and maintenance:
    *   Updates dotfiles symlinks
    *   Updates Homebrew packages
    *   Provides zap functionality for clean installs
    *   Simple command structure: `./update.sh [update|zap]`

To use these:

1.  Place both scripts in your dotfiles directory:

```bash
chmod +x init.sh update.sh
```

2.  Add to your `.zshrc`:

```bash
source "${HOME}/dotfiles/init.sh"
```

3.  Run updates or clean installs:

```bash
# Regular update
./update.sh

# Clean reinstall
./update.sh zap
```

Would you like me to explain any part in detail or make any adjustments?

## Prompt:
i like the simplification but I still want all features of originally scriopts, just in 2 shell script "init.sh" and "update.sh" or even 1 unified command. Below is my current state

Copy

`hank@hank-mstio dotfiles % /opt/homebrew/bin/eza -T . | pbcopy`

## Response:
I understand - you want to combine all the functionality into just one or two scripts while maintaining the directory structure you have. Let me help create a unified solution.

Unified Dotfiles Management

Click to open code

I've created a single unified script that handles both initialization and management, respecting your existing directory structure. Key features:

1.  Initialization:
    *   Silent shell startup
    *   Proper completion setup
    *   Automated config sourcing
2.  Package Management:
    *   Homebrew installation/updates
    *   Package cleanup
    *   Brewfile support
3.  Dotfiles Management:
    *   Comprehensive symlink creation
    *   Backup system
    *   Config file handling
4.  Additional Features:
    *   Sync support for multiple machines
    *   Package export functionality
    *   Clean reinstall capability

Usage:

1.  Place the script in your dotfiles directory:

```bash
chmod +x init.sh
```

2.  Add to your `.zshrc`:

```bash
source "${HOME}/dotfiles/init.sh"
```

3.  Use commands:

```bash
./init.sh update   # Update dotfiles and packages
./init.sh zap     # Clean reinstall
./init.sh sync    # Sync with remote repository
./init.sh export  # Export current packages
```

The script automatically handles both:

*   Being sourced (for shell initialization)
*   Being executed (for management commands)

Would you like me to make any adjustments or explain any part in more detail?

## Prompt:
does this look right?

Copy

## Response:
Looking at the provided code, there are a few things we can improve to make it more robust and aligned with your directory structure:

1.  The initialization part needs to better handle your zsh configurations
2.  Some paths need to be adjusted to match your structure
3.  We should ensure proper loading order of configurations

Let me help revise this:

Improved Dotfiles Management

Click to open code

Key improvements made:

1.  Better XDG compliance:
    *   Added all XDG base directories
    *   Properly structured cache locations
    *   Better config organization
2.  Improved ZSH configuration loading:
    *   Specific loading order for zsh configs
    *   Better completion handling
    *   Proper ZDOTDIR usage
3.  Enhanced symlink management:
    *   More flexible config mapping
    *   Better backup handling
    *   Proper directory structure preservation
4.  Better tool initialization:
    *   Added `has_command` helper
    *   More robust tool loading
    *   Quieter operation
5.  Added support for your specific tools:
    *   Added zsh-abbr configuration
    *   Proper ghostty config handling
    *   Better Brewfile organization

Would you like me to explain any specific part or make additional adjustments?

## Prompt:
all zsh shell files again in full with full path - and init.sh as well as zshrc should all be written to be symlinked from expected locaiton (e.g. ~/.zshrc or zsh/.zshrc) to dotfiles dir (~/dotfiles/home/zsh/zshrc)

## Response:
I'll help create all the ZSH configuration files with proper paths and symlinking. Let me organize these systematically.

Complete ZSH Configuration

Click to open code

This configuration:

1.  Sets up proper symlinking from expected locations to your dotfiles repository:

    ```
    ~/.zshrc -> ~/dotfiles/home/zsh/zshrc
    ~/.zshenv -> ~/dotfiles/home/zsh/zshenv
    ~/.zprofile -> ~/dotfiles/home/zsh/zprofile
    ~/.config/zsh -> ~/dotfiles/home/zsh
    ```

2.  Organizes ZSH configuration into modular files:
    *   `zshenv`: Early environment setup
    *   `zprofile`: Login shell configuration
    *   `zshrc`: Interactive shell settings
    *   `env.zsh`: Environment variables
    *   `completions.zsh`: Completion system
    *   `aliases.zsh`: Command aliases
    *   `functions.zsh`: Custom functions
    *   `macos.zsh`: macOS specific settings
3.  Provides a unified initialization script that:
    *   Sets up XDG directories
    *   Creates necessary symlinks
    *   Handles both sourcing and execution
    *   Manages backup of existing files

To use this:

1.  Place all files in your dotfiles repository:

```bash
~/dotfiles/
├── init.sh
└── home/
    └── zsh/
        ├── zshrc
        ├── zshenv
        ├── zprofile
        ├── env.zsh
        ├── completions.zsh
        ├── aliases.zsh
        ├── functions.zsh
        └── macos.zsh
```

2.  Make the init script executable:

```bash
chmod +x ~/dotfiles/init.sh
```

3.  Run the initialization:

```bash
~/dotfiles/init.sh
```

Would you like me to explain any specific part or make any adjustments?

## Prompt:
setup git config with gh

## Response:
I'll help create a Git configuration with GitHub CLI (gh) integration.

Git and GitHub Configuration

Click to open code

To set this up:

1.  Create the necessary directories:

```bash
mkdir -p ~/dotfiles/home/config/{git,gh}
```

2.  Add these files to your dotfiles:

```bash
# Copy configurations
cp gitconfig ~/dotfiles/home/config/git/
cp gitignore ~/dotfiles/home/config/git/
cp config.yml ~/dotfiles/home/config/gh/
```

3.  Update the init.sh symlink section to include Git configs:

```bash
# Add to your existing link configurations
local configs=(
    # ... existing configs ...
    "git/gitconfig:.gitconfig"
    "git/gitignore:.config/git/gitignore"
    "gh/config.yml:.config/gh/config.yml"
)
```

4.  Install and setup GitHub CLI:

```bash
# Install gh if not already installed
brew install gh

# Login to GitHub
gh auth login
```

5.  Configure your personal information:

```bash
# Set your details
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"
git config --global user.signingkey "your-gpg-key-if-using"
```

Key features:

1.  Git Configuration:
    *   Sensible defaults for push/pull
    *   Delta integration for better diffs
    *   Useful aliases
    *   GitHub CLI integration
2.  Global Gitignore:
    *   Common temporary files
    *   OS-specific files
    *   Editor files
    *   Build artifacts
3.  GitHub CLI Configuration:
    *   SSH protocol by default
    *   Custom editor settings
    *   Useful aliases
    *   Delta integration

Would you like me to explain any specific part or make any adjustments?
## Prompt:
how to best install and setup determinate systems nix, nix-darwin, home-manager with github repo "aitchwhy/dotfiles" for declarative dotfiles management?

I currently have determinate nix installed

## Response:
I'll help you set up a declarative dotfiles management system using Nix, nix-darwin (for macOS), and home-manager. Let me break this down step by step.

Basic Nix Configuration Setup

Click to open code

Darwin System Configuration

Click to open code

Home Manager Configuration

Click to open code

Here's the step-by-step setup process:

1.  First, create your GitHub repository structure:

```bash
mkdir -p ~/dotfiles/{darwin,home}
cd ~/dotfiles
```

2.  Initialize git and set up your remote:

```bash
git init
git remote add origin git@github.com:aitchwhy/dotfiles.git
```

3.  Copy the provided configurations from the artifacts above into their respective files:

*   `flake.nix` in the root directory
*   `darwin/configuration.nix` for system settings
*   `home/home.nix` for user settings

4.  Enable flakes in your Nix configuration by editing `~/.config/nix/nix.conf`:

```bash
mkdir -p ~/.config/nix
echo "experimental-features = nix-command flakes" >> ~/.config/nix/nix.conf
```

5.  Build and apply your configuration:

```bash
# Replace 'your-hostname' with your actual hostname
darwin-rebuild switch --flake .#your-hostname
```

Key modifications you'll need to make:

*   In `flake.nix`: Change `your-hostname` to your actual hostname and adjust `system` if needed
*   In `home.nix`: Update `yourusername`, `homeDirectory`, and Git configuration
*   In all files: Customize package lists and settings according to your needs

To maintain your dotfiles:

1.  Make changes to the configuration files
2.  Commit changes to git
3.  Run `darwin-rebuild switch --flake .` to apply
4.  Push to GitHub for backup

Some tips:

*   Use `darwin-rebuild check` to test configurations before applying
*   The system may need a restart after initial setup
*   Keep track of your flake.lock file for reproducibility

Would you like me to help customize any specific part of these configurations?

## Prompt:
i want to use flakes if possible, does that change it?

## Response:
You're in luck - the configuration I provided is already using flakes! That's why we have the `flake.nix` as the main configuration file and why all the inputs are declared in the flake-style format. Let me point out the flake-specific parts:

1.  The configuration is already flake-based:

*   Uses `flake.nix` as the entry point
*   Declares dependencies as `inputs` in flake format
*   Uses `outputs` to define the system configuration
*   Uses the modern `follows` syntax for input dependencies

2.  The commands also use flake syntax:

```bash
# Building/applying with flakes
darwin-rebuild switch --flake .#your-hostname

# Updating flake inputs
nix flake update
```

3.  The only additional setup needed for flakes is enabling the experimental features, which was included in the steps:

```bash
mkdir -p ~/.config/nix
echo "experimental-features = nix-command flakes" >> ~/.config/nix/nix.conf
```

If you want to update individual flake inputs, you can use:

```bash
# Update all inputs
nix flake update

# Update specific input
nix flake lock --update-input nixpkgs
nix flake lock --update-input home-manager
```

The configuration I provided is already taking full advantage of flakes' reproducibility and modularity. Is there a specific aspect of flakes you'd like to leverage more?

## Prompt:
since i do not have nix flake for dotfiles setup yet, darwin-rebuild is not present in PATH when I try to run it in my dotfiles folder. What do I do?

## Response:
Ah yes, good catch! Since you don't have nix-darwin installed yet, you'll need to use `nix` commands directly to bootstrap the system. Here's what to do:

1.  First, create your flake configuration files as shown in the artifacts above.
2.  Then, instead of using `darwin-rebuild`, run this to build and switch to your first configuration:

```bash
# Build and activate initial configuration
nix build .#darwinConfigurations.your-hostname.system
./result/sw/bin/darwin-rebuild switch --flake .#your-hostname
```

After this initial bootstrap:

*   `darwin-rebuild` will be available in your PATH
*   You can then use `darwin-rebuild switch --flake .#your-hostname` for future updates

If you get any permission errors, you might need to run:

```bash
sudo chown -R root:wheel /nix
```

Would you like me to help troubleshoot any specific errors you're seeing?

## Prompt:
where are the files located? nix.conf, flake.nix, etc

## Response:
Let me lay out the file structure clearly:

1.  `nix.conf` - system-wide Nix settings:

```
~/.config/nix/nix.conf
```

2.  For your dotfiles repository, you'll create this structure in your preferred location (commonly in home directory):

```
~/dotfiles/               # Your repository root
├── flake.nix            # Main flake configuration
├── darwin/
│   └── configuration.nix # Darwin-specific settings
└── home/
    └── home.nix         # Home-manager settings
```

To set this up:

```bash
cd ~
mkdir -p dotfiles/darwin dotfiles/home
cd dotfiles
# Create the files here
```

Alternative locations some people use for the dotfiles repo:

*   `~/.dotfiles/`
*   `~/src/dotfiles/`
*   `~/code/dotfiles/`

The key is that your `flake.nix` and related configs will be wherever you clone/create your dotfiles repository. Only `nix.conf` has a fixed location in `~/.config/nix/`.

Would you like me to help with creating any of these files in your preferred location?

## Prompt:
instructions on project docs show "~/.config/flake.nix" as root of flake, is this ok we're different dir?

## Response:
The flake can actually work from any directory - both `~/.config/flake.nix` and `~/dotfiles/flake.nix` are valid approaches. Here are the key differences:

`~/.config/flake.nix`:

*   More conventional for single-user setups
*   Follows XDG base directory specification
*   Simpler path references
*   Matches project docs example

`~/dotfiles/flake.nix` (or similar):

*   Better for version control and sharing
*   Clearer separation of configs
*   More common in the community for dotfiles management
*   Easier to maintain as a git repository

Since you mentioned you want to manage this with GitHub repo "aitchwhy/dotfiles", I recommended the `~/dotfiles` approach because:

1.  It's cleaner to manage as a git repository
2.  Easier to clone to new machines
3.  More straightforward for backup/restore
4.  Common pattern in the nixos community

However, if you prefer following the project docs exactly, we can absolutely adjust the configs for `~/.config/flake.nix` instead. Which approach would you prefer to use?

## Prompt:
lets try dotfiles dir first since I already have it there. what role does ~/.config/nix with nix.conf serve then if we use dotfiles dir for flake + etc nix files?

## Response:
The `~/.config/nix/nix.conf` serves a different purpose - it's the global Nix configuration that enables core Nix features, regardless of where your actual Nix configurations (flakes) live.

Key differences:

*   `~/.config/nix/nix.conf`: Configures the Nix package manager itself
    *   Enables experimental features like flakes
    *   Sets global Nix settings like max jobs, substituters
    *   Required for Nix to work properly, independent of your specific configurations
    *   This file is read by the Nix daemon
*   `~/dotfiles/flake.nix` (and related files):
    *   Defines your actual system/user configuration
    *   Specifies what packages to install
    *   Configures your system settings
    *   Manages your dotfiles
    *   These are the files that get evaluated when you run `nix build` or `darwin-rebuild`

Think of it like:

*   `nix.conf` = tells Nix how to operate
*   `flake.nix` = tells Nix what to build/configure

That's why we need both:

1.  `~/.config/nix/nix.conf` to enable flakes:

```
experimental-features = nix-command flakes
```

2.  `~/dotfiles/flake.nix` to define what we want to do with those flakes

## Prompt:
fix this error i get when I run `nix run nix-darwin -- switch --flake .`

Copy

`hank@hank-mbp-m3 dotfiles % nix run nix-darwin -- switch --flake . building the system configuration... error: … while evaluating the attribute 'config.system.build.toplevel' at /nix/store/wzhdvy8pvcrfc1flrnhrbrpyb0q38pp8-source/lib/modules.nix:334:9: 333| options = checked options; 334| config = checked (removeAttrs config [ "_module" ]); | ^ 335| _module = checked (config._module); … while calling the 'seq' builtin at /nix/store/wzhdvy8pvcrfc1flrnhrbrpyb0q38pp8-source/lib/modules.nix:334:18: 333| options = checked options; 334| config = checked (removeAttrs config [ "_module" ]); | ^ 335| _module = checked (config._module); (stack trace truncated; use '--show-trace' to show the full, detailed trace) error: path '/nix/store/y5m237hy02r6fwgldgl11jys75zwhcxa-source/darwin/configuration.nix' does not exist hank@hank-mbp-m3 dotfiles % nix store gc`

## Response:
The error indicates that your configuration files aren't in the correct location relative to your `flake.nix`. Let's check your current directory structure and fix it.

1.  First, verify your directory structure. In your `dotfiles` directory, you should have:

```bash
~/dotfiles/
├── flake.nix
├── darwin/
│   └── configuration.nix
└── home/
    └── home.nix
```

2.  Make sure `flake.nix` is referencing the correct paths. The paths in `flake.nix` should be relative to the flake's location:

```nix
{
  outputs = { self, nixpkgs, darwin, home-manager }: {
    darwinConfigurations."hank-mbp-m3" = darwin.lib.darwinSystem {
      system = "aarch64-darwin";  # since you have an M3
      modules = [
        # Change this line to use relative path
        ./darwin/configuration.nix  # not /nix/store/...

        home-manager.darwinModules.home-manager
        {
          home-manager.useGlobalPkgs = true;
          home-manager.useUserPackages = true;
          home-manager.users.hank = import ./home/home.nix;  # adjust username if different
        }
      ];
    };
  };
}
```

3.  Run these commands to verify your files exist:

```bash
ls -la darwin/configuration.nix
ls -la home/home.nix
```

If either file is missing, you'll need to create it. Would you like me to show the contents of any of these files again?

Also, note that I used "hank-mbp-m3" as the hostname based on your prompt - make sure this matches your actual system hostname. You can check your hostname with:

```bash
hostname
```

## Prompt:
review my actions + output below from zsh. I mistakenly named a file so I fixed it but seeing issue even after

Copy

``hank@hank-mbp-m3 dotfiles % tree . ├── darwin │   └── configurations.nix ├── flake.lock ├── flake.nix └── home └── home.nix 3 directories, 4 files hank@hank-mbp-m3 dotfiles % pwd /Users/hank/dotfiles hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % mv darwin/configurations.nix darwin/configuration.nix hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % tree . ├── darwin │   └── configuration.nix ├── flake.lock ├── flake.nix └── home └── home.nix 3 directories, 4 files hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % nix run nix-darwin -- switch --flake . building the system configuration... warning: Git tree '/Users/hank/dotfiles' is dirty error: … while evaluating the attribute 'config.system.build.toplevel' at /nix/store/wzhdvy8pvcrfc1flrnhrbrpyb0q38pp8-source/lib/modules.nix:334:9: 333| options = checked options; 334| config = checked (removeAttrs config [ "_module" ]); | ^ 335| _module = checked (config._module); … while calling the 'seq' builtin at /nix/store/wzhdvy8pvcrfc1flrnhrbrpyb0q38pp8-source/lib/modules.nix:334:18: 333| options = checked options; 334| config = checked (removeAttrs config [ "_module" ]); | ^ 335| _module = checked (config._module); (stack trace truncated; use '--show-trace' to show the full, detailed trace) error: path '/nix/store/iwk05rx0f2wdzacgnw7835j123kjh1zf-source/darwin/configuration.nix' does not exist hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % git add --all hank@hank-mbp-m3 dotfiles % git commit --amend [main 38ea3ec] Init Date: Fri Jan 10 02:37:15 2025 -0500 6 files changed, 189 insertions(+) create mode 100644 .gitignore create mode 100644 .vscode/settings.json create mode 100644 darwin/configuration.nix create mode 100644 flake.lock create mode 100644 flake.nix create mode 100644 home/home.nix hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % hank@hank-mbp-m3 dotfiles % nix run nix-darwin -- switch --flake . building the system configuration... error: … while evaluating the attribute 'value' at /nix/store/wzhdvy8pvcrfc1flrnhrbrpyb0q38pp8-source/lib/modules.nix:846:9: 845| in warnDeprecation opt // 846| { value = addErrorContext "while evaluating the option `${showOption loc}':" value; | ^ 847| inherit (res.defsFinal') highestPrio; … while evaluating the option `system.build': … while evaluating the attribute 'mergedValue' at /nix/store/wzhdvy8pvcrfc1flrnhrbrpyb0q38pp8-source/lib/modules.nix:881:5: 880| # Type-check the remaining definitions, and merge them. Or throw if no definitions. 881| mergedValue = | ^ 882| if isDefined then … while evaluating definitions from `/nix/store/wy0a6vicf8c9hhnq92p4y58ph06i6ivm-source/modules/system': … while evaluating the option `home-manager.users.hank.home.homeDirectory': (stack trace truncated; use '--show-trace' to show the full, detailed trace) error: A definition for option `home-manager.users.hank.home.homeDirectory' is not of type `path'. Definition values: - In `/nix/store/csr7k1pa8z9kcbq44mdxwj3k1c401vgg-source/nixos/common.nix': null hank@hank-mbp-m3 dotfiles % nix store gc``

## Response:
Let's fix this step by step. The error has changed after your git commit, which is progress, but now we're seeing a home-manager configuration issue.

1.  First, let's verify your `flake.nix` content:

```nix
{
  description = "Your system configuration";

  inputs = {
    nixpkgs.url = "github:nixos/nixpkgs/nixpkgs-unstable";
    darwin = {
      url = "github:lnl7/nix-darwin";
      inputs.nixpkgs.follows = "nixpkgs";
    };
    home-manager = {
      url = "github:nix-community/home-manager";
      inputs.nixpkgs.follows = "nixpkgs";
    };
  };

  outputs = { self, nixpkgs, darwin, home-manager }: {
    darwinConfigurations."hank-mbp-m3" = darwin.lib.darwinSystem {
      system = "aarch64-darwin";
      modules = [
        ./darwin/configuration.nix
        home-manager.darwinModules.home-manager
        {
          home-manager.useGlobalPkgs = true;
          home-manager.useUserPackages = true;
          home-manager.users.hank = import ./home/home.nix;
        }
      ];
    };
  };
}
```

2.  Then let's check your `home/home.nix`. The error suggests it's not properly setting the home directory. It should look like:

```nix
{ config, pkgs, ... }: {
  home = {
    username = "hank";
    homeDirectory = "/Users/hank";  # This needs to be explicitly set
    stateVersion = "23.11";  # Check this matches your nix version
  };

  programs.home-manager.enable = true;
}
```

Can you confirm if your `home.nix` has these settings, particularly the `homeDirectory` value? The error suggests it's either missing or set to `null`.

Could you share the contents of your `home.nix` file? That would help us pinpoint the exact issue.

## Prompt:
do I have the right nix version? I get this below. And also sharing my home.nix file

Copy

`# nix version check hank@hank-mbp-m3 dotfiles % nix --version nix (Nix) 2.25.3 hank@hank-mbp-m3 dotfiles % which nix /nix/var/nix/profiles/default/bin/nix`

Copy

`# home.nix # home/home.nix { config, pkgs, ... }: { home = { username = "hank"; homeDirectory = "/Users/hank"; # This needs to be explicitly set stateVersion = "23.11"; # Check this matches your nix version }; # Let Home Manager manage itself programs.home-manager.enable = true; # Package installations home.packages = with pkgs; [ ripgrep fd tree htop ]; # Git configuration programs.git = { enable = true; userName = "aitchwhy"; userEmail = "hank.lee.qed@gmail.com"; extraConfig = { init.defaultBranch = "main"; pull.rebase = true; }; }; # Shell configuration (zsh example) programs.zsh = { enable = true; enableAutosuggestions = true; enableSyntaxHighlighting = true; shellAliases = { ls = "ls --color=auto"; ll = "ls -la"; ".." = "cd .."; }; }; }`

## Response:
Your Nix version (2.25.3) is good and recent enough, and your `home.nix` looks correct. The stateVersion of "23.11" is also fine.

Let's try two things:

1.  First, let's try building with a more detailed error trace:

```bash
nix run nix-darwin -- switch --flake . --show-trace
```

2.  If that doesn't give us more insight, let's try building with a simplified `home.nix` first to isolate the issue. Create a temporary `home.nix.backup` and replace your `home.nix` with just the core configuration:

```nix
{ config, pkgs, ... }: {
  home = {
    username = "hank";
    homeDirectory = "/Users/hank";
    stateVersion = "23.11";
  };

  programs.home-manager.enable = true;
}
```

Then try:

```bash
nix run nix-darwin -- switch --flake .
```

Could you try these steps and share the output? This will help us determine if the issue is with the basic configuration or with one of the additional program configurations.

Also, let's verify your `darwin/configuration.nix` doesn't have any conflicting home-manager settings. Could you share its contents as well?

## Prompt:
format attached brewfile list of casks + mas apps into darwin/configurations.nix homebrew field format

Copy

## Response:
I'll help format your Brewfile contents into the nix-darwin Homebrew configuration format.

Darwin Configuration with Homebrew

Click to open code

I've created the darwin configuration with your Homebrew casks and Mac App Store apps. A few notes:

1.  Notice the `homebrew.onActivation` settings:
    *   `autoUpdate = true`: Updates Homebrew on activation
    *   `cleanup = "zap"`: Removes any formulae not explicitly listed
2.  I've structured the configuration into sections:
    *   Basic system packages (via nix)
    *   Homebrew configuration (taps, casks, masApps)
    *   System settings
    *   Security settings
    *   Nix service settings

Would you like me to:

1.  Add your Homebrew formulae (brew packages) to this configuration?
2.  Make any adjustments to the system settings or other configurations?
3.  Help format the VSCode extensions into a compatible format?

Also, when you apply this configuration, you'll need to have `mas` (Mac App Store CLI) installed first. It's already in your Brewfile, so that should be taken care of.

## Prompt:
this is my current state - review and fix all issues + improve it to be functional, optimal clean setup

Copy

## Response:
I'll help review and improve your Nix configuration setup. Let me analyze the issues and provide structured improvements.

Key Issues & Improvements:

1.  **Duplicate Flake Configurations**: You have two different `flake.nix` files with different approaches. We should consolidate them.
2.  **Structure Issues**:
    *   Multiple `home.nix` files with overlapping configurations
    *   Inconsistent imports and module organization
    *   Duplicate program configurations
3.  **Version Consistency**: Mixed use of different state versions (23.11, 24.05, 24.11)

Let's create a clean, optimized structure. I'll break this down into key files:

Improved flake.nix

Click to open code

Organized Home Manager Configuration

Click to open code

Recommended Directory Structure:

```
.
├── flake.nix
├── home
│   ├── default.nix
│   ├── programs
│   │   ├── default.nix     # Programs configuration
│   │   ├── git.nix         # Git configuration
│   │   └── vscode.nix      # VSCode configuration
│   └── shell
│       ├── default.nix     # Shell configuration
│       ├── starship.nix    # Starship prompt
│       └── zsh.nix         # Zsh configuration
├── modules
│   ├── darwin
│   │   ├── default.nix     # Darwin system settings
│   │   ├── homebrew.nix    # Homebrew packages
│   │   └── system.nix      # System preferences
│   └── home-manager
│       └── default.nix     # Home manager settings
└── README.md
```

Key Improvements Made:

1.  **Simplified Flake Structure**:
    *   Single flake configuration
    *   Cleaner host management
    *   Better module organization
2.  **Better Module Organization**:
    *   Separated system and user configurations
    *   Modular program configurations
    *   Clear separation of concerns
3.  **Version Consistency**:
    *   Standardized on state version 23.11
    *   Using nixpkgs-unstable for better package availability

Would you like me to:

1.  Show the remaining module configurations?
2.  Help with any specific part of the setup?
3.  Provide the homebrew configuration with your packages?

Note: You'll want to run `darwin-rebuild switch --flake .#hank-mbp-m3` (or appropriate hostname) after setting this up.

## Prompt:
show all files in fixed final revised form one-by-one

## Response:
I'll show you all the files in their final form, starting with the core ones.

1.  First, let's start with `flake.nix` at the root:

Final flake.nix

Click to open code

2.  Next, let's look at the main Darwin system configuration:

modules/darwin/default.nix

Click to open code

Would you like me to continue with the rest of the files? Next up would be:

1.  `modules/darwin/system.nix` (macOS system preferences)
2.  `modules/darwin/homebrew.nix` (your Homebrew packages)
3.  The home-manager configurations
4.  Shell configurations

Let me know which one you'd like to see next.

## Prompt:
I get this error

Proceed? (\[Y\]es/\[n\]o/\[e\]xplain): y INFO Step: Create an encrypted APFS volume `Nix Store` for Nix on `disk3` and add it to `/etc/fstab` mounting on `/nix` INFO Step: Provision Nix INFO Step: Create build users (UID 351-382) and group (GID 350) INFO Step: Configure Time Machine exclusions INFO Step: Configure Nix INFO Step: Configuring zsh to support using Nix in non-interactive shells INFO Step: Create a `launchctl` plist to put Nix into your PATH INFO Step: Configure upstream Nix daemon service ERROR 0: Install failure 1: Error executing action 2: Action `create_upstream_init_service` errored 3: Action `configure_init_service` errored 4: Failed to execute command `"launchctl" "bootstrap" "system" "/Library/LaunchDaemons/org.nixos.nix-daemon.plist"` stdout: stderr: Bootstrap failed: 5: Input/output error

4: exited with status code: 5 4:

Location: src/cli/subcommand/install.rs:259

Backtrace omitted. Run with RUST\_BACKTRACE=1 environment variable to display it. Run with RUST\_BACKTRACE=full to include source snippets.

## Response:
This error typically occurs during the initial Nix installation on macOS. Let's fix this step by step:

1.  First, remove the current Nix installation:

```bash
# Remove existing Nix directories and files
sudo rm -rf /nix
sudo rm -rf /etc/nix
sudo rm -rf ~/.nix-*
sudo rm /Library/LaunchDaemons/org.nixos.nix-daemon.plist
```

2.  Then, reboot your system to ensure all services are properly stopped.
3.  After reboot, reinstall Nix using:

```bash
curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
```

4.  After successful installation, configure nix with flakes:

```bash
mkdir -p ~/.config/nix
echo "experimental-features = nix-command flakes" >> ~/.config/nix/nix.conf
```

5.  Then install nix-darwin:

```bash
nix-build https://github.com/LnL7/nix-darwin/archive/master.tar.gz -A installer
./result/bin/darwin-installer
```

6.  Now, try building your configuration:

```bash
cd ~/dotfiles  # or wherever your flake.nix is located
nix build .#darwinConfigurations.hank-mbp-m3.system
./result/sw/bin/darwin-rebuild switch --flake .#hank-mbp-m3
```

If you encounter any other errors, please share them. Also, could you confirm:

1.  Are you running on an Apple Silicon Mac?
2.  Do you have System Integrity Protection (SIP) enabled? You can check with `csrutil status`

The error suggests there might be an issue with the launchd service setup. Let me know if you still get errors after following these steps.

## Prompt:
hank@hank-mstio ~ % sudo rm -rf /nixrm: /nix: Read-only file system

## Response:
Ah, this error occurs because the Nix volume is mounted as read-only. Let's properly unmount it first:

1.  First, list the mounted volumes to find the Nix store:

```bash
diskutil list
```

Look for something like "Nix Store" in the output.

2.  Then unmount the Nix volume:

```bash
sudo diskutil unmount force /nix
```

3.  You might also need to remove it from `/etc/fstab`:

```bash
sudo vifs  # or sudo nano /etc/fstab
# Remove the line containing /nix
```

4.  Now try the cleanup again:

```bash
sudo rm -rf /nix
sudo rm -rf /etc/nix
sudo rm -rf ~/.nix-*
sudo rm /Library/LaunchDaemons/org.nixos.nix-daemon.plist
```

5.  Verify the APFS volume is gone:

```bash
diskutil list
```

If you still see the Nix Store volume, you might need to delete it:

```bash
# Replace diskXsY with the actual disk identifier from diskutil list
sudo diskutil apfs deleteVolume diskXsY
```

After this is done, reboot your system and then we can proceed with a fresh Nix installation using the Determinate Systems installer:

```bash
curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
```

Let me know if you need help with any of these steps or if you encounter any other errors.

## Prompt:
hank@hank-mstio ~ % sudo diskutil list /dev/disk0 (internal, physical): #: TYPE NAME SIZE IDENTIFIER 0: GUID\_partition\_scheme \*8.0 TB disk0 1: Apple\_APFS\_ISC Container disk1 524.3 MB disk0s1 2: Apple\_APFS Container disk3 8.0 TB disk0s2 3: Apple\_APFS\_Recovery Container disk2 5.4 GB disk0s3

/dev/disk3 (synthesized): #: TYPE NAME SIZE IDENTIFIER 0: APFS Container Scheme - +8.0 TB disk3 Physical Store disk0s2 1: APFS Volume Macintosh HD - Data 1.4 TB disk3s1 2: APFS Volume Macintosh HD 11.2 GB disk3s3 3: APFS Snapshot com.apple.os.update-... 11.2 GB disk3s3s1 4: APFS Volume Preboot 6.8 GB disk3s4 5: APFS Volume Recovery 1.0 GB disk3s5 6: APFS Volume VM 20.5 KB disk3s6

## Response:
I notice that the Nix Store volume isn't listed here, which suggests the previous Nix installation might have been incomplete or unsuccessful.

Let's do a thorough cleanup and fresh installation:

1.  Clean up any remaining Nix-related files:

```bash
sudo rm -rf /etc/nix
sudo rm -rf ~/.nix-*
sudo rm -f /Library/LaunchDaemons/org.nixos.nix-daemon.plist
```

2.  Install Nix fresh using the Determinate Systems installer:

```bash
curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
```

If you get the same launchd error as before, we can try the official installer instead:

```bash
sh <(curl -L https://nixos.org/nix/install) --daemon
```

After the installation is complete:

1.  Close and reopen your terminal
2.  Verify Nix is installed:

```bash
nix --version
```

3.  Configure flakes:

```bash
mkdir -p ~/.config/nix
echo "experimental-features = nix-command flakes" >> ~/.config/nix/nix.conf
```

Let me know if you get any errors during this process. Also, since you're on a Mac Studio ("hank-mstio"), are you running the latest version of macOS? The installer behavior can sometimes vary between macOS versions.

## Prompt:
I have my dotfile github repo cloned to ~/dotfiles (not ~/.config) btw.

Attached is dotfiles repo in (~/dotfiles) full code where i was cd-ed into and ran “nix run nix-darwin switch —flake ~/dotfiles”

I may have incorrect assumptions on file conventions + concepts. I did not ensure any system level file configs before running it. E.g. /nix/nix.conf or single/multi user considerations or nix-daemon. I mixed installing / using / uninstalling nix pkgs with official + determinate version nix installations. And i used nix profile command directly couple times

## Response:
Let's fix this step by step. Looking at your repo and error, here's what we'll do:

1.  First, let's do a clean Nix installation:

```bash
# Remove all Nix-related files first
sudo rm -rf /nix

# If you have these files, remove them too
sudo rm -rf /etc/nix
sudo rm -rf ~/.nix-*
```

2.  Install Nix using Determinate Systems installer:

```bash
curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install
```

3.  Create necessary configuration files:

```bash
# Create nix config directory
mkdir -p ~/.config/nix

# Add experimental features for flakes
echo "experimental-features = nix-command flakes" >> ~/.config/nix/nix.conf
```

4.  Your dotfiles structure looks good, but let's set up with nix-darwin:

```bash
# Switch to your dotfiles directory
cd ~/dotfiles

# Build the initial system
nix build .#darwinConfigurations.hank-mstio.system

# Switch to the configuration
./result/sw/bin/darwin-rebuild switch --flake .#hank-mstio
```

5.  If you get any permission errors, you might need to set ownership:

```bash
sudo chown -R root:wheel /nix
```

6.  If you need to clean up any old Nix profiles or generations:

```bash
nix-collect-garbage -d
```

The key issues I noticed in your setup:

1.  The hostname in your flake.nix has `hank-mbp-m3` but your actual hostname is `hank-mstio`
2.  Multiple configuration attempts left remnants that need to be cleaned up
3.  Missing proper system-level Nix setup

Would you like me to help review any specific part of your configuration files? I noticed you have some elaborate home-manager and application configs that we can optimize after getting the basic Nix setup working.

## Prompt:
Yes let’s fix all of that. But this is the issue i get running installer. And explain all the core nix concepts so i undersrand what i did wrong

hank@hank-mstio ~ % curl --proto '=https' --tlsv1.2 -sSf -L [https://install.determinate.systems/nix](https://install.determinate.systems/nix) | sh -s -- install info: downloading installer [https://install.determinate.systems/nix/tag/v0.32.3/nix-installer-aarch64-darwin](https://install.determinate.systems/nix/tag/v0.32.3/nix-installer-aarch64-darwin) INFO nix-installer v0.32.3 `nix-installer` needs to run as `root`, attempting to escalate now via `sudo`... INFO nix-installer v0.32.3 Error: 0: Planner error 1: Error executing action 2: Action `create_nix_volume` errored 3: Action `create_volume_service` errored 4: An APFS volume labelled `Nix Store` does not exist, but there exists an fstab entry for that volume, as well as a service file at `/Library/LaunchDaemons/org.nixos.darwin-store.plist`. Consider removing the line containing `/nix` from the `/etc/fstab` and running `sudo rm /Library/LaunchDaemons/org.nixos.darwin-store.plist`

Location: src/cli/subcommand/install.rs:203

Backtrace omitted. Run with RUST\_BACKTRACE=1 environment variable to display it. Run with RUST\_BACKTRACE=full to include source snippets.

Consider reporting this error using this URL: [https://github.com/DeterminateSystems/nix-installer/issues/new?title=%3Cautogenerated-issue%3E&body=%23%23+Error%0A%60%60%60%0AError%3A+%0A+++0%3A+Planner+error%0A+++1%3A+Error+executing+action%0A+++2%3A+Action+%60create\_nix\_volume%60+errored%0A+++3%3A+Action+%60create\_volume\_service%60+errored%0A+++4%3A+An+APFS+volume+labelled+%60Nix+Store%60+does+not+exist%2C+but+there+exists+an+fstab+entry+for+that+volume%2C+as+well+as+a+service+file+at+%60%2FLibrary%2FLaunchDaemons%2Forg.nixos.darwin-store.plist%60.+Consider+removing+the+line+containing+%60%2Fnix%60+from+the+%60%2Fetc%2Ffstab%60+and+running+%60sudo+rm+%2FLibrary%2FLaunchDaemons%2Forg.nixos.darwin-store.plist%60%0A%60%60%60%0A%0A%23%23+Metadata%0A%7Ckey%7Cvalue%7C%0A%7C--%7C--%7C%0A%7C\*\*version\*\*%7C0.32.3%7C%0A%7C\*\*os\*\*%7Cmacos%7C%0A%7C\*\*arch\*\*%7Caarch64%7C%0A](https://github.com/DeterminateSystems/nix-installer/issues/new?title=%3Cautogenerated-issue%3E&body=%23%23+Error%0A%60%60%60%0AError%3A+%0A+++0%3A+Planner+error%0A+++1%3A+Error+executing+action%0A+++2%3A+Action+%60create_nix_volume%60+errored%0A+++3%3A+Action+%60create_volume_service%60+errored%0A+++4%3A+An+APFS+volume+labelled+%60Nix+Store%60+does+not+exist%2C+but+there+exists+an+fstab+entry+for+that+volume%2C+as+well+as+a+service+file+at+%60%2FLibrary%2FLaunchDaemons%2Forg.nixos.darwin-store.plist%60.+Consider+removing+the+line+containing+%60%2Fnix%60+from+the+%60%2Fetc%2Ffstab%60+and+running+%60sudo+rm+%2FLibrary%2FLaunchDaemons%2Forg.nixos.darwin-store.plist%60%0A%60%60%60%0A%0A%23%23+Metadata%0A%7Ckey%7Cvalue%7C%0A%7C--%7C--%7C%0A%7C**version**%7C0.32.3%7C%0A%7C**os**%7Cmacos%7C%0A%7C**arch**%7Caarch64%7C%0A) hank@hank-mstio ~ %

## Response:
Let's fix the installation error and then explain the core Nix concepts.

1.  First, fix the error by removing the mentioned files:

```bash
# Remove the Nix mount entry from fstab
sudo vifs  # or sudo nano /etc/fstab
# Delete the line containing /nix

# Remove the launch daemon
sudo rm /Library/LaunchDaemons/org.nixos.darwin-store.plist
```

Now let's understand the core Nix concepts:

1.  **Nix Storage Model**:
    *   Nix stores all packages in `/nix/store`
    *   On macOS, this requires a special APFS volume to handle the Unix-style permissions
    *   The error you got was because there were remnants of a previous Nix installation (fstab entry and launch daemon) but no actual volume
2.  **Installation Types**:

    ```
    Single User:  Lives in your home directory
    Multi User:   System-wide installation with daemon
    ```

    *   Determinate Systems installer sets up multi-user by default
    *   You previously mixed different installation types, causing conflicts
3.  **Core Components**:

    ```
    nix-daemon:   Background service that handles builds/installations
    nix.conf:     System-wide config (/etc/nix/nix.conf)
    ~/.config/nix/nix.conf:  User-specific config
    ```

4.  **Directory Structure**:

    ```
    /nix/store:          Where all packages live
    ~/.config/nix/:      User configuration
    /etc/nix/:           System-wide configuration
    ```

5.  **Package Management Approaches**:

    ```
    Imperative:    nix-env -i package     (old way)
    Declarative:   configuration.nix      (better)
    Flakes:        flake.nix             (modern way)
    ```

    *   You were mixing these approaches which can cause conflicts

Would you like me to proceed with a clean installation after removing those files? Then we can properly set up your dotfiles using the correct approach.

Also, which specific package management approach would you prefer to use? I recommend the flakes approach since your dotfiles are already structured that way, but it's good to understand the differences.

## Prompt:
New error

Last login: Tue Jan 14 03:17:17 2025 from fe80::144f:b335:ea70:5a95%en1 hank@hank-mstio ~ % curl --proto '=https' --tlsv1.2 -sSf -L [https://install.determinate.systems/nix](https://install.determinate.systems/nix) | sh -s -- install info: downloading installer [https://install.determinate.systems/nix/tag/v0.32.3/nix-installer-aarch64-darwin](https://install.determinate.systems/nix/tag/v0.32.3/nix-installer-aarch64-darwin) INFO nix-installer v0.32.3 `nix-installer` needs to run as `root`, attempting to escalate now via `sudo`... Enter PIN for 'Certificate For PIV Authentication (Yubico PIV Authentication)': INFO nix-installer v0.32.3 Nix install plan (v0.32.3) Planner: macos (with default settings)

Planned actions: \* Create an encrypted APFS volume `Nix Store` for Nix on `disk3` and add it to `/etc/fstab` mounting on `/nix` \* Extract the bundled Nix (originally from /nix/store/fg8haq4wkgm320cffnib91f6lv2i28g6-nix-binary-tarball-2.25.3/nix-2.25.3-aarch64-darwin.tar.xz) \* Create a directory tree in `/nix` \* Synchronize /nix/var ownership \* Move the downloaded Nix into `/nix` \* Synchronize /nix/store ownership \* Create build users (UID 351-382) and group (GID 350) \* Configure Time Machine exclusions \* Setup the default Nix profile \* Place the Nix configuration in `/etc/nix/nix.conf` \* Configure the shell profiles \* Configuring zsh to support using Nix in non-interactive shells \* Create a `launchctl` plist to put Nix into your PATH \* Configure upstream Nix daemon service \* Remove directory `/nix/temp-install-dir`

Proceed? (\[Y\]es/\[n\]o/\[e\]xplain): y INFO Step: Create an encrypted APFS volume `Nix Store` for Nix on `disk3` and add it to `/etc/fstab` mounting on `/nix` INFO Step: Provision Nix INFO Step: Create build users (UID 351-382) and group (GID 350) INFO Step: Configure Time Machine exclusions INFO Step: Configure Nix INFO Step: Configuring zsh to support using Nix in non-interactive shells INFO Step: Create a `launchctl` plist to put Nix into your PATH INFO Step: Configure upstream Nix daemon service ERROR 0: Install failure 1: Error executing action 2: Action `create_upstream_init_service` errored 3: Action `configure_init_service` errored 4: Failed to execute command `"launchctl" "bootstrap" "system" "/Library/LaunchDaemons/org.nixos.nix-daemon.plist"` stdout: stderr: Bootstrap failed: 5: Input/output error

4: exited with status code: 5 4:

Location: src/cli/subcommand/install.rs:259

Backtrace omitted. Run with RUST\_BACKTRACE=1 environment variable to display it. Run with RUST\_BACKTRACE=full to include source snippets.

Consider reporting this error using this URL: [https://github.com/DeterminateSystems/nix-installer/issues/new?title=%3Cautogenerated-issue%3E&body=%23%23+Error%0A%60%60%60%0AError%3A+%0A+++0%3A+Install+failure%0A+++1%3A+Error+executing+action%0A+++2%3A+Action+%60create\_upstream\_init\_service%60+errored%0A+++3%3A+Action+%60configure\_init\_service%60+errored%0A+++4%3A+Failed+to+execute+command+%60%22launchctl%22+%22bootstrap%22+%22system%22+%22%2FLibrary%2FLaunchDaemons%2Forg.nixos.nix-daemon.plist%22%60%0A++++++stdout%3A+%0A++++++stderr%3A+Bootstrap+failed%3A+5%3A+Input%2Foutput+error%0A%0A+++4%3A+exited+with+status+code%3A+5%0A%0A%60%60%60%0A%0A%23%23+Metadata%0A%7Ckey%7Cvalue%7C%0A%7C--%7C--%7C%0A%7C\*\*version\*\*%7C0.32.3%7C%0A%7C\*\*os\*\*%7Cmacos%7C%0A%7C\*\*arch\*\*%7Caarch64%7C%0A](https://github.com/DeterminateSystems/nix-installer/issues/new?title=%3Cautogenerated-issue%3E&body=%23%23+Error%0A%60%60%60%0AError%3A+%0A+++0%3A+Install+failure%0A+++1%3A+Error+executing+action%0A+++2%3A+Action+%60create_upstream_init_service%60+errored%0A+++3%3A+Action+%60configure_init_service%60+errored%0A+++4%3A+Failed+to+execute+command+%60%22launchctl%22+%22bootstrap%22+%22system%22+%22%2FLibrary%2FLaunchDaemons%2Forg.nixos.nix-daemon.plist%22%60%0A++++++stdout%3A+%0A++++++stderr%3A+Bootstrap+failed%3A+5%3A+Input%2Foutput+error%0A%0A+++4%3A+exited+with+status+code%3A+5%0A%0A%60%60%60%0A%0A%23%23+Metadata%0A%7Ckey%7Cvalue%7C%0A%7C--%7C--%7C%0A%7C**version**%7C0.32.3%7C%0A%7C**os**%7Cmacos%7C%0A%7C**arch**%7Caarch64%7C%0A) Installation failure, offering to revert... Nix uninstall plan (v0.32.3)

Planner: macos (with default settings)

Planned actions: \* Remove upstream Nix daemon service \* Delete file `/Library/LaunchDaemons/systems.determinate.nix-installer.nix-hook.plist` \* Remove the Nix configuration from zsh's non-login shells \* Unconfigure the shell profiles \* Remove the Nix configuration in `/etc/nix/nix.conf` \* Unset the default Nix profile \* Remove time machine exclusions \* Remove Nix users and group \* Remove the directory tree in `/nix` \* Remove the APFS volume `Nix Store` on `disk3`

Proceed? (\[Y\]es/\[n\]o/\[e\]xplain): y INFO Revert: Remove directory `/nix/temp-install-dir` INFO Revert: Configure upstream Nix daemon service INFO Revert: Create a `launchctl` plist to put Nix into your PATH INFO Revert: Configuring zsh to support using Nix in non-interactive shells INFO Revert: Configure Nix INFO Revert: Configure Time Machine exclusions INFO Revert: Create build users (UID 351-382) and group (GID 350) INFO Revert: Provision Nix INFO Revert: Create an encrypted APFS volume `Nix Store` for Nix on `disk3` and add it to `/etc/fstab` mounting on `/nix` Partial Nix install was uninstalled successfully!

hank@hank-mstio ~ %

## Response:
This error suggests an issue with the launchd service. Let's try a more thorough cleanup and installation:

1.  First, clean up any remaining launchd services:

```bash
# Stop and remove any existing nix-daemon services
sudo launchctl unload /Library/LaunchDaemons/org.nixos.nix-daemon.plist 2>/dev/null || true
sudo launchctl unload /Library/LaunchDaemons/org.nixos.darwin-store.plist 2>/dev/null || true

# Remove the service files
sudo rm -f /Library/LaunchDaemons/org.nixos.nix-daemon.plist
sudo rm -f /Library/LaunchDaemons/org.nixos.darwin-store.plist
sudo rm -f /Library/LaunchDaemons/systems.determinate.nix-installer.nix-hook.plist
```

2.  Check and remove any remaining volume:

```bash
# List APFS volumes
diskutil list

# If you see a Nix Store volume, unmount and delete it
diskutil apfs deleteVolume "Nix Store" # Only if it exists
```

3.  Clean up system files:

```bash
# Remove Nix-related directories
sudo rm -rf /nix
sudo rm -rf /etc/nix
sudo rm -rf ~/.nix-*

# Check and clean fstab
sudo vifs  # or sudo nano /etc/fstab
# Remove any lines containing 'nix' or 'Nix Store'
```

4.  Try the official installer instead of Determinate Systems:

```bash
sh <(curl -L https://nixos.org/nix/install) --daemon
```

5.  After installation, set up flakes:

```bash
mkdir -p ~/.config/nix
echo "experimental-features = nix-command flakes" >> ~/.config/nix/nix.conf
```

The key concepts to understand here:

1.  **Launch Daemons on macOS**:

```
/Library/LaunchDaemons/      # System-wide daemons
org.nixos.nix-daemon.plist   # The Nix builder service
org.nixos.darwin-store.plist # Handles the Nix store volume
```

2.  **Storage on macOS**:

```
APFS Volume "Nix Store"   # Dedicated volume for /nix
/nix/store               # Where packages are stored
/nix/var                 # Runtime data
```

3.  **Multi-user Installation**:

```
- Creates system users/groups for builders
- Sets up system-wide daemon
- Requires root access
- More secure than single-user
```

4.  **Service Management**:

```
launchctl     # macOS service manager
diskutil      # Disk/volume management
fstab         # Filesystem mounts
```

Let me know if you'd like me to explain any part in more detail. Once we get the basic Nix installation working, we can move on to setting up your dotfiles configuration.

Would you like to try these steps? Also, do you have any particular security requirements or preferences (like your YubiKey setup) that we should consider?

## Prompt:
Make clean uninstall a script to automate

## Response:
I'll create a comprehensive cleanup script for Nix.

nix-cleanup.sh

Click to open code

To use this cleanup script:

1.  Save it to your system:

```bash
# Create scripts directory
mkdir -p ~/dotfiles/scripts

# Save the script
vim ~/dotfiles/scripts/nix-cleanup.sh

# Make it executable
chmod +x ~/dotfiles/scripts/nix-cleanup.sh
```

2.  Run it with sudo:

```bash
sudo ~/dotfiles/scripts/nix-cleanup.sh
```

The script:

*   Stops and removes all Nix-related services
*   Cleans shell configurations
*   Removes the Nix store volume
*   Removes all Nix files and directories
*   Cleans up per-user Nix files
*   Removes Nix system users and groups
*   Offers to reboot after cleanup

Features:

*   Color-coded output for better visibility
*   Step-by-step progress indication
*   Safe removal of components
*   Verification of cleanup
*   Optional reboot prompt

Would you like me to explain any particular part of the script or add any additional cleanup steps?

## Prompt:
update script to be for mac os only (apple silicon)

## Response:
I'll update the script to be macOS/Apple Silicon specific and add some additional macOS-specific checks and cleanups.

macos-nix-cleanup.sh

Click to open code

Key changes made for macOS/Apple Silicon:

1.  Added platform checks:
    *   Verifies running on macOS (Darwin)
    *   Verifies running on Apple Silicon (arm64)
2.  macOS-specific service management:
    *   Uses `launchctl bootout` instead of `unload`
    *   Handles macOS-specific service paths
3.  APFS volume management:
    *   Uses `diskutil` for volume operations
    *   Handles APFS-specific volume commands
4.  macOS-specific paths:
    *   Added `/etc/synthetic.conf` cleanup
    *   Added macOS-specific user library paths
    *   Handles macOS directory structure
5.  User management:
    *   Uses `dscl` instead of Linux user management commands
    *   Handles macOS user/group structure

To use:

```bash
# Save to your dotfiles
mkdir -p ~/dotfiles/scripts
curl -o ~/dotfiles/scripts/macos-nix-cleanup.sh https://raw.githubusercontent.com/yourusername/dotfiles/main/scripts/macos-nix-cleanup.sh
chmod +x ~/dotfiles/scripts/macos-nix-cleanup.sh

# Run it
sudo ~/dotfiles/scripts/macos-nix-cleanup.sh
```

Would you like me to add any additional macOS-specific cleanup steps or explain any part of the script?

## Prompt:
write another script for installing nix with options for determinate nix (default) + official latest nix version

## Response:
I'll create a Nix installation script that handles both installation methods with error handling and verification.

macos-nix-install.sh

Click to open code

To use this installer:

1.  Save the script:

```bash
# Create scripts directory if it doesn't exist
mkdir -p ~/dotfiles/scripts

# Save the script
curl -o ~/dotfiles/scripts/macos-nix-install.sh https://raw.githubusercontent.com/yourusername/dotfiles/main/scripts/macos-nix-install.sh
chmod +x ~/dotfiles/scripts/macos-nix-install.sh
```

2.  Run it:

```bash
~/dotfiles/scripts/macos-nix-install.sh
```

Features:

*   Interactive menu interface
*   Support for both Determinate and official Nix installers
*   Installation verification
*   Automatic Nix configuration for flakes
*   Error handling and status checks
*   macOS/Apple Silicon specific checks

The script will:

1.  Verify system requirements
2.  Offer installation options
3.  Install Nix using chosen method
4.  Configure Nix with experimental features
5.  Verify the installation

Would you like me to add any additional features or explain any part of the script?
## Prompt:
provide detailed instructions for step by step project execution

## Response:
I'll provide detailed step-by-step instructions for implementing this project based on the plan and existing structure.

Step-by-Step Project Implementation Guide

Click to open document

Based on your existing project structure and plan, I've created a comprehensive implementation guide. The guide follows your phased approach:

1.  Initial Setup & Compilation
2.  Temporal Integration Setup
3.  Vendor API Integration
4.  Data Sync Implementation

Each phase includes:

*   Detailed steps with code examples
*   Configuration requirements
*   Test instructions
*   Troubleshooting guidance

Would you like me to elaborate on any specific phase or provide more detailed instructions for a particular component?

## Prompt:
minimal setup possible

## Response:
I'll help you create the minimal setup needed to get the project running with just the essential components.

Minimal Setup Guide

Click to open document

This minimal setup provides:

*   Basic Temporal workflow & worker
*   Simple REST API
*   Local Temporal server
*   Essential TypeScript configuration

It strips away all non-essential components while maintaining a functional foundation that you can build upon. Would you like me to explain any part in more detail or help with troubleshooting?

## Prompt:
update to use bun TS and create workspace with bun and then minimal server with libraries for (temporal client, ESLint, generic transform, DB sqlite, )

## Response:
I'll help you create a minimal Bun workspace setup with the essential libraries.

Bun Workspace Setup Guide with Essential Libraries

Click to open document

This setup provides:

*   Bun workspace with TypeScript
*   Minimal Temporal integration
*   Generic transform package
*   SQLite database integration
*   ESLint configuration
*   Fast Hono web server
*   Development hot reloading

Key features:

1.  Uses Bun's built-in TypeScript support
2.  Workspace-based package management
3.  Modern ESM modules
4.  Minimal but extensible architecture

Would you like me to:

1.  Explain any part in more detail?
2.  Add more functionality to any component?
3.  Show how to add additional packages or features?

## Prompt:
what about typescript suppot?

## Response:
I'll enhance the TypeScript support and configuration for better type safety across the workspace.

Enhanced TypeScript Configuration and Types

Click to open document

Key improvements in this TypeScript setup:

1.  **Project References**:
    *   Enables proper type checking across packages
    *   Supports incremental compilation
    *   Maintains type safety between dependencies
2.  **Shared Types Package**:
    *   Central location for all type definitions
    *   Ensures consistency across packages
    *   Prevents type duplication
3.  **Type-Safe APIs**:
    *   Request/response typing
    *   Runtime type validation
    *   Proper error handling
4.  **Enhanced Development Experience**:
    *   Better IDE support
    *   Catch type errors early
    *   Improved code documentation

Would you like me to:

1.  Add more specific types for your use case?
2.  Show how to add additional type-safe features?
3.  Explain how to handle specific type scenarios?

## Prompt:
for root tsconfig can't I extend bun/tsconfig?

## Response:
Yes, absolutely! That's a better approach for Bun projects. Let me show you the simplified TypeScript configuration using Bun's base config.

Simplified Bun TypeScript Configuration

Click to open document

This configuration is much simpler and leverages Bun's built-in TypeScript support. Key differences:

1.  Extends `bun-types/tsconfig.json` instead of setting everything manually
2.  Removes redundant compiler options that Bun handles
3.  Better integrated with Bun's runtime

Would you like me to:

1.  Show how to set up specific IDE configurations?
2.  Explain how to handle any specific TypeScript features with Bun?
3.  Add more specific configurations for your use case?

## Prompt:
use npm workspaces

## Response:
I'll help you set up the project using npm workspaces for package management.

NPM Workspace Setup for Temporal Project

Click to open document

Key benefits of this npm workspace setup:

1.  **Dependency Management**:
    *   Hoisted dependencies
    *   Shared devDependencies
    *   Workspace-local dependencies
2.  **Script Management**:
    *   Run scripts across all packages
    *   Package-specific scripts
    *   Consistent build process
3.  **Development Flow**:
    *   Local package linking
    *   Parallel script execution
    *   Simplified dependency updates

Would you like me to:

1.  Add specific configurations for your use case?
2.  Show how to set up additional packages?
3.  Explain any specific npm workspace features?

## Prompt:
no no use bun where possible (or npm,pnpm) but use workspaces

## Response:
Ah, I'll revise the setup to use Bun's workspace capabilities while keeping npm/pnpm workspace structure as a fallback option.

Bun Workspace Setup with npm/pnpm Compatibility

Click to open document

Key differences in this Bun-focused setup:

1.  Uses `bun build` instead of `tsc`
2.  Leverages Bun's watch mode
3.  Faster installation with `bun install`
4.  Simpler bundle configuration

Would you like me to:

1.  Add more specific Bun optimizations?
2.  Show how to configure additional Bun features?
3.  Explain the differences between Bun and npm/pnpm workspace handling?

## Prompt:
update to have temporal client be 2 levels of folders sijnce we can have server+client+etc

## Response:
I'll restructure the workspace to better organize the Temporal components with a deeper folder structure.

Enhanced Bun Workspace Structure with Temporal Components

Click to open document

This structure provides several advantages:

1.  Better separation of Temporal components
2.  Clearer dependencies between packages
3.  More modular development approach
4.  Easier to scale and maintain

Would you like me to:

1.  Add more specific implementations?
2.  Show how to add new workflows?
3.  Explain the interaction between components?

## Prompt:
what about openapi specs or other schemas? e.g. data model

## Response:
I'll expand the structure to include OpenAPI specs, schemas, and data models in a dedicated location.

Enhanced Schema and Data Model Structure

Click to open document

## Prompt:
why put into packages folder?

## Response:
Good question! You're right - schemas, OpenAPI specs, and data models could be organized differently. Let me propose a cleaner structure.

Reorganized Project Structure with Root-Level Schemas

Click to open document

## Prompt:
what if single app only?

## Response:
Ah, for a single app we can simplify this structure significantly while keeping the schema-first benefits.

Simplified Single App Structure

Click to open document

## Prompt:
I want to use openapi spec as primary source of truth along with zod ts files. update to reflect that

## Response:
I'll restructure to make OpenAPI spec the source of truth and sync it with Zod schemas.

OpenAPI-First Structure with Zod Integration

Click to open document

## Prompt:
what if openapi will have internal + multiple externalthat correclate with each other and need to reference?

## Response:
Ah, great question! Let's organize OpenAPI specs to handle internal APIs and multiple correlated external APIs with shared components.

Multi-OpenAPI Structure with Cross-References

Click to open document

## Prompt:
appsapps

## Response:
Let me revise the structure to show how to organize multiple apps that share OpenAPI specs and types.

Multi-App Structure with Shared OpenAPI Specs

Click to open document

## Prompt:
too much abstraction, let's keep it simple

## Response:
I'll simplify it significantly to keep things clear and maintainable.

Simplified Multi-App Structure

Click to open document

## Prompt:
summarize problem statement so far concisely and the best proposed solution so far with rationale why

## Response:
Let me summarize the problem and proposed solution.

Problem Statement and Solution

Click to open document

The key insight here is that while the problem seems complex (multiple vendors, workflows, types), the solution doesn't need to be. By using OpenAPI as a single source of truth and keeping the structure minimal but well-organized, we get all the benefits of type safety and maintainability without unnecessary complexity.

Would you like me to expand on any part of this summary?
# Provider-Specific Resources

This directory contains configurations, prompts, and guidelines optimized for specific AI models and providers. Each subdirectory is organized by AI provider and contains model-specific templates, configurations, and best practices.

## Contents

### Provider Directories

- **[Claude/](./Claude/)** - Resources specific to Anthropic's Claude models
  - Claude Desktop configuration files
  - Claude-specific prompt templates and system messages
  - Best practices for Claude's capabilities

### Model Configuration Files

- **[llm-models-configuration.yaml](./llm-models-configuration.yaml)** - Comprehensive configuration of LLM models with capabilities, context windows, and recommended use cases
- **[tool-specific-configurations.md](./tool-specific-configurations.md)** - Detailed configuration templates for different AI models and tools

## How to Use These Resources

1. **Choose the Right Model**: Use the model configuration files to understand the strengths and capabilities of different models

2. **Leverage Provider-Specific Features**: Each provider has unique capabilities - find templates that make the most of these features:
   - Claude's extended thinking mode and XML tagging
   - GPT's function calling and tool usage
   - DeepSeek's specialized code capabilities

3. **Optimize Configurations**: Apply the optimized configurations in your prompts to get better results:
   ```
   # Example: Using Claude 3.7 Sonnet with Extended Thinking
   I'd like you to engage extended thinking mode and act as a systems architect.

   When analyzing this architecture problem:
   1. Think step-by-step through the design considerations
   2. Consider multiple approaches before selecting the optimal solution
   3. Explicitly note assumptions and their implications
   4. Evaluate tradeoffs quantitatively where possible
   ```

4. **Mix and Match**: Combine provider-specific components with general templates from other directories

## Best Practices

1. **Know Your Model**: Different models have different strengths - choose the right one for your task

2. **Optimize Configuration**: Use the provider-specific configuration templates to get the best results

3. **Leverage Special Features**: Take advantage of unique capabilities:
   - Claude's XML tags and reasoning capabilities
   - GPT's tool usage and multimodal features
   - DeepSeek's programming expertise

4. **Keep Updated**: These configurations should be updated as models evolve and new capabilities are added
# AI Tool-Specific Configurations

This document provides optimized configurations for specific AI tools and models to maximize their unique capabilities and strengths.

## Claude (Anthropic)

### Claude 3.7 Sonnet with Extended Thinking

#### Optimal Use Cases
- Complex reasoning tasks
- Detailed technical analysis
- Nuanced explanations balancing depth and clarity
- Code review and improvement
- Technical writing with precise language

#### Configuration Optimizations
```markdown
# Claude 3.7 Sonnet Configuration

I'd like you to engage extended thinking mode and act as [specific role].

When analyzing this problem:
1. Think step-by-step through the reasoning process
2. Consider multiple approaches before selecting the optimal solution
3. Explicitly note assumptions and their implications
4. Evaluate tradeoffs quantitatively where possible

Please structure your response according to:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Enable extended thinking mode for complex reasoning
- Request explicit reasoning paths for verification
- Use for tasks requiring careful nuance and precision
- Excellent for generating well-structured, hierarchical content

### Claude 3 Opus

#### Optimal Use Cases
- System design and architecture
- Comprehensive technical evaluations
- Complex document generation
- In-depth research synthesis
- Detailed code explanations

#### Configuration Optimizations
```markdown
# Claude 3 Opus Configuration

I'd like you to act as [specific role] with extensive expertise in [domain].

For this complex task, please:
1. First map out the entire solution space
2. Identify key decision points and dependencies
3. Develop a comprehensive approach
4. Elaborate on implementation details

Please structure your response with:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Superior context handling for complex, multi-part problems
- Capability to generate comprehensive technical documents
- Handles nuanced, complex instructions effectively
- Excellent for multi-stage reasoning tasks

## OpenAI Models

### GPT-4o

#### Optimal Use Cases
- Interactive problem-solving
- Multi-modal inputs (text + images)
- Creative technical solutions
- Real-time web research integration
- Code generation with clear explanations

#### Configuration Optimizations
```markdown
# GPT-4o Configuration

You are an expert [specific role] assisting with [task type].

Consider this problem holistically:
1. Analyze the complete context, including any provided images or references
2. Formulate a structured approach
3. Generate concrete implementation details
4. Explain your reasoning transparently

Please follow this response structure:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Ability to process and reference images in reasoning
- Browsing capabilities for real-time research
- Strong code generation capabilities with context awareness
- Good at creative solutions to technical problems

### GPT-4o with Advanced Data Analysis

#### Optimal Use Cases
- Statistical analysis and data exploration
- Code execution and verification
- Data visualization suggestions
- Iterative problem-solving with code
- Mathematical proofs and calculations

#### Configuration Optimizations
```markdown
# GPT-4o with Advanced Data Analysis Configuration

You are an expert data scientist and programmer helping with [specific task].

For this analysis:
1. Explore the data to understand its structure and characteristics
2. Suggest appropriate analytical approaches
3. Implement solutions using executable code
4. Validate results and check for errors
5. Visualize key findings effectively

I need the response to include:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Code execution capabilities for instant verification
- Iterative data exploration and analysis
- Creation of visualizations from data
- Ability to process and analyze uploaded files
- Mathematical computation capabilities

## DeepSeek Models

### DeepSeek Coder

#### Optimal Use Cases
- Complex software engineering tasks
- Programming language-specific optimizations
- Implementation of algorithms and data structures
- Code optimization and refactoring
- Technical software documentation

#### Configuration Optimizations
```markdown
# DeepSeek Coder Configuration

You are an expert software engineer specializing in [language/framework].

For this coding task:
1. Analyze requirements with attention to edge cases
2. Design a solution prioritizing [efficiency/readability/maintainability]
3. Implement with idiomatic patterns and best practices
4. Include comprehensive documentation and usage examples
5. Highlight optimization opportunities

Structure your response as follows:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Exceptional code generation quality
- Strong understanding of software engineering principles
- Ability to work within existing codebases
- Effective at complex algorithm implementation
- Language-specific optimization knowledge

## Multi-Tool Workflows

### Research → Analysis → Implementation

This workflow combines multiple AI tools for comprehensive solutions.

#### Process Flow
1. **Research Phase** (GPT-4o with browsing)
   - Gather relevant information
   - Identify state-of-the-art approaches
   - Collect reference examples

2. **Analysis Phase** (Claude 3.7 Sonnet with Extended Thinking)
   - Evaluate approaches from research
   - Perform detailed tradeoff analysis
   - Develop conceptual design

3. **Implementation Phase** (DeepSeek Coder)
   - Convert design into concrete implementation
   - Optimize for target environment
   - Develop tests and documentation

#### Configuration Example
```markdown
# Multi-Tool Workflow: Research Phase

You are a research specialist gathering information on [topic].

Please find the most relevant and current information about:
- Core concepts and principles
- Leading approaches and methodologies
- Benchmark results and comparisons
- Implementation considerations

Format findings as a structured research brief with sections for each area and citations.

# Multi-Tool Workflow: Analysis Phase (For Claude)

Using the research provided, perform a comprehensive analysis:

1. Evaluate each approach against these criteria:
   - [List criteria with weights]
2. Identify key tradeoffs and dependencies
3. Recommend an optimal approach with justification
4. Outline a conceptual design

# Multi-Tool Workflow: Implementation Phase (For DeepSeek)

Based on the provided conceptual design, implement a solution that:
1. Follows the specified approach
2. Optimizes for [specific requirements]
3. Includes error handling and edge cases
4. Provides documentation and usage examples
```

### Design → Feedback → Refinement

This iterative workflow improves quality through multiple models.

#### Process Flow
1. **Initial Design** (GPT-4o or Claude Opus)
   - Generate comprehensive first draft
   - Include all required components
   - Document design decisions

2. **Critical Feedback** (Claude with Extended Thinking)
   - Analyze for weaknesses and gaps
   - Identify optimization opportunities
   - Suggest specific improvements

3. **Refinement** (Original or specialized model)
   - Incorporate feedback
   - Optimize critical components
   - Finalize the solution

#### Configuration Example
```markdown
# Initial Design Phase

Create a comprehensive design for [system/document/code].

Requirements:
- [List key requirements]
- [List constraints]
- [List quality attributes]

Provide a complete initial design including:
- [List expected deliverables]

# Feedback Phase (For Claude)

Review the provided design critically:

1. Evaluate against these criteria:
   - [List evaluation criteria]
2. Identify potential weaknesses, including:
   - Performance bottlenecks
   - Scalability issues
   - Security concerns
   - Maintenance challenges
3. Suggest specific improvements for each issue
4. Prioritize recommendations by impact

# Refinement Phase

Based on the feedback provided, refine the initial design:

1. Address each identified issue with specific changes
2. Optimize the critical components
3. Justify any feedback points not incorporated
4. Provide a complete, refined version
```

## Custom Configuration Templating

### Configuration Template Structure

Create your own tool-specific configurations using this template:

```markdown
# [Tool Name] Configuration for [Task Type]

## Context Setting
[Establish role, expertise level, and general approach]

## Task Instructions
[Specific task details and requirements]

## Processing Guidelines
[How the AI should approach the problem]
- Step 1: [Specific instruction]
- Step 2: [Specific instruction]
- etc.

## Output Format
[Response structure and formatting requirements]

## Additional Parameters
[Any special modes or settings to enable]
```

### Calibration Process

To optimize configurations for specific models:

1. **Baseline Testing**
   - Test the same prompt across different models
   - Note differences in output quality and approach
   - Identify relative strengths and weaknesses

2. **Parameter Tuning**
   - Adjust instructions based on model strengths
   - Modify level of detail in task breakdown
   - Calibrate output format specificity

3. **Iteration**
   - Refine based on output quality
   - Create model-specific instruction sets
   - Document effective patterns

# Developer Profile

## Core Identity
- Senior software engineer specializing in systems, AI infrastructure, and developer tooling
- Background in distributed systems, data engineering, and systems architecture
- Preference for intelligent, reproducible, and automated solutions
- Values high-quality implementations with minimal technical debt

## Technical Environment

### Hardware/OS
- Apple Silicon Mac (primary development environment)
- Multi-machine synchronization requirements
- Local-first architecture preferences

### Development Environment
- **Package Management**: Nix, Homebrew, uv (Python)
- **Configuration**: Git-tracked dotfiles, declarative configurations
- **Editor Ecosystem**: Neovim (LazyVim), Cursor IDE
- **Terminal**: Ghostty with ZSH
- **Window Management**: Raycast, Swish, Yabai, Hammerspoon

### Technical Stack
- **Primary Languages**:
  - TypeScript (eslint + prettier)
  - Python (ruff for linting)
  - Exposure to Rust, Go, and other systems languages
- **Tooling Preferences**:
  - Modern CLI utilities (ripgrep, fd, zoxide, atuin, starship)
  - AI-augmented development workflows
  - Keyboard-driven interfaces with minimal mouse usage

## Workflow Characteristics

### General Approach
- Preference for keyboard-driven, efficient workflows
- Strong Vim expertise and modal editing proficiency
- Value tools that reduce cognitive load and automate routine tasks
- Emphasis on reproducibility and declarative configuration

### Technical Values
- Intelligence/capability prioritized over resource usage
- Clear, maintainable code valued over cleverness
- Preference for functional programming patterns where appropriate
- Appreciation for elegant solutions that balance simplicity and power

### Learning Style
- Preference for bottom-up understanding with comprehensive examples
- Value explanations that balance theory and practical application
- Appreciate insights into underlying mechanisms and design decisions
- Learn best through hands-on implementation after conceptual understanding

## Communication Preferences

### Technical Documentation
- Structured, hierarchical organization with clear navigation
- Balanced detail: comprehensive without overwhelming
- Code examples that demonstrate real-world usage
- Visual aids for complex relationships and architectures

### Decision Processes
- Evidence-based recommendations with explicit reasoning
- Clear articulation of tradeoffs and constraints
- Preference for quantitative evaluation when possible
- Appreciation for multiple perspectives before conclusions

## AI Collaboration Context

### Interaction Mode
- Direct, efficient communication without unnecessary niceties
- Preference for precise, technical language
- Appreciation for both depth (when needed) and brevity (when appropriate)

### Output Preferences
- Structured, scannable content with clear section demarcation
- Visual representations for complex relationships
- Code that balances readability and efficiency
- Practical, implementable recommendations

### Follow-up Style
- Iterative refinement rather than extended initial responses
- Focused questions to clarify ambiguities
- Building on previous context rather than repetition
# Template Components

This directory contains reusable components that can be incorporated into various AI prompts and instructions. These components are designed to be modular, allowing you to mix and match them based on your specific needs.

## What's Included

### User Profiles
- **[developer-profile.md](./developer-profile.md)** - Detailed developer profile for personalizing AI responses to match technical preferences and workflow patterns

### Response Format Templates
- Coming soon: Response formatting guidelines for different output types (code, analysis, documentation)

### System Role Components
- Coming soon: Modular components for building custom system roles

## How to Use These Components

These components can be included in your prompts in several ways:

1. **Direct Inclusion** - Copy and paste relevant sections into your prompt
2. **Reference by Link** - If your AI assistant can access files, reference these components by path
3. **Mixed Approach** - Include the most relevant parts directly, reference others by path

Example:

```
I'd like you to help me design a new web service architecture.
Please refer to my developer profile at /Templates_Components/developer-profile.md
to understand my preferences and technical environment.

For this task, I need:
[your specific requirements]

Please format your response according to my communication preferences in the profile.
```

## Customization

These templates should be customized to match your specific needs:

1. For the developer profile, update sections to match your actual preferences
2. Add, remove, or modify sections based on what's relevant to your work
3. Consider creating domain-specific variants for different types of projects

## Best Practices

- Keep components modular so they can be mixed and matched
- Update components as your preferences and environment evolve
- Use consistent terminology across all components
- Include examples where appropriate to illustrate usage
# Task-Specific Templates

These templates provide specialized prompting patterns for common task types. They are designed to be combined with your Developer Profile and Response Format preferences.

## Code Generation Template

```markdown
# Code Generation Request

## Task Description
[Brief description of the code you need]

## Requirements
- Language: [Programming language and version]
- Framework/Libraries: [Any specific libraries or frameworks]
- Performance needs: [Time/space complexity requirements]
- Edge cases to handle: [List specific edge cases]

## Expected Functionality
[Detailed description of what the code should do]

## Output Format
- Provide clean, efficient implementation with type annotations
- Include relevant error handling
- Add comments explaining complex logic or design decisions
- Demonstrate usage with example inputs/outputs
- Discuss any performance considerations or tradeoffs

## Additional Context
[Any other relevant information or constraints]
```

## Code Review Template

```markdown
# Code Review Request

## Code to Review
```[language]
[Paste code here]
```

## Review Focus
- [Specific aspects to focus on, e.g., "performance", "security", "readability"]
- [Any specific concerns or questions]

## Expected Output
- Identify potential issues (bugs, performance problems, security vulnerabilities)
- Suggest improvements with specific code examples
- Highlight good practices already present
- Analyze overall code structure and organization
- Provide a table scoring the code on key dimensions (1-10):
  - Correctness, Performance, Maintainability, Security, Readability

## Additional Context
[Relevant background information about the code purpose, environment, etc.]
```

## System Design Template

```markdown
# System Design Request

## Design Task
[Brief description of the system to be designed]

## Requirements
- Functionality: [Key functional requirements]
- Scale: [Expected scale/load characteristics]
- Constraints: [Budget, technology, or other constraints]
- Quality attributes: [Reliability, performance, security needs]

## Expected Deliverables
- High-level architecture diagram
- Component breakdown with responsibilities
- Data model and flow
- API design (if applicable)
- Technology stack recommendations with rationale
- Scalability considerations
- Potential failure modes and mitigations

## Output Format Preferences
- Start with executive summary of the approach
- Include visual architecture diagram
- Provide component tables with responsibilities and interfaces
- Include sequence diagrams for key flows
- List technology choices with explicit rationale

## Additional Context
[Any background information or existing systems to consider]
```

## Data Analysis Template

```markdown
# Data Analysis Request

## Analysis Goal
[What insight or outcome you're seeking]

## Data Description
- Source: [Where the data comes from]
- Format: [CSV, JSON, etc.]
- Size: [Approximate size or dimensions]
- Key fields: [Important columns or attributes]
- Special characteristics: [Missing values, outliers, etc.]

## Analysis Requirements
- Specific questions to answer: [List questions]
- Metrics to calculate: [List metrics]
- Visualizations needed: [Types of charts/plots]
- Statistical methods to apply: [If applicable]

## Output Format Preferences
- Present key findings first as executive summary
- Include properly labeled visualizations
- Provide tables summarizing statistical results
- Document methodology and assumptions
- Include recommendations based on findings

## Additional Context
[Any relevant background information or hypotheses]
```

## Technical Documentation Template

```markdown
# Technical Documentation Request

## Documentation Purpose
[What this documentation will be used for and by whom]

## Subject Matter
[The system, code, or process to document]

## Documentation Requirements
- Target audience: [Who will read this]
- Technical depth: [Beginner, intermediate, expert]
- Sections to include: [List required sections]
- Examples needed: [Specific examples to include]

## Format Preferences
- Include table of contents
- Use hierarchical structure with clear navigation
- Provide code examples for key operations
- Include diagrams for complex concepts
- Add troubleshooting section for common issues

## Additional Context
[Any background information or existing documentation to reference]
```

## Research Synthesis Template

```markdown
# Research Synthesis Request

## Research Topic
[The subject to research]

## Research Objectives
- Key questions to answer: [List specific questions]
- Scope boundaries: [What to include/exclude]
- Required depth: [How comprehensive should this be]

## Source Preferences
- Preferred authorities: [Any specific sources to prioritize]
- Recency requirements: [How recent should sources be]
- Source types: [Academic papers, industry reports, etc.]

## Output Format
- Begin with executive summary of key findings
- Structure information hierarchically by subtopic
- Include comparison tables for competing approaches/views
- Provide citations for all significant claims
- End with synthesis of implications and next steps

## Additional Context
[Your current knowledge level, specific applications of the research, etc.]
```

## Technical Comparison Template

```markdown
# Technical Comparison Request

## Items to Compare
[List the technologies, methods, or tools to compare]

## Comparison Criteria
[List the 5-7 most important factors for evaluation]

## Usage Context
[How and where the solution will be used]

## Constraints
[Any limitations that would affect the decision]

## Output Format
- Start with brief overview of each option
- Create weighted comparison table (criteria weighted by importance)
- Score each option 1-10 on each criterion with clear rationale
- Include pros/cons section for each option
- Provide final recommendation with justification
- Address potential objections to the recommendation

## Additional Context
[Your experience level with these options, specific requirements, etc.]
```

## Troubleshooting Template

```markdown
# Troubleshooting Request

## Problem Description
[Detailed description of the issue]

## Environment Details
- Hardware: [Relevant hardware specifications]
- Software: [OS, application versions, etc.]
- Configuration: [Relevant configuration details]
- Recent changes: [Any recent changes that might be relevant]

## Observed Behavior
[What's actually happening]

## Expected Behavior
[What should be happening]

## Troubleshooting Already Attempted
[Steps you've already taken to diagnose or fix]

## Output Format
- Analyze possible root causes systematically
- Provide diagnostic steps in order of likelihood
- Include specific commands or code to run for diagnosis
- Suggest solutions with step-by-step implementation
- Explain preventative measures for the future

## Additional Context
[Error logs, screenshots, or other relevant information]
```

## Project Planning Template

```markdown
# Project Planning Request

## Project Description
[Brief description of the project]

## Project Goals
- Primary objectives: [List main goals]
- Success criteria: [How success will be measured]
- Constraints: [Time, budget, resource constraints]

## Scope
- In scope: [Features/components definitely included]
- Out of scope: [What's explicitly excluded]
- Potential extensions: [Nice-to-haves if time permits]

## Output Format
- Provide project breakdown structure with major components
- Create task dependency diagram
- Include risk assessment table with mitigation strategies
- Suggest timeline with milestones
- Recommend resource allocation approach
- Include key decision points and criteria

## Additional Context
[Team composition, expertise areas, relevant experience, etc.]
```
# Workflow Integration Guide

This guide explains how to effectively integrate AI tools into your development, writing, and analysis workflows.

## Core Integration Principles

### 1. Task Decomposition
- Break complex tasks into discrete steps
- Identify which steps benefit most from AI assistance
- Create clear handoffs between human and AI work

### 2. Context Management
- Maintain consistent context across interactions
- Reference previous outputs explicitly
- Provide explicit updates when context changes

### 3. Output Refinement
- Use follow-up prompts to refine outputs
- Iterate based on specific feedback
- Save effective prompt+output combinations as templates

### 4. Version Control Integration
- Include AI prompts in version control
- Document AI-assisted components
- Attribute significant AI contributions appropriately

## Development Workflow Integration

### Code Generation Workflow

#### Initial Planning Phase
1. **Define Requirements** (Human)
   - Specify functionality, inputs, outputs
   - Identify constraints and edge cases
   - Determine performance requirements

2. **Generate Implementation Strategy** (AI)
   - Use the "Code Solution Template"
   - Request multiple approach options
   - Ask for tradeoff analysis

3. **Select and Refine Approach** (Human)
   - Choose preferred implementation approach
   - Identify areas needing customization
   - Note integration requirements

#### Implementation Phase
4. **Generate Initial Implementation** (AI)
   - Use "Code Generation Template" with selected approach
   - Request complete implementation with tests
   - Specify naming conventions and style preferences

5. **Review and Customize** (Human)
   - Evaluate generated code against requirements
   - Identify areas needing modification
   - Make necessary integrations with existing codebase

6. **Refine Implementation** (AI+Human)
   - Request specific optimizations
   - Address edge cases and error handling
   - Finalize documentation and examples

### Code Review Workflow

1. **Prepare Code for Review** (Human)
   - Select code segments for review
   - Provide context and requirements
   - Specify review focus areas

2. **Generate Review** (AI)
   - Use "Code Review Template"
   - Request specific feedback categories
   - Specify output format for actionable review

3. **Triage Feedback** (Human)
   - Categorize issues by severity
   - Identify patterns in feedback
   - Prioritize improvements

4. **Implement Improvements** (Human+AI)
   - Request specific optimizations based on review
   - Generate alternative implementations for problem areas
   - Use "Advanced Techniques" for complex refactoring

### System Design Workflow

1. **Define Requirements** (Human)
   - Specify functional requirements
   - Establish constraints and scale
   - Define quality attributes

2. **Generate Architecture** (AI)
   - Use "System Design Template"
   - Request component breakdown with responsibilities
   - Specify required diagrams

3. **Review and Refine** (Human)
   - Identify areas needing more detail
   - Question assumptions and tradeoffs
   - Note integration challenges

4. **Detailed Design** (AI+Human)
   - Generate detailed specifications for key components
   - Create interface definitions
   - Develop data models and flows

5. **Implementation Planning** (Human+AI)
   - Break down into implementation tasks
   - Identify dependencies and critical path
   - Create implementation timeline

## Writing and Documentation Workflow

### Technical Documentation Workflow

1. **Define Documentation Scope** (Human)
   - Identify target audience
   - Determine required sections and depth
   - Specify format and style requirements

2. **Generate Structure** (AI)
   - Use "Technical Documentation Template"
   - Request outline with section breakdown
   - Define content types for each section

3. **Generate Content** (AI)
   - Develop content for each section
   - Include required examples and diagrams
   - Format according to specifications

4. **Review and Refine** (Human)
   - Check technical accuracy
   - Ensure completeness and clarity
   - Note areas needing expansion or correction

5. **Finalize Documentation** (AI+Human)
   - Address specific feedback
   - Format for final delivery
   - Create supplementary materials if needed

### Research Synthesis Workflow

1. **Define Research Questions** (Human)
   - Specify primary questions to answer
   - Determine scope and boundaries
   - Identify key sources or data to consider

2. **Generate Research Plan** (AI)
   - Use "Research Synthesis Template"
   - Create structured approach to investigation
   - Define synthesis methodology

3. **Conduct Research** (AI+Human)
   - Gather and organize information
   - Analyze patterns and contradictions
   - Identify key findings

4. **Generate Synthesis** (AI)
   - Create comprehensive synthesis of findings
   - Highlight implications and applications
   - Suggest areas for further investigation

5. **Review and Extend** (Human)
   - Validate accuracy and completeness
   - Identify applications to current work
   - Determine next steps based on findings
# AI Content Migration Progress Report

## Migration Status

This report documents the progress of migrating and organizing AI-related content from the original `AI` directory to the new `AI_Prompts_Master_Collection` structure.

### Completed Migrations

#### Core System Files
- ✅ `AI/CLAUDE.md` → `AI_Prompts_Master_Collection/Provider_Specific/Claude/claude-code-repository-guidance.md`
- ✅ `AI/prompts/base/system-role.md` → `AI_Prompts_Master_Collection/Core_Prompting/System_Roles/base-system-role.md`
- ✅ `AI/prompts/base/code-generation.md` → `AI_Prompts_Master_Collection/Core_Prompting/Base_Templates/code-generation-template.md`

#### Provider-Specific Content
- ✅ `AI/claude/claude_desktop_config.json` → `AI_Prompts_Master_Collection/Provider_Specific/Claude/claude_desktop_config.json`
- ✅ `AI/models/llms.yaml` → `AI_Prompts_Master_Collection/Provider_Specific/llm-models-configuration.yaml`
- ✅ `AI/prompts/tools.md` → `AI_Prompts_Master_Collection/Provider_Specific/tool-specific-configurations.md`

#### Documentation & Guides
- ✅ `AI/PROMPTING.md` → `AI_Prompts_Master_Collection/Documentation_Reference/prompt-engineering-guide.md`
- ✅ `AI/STYLE_GUIDE.md` → `AI_Prompts_Master_Collection/Style_Guides/repo-style-guide.md`
- ✅ `AI/CONVENTIONAL_COMMITS.md` → `AI_Prompts_Master_Collection/Documentation_Reference/conventional-commits-workflow.md`
- ✅ `AI/PDF_CHAT.md` → `AI_Prompts_Master_Collection/Documentation_Reference/pdf-chat-integration.md`
- ✅ `AI/NOGGIN.md` → `AI_Prompts_Master_Collection/Development_Workflow/Integration_Guides/noggin-frontend-backend-integration.md`

#### Templates & Components
- ✅ `AI/prompts/dev-profile.md` → `AI_Prompts_Master_Collection/Templates_Components/developer-profile.md`
- ✅ `AI/prompts/advanced-techniques.md` → `AI_Prompts_Master_Collection/Advanced_Techniques/advanced-prompting-techniques.md`

#### Code Generation
- ✅ `AI/CODEGEN.md` → `AI_Prompts_Master_Collection/Use_Cases/Code_Generation/code-generation-platform.md`
- ✅ `AI/prompts/code/python.md` → `AI_Prompts_Master_Collection/Use_Cases/Code_Generation/python-code-generation.md`
- ✅ `AI/prompts/code/typescript.md` → `AI_Prompts_Master_Collection/Use_Cases/Code_Generation/typescript-code-generation.md`

#### Tools Integration
- ✅ `AI/windsurf-ai/tools.json` → `AI_Prompts_Master_Collection/Tools_Integration/windsurf-tool-schemas.json`

### Identified Duplications

1. **Advanced Techniques Content**
   - `AI_Prompts_Master_Collection/Core_Prompting/Best_Practices/advanced-techniques.md`
   - `AI_Prompts_Master_Collection/Advanced_Techniques/advanced-prompting-techniques.md`

   These files contain overlapping but distinct content. The Core_Prompting version is a more concise summary, while the Advanced_Techniques version is more comprehensive.

   **Recommendation**: Keep both but add cross-references between them, clarifying that one is a summary and the other is a full guide.

2. **README Files**
   - Several README.md files were created with overlapping organization information

   **Recommendation**: Keep these as they serve as navigation aids within their respective directories.

3. **Template Components**
   - Some components appear in multiple locations with slight variations

   **Recommendation**: Consolidate into the Templates_Components directory with clear cross-references.

### Remaining Content to Migrate

1. **Prompt Files**
   - Multiple remaining .md files in `/AI/prompts/` directory
   - Need to evaluate and migrate non-duplicative content

2. **Provider-Specific Content**
   - Additional provider-specific configurations to organize

3. **Legacy Content**
   - Evaluate content in `/AI/legacy/` directory for potential archival

## Deduplication Strategy

For the identified duplications, we propose the following approach:

1. **Content Merging**:
   - For files with significant content overlap, merge into a single comprehensive file
   - Preserve all unique information from each source
   - Place in the most logical location based on content type

2. **Cross-Referencing**:
   - For related content that serves different purposes (e.g., summary vs. detailed guide)
   - Add clear references between the files
   - Explain the relationship and different use cases

3. **Hierarchical Organization**:
   - Maintain parent-child relationships between general principles and specific applications
   - Use README files to clarify the organization and relationships

## Next Steps

1. **Complete Remaining Migrations**:
   - Evaluate and migrate remaining files from the original AI directory

2. **Perform Final Deduplication**:
   - Apply the deduplication strategy to identified overlaps
   - Create consolidated versions where appropriate

3. **Enhance Documentation**:
   - Update all README files with comprehensive descriptions
   - Add cross-references between related content

4. **Final Review**:
   - Perform a complete review of the new structure
   - Ensure all valuable content has been preserved
   - Validate all links and references
# System Role Template

You are an AI assistant specialized in software development and technical tasks. Your role is to provide helpful, accurate, and concise assistance with coding, debugging, documentation, and other development-related tasks.

## Principles

- **Accuracy**: Provide factually correct information and working code examples.
- **Clarity**: Communicate clearly and concisely, avoiding jargon when possible.
- **Helpfulness**: Focus on solving the user's specific problem or request.
- **Completeness**: Provide comprehensive solutions that address all aspects of the request.
- **Contextual Awareness**: Consider the environment, tools, and frameworks mentioned in the conversation.

## Guidelines

1. **Code Generation**: When writing code, ensure it follows best practices, includes proper error handling, and is well-documented.
2. **Troubleshooting**: When helping debug issues, analyze potential causes systematically and suggest specific solutions.
3. **Explanation**: When explaining concepts, provide both high-level overviews and specific details, with examples when helpful.
4. **Documentation**: When generating documentation, follow appropriate formatting and include all necessary details.
5. **Tool Recommendations**: When suggesting tools or libraries, consider the specific context and requirements.

## Responses

- Provide concise, direct responses that address the specific question or task.
- Format code using appropriate Markdown syntax with language tags.
- Use bullet points, headings, and other formatting to enhance readability.
- For complex topics, structure your explanation in a logical progression.
- Include references to documentation or additional resources when appropriate.

## Limitations

- You should acknowledge when you don't know something or when a task is outside your capabilities.
- You should ask for clarification when the request is ambiguous or incomplete.
- You should not generate harmful, unethical, or illegal content.
- You should not pretend to be a human or claim capabilities you don't have.

Remember to adapt this system role to the specific context of each interaction while maintaining these core principles and guidelines.
# Noggin Chat: Smart PDF Citations Demo

## Overview

This demo showcases the new PDF citation feature in Noggin Chat. This feature allows the AI to reference specific parts of uploaded PDF documents, creating a more transparent and verifiable chat experience for users working with medical documentation.

## Key Features

### 1. Intelligent PDF Citations

The AI can now reference precise sections of uploaded PDFs in its responses. Each citation:
- Links directly to the source document
- Highlights the exact text being referenced
- Shows the page number and position in the document

### 2. PDF Navigation Through AI

Users can click on citations to jump directly to the referenced section in the PDF, allowing for:
- Quick verification of AI responses against source material
- Efficient document exploration guided by AI insights
- Better understanding of complex medical documentation

### 3. Transparent AI Responses

All AI claims about document content can now be traced back to their source:
- Citations are presented inline within the AI's response
- Multiple documents can be referenced in a single response
- Users can easily distinguish between AI analysis and direct quotes

## Technical Implementation

The citation system works through a sophisticated pipeline:

1. **Document Processing**
   - PDFs are processed to extract text blocks with precise coordinate information
   - Text extracts are organized by document ID and page number

2. **Intelligent LLM Prompting**
   - The system builds structured XML prompts containing relevant document extracts
   - LLM is instructed to include citations using a consistent XML format

3. **Citation Processing**
   - LLM responses containing citation XML tags are parsed
   - Citation references are mapped back to the original document sections
   - Content offsets are calculated to maintain precise text references

4. **Response Formatting**
   - Citations are transformed into structured data
   - Response text is enhanced with metadata linking to source documents

## Demo Scenarios

### Scenario 1: Medical Criteria Verification

**User:** "What criteria need to be met for approval of this treatment?"

**Noggin Chat:**
"According to the guidelines, approval requires [three main criteria](#citation1):
1. Confirmed diagnosis by a specialist
2. Failure of at least two conventional therapies
3. No contraindications for the requested treatment

For patients under 18, [additional documentation](#citation2) from a pediatric specialist is required."

*Citations link directly to the relevant sections in the medical criteria documents*

### Scenario 2: Clinical Information Exploration

**User:** "Summarize the patient's cardiac history"

**Noggin Chat:**
"The patient has a history of [atrial fibrillation diagnosed in 2018](#citation1) with [two cardioversion procedures](#citation2) performed in 2019. They are currently on [anticoagulation therapy](#citation3) and have [stable cardiac function](#citation4) with an ejection fraction of 55% as of their last echocardiogram."

*Each citation links to the specific clinical note containing the information*

## Business Value

This feature delivers significant value by:

1. **Increasing Trust & Transparency**
   - Users can verify AI responses against source material
   - Reduces the risk of hallucinated or incorrect information

2. **Improving Efficiency**
   - Faster document navigation through AI-guided citations
   - Reduced time spent manually verifying information

3. **Enhancing Compliance**
   - Clear audit trail of information sources
   - Better documentation of decision-making rationale

4. **Supporting Better Decisions**
   - More informed clinical and operational decisions
   - Reduced risk of errors in document interpretation

## Technical Notes

- Citations use normalized coordinates (0-1) for PDF bounding boxes
- XML-based citation format: `<extract ids="pdf_ID#extract_ID">text</extract>`
- Content offsets track precise character positions in responses
- System handles multiple document references in a single response
# Orchestrator Architecture

## Overview

The Orchestrator architecture handles LLM provider interactions in the Chat module. It provides a unified interface for making requests to different LLM providers (Anthropic, Gemini) while maintaining type safety and promoting code reuse.

This document explains the design decisions, implementation details, and usage patterns for the orchestrator components.

## Key Components

### 1. Core Interface & Base Class

- **`Orchestrator` Interface**: Defines the contract that all provider implementations must follow
- **`BaseOrchestrator` Abstract Class**: Implements common functionality shared across providers
- **Provider-Specific Implementations**: `AnthropicOrchestrator` and `GeminiOrchestrator`

### 2. Factory Pattern

- **`OrchestratorFactory`**: Type for factory functions that instantiate orchestrators
- **`createOrchestrator`**: Factory implementation that selects the appropriate orchestrator based on provider

### 3. Type Safety

- **Provider-Specific Message Types**: `AnthropicMessage`, `GeminiMessage`
- **Zod Schemas**: `anthropicMessageSchema`, `geminiMessageSchema`

## Design Decisions

### Why an Interface + Abstract Class Pattern?

1. **Interface Definition**: The `Orchestrator` interface defines the clear contract that all providers must implement.
2. **Code Reuse**: The `BaseOrchestrator` abstract class implements common functionality to avoid duplication.
3. **Flexibility**: Each provider can extend the base class while adding provider-specific functionality.

```typescript
export interface Orchestrator {
  getModelName(): string;
  getClient(): Client;
  getSystemPrompt(): string;
  parseMessages<T>(messages: ChatMessage[]): T[];
  processChat(messages: ChatMessage[]): Promise<ChatMessage>;
}

export abstract class BaseOrchestrator implements Orchestrator {
  // Common implementation
  // Abstract methods must be implemented by subclasses
}
```

### Why a Factory Pattern?

1. **Dynamic Selection**: The factory can choose the appropriate orchestrator at runtime.
2. **Configuration Injection**: Common dependencies are injected once.
3. **Clean API**: Client code doesn't need to know implementation details.

```typescript
const orchestratorFactory = createOrchestrator(ctx);
const orchestrator = orchestratorFactory("anthropic"); // or "gemini"
```

### Why Type-Safe Messages with Zod?

1. **Runtime Validation**: Zod validates messages at runtime to catch potential issues.
2. **Type Inference**: TypeScript types are derived from Zod schemas.
3. **Consistency**: Ensures consistent message formats across the application.

```typescript
export const anthropicMessageSchema = z.object({
  role: z.enum(["user", "assistant", "system"]),
  content: z.string(),
}) satisfies SchemaLike<AnthropicMessage>;
```

## Implementation Details

### Message Parsing

Each provider implements a `parseMessages<T>` method that transforms our internal `ChatMessage` format into the provider-specific format:

- **Anthropic**: Uses string content fields
- **Gemini**: Uses arrays of parts with text fields

### Response Handling

The `createResponseMessage` helper method in the base class ensures consistent response formatting:

```typescript
protected createResponseMessage(text: string, modelName: string, finishReason?: string): ChatMessage {
  return {
    message_id: generateRandomId("msg"),
    timestamp: new Date().toISOString(),
    role: "assistant",
    content: [{ content_type: "text", text }],
    metadata: {
      model_name: modelName,
      finish_reason: finishReason,
    },
  };
}
```

### Error Handling & Logging

All error handling and logging follows a consistent pattern with standardized log messages.

## Provider Implementations

### Anthropic (`AnthropicOrchestrator`)

- Uses the Claude API for chat completions
- Supports tool usage (with the MnrTool for medical necessity reviews)
- Handles multi-step tool interactions

### Gemini (`GeminiOrchestrator`)

- Uses Google's Gemini API
- Simpler implementation without tool support (currently)

## Usage Examples

### Basic Usage

```typescript
// Create the factory with dependencies
const orchestratorFactory = createOrchestrator({
  logger,
  platform,
  clients: {
    anthropic: new Anthropic({ apiKey: "..." }),
    gemini: new GoogleGenAI({ apiKey: "..." }),
  },
});

// Get an orchestrator instance
const orchestrator = orchestratorFactory("anthropic");

// Process messages
const response = await orchestrator.processChat(messages);
```

## Testing

The architecture is designed for testability:

- **Unit Tests**: Test individual orchestrators and their components
- **Integration Tests**: Test complete provider implementations with mocked clients
- **Mocks**: Easy to create mock implementations for testing

## Future Extensibility

To add a new provider:

1. Create a new implementation class extending `BaseOrchestrator`
2. Define provider-specific message types and schemas
3. Add the provider type to the `Provider` union type
4. Update the factory function to handle the new provider

## Performance Considerations

- **Lazy Loading**: Provider implementations are loaded dynamically to avoid circular dependencies
- **Reused Clients**: Client instances are created once and reused
- **Structured Logging**: Performance metrics are logged consistently

## Conclusion

The orchestrator architecture provides a clean, type-safe approach to working with multiple LLM providers. It balances flexibility and standardization while making it easy to add new providers in the future.
# PDF Citation System Implementation

## Problem Statement

The platform needed a new feature called "Flopilot Chat + Citations" that would allow users to upload PDF documents, ask questions about them, and receive answers with precise citations back to the source documents. This would eliminate manual document skimming, speed up review processes, and provide defensible evidence for audits.

The core technical requirement was to build a chat system that could:
1. Accept and process multiple PDF documents
2. Allow natural-language questions about the PDF content
3. Stream back responses with accurate, clickable citations to source documents
4. Validate all responses against proper JSON schemas

## Requirements Analysis

Based on the PRD, the key technical requirements included:

### Core Features

1. **PDF Upload & Parsing**: Already handled by existing systems that convert PDFs to structured extract blocks
2. **Parse Status Polling**: Already implemented via UI components
3. **Chat With Citations**: The focus of this implementation, requiring schema-based payloads
4. **Schema-First Contracts**: All payloads defined in JSON-Schema Draft-07

### Technical Architecture Components

- UI: PromptBoxComponent, ChatComponent, Controllers (already implemented)
- API: Fastify endpoints (our focus)
- Worker: FlowScheduler + Prefect (already implemented)
- Parser: Reducto (already implemented)
- LLM: Claude (our integration point)
- Store: S3 (or MinIO) (already implemented)

### Data Model Requirements

- TextSectionV1: Segments of assistant response with citations
- PdfSectionV1: Bounding box location in source documents
- Structured prompt to LLM and structured response parsing

## Design Decisions & Approach

### 1. Schema Integration

The first design decision was how to work with the schemas. Two options were considered:

1. **Manual Schema Implementation**: Create our own schema definitions in code
2. **Generated Schema Usage**: Use auto-generated TypeScript schemas from JSON Schema definitions

Decision: We chose to use the generated schema approach since the JSON Schema files were already defined and the generation pipeline existed. This approach:
- Ensures consistency between backend validation and frontend types
- Reduces duplication of schema definitions
- Leverages the existing type generation system
- Minimizes maintenance burden when schemas change

### 2. LLM Integration Method

For interacting with the Claude LLM, two approaches were considered:

1. **Orchestrator Pattern**: Use an intermediary orchestrator service to handle LLM calls
2. **Direct LLM Integration**: Call the LLM API directly from the chat endpoint

Decision: We chose to bypass the orchestrator and call Claude directly because:
- It simplifies the code path for this specific use case
- It gives us more control over the exact prompt format for citation extraction
- It reduces latency by eliminating a service hop
- The orchestrator wasn't adding significant value for this specific feature

### 3. Response Format

For sending responses back to the client, two options were evaluated:

1. **Streaming Response**: Use SSE (Server-Sent Events) to stream responses as they're generated
2. **Standard REST API**: Return a single JSON response when processing is complete

Decision: We opted for a standard REST API response because:
- It simplifies error handling
- Simplifies the client implementation
- Makes it easier to validate the full response against the schema
- Streaming wasn't a core requirement for the initial implementation

### 4. Citation Extraction Approach

For handling the citation extraction from Claude responses, we considered:

1. **XML Format**: Have Claude return responses with XML tags for citations
2. **JSON Format**: Have Claude format responses in a JSON structure

Decision: We chose the XML approach because:
- It's more natural for language models to insert inline citations as XML tags
- JSON structures would require more complex prompt engineering
- The XML format is more readable when debugging
- It provides a clear separation between citation metadata and text content

## Implementation Details

### Schema Structure

We implemented the schema structure based on the JSON Schema definitions:

1. **TextSection**: Represents segments of text with citations
   - text: The content text
   - section_type: Identifier ("text_section_v01_20250514")
   - start/end: Character positions in the source
   - citations: Array of ExtractCitation references

2. **PdfSection**: Represents a location in a PDF document
   - section_type: Identifier ("pdf_section_v01_20250514")
   - page: Page number (1-based)
   - bounding_box: Rectangle coordinates (top, right, bottom, left)

3. **ExtractCitation**: Links text to source documents
   - pdf_id: Identifier for the PDF
   - extract_id: Identifier for the extract
   - cited_text: The cited content
   - start_char/end_char: Character positions
   - pdf_section: Optional bounding box location

### Prompt Engineering

The prompt to Claude was structured as follows:

```
You are an expert research assistant tasked with answering questions based on provided PDF document extracts.

Here are the PDF documents that you will be referencing, formatted in XML and broken down into extracts referenced by extract ID:

<pdfs>
<pdf1>
<extract ids="pdf_id.extract_0">Extract text content</extract>
...
</pdf1>
...
</pdfs>

The question you need to answer is:
<question>
{User's question}
</question>

To complete this task, follow these steps:
1. Review the PDF documents and identify relevant quotes
2. List the relevant quotes in numbered order
3. Formulate your answer, starting with "Answer:"
4. When citing information, use the format:
   <extract ids="pdf_id.ext_id.block_id">Cited text goes here.</extract>
```

This prompt instructs Claude to respond with explicit XML citation tags that reference the source documents.

### Response Parsing

The response parsing logic:

1. Checks for presence of `<extract>` tags
2. Parses the response to extract citation sections
3. For each citation:
   - Extracts the `ids` attribute
   - Parses cited text
   - Creates a PDF section reference
   - Maps text positions to source document locations
4. Constructs a properly structured JSON response with sections and citations

### Error Handling

The implementation includes error handling for:
- Malformed XML in responses
- Missing citation tags
- Invalid PDF or extract references
- LLM API errors

Each error case falls back to simpler response formats when necessary, ensuring the user always gets a response.

## Assumptions

Several assumptions were made during implementation:

1. **PDF Processing**: We assumed the PDF extraction process is already in place and functions correctly
2. **Schema Stability**: We assumed the schemas are stable and won't change frequently
3. **LLM Reliability**: We assumed Claude can reliably follow instructions to produce correctly formatted citations
4. **Bounding Box Accuracy**: For the initial implementation, we used placeholder values for bounding boxes
5. **Text Position Accuracy**: We simplified text position mapping, assuming exact matches can be found

## Potential Future Improvements

Several areas for future enhancement were identified:

1. **Response Streaming**: Add streaming capabilities to show results as they're generated
2. **Citation Accuracy**: Improve exact text position mapping with fuzzy matching
3. **Error Recovery**: More sophisticated error recovery for malformed LLM responses
4. **Schema Validation**: Add explicit validation of responses against schemas before returning
5. **Performance Optimization**: Implement caching of extracted content and common queries
6. **UI Integration**: Add hover states and visual indicators for citations in the front-end

## Conclusion

The implemented solution provides a robust foundation for the PDF citation feature. By leveraging schema-first design, direct LLM integration, and structured XML prompting, we've created a system that can accurately extract information from PDFs and provide well-cited responses to user queries.

The implementation balances simplicity with functionality, focusing on reliable citation extraction while maintaining a clear code structure. The approach is extensible for future enhancements while meeting the immediate requirements specified in the PRD.
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Tool Configuration

These are the optimized settings for various CLI tools in the dotfiles:

| Tool      | Key Configurations                                   |
|-----------|------------------------------------------------------|
| Aerospace | `hyper-; = 'focus window-hint'` (window jumping)     |
| Atuin     | `workspaces=true`, `enter_accept=true`, `sync_frequency="60m"` |
| Bat       | `BAT_THEME="--theme=OneHalfDark"`, `DELTA_PAGER="bat --plain --paging=never"` |
| FZF       | 40% height, reverse layout, keybindings (ctrl-j/k navigation) |
| Git       | Safe pushes (`--force-with-lease = false`), current branch default |
| Glow      | Uses bat as pager for markdown preview               |
| Homebrew  | Analytics disabled                                   |
| htop      | IO Read/Write columns, optimized fields display      |
| just      | `summary = "on"`, zsh shell mode                     |
| yazi      | 5MB max file size for previews                       |
| starship  | Right prompt with time/battery, Tokyo Night theme    |

## Commands

### Linting & Formatting
- Format Lua: `stylua -c config/nvim/`
- Lint Lua: `selene config/nvim/`
- Python formatting: `ruff format <file.py>`
- Shell formatting: `shfmt -i 2 -ci <file.sh>`
- JavaScript/TypeScript linting: `eslint_d <file.js>`
- Validate Lua configuration: `nvim --headless -c "lua require('config.lazy')" -c "q"`

### Testing
- Individual Lua test: `busted -m config/nvim/lua/tests <test_file>`
- Single Neovim plugin test: `cd config/nvim && plenary-test spec/<plugin_name>_spec.lua`

## Style Guidelines

- **Indentation**: 2 spaces for most files, 4 spaces for Python
- **Line Length**: 120 characters maximum (per stylua.toml)
- **String Quotes**: Single quotes for Lua, double quotes for other languages
- **Naming**: camelCase for functions, snake_case for variables
- **Imports**: Group imports by type, sort alphabetically
- **Error Handling**: Use pcall/xpcall in Lua, try/catch in JavaScript
- **Comments**: Minimal, only when necessary to explain complex logic
- **Configuration**: Follow LazyVim conventions for Neovim configs
- **Cursor Rules**: Keep rules DRY, include both DO and DON'T examples
- **Commits**: Clear, atomic changes with descriptive messages
# Claude Chat History

## FormData File Array Handling - May 8, 2025

### User Request
I was trying to find other tests in the current directory that create arrays of files before a test, and then use vitest to check if the results are the same array of items.

### Files Found
After searching through test files in the current directory, I found several examples of tests that create arrays and check them with vitest:

1. In `app.test.ts`:
```typescript
test("handles File objects", async () => {
  // Create test files
  const f1 = new File(["x"], "f1.txt");
  const f2 = new File(["y"], "f2.txt");

  // Prepare form data with single and array file fields
  const fd = new FormData();
  fd.append("doc", f1);
  fd.append("docs[]", f1);
  fd.append("docs[]", f2);

  const req = new Request("https://example.com", { method: "POST", body: fd });
  const result = await extractBodyFormData(req);

  expect(result).toEqual(
    expect.objectContaining({
      doc: expect.any(File),
      docs: expect.arrayContaining([expect.any(File), expect.any(File)]),
    })
  );
}
```

2. In `blob.test.ts`:
```typescript
for (const [name, bloblike] of [
  ["plainText", "hello"] as const,
  ["buffer", Buffer.from("hello")] as const,
  ["arrayBuffer", new TextEncoder().encode("hello").buffer] as const,
]) {
  test(`writes reads and deletes ${name} blobs`, async () => {
    const key = generateRandomId("test-blob");
    // ...test implementation...
  });
}
```

3. In `events.test.ts`:
```typescript
test("reads forward", async () => {
  const stream = makeStream(store);
  await expect(stream.save(CREATE));
  await expect(stream.save(UPDATE));
  const events = await Array.fromAsync(stream.readEvents("forward"));
  const payloads = events.map((e) => e.data);
  expect(payloads).toEqual([CREATE, UPDATE]);
});
```

4. In `object.test.ts`:
```typescript
test("it converts arrays of snake_case to camelCase", () => {
  const obj = [{ foo_bar: "baz" }, { baz_qux: "quux" }];
  expect(recursiveCamelCase(obj)).toEqual([{ fooBar: "baz" }, { bazQux: "quux" }]);
});
```

### Build Issues
When trying to uncomment and use the File test in app.test.ts, there were TypeScript build errors:
```
src/app.test.ts:173:10 - error TS2532: Object is possibly 'undefined'.
173   expect(arrayFiles[0].name).toBe("f1.txt");
            ~~~~~~~~~~~~~
src/app.test.ts:174:10 - error TS2532: Object is possibly 'undefined'.
174   expect(arrayFiles[1].name).toBe("f2.txt");
            ~~~~~~~~~~~~~
```

### Solution
I fixed the TypeScript errors by modifying the test to use type-safe array handling:

```typescript
test("handles File objects", async () => {
  // Create test files
  const f1 = new File(["x"], "f1.txt");
  const f2 = new File(["y"], "f2.txt");

  // Prepare form data with single and array file fields
  const fd = new FormData();
  fd.append("doc", f1);
  fd.append("docs[]", f1);
  fd.append("docs[]", f2);

  const req = new Request("https://example.com", { method: "POST", body: fd });
  const result = await extractBodyFormData(req);

  // Type-safe validation using structure matching
  expect(result).toEqual(
    expect.objectContaining({
      doc: expect.any(File),
      docs: expect.arrayContaining([expect.any(File), expect.any(File)]),
    })
  );

  // Type-safe array validation
  const formData = result as Record<string, any>;
  const docsArray = formData['docs'];
  if (Array.isArray(docsArray)) {
    expect(docsArray.length).toBe(2);
  }
});
```

The fix ensures TypeScript doesn't complain about possibly undefined array indexes while still checking the array's length. The original test was later modified to use a more comprehensive approach with full assertions on File properties.

### Observations on FormData Array Syntax
The implementation in object.ts shows how array fields are handled when using the "[]" suffix in form field names:

1. In `fromEach` function:
```typescript
if (isArr) {
  const prev = result[key];
  result[key] =
    prev === undefined ? [val] : Array.isArray(prev) ? [...prev, val] : [prev, val];
} else {
  result[key] = val;
}
```

This implements browser-standard duplicate-key handling with these rules:
- `foo, foo` → last value wins
- `foo[], foo[]` → values append to array
- `foo, foo[]` → scalar folded into array
- `foo[], foo` → last scalar overwrites array
EOF < /dev/null
# AI Directory Structure

This directory contains all AI-related configurations, tools, and utilities for enhancing your development workflows through AI integration.

## Directory Structure

```
config/ai/
├── claude/               # Claude-specific configuration files
├── cline/                # Cline (Claude CLI) configuration
├── history/              # AI conversation history exports
├── integrations/         # Integration files for various tools
│   └── api-justfile      # API namespace for REST API utilities
├── justfile              # Main AI command structure
├── models/               # AI model definitions and capabilities
│   └── llms.yaml         # LLM definitions with capabilities
├── preferences/          # User preferences extracted from AI interactions
├── prompts/              # Prompt templates for different AI tasks
│   ├── language-specific/  # Templates for different programming languages
│   └── task-specific/      # Templates for different task types
├── providers/            # Provider-specific configuration
│   ├── anthropic/
│   ├── claude/
│   ├── cursor/
│   ├── gemini/
│   └── openai/
├── templates/            # Code, document, and prompt templates
└── utils/                # Utility scripts for AI operations
    ├── ai_bash.sh        # Bash scripts for AI commands
    └── typescript/       # TypeScript library for AI commands
```

## Main Components

### Claude Code Integration

The `claude` directory contains configuration files for Claude Code, Anthropic's AI coding assistant. The `history` directory stores conversation exports from Claude Code sessions.

### Command Structure

The `justfile` provides a comprehensive set of AI commands organized into namespaces:

- `ai:` - Core AI operations
- `ai:typescript` - TypeScript-specific AI commands
- `ai:python` - Python-specific AI commands
- `ai:shell` - Shell scripting AI commands
- `ai:nix` - Nix-related AI commands
- `ai:run` - AI-powered operations
- `api:` - REST API utilities powered by AI

### Model Definitions

The `models/llms.yaml` file contains detailed information about AI models, their capabilities, and recommended use cases. This serves as a central reference for all AI model configuration.

### Provider Configuration

Each AI provider has its own directory in `providers/` containing configuration files, API settings, and provider-specific utilities.

### Templates and Prompts

The `templates/` and `prompts/` directories contain reusable templates for different tasks, languages, and scenarios, making it easy to create consistent AI-driven workflows.

## Usage

To use these AI tools, you can leverage the justfile command structure:

```bash
# List all available AI commands
just ai

# Run a specific AI command
just ai:code typescript "Write a function that does X"

# Generate a commit message with AI
just ai:commit-msg

# Run the API utilities
just api
```

## Adding New AI Tools

To add a new AI tool or configuration:

1. Create a new directory in the appropriate location (e.g., providers/new-provider/)
2. Add configuration files and templates
3. Update the justfile to include commands for the new tool
4. Document the new tool in this README.md

## Related Files

- `/Users/hank/dotfiles/SPECIFICATIONS.yml` - Contains specifications for AI tools
- `/Users/hank/dotfiles/config/git/gitconfig` - Git integration with AI for commit messages
- `/Users/hank/dotfiles/config/git/gitmessage` - Commit message template with AI assistance
# Dotfiles Specifications

This document provides detailed technical specifications for the dotfiles repository, serving as a reference for development and contributions.

## Core Principles

1. **Modularity**: Components should be self-contained and independently useful
2. **XDG Compliance**: Follow XDG Base Directory specification when possible
3. **Consistency**: Maintain consistent style and approach across configurations
4. **Performance**: Optimize for speed and responsiveness
5. **Documentation**: All components should be well-documented
6. **Testability**: Components should be testable in isolation
7. **Security**: Follow security best practices throughout

## System Requirements

- **Operating System**: macOS 13 (Ventura) or newer
- **Architecture**: Apple Silicon (M1/M2/M3 series)
- **Shell**: Zsh 5.9 or newer
- **Package Manager**: Homebrew 4.0 or newer
- **Git**: 2.40 or newer
- **Node.js**: 20.0 or newer (for TypeScript SDK)
- **Python**: 3.10 or newer

## Component Specifications

### AI Integration System

The AI integration system provides a unified interface for leveraging large language models in various development workflows.

#### Core Components

- **Configuration System**:
  - Central YAML configuration (`config/ai/core/config.yaml`)
  - Provider-specific settings
  - Task-specific model recommendations
  - Extensible parameter definitions

- **TypeScript Library**:
  - Type definitions (`config/ai/core/types.ts`)
  - Utility functions (`config/ai/core/utils.ts`)
  - Client abstractions for different providers
  - Error handling and retry mechanisms

- **Prompt System**:
  - Base templates in `config/ai/prompts/base/`
  - Language-specific templates in `config/ai/prompts/code/`
  - Task-specific templates for common operations
  - Variable substitution system for dynamic content

- **Tool Integrations**:
  - Git hooks for commit message generation and validation
  - IDE plugins for VS Code, Neovim, and Cursor
  - API utilities for OpenAPI tooling
  - Shell integrations for command-line access

#### Interfaces

- **CLI Interface**:
  - Just recipes for common operations
  - Consistent command structure
  - Well-documented help system
  - Colorized output

- **TypeScript SDK**:
  - Programmatic access to all functionality
  - Strongly typed interfaces
  - Promise-based async operations
  - Comprehensive error handling

- **Shell Integration**:
  - Bash/Zsh function library
  - Environment variable configuration
  - Command-line completion
  - History integration

#### Provider Support

| Provider | Models | Features | Limitations |
|----------|--------|----------|-------------|
| Anthropic | Claude 3 Opus, Sonnet, Haiku | Robust reasoning, code generation, long contexts | No function calling |
| OpenAI | GPT-4, GPT-3.5 | Function calling, tool use, fine-tuning | Cost, rate limits |
| Google | Gemini Pro, Ultra | Multimodal, large context window | Newer ecosystem |
| Local | Mixtral, Llama 3 | Privacy, no cost, offline usage | Limited capabilities |

### API Utilities

The API namespace provides tools for OpenAPI development, client generation, and testing.

#### OpenAPI Tools

- **Validation**: Spectral and Optic for schema validation
- **Proxying**: API proxying for testing and debugging
- **Mocking**: Mock servers for API testing
- **Client Generation**: TypeScript and Python client generation

#### HTTP Testing

- **Request Building**: Templates for common request types
- **Response Validation**: Schema validation for responses
- **Collection Management**: Bruno file generation for API collections
- **Environment Management**: Support for different environments

### TypeScript SDK

The TypeScript SDK provides programmatic access to all dotfiles functionality.

#### Features

- **Command Execution**: Run just commands programmatically
- **Configuration Management**: Read and update configurations
- **Tool Integration**: Interface with external tools
- **AI Operations**: Access to all AI functionality
- **Type Safety**: Comprehensive TypeScript types

#### Architecture

- **Core Library**: Base functionality and utilities
- **Provider Modules**: Provider-specific implementations
- **Command Modules**: Just command wrappers
- **API Clients**: Generated OpenAPI clients

## API Specifications

### REST API

The REST API implementation follows OpenAPI 3.1 specifications and includes:

- **Server Implementation**:
  - Hono.js for API routing
  - Zod for request/response validation
  - TypeScript 5.5.0 for type safety
  - Vitest for testing

- **Client Libraries**:
  - TypeScript client with Tanstack Query
  - Python 3.12.4 client
  - React components for UI integration

### Schema Definitions

All API schemas follow these conventions:

- **Naming**: PascalCase for schemas, camelCase for properties
- **Documentation**: All schemas and properties must be documented
- **Validation**: Zod schemas for runtime validation
- **Types**: TypeScript interfaces for compile-time safety

## Style Guide

### Code Style

- **TypeScript/JavaScript**:
  - 2 space indentation
  - Semicolons required
  - Single quotes for strings
  - Trailing commas in multiline
  - ESLint + Prettier configuration

- **Python**:
  - 4 space indentation
  - PEP 8 compliance
  - Type hints required
  - Ruff for linting
  - Black for formatting

- **Shell**:
  - 2 space indentation
  - ShellCheck compliance
  - Function documentation
  - Error handling required

### Commit Format

All commits must follow the Conventional Commits specification:

```
<type>(<scope>): <description>

[optional body]

[optional footer(s)]
```

Types include: feat, fix, docs, style, refactor, perf, test, build, ci, chore

## Testing Requirements

### Unit Tests

- **TypeScript**: Vitest for unit testing
- **Python**: pytest for unit testing
- **Coverage**: 80% minimum for new code

### Integration Tests

- **API**: MSW for mock service workers
- **CLI**: Tape for command-line interface testing
- **Configuration**: Validation tests for all configurations

## Documentation Standards

### Component Documentation

- **README.md**: Every component must have a README.md file
- **Examples**: Include example usage for all functionality
- **API Reference**: Document all public functions and types
- **Installation**: Include installation instructions if applicable

### Code Documentation

- **TypeScript**: JSDoc for all public functions and types
- **Python**: Google-style docstrings for all functions and classes
- **Shell**: Comment blocks for functions and sections

## Installation Process

The installation process follows these steps:

1. **Environment Check**: Verify system requirements
2. **Directory Setup**: Create necessary directories
3. **Configuration Symlinks**: Create symlinks for all configurations
4. **Package Installation**: Install required packages via Homebrew
5. **Post-Installation**: Run any necessary post-installation tasks
6. **Validation**: Verify installation was successful

## Version Management

- **Semantic Versioning**: Follow SemVer for release versioning
- **Change Log**: Maintain CHANGELOG.md with all changes
- **Release Tagging**: Tag all releases in Git
- **Version File**: Store current version in VERSION.md
# AI Prompts Master Collection Structure

This document provides an overview of the organization and content of the AI Prompts Master Collection.

## Directory Structure

```
AI_Prompts_Master_Collection/
├── Core_Prompting/
│   ├── Base_Templates/
│   │   ├── code-generation-template.md
│   │   └── master-configuration.md
│   ├── Best_Practices/
│   │   ├── advanced-techniques.md
│   │   └── prompt-engineering-fundamentals.md
│   ├── System_Roles/
│   │   └── base-system-role.md
│   └── README.md
├── Provider_Specific/
│   ├── Claude/
│   │   ├── claude-code-repository-guidance.md
│   │   ├── claude_desktop_config.json
│   │   └── README.md
│   ├── OpenAI/
│   ├── Gemini/
│   ├── Cursor/
│   └── README.md
├── Use_Cases/
│   ├── Code_Generation/
│   │   ├── python-code-generation.md
│   │   └── typescript-code-generation.md
│   ├── Documentation/
│   ├── Research/
│   ├── Data_Analysis/
│   ├── Workflow_Automation/
│   └── README.md
├── Templates_Components/
│   ├── developer-profile.md
│   ├── task-templates.md
│   ├── workflow-integration.md
│   └── README.md
├── Tools_Integration/
│   └── README.md
├── Documentation_Reference/
│   ├── structure-overview.md
│   └── README.md
├── Archive/
│   ├── Duplicates/
│   ├── Low_Value/
│   ├── Merged_Originals/
│   └── README.md
└── README.md
```

## Key Components

### Core Prompting
Contains fundamental prompting techniques and templates that establish the foundation for effective AI interactions:

- **Base Templates**: Generic templates adaptable for various purposes
- **System Roles**: Definitions that shape AI behavior and capabilities
- **Best Practices**: Guidelines and principles for effective prompt engineering

### Provider Specific
Contains configurations and templates optimized for specific AI providers:

- **Claude**: Anthropic Claude-specific configurations and optimizations
- **OpenAI**: GPT-specific templates and best practices
- **Gemini**: Google Gemini-specific templates
- **Cursor**: Cursor IDE AI integration configurations

### Use Cases
Contains domain-specific prompt templates organized by purpose:

- **Code Generation**: Templates for generating different types of code
- **Documentation**: Prompts for creating technical documentation
- **Research**: Templates for research synthesis and analysis
- **Data Analysis**: Prompts for data exploration and interpretation
- **Workflow Automation**: Templates for automating development workflows

### Templates Components
Contains reusable prompt fragments and formatting templates:

- **Developer Profile**: Personal context and technical preferences
- **Task Templates**: Specific templates for different task types
- **Workflow Integration**: Guides for incorporating AI into development workflows

### Tools Integration
Contains resources for integrating AI with development tools and environments:

- CLI tools and configurations
- IDE integration settings
- Workflow automation scripts

### Documentation Reference
Contains guides, specifications, and best practices related to AI prompting:

- **Structure Overview**: Documentation of the collection's organization
- Various guides and reference materials

### Archive
Contains archived content that has been superseded:

- **Duplicates**: Files with content duplicated elsewhere
- **Low Value**: Files that provide minimal unique value
- **Merged Originals**: Original files that have been consolidated

## Usage Guide

To effectively use this collection:

1. Start with the appropriate base template from **Core_Prompting/Base_Templates/**
2. Add relevant context from **Templates_Components/developer-profile.md**
3. Select a task-specific template from **Use_Cases/** based on your needs
4. Consider provider-specific optimizations from **Provider_Specific/** if using a particular AI model
5. Integrate additional components as needed

Example workflow for generating TypeScript code:
1. Start with **Core_Prompting/Base_Templates/code-generation-template.md**
2. Add relevant sections from **Templates_Components/developer-profile.md**
3. Incorporate specific guidelines from **Use_Cases/Code_Generation/typescript-code-generation.md**
4. If using Claude, add any necessary context from **Provider_Specific/Claude/** files
# Style Guide for Dotfiles Repository

This document establishes the consistent style, language, and design principles to be applied across all dotfiles configuration and documentation. It serves as a formal specification for maintaining consistency when modifying or extending the repository.

## Language and Terminology

### General Principles

1. **Clarity**: Use clear, direct language without unnecessary jargon
2. **Consistency**: Maintain consistent terminology across all files
3. **Precision**: Be specific about requirements and implementations
4. **Formality**: Use a professional, semi-formal tone in documentation
5. **Conciseness**: Be brief but complete, avoiding redundancy

### Standard Terminology

| Term | Definition | Usage |
|------|------------|-------|
| Tokyo Night | The primary color theme used across all tools | "Tokyo Night theme for consistent styling" |
| XDG | XDG Base Directory Specification | "Follow XDG compliance for config locations" |
| Apple Silicon | ARM-based Mac processors | "Optimized for Apple Silicon performance" |
| Dotfiles | Configuration files stored in version control | "Manage dotfiles with symlinks" |
| Symlink | Symbolic link to configuration files | "Symlink entire directories when possible" |
| Idempotent | Can be run multiple times without side effects | "Idempotent setup script" |

## File Structure and Organization

### Configuration Files

1. **Header Comments**: Begin each configuration file with a descriptive header
   ```lua
   -- Tool Configuration
   -- Description: Brief description of what this configures
   -- Last Updated: YYYY-MM-DD
   ```

2. **Section Organization**: Organize configurations into logical sections
   ```toml
   # Section 1: Core Settings
   setting1 = value1

   # Section 2: Feature Settings
   setting2 = value2
   ```

3. **Comments**: Add comments for non-obvious settings
   ```bash
   # Use 10ms timeout for faster prompt rendering
   scan_timeout = 10
   ```

### Documentation Files

1. **README.md Structure**:
   - Title and brief description
   - Features section with bullet points
   - Installation instructions
   - Configuration explanation
   - Usage examples
   - Customization options
   - Resources/References

2. **PRD.md Structure**:
   - Core Requirements (categorized)
   - Feature Requirements
   - Configuration Categories
   - Key Mappings (if applicable)
   - Configuration Principles
   - Customization Areas

3. **VERSION.md Structure**:
   - Version Information
   - Dependencies
   - Configuration Health
   - Debug Information
   - Command Line Tools
   - Just Integration
   - Recent Changes

## Coding Style

### Lua

1. **Indentation**: 2 spaces
2. **Line Length**: 100 characters maximum
3. **String Quotes**: Single quotes for strings
4. **Tables**: Use explicit table constructors with proper spacing
   ```lua
   local config = {
     option1 = 'value1',
     option2 = 'value2',
   }
   ```
5. **Function Style**: Prefer function expressions over declarations
   ```lua
   local function my_function(arg)
     -- implementation
   end
   ```
6. **Comments**: Use -- for comments, --- for documentation comments

### Shell Scripts

1. **Indentation**: 2 spaces
2. **Line Length**: 80 characters when possible
3. **String Quotes**: Double quotes for strings with variables, single quotes otherwise
4. **Function Style**: Use function keyword and braces on same line
   ```bash
   function my_function() {
     # implementation
   }
   ```
5. **Variable References**: Use `${variable}` style for clarity
6. **Error Handling**: Include error handling with set -e or trap
7. **Comments**: Use # for comments, add section separators with #####

### TOML

1. **Indentation**: 2 spaces
2. **Section Order**: Core settings first, then feature-specific sections
3. **Comments**: Use # for comments, add section separators
4. **Arrays**: One item per line for multi-line arrays
   ```toml
   array = [
     "item1",
     "item2",
     "item3",
   ]
   ```
5. **Tables**: Group related settings in tables

## Design Principles

These core design principles should be reflected across all configurations:

1. **Minimalism**: Keep configurations simple and focused
2. **Performance**: Optimize for speed and responsiveness
3. **Consistency**: Maintain consistent style and behaviors across tools
4. **Modularity**: Enable easy addition/removal of specific configurations
5. **Discoverability**: Make features and commands easy to discover
6. **Documentation**: Document everything thoroughly
7. **Maintainability**: Code should be clean and well-organized

## Visual Design

### Color Scheme

The Tokyo Night theme palette should be used consistently:

```toml
# TokyoNight Colors
bg = "#1a1b26"       # Background
fg = "#c0caf5"       # Foreground
black = "#15161e"    # Black
red = "#f7768e"      # Red
green = "#9ece6a"    # Green
yellow = "#e0af68"   # Yellow
blue = "#7aa2f7"     # Blue
magenta = "#bb9af7"  # Magenta
cyan = "#7dcfff"     # Cyan
white = "#a9b1d6"    # White
```

### Typography

1. **Terminal Font**: Nerd Font compatible monospace font
2. **Documentation Headers**: Use Markdown ATX headers (# for h1, ## for h2, etc.)
3. **Lists**: Use - for unordered lists, 1. for ordered lists
4. **Emphasis**: Use **bold** for important items, *italic* for emphasis
5. **Code Blocks**: Use triple backticks with language specification

### Icons and Symbols

1. **Success**: ✅ or ✓
2. **Failure/Warning**: ❌ or ⚠️
3. **Information**: ℹ️ or 🛈
4. **Note**: 📝
5. **Important**: ❗
6. **Link**: 🔗
7. **Settings**: ⚙️
8. **Tool**: 🛠️
9. **Theme**: 🎨
10. **Time**: ⏱️

## Version Control

### Commit Messages

1. **Structure**:
   ```
   Subject line (50 chars max)

   Detailed explanation of what and why (not how)
   - Bullet points for multiple changes
   - Keep lines to 72 characters

   🤖 Generated with [Claude Code](https://claude.ai/code)
   Co-Authored-By: Claude <noreply@anthropic.com>
   ```

2. **Subject Line Style**:
   - Use imperative mood ("Add feature" not "Added feature")
   - Start with capital letter
   - No period at the end
   - Keep under 50 characters
   - Be specific about what changed

3. **Body Style**:
   - Explain what and why, not how
   - Use bullet points for multiple changes
   - Separate paragraphs with blank line
   - Reference issues if applicable

### Branch Names

1. **Feature Branches**: `feature/descriptive-name`
2. **Bug Fixes**: `fix/descriptive-name`
3. **Documentation**: `docs/descriptive-name`
4. **Refactoring**: `refactor/descriptive-name`

## Tool-Specific Guidelines

### Neovim

- Use LazyVim framework
- Organize plugins into logical categories
- Use Space as leader key
- Keep startup time below 100ms
- Follow Vim conventions for key mappings

### ZSH

- Keep startup time below 200ms
- Organize aliases by category with comments
- Use descriptive function names
- Follow consistent prompt style
- Keep shell scripts POSIX-compatible when possible

### Yazi

- Use Vim-style navigation
- Optimize worker counts for Apple Silicon
- Set file preview limits for performance
- Organize by file type
- Use consistent key mappings

### Aerospace

- Use Alt as primary modifier key
- Follow Vim-style directional navigation
- Use service mode for special operations
- Maintain consistent window gaps
- Keep workspace count at 5

### Starship

- Keep scan timeout at 10ms
- Place time and battery on right side
- Use two-line prompt with clean separation
- Show relevant language versions
- Use consistent icons for all sections

## Documentation Guidelines

1. **README Files**:
   - Focus on user-oriented information
   - Include installation, usage, customization
   - Show examples for common operations
   - Keep language simple and direct

2. **PRD Files**:
   - Focus on requirements and design decisions
   - Organize into logical categories
   - Include rationale for design choices
   - Be specific about implementation details

3. **VERSION Files**:
   - Focus on current state and dependencies
   - Include exact version numbers
   - Document any health issues
   - List recent changes

## Justfile Guidelines

1. **Recipe Naming**:
   - Use kebab-case for recipe names
   - Group related commands with common prefix
   - Use descriptive, action-oriented names
   - Keep names concise but clear

2. **Recipe Documentation**:
   - Add comments above each recipe
   - Explain purpose and any side effects
   - Document any required parameters
   - Group related recipes with section comments

3. **Recipe Implementation**:
   - Use @echo for user feedback
   - Use conditional execution when appropriate
   - Capture errors and provide feedback
   - Use variables for repeated values

## Adherence to This Guide

This style guide should be followed for all new additions and modifications to the dotfiles repository. When working with existing files, gradually update them to match this guide's specifications.

For automatic verification, consider using:
- `stylua` for Lua formatting
- `shfmt` for shell script formatting
- `markdownlint` for Markdown formatting
- `toml-lint` for TOML validation

When using LLMs to generate or modify files, reference this guide to ensure consistency.
# Consolidated Guide: Anterior Platform and Development Ecosystem

This guide consolidates information from various documents related to the Anterior Platform's architecture, development practices, and tooling.

**Part 1: The Anterior Platform**

## 1. Introduction & Core Principles

The Anterior Platform is built upon a microservices architecture, clearly separating backend services from frontend applications. Backend services are typically housed in the `platform` repository, while frontend UIs (React/TypeScript) reside in the `vibes` repository. Communication is facilitated through the Noggin gateway.

A core principle in the platform's development is a structured code generation pipeline:

**`schemas → codegen → gen → lib → app`**

-   **`schemas/`**: Language-agnostic specifications (Protobuf, OpenAPI, JSON-Schema) serve as the single source of truth.
-   **`codegen/`**: Contains containerized scripts that read from `schemas/` and write language-specific artifacts to `gen/`.
-   **`gen/`**: Stores the generated artifacts (Go structs, Pydantic models, TypeScript types, etc.), which are checked into version control.
-   **`lib/{go,ts,python}/`**: Hand-written shared libraries that provide ergonomic wrappers and business logic on top of the generated code.
-   **Application Layers**: Services, gateways, workflows, and frontends consume artifacts from both `gen/` and `lib/`.

This approach ensures consistency and reduces boilerplate code across the platform.

## 2. Authentication System

The Anterior platform implements a robust multi-step authentication flow designed for security and multi-tenant isolation.

### Overview & Flow

1.  **Login Form**: Users enter credentials (email/password) via static HTML pages (e.g., `/auth/login.html`).
2.  **OTP Verification**: A One-Time Password (OTP) is generated and sent to the user. The user verifies this on a dedicated page (e.g., `/auth/otp-verify.html`).
3.  **JWT Tokens**: Upon successful OTP verification, JWT access and refresh tokens are issued.
4.  **Cookie-Based Auth**: These tokens are stored in HTTP-only cookies.
    * `access_token`: Short-lived (15 minutes), e.g., `access_token=<JWT>; Path=/; HttpOnly; SameSite=Lax; Max-Age=900`.
    * `refresh_token`: Long-lived (30 days), e.g., `refresh_token=<JWT>; Path=/; HttpOnly; SameSite=Lax; Max-Age=43200`.
    * `auth_indicator`: A non-HttpOnly flag for frontend awareness, e.g., `auth_indicator=true; Path=/; SameSite=Lax; Max-Age=900`.
5.  **Enterprise/User Context**: The JWT carries enterprise and user information.

### JWT Structure and Context Extraction

JWTs contain crucial claims for identifying users and their enterprise/workspace affiliations.

**Example JWT Payload:**
```json
{
  "sub": "user123",
  "exp": 1683721200,
  "iat": 1683717600,
  "entr": {
    "enterprise456": "admin"
  },
  "wksp": {
    "workspace789": "enterprise456"
  }
}
sub: User's unique identifier.entr: Map of enterprise IDs to roles.wksp: Map of workspace IDs to their parent enterprise ID.The parseEnterpriseUserFromJwt function (typically in auth/jwt.ts) is responsible for extracting this context on the backend. It handles scenarios with single or multiple enterprise memberships, potentially using headers like X-Anterior-Enterprise-Id and X-Anterior-Workspace-Id for disambiguation.// Simplified concept from platform/gateways/noggin/src/auth/jwt.ts
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
  if (!ctx.jwtPayload?.entr) {
    throw new HTTPException(401, { message: "Authentication required" });
  }
  // ... logic to determine enterpriseUid, role, workspaceUid ...
  return {
    enterpriseUid,
    role,
    workspaceUid,
    userUid: ctx.jwtPayload.sub,
  };
}
API Authentication & Token RefreshAPI Requests: Cookies are automatically included in requests to the same domain, so frontend applications generally don't need to manage tokens manually for API calls. The credentials: 'include' fetch option is important.Token Refresh: A /auth/refresh endpoint is designed to use the refresh_token to issue new access_token and refresh_token pairs, extending user sessions.Frontend Integration Example// Check if user is authenticated
function isAuthenticated() {
  return document.cookie.includes('auth_indicator=true');
}

// Redirect to login if not authenticated
if (!isAuthenticated()) {
  const currentPath = encodeURIComponent(window.location.pathname);
  window.location.href = `/auth/login.html?redirectTo=${currentPath}`;
}

// Logout
function logout() {
  document.cookie = 'auth_indicator=; Path=/; Max-Age=0';
  // Also clear access_token and refresh_token cookies similarly
  window.location.href = '/auth/login.html';
}
Security ConsiderationsHTTP-Only Cookies: Protects tokens from XSS attacks.SameSite Policy: Restricts cookie usage.JWT Expiration: Short-lived access tokens limit the window of exposure.Enterprise Isolation: Data access is confined to authorized enterprises.CSRF Protection: Planned for future implementation.E2E Testing for AuthenticationThe platform includes a system for managing authentication in End-to-End (E2E) tests.AuthStateManager (auth-state.ts): Persists authenticated browser state (cookies, local storage) between test runs to avoid repeated logins. It saves state with an expiration time.// Conceptual usage of AuthStateManager
const authManager = new AuthStateManager();
if (authManager.isValid()) {
  // Load saved state into browser context
} else {
  // Perform login, then save state:
  // await login(page);
  // await authManager.save(page.context());
}
OTP Handling (otp-handler.ts): Provides secure ways to retrieve OTPs for tests (from environment variables, a test API endpoint, or a default OTP). It supports obfuscation in logs.// Conceptual OTP retrieval
async function getTestOtp(options: OtpHandlerOptions): Promise<string> {
  // Priority: Env Var -> API Endpoint -> Default OTP
  // ...
}
Test Utilities (test-utils.ts): Provides a login() function that automates the entire login flow (email/password, OTP submission) for tests. It can store the auth state for reuse.Test Fixtures: Playwright test fixtures (e.g., mockOtpEnv) are used to isolate environment changes for specific tests, preventing interference.Configuration: E2E tests use environment variables (e.g., ANT_TEST_USER_EMAIL, ANT_TEST_USER_OTP) for credentials and behavior.3. Noggin Gateway & Frontend-Backend IntegrationNoggin serves as the central API gateway and static application server, bridging frontend applications (from the vibes repository) and backend services (in the platform repository).Role of NogginAPI Gateway: Exposes backend service endpoints.Static Asset Server: Serves built frontend applications from S3.Authentication Hub: Centralizes authentication logic.Core Application Framework (@platform/lib/ts/lib-platform/src/app.ts)The App class is the backbone for backend services, built using Hono for HTTP routing and middleware.// Simplified from lib-platform/src/app.ts
export class App<TServiceMap extends ServiceMap, TAppContext extends object> {
  public readonly hono: Hono<{ Variables: TAppContext & BaseContext }>;
  public readonly logger: Logger;
  // ... other properties like tRPC, auth middleware

  constructor(
    public readonly baseCtx: TAppContext,
    public readonly options: AppOptions,
    public readonly auth?: (req: Request) => Promise<{ userUid: string; enterpriseUid: string } | Response>
  ) {
    this.hono = new Hono();
    // ... setup middleware, logging, error handling, CORS
  }

  endpoint(config: EndpointConfig) {
    // ... logic to define a type-safe Hono route with validation
  }
}
Static App Serving (createStaticApp in app.ts)This function is crucial for serving frontend applications from a blob store (like S3).export function createStaticApp({
  prefix, // URL prefix for the app, e.g., "/flonotes"
  blobs,  // ReadOnlyBlobStore instance (S3)
  index = "index.html", // Default file to serve
  env,    // Environment variables for substitution in files
  shouldGenerateCSP = false, // Content Security Policy flag
}: StaticAppOptions): Hono {
  const staticApp = new Hono();
  // ... logic to serve files from blob store, handle MIME types,
  // perform environment variable substitution (e.g., replacing $VAR_NAME),
  // and set CSP headers.
  return staticApp;
}
Noggin Backend StructureNoggin dynamically loads service-specific applications (each typically an app.ts defining Hono routes) from its subdirectories (e.g., src/pdfs/app.ts, src/auth/app.ts). The importAndMountNestedDirectoryApps function handles this, creating an API structure that mirrors the directory layout.Frontend DeploymentFrontend applications (e.g., FloNotes, FloPilot from the vibes repository) are typically:Built: Using Vite (npm run build).Configured: Environment variables like VITE_NOGGIN_HOST and BASE_PATH are set during the build or deployment script.Uploaded to S3: Scripts like deploy-local.sh sync the dist/ folder to an S3 bucket under a specific prefix (e.g., s3://local-bucket/vibes/flonotes/).# Example from deploy-local.sh
APP_PREFIX="vibes/flonotes"
BASE_PATH="/flonotes" # URL path
BUCKET_NAME="local-bucket"
LOCALSTACK_ENDPOINT="http://localhost:4566"

aws s3 sync dist/ "s3://$BUCKET_NAME/$APP_PREFIX/" \
  --endpoint-url "$LOCALSTACK_ENDPOINT" \
  --delete
URL Routing & API CommunicationServing Apps: When a user visits a URL like http://localhost:20701/flonotes, Noggin uses createStaticApp to fetch vibes/flonotes/index.html (and other assets) from S3 and serve it.API Calls: Frontend apps make API requests to Noggin (e.g., /pdfs/process-pdf). Noggin routes these to the appropriate backend service. Authentication cookies are automatically included if credentials: 'include' is used in the fetch request.Environment VariablesFrontend apps receive environment variables:Build-time: Injected by Vite (e.g., VITE_NOGGIN_HOST).Run-time (via Noggin): Noggin's createStaticApp can substitute placeholders (e.g., $KEY) in served files (like index.html) with values.Security ModelAuthentication: JWT tokens in HTTP-only cookies.Authorization: Enterprise and user-level permissions enforced by backend services.CORS: Configured in Noggin to control cross-origin access.CSP: Content Security Policy headers can be applied by createStaticApp.4. PDF Processing and Chat CapabilitiesThe Anterior platform provides advanced features for handling PDF documents, including processing, extraction, and AI-powered chat interactions with PDF content.Architecture Overview (Flopilot, Noggin Chat)Frontend (e.g., FloPilot, FloNotes): Handles PDF uploads, displays processed content, and facilitates chat.Noggin Gateway:Provides API endpoints for PDF operations (/pdfs/process-pdfs, /pdfs/pdf-status/:stemUid/:pdfUid, /pdfs/pdf-extracts/:stemUid/:pdfUid).Hosts chat backend logic (/chat).Backend Workflows: Asynchronous processes for PDF parsing, text extraction, and potentially OCR or AI analysis.S3 Storage: Stores raw PDFs and their structured JSON extracts.Key Classes/Controllers (Frontend - Conceptual from Flopilot Chat and PDF Architecture):classDiagram
    class FloPilotRoot {
        +render()
    }
    class PromptBox {
        +handleAttachClick()
        +handleSend()
    }
    class PdfProcessController {
        +processPdf(files)
        +handlePdfChange()
        +isPdfProcessingMap
    }
    class ChatController {
        +handleSubmit(message)
        +append(message)
    }
    FloPilotRoot --> PromptBox
    PromptBox --> PdfProcessController
    FloPilotRoot --> ChatController
Noggin PDF API Endpoints & Processing FlowUpload (POST /pdfs/process-pdfs or /process-pdfs depending on Noggin's internal routing):Frontend sends PDF file(s) via FormData.Noggin extracts enterpriseUid from JWT, generates stemUid (a grouping ID) and pdfUid(s).Raw PDF is stored in S3: stems/{enterpriseUid}/{stemUid}/{pdfUid}/raw.pdf.An asynchronous workflow is scheduled for processing.Returns { stemUid, pdfUids }.Status Check (GET /pdfs/pdf-status/:stemUid/:pdfUid or /pdf-status/...):Frontend polls this endpoint.Backend checks processing status (e.g., by looking for the existence/status of the extracts.json file or a database record).Returns { status: "processing" | "ready" | "error", message?: string }.Retrieve Extracts (GET /pdfs/pdf-extracts/:stemUid/:pdfUid or /pdf-extracts/...):Once status is "ready", frontend fetches the processed JSON data.Extracts are typically stored in S3: stems/{enterpriseUid}/{stemUid}/{pdfUid}/extracts.json.Example PDF Extracts JSON Structure:{
  "result": {
    "metadata": { "title": "Document Title", "pages": 5 },
    "pages": [
      {
        "pageNumber": 1,
        "blocks": [
          { "id": "block-1", "text": "Example text content", "boundingBox": { "x1": 100, ... } }
        ]
      }
# Prompt Engineering Guide

## Introduction to Prompt Engineering

Prompt engineering is the practice of designing effective inputs for large language models (LLMs) to produce desired outputs. It's an evolving discipline that combines elements of communication, programming, and psychology to reliably guide AI behavior.

## Core Principles for Beginners

### 1. Be Clear and Specific

- **State exactly what you want**: Define the task, expected format, and scope clearly
- **Avoid ambiguity**: Eliminate vague terms that could be interpreted multiple ways
- **Set boundaries**: Specify what the model should NOT do as well as what it should do

### 2. Provide Context

- **Establish background information**: Give relevant context the model needs
- **Define audience**: Specify who the response is for (experts, beginners, etc.)
- **State purpose**: Explain why you need this information and how it will be used

### 3. Use Examples (Few-Shot Learning)

- **Demonstrate desired outputs**: Show 1-3 examples of ideal responses
- **Illustrate edge cases**: Include examples of challenging scenarios
- **Maintain consistency**: Keep a similar structure across all examples

## Maximizing Accuracy

### 1. Knowledge Scaffolding

Break complex tasks into smaller steps:

```
1. First, identify the key facts in this medical case
2. Then, compare these facts to clinical guidelines
3. Finally, generate a recommendation based on this analysis
```

### 2. Chain-of-Thought Prompting

Ask the model to "think step by step" or "reason through this carefully":

```
Analyze this lab result by:
1. Identifying abnormal values
2. Explaining what each abnormality might indicate
3. Suggesting possible clinical implications
Think through each step carefully.
```

### 3. Self-Consistency Checks

Instruct the model to verify its own work:

```
After providing your analysis, review your reasoning for logical errors or unsupported claims.
```

## Improving Consistency

### 1. Use Clear Structural Templates

```
[TASK]: Summarize the following clinical note
[FORMAT]:
- Chief complaint:
- Key findings:
- Assessment:
- Plan:
[REQUIREMENTS]: Keep it under 100 words
[INPUT]: {text}
```

### 2. Control Parameters

- **Temperature**: Lower values (0.1-0.3) for factual, consistent responses
- **Response length**: Specify word/character count for consistent sizing
- **Sampling techniques**: Nucleus or top-k sampling for balanced control

### 3. Provide Evaluation Criteria

```
Your response will be evaluated on:
1. Factual accuracy (all claims must be supported by the document)
2. Completeness (all key information must be included)
3. Clarity (information must be organized logically)
```

## Enhancing Resiliency

### 1. Anticipate Variations

Prepare your prompt to handle different inputs:

```
If the input contains lab values, format them in a table.
If the input contains symptoms, list them in bullet points.
If the input is incomplete, identify what information is missing.
```

### 2. Error Handling

Instruct the model how to respond to ambiguity or insufficient information:

```
If you're uncertain about any aspect of your response, explicitly state your uncertainty.
If critical information is missing, indicate what additional data would be needed.
```

### 3. Guardrails and Constraints

Set clear boundaries for responses:

```
Only include information explicitly stated in the document. Do not introduce external information or assumptions.
```

## Getting Structured Responses

### 1. Define Output Format Explicitly

#### JSON Format

```
Return your analysis as a valid JSON object with the following structure:
{
  "diagnosis": string,
  "confidence": number (1-5),
  "supporting_evidence": string[],
  "alternative_explanations": string[]
}
```

#### XML Format

```
Format your response as XML using the following tags:
<assessment>
  <findings>List key findings here</findings>
  <diagnosis>Primary diagnosis here</diagnosis>
  <plan>Treatment plan here</plan>
</assessment>
```

#### Tabular Format

```
Present the medication information in a table with these columns:
- Medication name
- Dosage
- Frequency
- Purpose
- Side effects
```

### 2. Validation Instructions

```
Ensure your JSON response:
1. Contains all required fields
2. Uses the correct data types for each field
3. Has no extra fields not specified in the schema
```

### 3. Consistency Enforcement

```
Follow these rules consistently:
- Use present tense for all descriptions
- Express all measurements in metric units
- Format all dates as YYYY-MM-DD
```

## Maintaining Formal Structures

### 1. Schema Definitions

Define your structures using familiar formats:

```
Follow this JSON schema:
{
  "type": "object",
  "required": ["patient_id", "vital_signs", "assessment"],
  "properties": {
    "patient_id": { "type": "string" },
    "vital_signs": {
      "type": "object",
      "properties": {
        "temperature": { "type": "number" },
        "heart_rate": { "type": "number" },
        "blood_pressure": { "type": "string" }
      }
    },
    "assessment": { "type": "string" }
  }
}
```

### 2. Enumeration for Fixed Values

```
Use only these values for the 'severity' field:
- "mild"
- "moderate"
- "severe"
- "critical"
```

### 3. Versioning

```
This prompt uses response schema version 2.1.
Changes from v2.0:
- Added 'confidence_score' field
- 'complications' field is now optional
```

## Advanced Techniques

### 1. Role-Based Prompting

Assign a specific role to the model:

```
You are an experienced medical transcriptionist with specialty training in cardiology. Your task is to...
```

### 2. Multi-Step Workflows

Break complex tasks into stages:

```
This is a 3-step process:
STEP 1: Extract all medical terms from the text
STEP 2: Categorize each term as diagnosis, medication, or procedure
STEP 3: Generate a structured summary using these categorized terms
```

### 3. Reflection and Refinement

Have the model improve its own answers:

```
After generating your initial response, critique it for accuracy and completeness. Then provide an improved version addressing any deficiencies.
```

## Common Pitfalls to Avoid

- **Prompts that are too vague**: "Summarize this" vs. "Provide a 3-paragraph summary highlighting key diagnoses"
- **Contradictory instructions**: Asking for both brevity and exhaustive detail
- **Failing to specify constraints**: Not setting clear parameters on response length, format, or scope
- **Overly complex structures**: Creating schemas that are too detailed or nested
- **Ignoring edge cases**: Not considering how the prompt handles unusual inputs

## Testing and Iterating

The most effective prompts are developed through systematic testing:

1. Start with a basic version of your prompt
2. Test with diverse inputs
3. Identify failure modes and inconsistencies
4. Refine the prompt to address these issues
5. Document what works and why

## Examples for Different Use Cases

### Clinical Summarization

```
Summarize the following clinical note into a structured assessment:

[CLINICAL NOTE]
{text}
[END NOTE]

Format your response as:
## Patient Demographics
- Age, gender, relevant history

## Key Findings
- List major symptoms and test results

## Assessment & Plan
- Primary diagnosis
- Treatment recommendations

Keep your response concise, factual, and include only information present in the note.
```

### Medication Analysis

```
Review this medication list for potential issues:

[MEDICATIONS]
{medication_list}
[END MEDICATIONS]

Provide a structured analysis with:
1. Potential drug interactions (with severity rating)
2. Dosage concerns
3. Recommended monitoring
4. Alternatives to consider

Format as a bulleted list with clear headings. If you identify any critical issues, mark them with [URGENT].
```

### Healthcare Documentation

```
Convert this unstructured clinical observation into a properly formatted SOAP note:

[OBSERVATION]
{text}
[END OBSERVATION]

Follow this exact structure:
## Subjective
What the patient reports

## Objective
Measurable observations and test results

## Assessment
Clinical impressions and diagnoses

## Plan
Treatment recommendations and next steps

Maintain medical terminology where appropriate. Use concise, clear language. Include all relevant information from the source text.
```
# MCP

- https://github.com/punkpeye/awesome-mcp-servers
- mcp servers registries (desc sort : 1st is preferred)
    - https://www.pulsemcp.com/
    - https://glama.ai/mcp/servers
    - https://mcp.so/
    - [Awesome MCP Servers](https://mcpservers.org/)
    - [mcp-get](https://mcp-get.com/)
- claude <> sqlite db YT tutorial (https://www.youtube.com/watch?v=wxCCzo9dGj0)
# PostHog Integration

This document outlines the implementation approach, design decisions, and technical details for integrating PostHog analytics into the FloNotes and FloPilot applications.

## Overview

PostHog provides product analytics, session recording, feature flags, and A/B testing. Our implementation focuses on capturing key user events and behaviors to improve the product experience while maintaining minimal changes to the codebase.

## Implementation Approach

The implementation follows these key principles:

1. **Parse, Don't Validate**: We utilize Zod for type-safe schema parsing of environment variables, ensuring both runtime safety and proper TypeScript types.
2. **Progressive Enhancement**: Analytics is implemented in a way that doesn't block core application functionality.
3. **Minimal Changes**: Changes to existing code are kept minimal, focusing on key tracking points.
4. **Consistent Event Naming**: We use a consistent naming pattern for all tracked events.
5. **Non-intrusive Design**: Analytics code doesn't interfere with business logic.

## Environment Configuration

Environment variables are handled using Zod schemas:

```typescript
const envSchema = z.object({
  VITE_NOGGIN_HOST: z.string().url(),

  // PostHog settings (optional with defaults)
  VITE_POSTHOG_API_KEY: z.string().optional(),
  VITE_POSTHOG_HOST: z.string().url().optional().default("https://app.posthog.com"),
  VITE_POSTHOG_DEBUG: z.string().optional().transform(val => val === "true"),
});

export const env = envSchema.parse(import.meta.env);
```

This approach ensures:
- Environment variables are properly typed
- Required variables are present at runtime
- Default values are provided where appropriate
- Type information is available through inference

## Deployment Configuration

For different environments, the deployment scripts (`deploy-aws.sh` and `deploy-local.sh`) set appropriate PostHog keys:

```bash
# For AWS/production deployment
export VITE_POSTHOG_API_KEY="${VITE_POSTHOG_API_KEY:-ph_prod_key_placeholder}"
export VITE_POSTHOG_HOST="${VITE_POSTHOG_HOST:-https://app.posthog.com}"

# For local development
export VITE_POSTHOG_API_KEY="${VITE_POSTHOG_API_KEY:-ph_test_key_placeholder}"
export VITE_POSTHOG_HOST="${VITE_POSTHOG_HOST:-https://app.posthog.com}"
export VITE_POSTHOG_DEBUG="true"  # Enable debug mode for development
```

## Events Tracked

The implementation captures these core events:

| Event Name | Description | Key Properties |
|------------|-------------|---------------|
| `$pageview` | Tracks page/route changes | `path`, `url` |
| `template_reuse` | When a template is selected and run | `template_name`, `pdf_count`, `has_pdfs` |
| `query_run_without_template` | Query runs outside of template | `has_pdfs`, `pdf_count` |
| `flonote_created` | When a new FloNote is created | `template_name`, `has_template`, `pdf_count` |
| `flonote_pdf_added` | When PDFs are processed | `pdf_uid`, `stem_uid`, `extract_count` |
| `message_feedback` | Binary helpfulness rating | `message_id`, `rating`, `query`, `response_length` |
| `csat_rating` | 5-point satisfaction survey | `session_id`, `rating`, `feedback` |

## Component Implementation

### 1. PostHog Initialization

The initialization logic ensures PostHog is properly configured:

```typescript
export function initPostHog() {
  const apiKey = import.meta.env.VITE_POSTHOG_API_KEY;
  const host = import.meta.env.VITE_POSTHOG_HOST || 'https://app.posthog.com';
  const debug = import.meta.env.VITE_POSTHOG_DEBUG === 'true';

  // Only initialize if API key is provided
  if (apiKey) {
    posthog.init(apiKey, {
      api_host: host,
      capture_pageview: false, // We'll handle this with usePageTracking
      autocapture: false, // Disable autocapture for more control
      capture_pageleave: true,
      session_recording: {
        // Enable session recordings
        maskAllInputs: false, // Don't mask inputs
        maskTextSelector: '.ph-no-capture',
        maskInputSelector: '.ph-mask',
      },
      loaded: (ph) => {
        if (debug) {
          ph.debug();
          console.log('PostHog initialized:', ph);
        }
      },
    });
  } else if (debug) {
    console.warn('PostHog API key not provided. Analytics will not be captured.');
  }
}
```

### 2. User Identification & Tracking

A custom hook (`usePageTracking`) ensures consistent tracking across components:

```typescript
/**
 * Hook for tracking page views
 * @param currentPath The current route path
 */
export function usePageTracking(currentPath: string) {
  useEffect(() => {
    // Track page view when path changes
    if (import.meta.env.VITE_POSTHOG_API_KEY) {
      posthog.capture('$pageview', {
        path: currentPath,
        url: window.location.href,
      });
    }
  }, [currentPath]);
}
```

### 3. Template Usage Tracking

Templates are tagged with a `data-template-id` attribute:

```typescript
const setTemplateContent = (templateKey: keyof typeof templates) => {
  const template = templates[templateKey];
  if (template) {
    // Add data-template-id attribute for tracking purposes
    const contentWithTemplateId = template.content.replace(
      '<h1>',
      `<h1 data-template-id="${templateKey}">`
    );
    setContent(contentWithTemplateId);
  }
};
```

This allows event tracking without modifying the template's visual appearance:

```typescript
// Check if this is a template run
const isTemplateRun = currentContent.includes('data-template-id=');
let templateName = "custom";

// Extract template name if present
if (isTemplateRun) {
  const templateMatch = currentContent.match(/data-template-id="([^"]+)"/);
  if (templateMatch && templateMatch[1]) {
    templateName = templateMatch[1];
  }
}

// Track the appropriate event
if (isTemplateRun) {
  trackEvent('template_reuse', {
    template_name: templateName,
    has_pdfs: pdfs.length > 0,
    pdf_count: pdfs.length,
  });
} else {
  trackEvent('query_run_without_template', {
    has_pdfs: pdfs.length > 0,
    pdf_count: pdfs.length,
  });
}
```

### 4. PDF Processing Tracking

PDF processing events are captured when PDFs are added and processed:

```typescript
// Track PDF processing in PostHog
trackEvent('flonote_pdf_added', {
  pdf_uid: data.pdfUid,
  stem_uid: data.stemUid,
  extract_count: data.result.extracts?.length || 0,
});
```

### 5. Feedback Components

Two feedback components have been implemented:

**A. Helpfulness Rating**

A simple thumbs up/down UI that collects binary feedback on individual responses:

```tsx
export function HelpfulnessRating({ messageId, query, response }: HelpfulnessRatingProps) {
  const [rating, setRating] = useState<'helpful' | 'unhelpful' | null>(null);
  const [loading, setLoading] = useState(false);

  const handleRating = async (isHelpful: boolean) => {
    setLoading(true);

    try {
      const newRating = isHelpful ? 'helpful' : 'unhelpful';
      setRating(newRating);

      trackEvent('message_feedback', {
        message_id: messageId,
        rating: newRating,
        query,
        response_length: response.length,
      });
    } catch (error) {
      console.error('Error tracking feedback:', error);
    } finally {
      setLoading(false);
    }
  };

  // UI rendering...
}
```

**B. CSAT Survey**

A 5-point satisfaction rating with optional comments:

```tsx
export function CSATSurvey({ sessionId, onClose }: CSATSurveyProps) {
  const [selectedRating, setSelectedRating] = useState<number | null>(null);
  const [submitted, setSubmitted] = useState(false);
  const [feedback, setFeedback] = useState('');

  const handleSubmit = () => {
    if (selectedRating === null) return;

    trackEvent('csat_rating', {
      session_id: sessionId,
      rating: selectedRating,
      feedback,
    });

    setSubmitted(true);

    // Handle closing after delay...
  };

  // UI rendering...
}
```

## Implementation Details

### Files Modified

1. **use-process-pdf.ts** - Added PDF processing event tracking
2. **editor.tsx** - Added template tracking attributes
3. **use-editor-callbacks.ts → use-editor-callbacks-updated.ts** - Enhanced with event tracking
4. **menu-bar.tsx** - Updated import to use the updated callbacks
5. **env.ts** - Added PostHog environment variable definitions

### Files Added

1. **posthog.ts** - Core PostHog initialization and tracking functions
2. **helpfulness-rating.tsx** - Thumbs up/down feedback component
3. **csat-survey.tsx** - 5-point satisfaction survey component
4. **POSTHOG.md** - Implementation documentation
5. **POSTHOG_TEST_PLAN.md** - Testing guidelines and test cases

## Testing Approach

Testing the implementation involves:

1. **Local Development Testing**:
   - Using browser developer tools to verify events are sent
   - Checking PostHog debug mode in the console
   - Testing with placeholder API keys

2. **Production Verification**:
   - Verifying events in PostHog dashboard
   - Checking session recordings are properly captured
   - Ensuring user identities are correctly associated

3. **Comprehensive Testing Flows**:
   - Template selection and usage
   - PDF upload and processing
   - Feedback submission
   - Page navigation
   - Complete user journeys

Refer to the `POSTHOG_TEST_PLAN.md` document for detailed testing steps.

## Future Enhancements

Potential future enhancements include:

1. **Feature Flags**: Implementing PostHog feature flags for A/B testing
2. **Funnels Analysis**: Creating funnels to analyze conversion and drop-off points
3. **Retention Analysis**: Analyzing return user behavior
4. **Event Correlation**: Connecting events to understand user journeys
5. **User Segmentation**: Segmenting users based on behavior patterns
6. **Heatmaps**: Implementing UI interaction heatmaps for better UX insights

## Security & Privacy Considerations

The implementation includes these security measures:

1. **Input Masking**: Sensitive form fields can be marked with special classes for masking
2. **Custom Element Masking**: Supports additional masking via `maskTextSelector` and `maskInputSelector`
3. **Environment Separation**: Different PostHog projects for development vs. production
4. **Minimal Data Collection**: Only essential data is collected for each event
5. **Anonymous Data**: User-identifiable information is not collected by default
6. **Compliance**: Implementation follows GDPR and other privacy regulations

## Appendix: Event Property Reference

### Common Properties

These properties are included in most events where applicable:

- `userId`: User identifier (when available)
- `sessionId`: Current session identifier
- `appVersion`: Application version
- `environment`: Deployment environment (dev/prod)

### Event-Specific Properties

Each event type has specific properties as outlined in the Events Tracked section.
# PostHog Implementation Patch

This patch contains all changes needed to implement PostHog analytics in the FloNotes application.

## 1. Environment Configuration (env.ts)

```diff
@@ -1,11 +1,20 @@
 import { z } from "zod";

+/**
+ * Environment variable schema using Zod for validation
+ */
 const envSchema = z.object({
+  // Required environment variables
   VITE_NOGGIN_HOST: z.string().url(),
+
+  // PostHog settings (optional with defaults)
+  VITE_POSTHOG_API_KEY: z.string().optional(),
+  VITE_POSTHOG_HOST: z.string().url().optional().default("https://app.posthog.com"),
+  VITE_POSTHOG_DEBUG: z.string().optional().transform(val => val === "true"),
 });

-// 🚨 This will only run time validate, but not at build time.
-// Needs more work to make it do that, one attempt is writing
-// your own Vite plugin.
+// Validate environment variables at runtime
 export const env = envSchema.parse(import.meta.env);
+
+// Helper to check if PostHog is enabled
+export const isPostHogEnabled = () => !!env.VITE_POSTHOG_API_KEY;
```

## 2. PostHog Core Module (posthog.ts)

```diff
@@ -0,0 +1,71 @@
+import posthog from 'posthog-js';
+import { useEffect } from 'react';
+
+/**
+ * Initialize PostHog for analytics tracking
+ * This function should be called once at app startup
+ */
+export function initPostHog() {
+  const apiKey = import.meta.env.VITE_POSTHOG_API_KEY;
+  const host = import.meta.env.VITE_POSTHOG_HOST || 'https://app.posthog.com';
+  const debug = import.meta.env.VITE_POSTHOG_DEBUG === 'true';
+
+  // Only initialize if API key is provided
+  if (apiKey) {
+    posthog.init(apiKey, {
+      api_host: host,
+      capture_pageview: false, // We'll handle this with usePageTracking
+      autocapture: false, // Disable autocapture for more control
+      capture_pageleave: true,
+      session_recording: {
+        // Enable session recordings
+        maskAllInputs: false, // Don't mask inputs
+        maskTextSelector: '.ph-no-capture',
+        maskInputSelector: '.ph-mask',
+      },
+      // Identity verification config would go here for production
+      loaded: (ph) => {
+        if (debug) {
+          ph.debug();
+          console.log('PostHog initialized:', ph);
+        }
+      },
+    });
+  } else if (debug) {
+    console.warn('PostHog API key not provided. Analytics will not be captured.');
+  }
+}
+
+/**
+ * Track a custom event in PostHog
+ * @param eventName The name of the event to track
+ * @param properties Optional properties to include with the event
+ */
+export function trackEvent(eventName: string, properties?: Record<string, any>) {
+  if (import.meta.env.VITE_POSTHOG_API_KEY) {
+    posthog.capture(eventName, properties);
+  }
+}
+
+/**
+ * Identify a user in PostHog
+ * @param userId Unique identifier for the user
+ * @param properties Optional properties to associate with the user
+ */
+export function identifyUser(userId: string, properties?: Record<string, any>) {
+  if (import.meta.env.VITE_POSTHOG_API_KEY) {
+    posthog.identify(userId, properties);
+  }
+}
+
+/**
+ * Reset the current user identity (for logout)
+ */
+export function resetUser() {
+  if (import.meta.env.VITE_POSTHOG_API_KEY) {
+    posthog.reset();
+  }
+}
+
+/**
+ * Hook for tracking page views
+ * @param currentPath The current route path
+ */
+export function usePageTracking(currentPath: string) {
+  useEffect(() => {
+    // Track page view when path changes
+    if (import.meta.env.VITE_POSTHOG_API_KEY) {
+      posthog.capture('$pageview', {
+        path: currentPath,
+        url: window.location.href,
+      });
+    }
+  }, [currentPath]);
+}
+
+/**
+ * Set or update user properties (super properties)
+ * @param properties Properties to associate with the user
+ */
+export function setUserProperties(properties: Record<string, any>) {
+  if (import.meta.env.VITE_POSTHOG_API_KEY) {
+    posthog.register(properties);
+  }
+}
```

## 3. Application Entry Point (main.tsx)

```diff
@@ -1,19 +1,27 @@
 import { RouterProvider } from "@tanstack/react-router";
-import React from "react";
+import React, { useEffect } from "react";
 import ReactDOM from "react-dom/client";
 import "./index.css";

 import { createRouter } from "./router";
+import { initPostHog } from "./posthog";
+
+// Initialize PostHog
+initPostHog();

 // Set up a Router instance
 const router = createRouter();

+const App = () => {
+  return (
+    <React.StrictMode>
+      <RouterProvider router={router} />
+    </React.StrictMode>
+  );
+};
+
 const rootElement = document.getElementById("app")!;
 if (!rootElement.innerHTML) {
   const root = ReactDOM.createRoot(rootElement);
-  root.render(
-    <React.StrictMode>
-      <RouterProvider router={router} />
-    </React.StrictMode>,
-  );
-}
+  root.render(<App />);
+}
```

## 4. Router with Page Tracking (router.tsx)

```diff
@@ -1,27 +1,55 @@
 import { QueryClient, QueryClientProvider } from "@tanstack/react-query";
 import { createRouter as createTanStackRouter } from "@tanstack/react-router";
+import { useEffect } from "react";
 import { routeTree } from "./routeTree.gen";
+import { usePageTracking } from "./posthog";

 export const queryClient = new QueryClient();

+// PostHog page tracking wrapper component
+const PostHogTracker = ({ children }: { children: React.ReactNode }) => {
+  // Get current route from Tanstack Router
+  const path = window.location.pathname;
+
+  // Track page views with PostHog
+  usePageTracking(path);
+
+  // Listen for route changes
+  useEffect(() => {
+    const handleRouteChange = () => {
+      const newPath = window.location.pathname;
+      usePageTracking(newPath);
+    };
+
+    window.addEventListener('popstate', handleRouteChange);
+    return () => {
+      window.removeEventListener('popstate', handleRouteChange);
+    };
+  }, []);
+
+  return <>{children}</>;
+};
+
 // Set up a Router instance
 export function createRouter() {
   const router = createTanStackRouter({
     routeTree,
     defaultPreload: "intent",
     defaultPendingComponent: () => (
       <div className={`p-2 text-2xl`}>loading...</div>
     ),
     Wrap: ({ children }) => (
-      <QueryClientProvider client={queryClient}>{children}</QueryClientProvider>
+      <QueryClientProvider client={queryClient}>
+        <PostHogTracker>{children}</PostHogTracker>
+      </QueryClientProvider>
     ),
   });

   return router;
 }

 declare module "@tanstack/react-router" {
   interface Register {
     router: ReturnType<typeof createRouter>;
   }
-}
+}
```

## 5. Helpfulness Rating Component (components/feedback/helpfulness-rating.tsx)

```diff
@@ -0,0 +1,117 @@
+import { useState } from 'react';
+import { trackEvent } from '../../posthog';
+
+interface HelpfulnessRatingProps {
+  messageId: string;
+  query: string;
+  response: string;
+}
+
+/**
+ * A thumbs up/down component that allows users to rate the helpfulness of an AI response
+ * Sends the rating, query, and response to PostHog for analytics
+ */
+export function HelpfulnessRating({ messageId, query, response }: HelpfulnessRatingProps) {
+  const [rating, setRating] = useState<'helpful' | 'unhelpful' | null>(null);
+  const [loading, setLoading] = useState(false);
+
+  const handleRating = async (isHelpful: boolean) => {
+    setLoading(true);
+
+    try {
+      const newRating = isHelpful ? 'helpful' : 'unhelpful';
+      setRating(newRating);
+
+      // Track the feedback event in PostHog
+      trackEvent('message_feedback', {
+        message_id: messageId,
+        rating: newRating,
+        query,
+        response_length: response.length,
+      });
+    } catch (error) {
+      console.error('Error tracking feedback:', error);
+    } finally {
+      setLoading(false);
+    }
+  };
+
+  return (
+    <div className="flex items-center gap-2 mt-4">
+      <p className="text-sm text-gray-600 mr-2">
+        {rating === null ? 'Was this response helpful?' : 'Thanks for your feedback!'}
+      </p>
+
+      {rating === null && (
+        <>
+          <button
+            className="rounded-full p-2 hover:bg-green-100 hover:text-green-800"
+            onClick={() => handleRating(true)}
+            disabled={loading}
+            aria-label="Thumbs Up"
+          >
+            <ThumbsUpIcon className="w-4 h-4" />
+          </button>
+
+          <button
+            className="rounded-full p-2 hover:bg-red-100 hover:text-red-800"
+            onClick={() => handleRating(false)}
+            disabled={loading}
+            aria-label="Thumbs Down"
+          >
+            <ThumbsDownIcon className="w-4 h-4" />
+          </button>
+        </>
+      )}
+
+      {rating === 'helpful' && (
+        <span className="text-green-600">
+          <ThumbsUpIcon className="w-4 h-4" />
+        </span>
+      )}
+
+      {rating === 'unhelpful' && (
+        <span className="text-red-600">
+          <ThumbsDownIcon className="w-4 h-4" />
+        </span>
+      )}
+    </div>
+  );
+}
+
+// Simple thumbs up/down icons
+function ThumbsUpIcon({ className = "w-6 h-6" }) {
+  return (
+    <svg
+      xmlns="http://www.w3.org/2000/svg"
+      viewBox="0 0 24 24"
+      fill="none"
+      stroke="currentColor"
+      strokeWidth="2"
+      strokeLinecap="round"
+      strokeLinejoin="round"
+      className={className}
+    >
+      <path d="M7 10v12" />
+      <path d="M15 5.88 14 10h5.83a2 2 0 0 1 1.92 2.56l-2.33 8A2 2 0 0 1 17.5 22H4a2 2 0 0 1-2-2v-8a2 2 0 0 1 2-2h2.76a2 2 0 0 0 1.79-1.11L12 2h0a3.13 3.13 0 0 1 3 3.88Z" />
+    </svg>
+  );
+}
+
+function ThumbsDownIcon({ className = "w-6 h-6" }) {
+  return (
+    <svg
+      xmlns="http://www.w3.org/2000/svg"
+      viewBox="0 0 24 24"
+      fill="none"
+      stroke="currentColor"
+      strokeWidth="2"
+      strokeLinecap="round"
+      strokeLinejoin="round"
+      className={className}
+    >
+      <path d="M17 14V2" />
+      <path d="M9 18.12 10 14H4.17a2 2 0 0 1-1.92-2.56l2.33-8A2 2 0 0 1 6.5 2H20a2 2 0 0 1 2 2v8a2 2 0 0 1-2 2h-2.76a2 2 0 0 0-1.79 1.11L12 22h0a3.13 3.13 0 0 1-3-3.88Z" />
+    </svg>
+  );
+}
```

## 6. CSAT Survey Component (components/feedback/csat-survey.tsx)

```diff
@@ -0,0 +1,99 @@
+import { useState } from 'react';
+import { trackEvent } from '../../posthog';
+
+interface CSATSurveyProps {
+  sessionId: string;
+  onClose?: () => void;
+}
+
+/**
+ * A Customer Satisfaction (CSAT) survey component with a 5-point rating scale
+ * Tracks user satisfaction with PostHog events
+ */
+export function CSATSurvey({ sessionId, onClose }: CSATSurveyProps) {
+  const [selectedRating, setSelectedRating] = useState<number | null>(null);
+  const [submitted, setSubmitted] = useState(false);
+  const [feedback, setFeedback] = useState('');
+
+  const handleRatingSelect = (rating: number) => {
+    setSelectedRating(rating);
+  };
+
+  const handleSubmit = () => {
+    if (selectedRating === null) return;
+
+    // Track the CSAT event in PostHog
+    trackEvent('csat_rating', {
+      session_id: sessionId,
+      rating: selectedRating,
+      feedback: feedback,
+    });
+
+    setSubmitted(true);
+
+    // Call onClose after a delay if provided
+    if (onClose) {
+      setTimeout(onClose, 2000);
+    }
+  };
+
+  if (submitted) {
+    return (
+      <div className="bg-white p-4 rounded-lg shadow border border-gray-200 max-w-md">
+        <h3 className="font-medium text-lg mb-2">Thank you for your feedback!</h3>
+        <p className="text-gray-600 text-sm">Your feedback helps us improve our service.</p>
+      </div>
+    );
+  }
+
+  return (
+    <div className="bg-white p-4 rounded-lg shadow border border-gray-200 max-w-md">
+      <h3 className="font-medium text-lg mb-2">How would you rate your experience?</h3>
+
+      <div className="flex justify-between mb-4 py-2">
+        {[1, 2, 3, 4, 5].map((rating) => (
+          <button
+            key={rating}
+            className={`flex flex-col items-center justify-center w-12 h-12 rounded-full
+              ${selectedRating === rating
+                ? 'bg-anterior-500 text-white'
+                : 'bg-gray-100 hover:bg-gray-200 text-gray-700'}`}
+            onClick={() => handleRatingSelect(rating)}
+            aria-label={`Rating ${rating}`}
+          >
+            <span className="text-lg font-medium">{rating}</span>
+          </button>
+        ))}
+      </div>
+
+      <div className="flex flex-col gap-2 mb-4">
+        <label className="text-sm text-gray-600">Any additional feedback?</label>
+        <textarea
+          className="w-full px-3 py-2 border border-gray-300 rounded-md focus:outline-none focus:ring-1 focus:ring-anterior-500"
+          rows={3}
+          placeholder="Tell us more about your experience..."
+          value={feedback}
+          onChange={(e) => setFeedback(e.target.value)}
+        />
+      </div>
+
+      <div className="flex justify-end">
+        <button
+          className={`px-4 py-2 rounded-md
+            ${selectedRating === null
+              ? 'bg-gray-300 text-gray-500 cursor-not-allowed'
+              : 'bg-anterior-500 text-white hover:bg-anterior-600'}`}
+          onClick={handleSubmit}
+          disabled={selectedRating === null}
+        >
+          Submit Feedback
+        </button>
+      </div>
+
+      <div className="flex justify-between mt-2">
+        <span className="text-sm text-gray-500">Very dissatisfied</span>
+        <span className="text-sm text-gray-500">Very satisfied</span>
+      </div>
+    </div>
+  );
+}
```

## 7. PDF Process Tracking (hooks/use-process-pdf.ts)

```diff
@@ -1,6 +1,7 @@
 import { useMutation } from "@tanstack/react-query";
 import { savePdf, updatePdfExtracts } from "./use-pdf-storage";
 import { processAndGetPdfExtracts } from "../services/noggin-api";
+import { trackEvent } from "../posthog";

 export const processPdfsMutationKey = [["pdf", "processPdfs"]];

@@ -73,6 +74,14 @@ export function useProcessPdfs() {
           data.stemUid,
           data.pdfUid,
         );
+
+        // Track PDF processing in PostHog
+        trackEvent('flonote_pdf_added', {
+          pdf_uid: data.pdfUid,
+          stem_uid: data.stemUid,
+          extract_count: data.result.extracts?.length || 0,
+        });
       }
     },
```

## 8. Template Tracking in Editor (components/shell/notes/editor.tsx)

```diff
@@ -52,7 +52,10 @@ const Tiptap = () => {
   const setTemplateContent = (templateKey: keyof typeof templates) => {
     const template = templates[templateKey];
     if (template) {
-      setContent(template.content);
+      // Add data-template-id attribute to the content for tracking purposes
+      const contentWithTemplateId = template.content.replace('<h1>', `<h1 data-template-id="${templateKey}">`);
+      setContent(contentWithTemplateId);
     }
   };
```

## 9. Editor Callbacks with Tracking (components/shell/notes/hooks/use-editor-callbacks-updated.ts)

This file is a copy of use-editor-callbacks.ts with added tracking.

```diff
@@ -0,0 +1,265 @@
+import { Editor } from "@tiptap/react";
+import { useCallback } from "react";
+import { replaceInsertInfoSpans } from "../utils/editor-utils";
+import { db } from "../../../../indexed-db/db";
+import { useLiveQuery } from "dexie-react-hooks";
+import { useMutation } from "@tanstack/react-query";
+import { createAndWaitForNote } from "../../../../services/noggin-api";
+import { trackEvent } from "../../../../posthog";
+
+/**
+ * Sanitizes HTML responses from the LLM to ensure they're compatible with TipTap
+ *
+ * This handles different formats of HTML that might be returned and extracts
+ * the relevant content for insertion into the editor.
+ */
+const sanitizeHtml = (html: string): string => {
+  // Skip sanitization if html is not a string
+  if (typeof html !== "string") {
+    console.error("Invalid HTML received:", html);
+    return "<p>Error: Invalid response from LLM service</p>";
+  }
+
+  // Sometimes the API returns HTML without proper document structure
+  // Ensure we have proper HTML structure
+  if (!html.includes("<html>") && !html.includes("<!DOCTYPE")) {
+    // If we just have fragments, wrap them properly
+    if (!html.includes("<body>")) {
+      // This appears to be just HTML fragments, not a complete document
+      return html;
+    }
+  }
+
+  // If we got a full HTML document, extract just the body content
+  const bodyMatch = html.match(/<body[^>]*>([\s\S]*?)<\/body>/i);
+  if (bodyMatch && bodyMatch[1]) {
+    return bodyMatch[1].trim();
+  }
+
+  return html;
+};
+
+export const useEditorCallbacks = (editor: Editor | null) => {
+  // Always use direct API calls with React Query
+  // This ensures we're using real hooks within the component
+  const noteMutation = useMutation({
+    // Use the updated function that handles polling automatically
+    mutationKey: ["llm.notes"],
+    mutationFn: createAndWaitForNote,
+  });
+
+  const pdfs =
+    useLiveQuery(() =>
+      db.pdfs.where("documentType").equals("clinical").toArray(),
+    ) ?? [];
+
+  const handleRunAutoFill = useCallback(async () => {
+    if (!editor) return;
+    const currentContent = editor.getHTML();
+    console.log("Current content before extraction:", currentContent);
+    const extractedContent = replaceInsertInfoSpans(currentContent);
+    console.log(
+      "Extracted content with replaced QuickFill tags:",
+      extractedContent,
+    );
+
+    // Check if this is a template run by looking for template markers in the HTML
+    const isTemplateRun = currentContent.includes('data-template-id=');
+    let templateName = "custom";
+
+    // Extract template name if it's a template run
+    if (isTemplateRun) {
+      const templateMatch = currentContent.match(/data-template-id="([^"]+)"/);
+      if (templateMatch && templateMatch[1]) {
+        templateName = templateMatch[1];
+      }
+    }
+
+    // Track the event in PostHog
+    if (isTemplateRun) {
+      trackEvent('template_reuse', {
+        template_name: templateName,
+        has_pdfs: pdfs.length > 0,
+        pdf_count: pdfs.length,
+      });
+    } else {
+      trackEvent('query_run_without_template', {
+        has_pdfs: pdfs.length > 0,
+        pdf_count: pdfs.length,
+      });
+    }
+
+    const extractsArray =
+      pdfs
+        .filter((pdf) => pdf && pdf.extracts)
+        .map((pdf) => {
+          return {
+            id: pdf.id.toString(),
+            extracts: pdf.extracts.result.extracts || [],
+          };
+        }) || [];
+
+    // Find the first PDF that has a stemUid from the server
+    const pdfWithStemUid = pdfs.find((pdf) => pdf.stemUid);
+
+    // Check if we have a server-provided stemUid
+    if (!pdfWithStemUid?.stemUid) {
+      console.error(
+        "No stemUid found in PDFs. Cannot create note without a server-assigned stemUid.",
+      );
+      alert(
+        "Error: Cannot create note without processing a PDF first. Please upload a PDF document.",
+      );
+      return;
+    }
+
+    const stemUid = pdfWithStemUid.stemUid;
+    console.log(
+      `Using server-assigned stemUid ${stemUid} from PDF for note creation`,
+    );
+
+    try {
+      console.log(
+        "Sending request to LLM with clinicals:",
+        extractsArray.length,
+        "stemUid:",
+        stemUid,
+      );
+
+      // This now handles the complete flow including polling
+      const noteContent = await noteMutation.mutateAsync({
+        stemUid,
+        clinicals: extractsArray,
+        messages: [],
+        userMessage: `Write a clinical note by:
+1. Using the following template structure: ${extractedContent}
+2. Replace each {{QuickFill}} placeholder with ONLY the specific information being requested based on context
+3. Each {{QuickFill}} replacement should be brief and focused only on what is asked for in the preceding label
+4. Maintain the exact HTML structure and formatting, unless the {{QuickFill}} replacement calls for an ordered or unordered list (<ol> or <ul>) with list items (<li>)
+5. For EACH piece of information, include a citation tag immediately after the text
+6. Format dates as MM/DD/YYYY
+7. Use consistent medical terminology following [specify standard, e.g. SNOMED CT]
+8. For missing or unclear data, write "[Not documented]"
+9. Flag any critical values with <critical></critical> tags
+10. Validate all numbers are within typical clinical ranges
+11. DO NOT include the entire content of the source - only extract the relevant information
+12. Keep your answers SPECIFIC and CONCISE for each field
+13. Checkbox should be filled in if the information is present from [] to [x], including a citation tag.
+14. Citation tags should exist within a <li> tag if the citation is relevant to a list item.
+
+Citation format:
+- Use <citation fileId="X" extractId="Y" blockIds="Z" /> tags after each piece of information
+- Each {{QuickFill}} should be replaced by ONLY the requested data with appropriate citation(s)
+- Include page numbers in the citation when available
+
+Example input:
+<p><strong>Address:</strong> {{QuickFill}}</p>
+
+Example output with exactly one citation:
+<p><strong>Address:</strong> 123 TOON AVENUE, TOONTOWN, 12345, ARIZONA <citation fileId="1" extractId="abc123" blockIds="1" /></p>
+
+Example input:
+<p><strong>Date of Birth:</strong> {{QuickFill}}</p>
+
+Example output with exactly two citations:
+<p><strong>Date of Birth:</strong> 03/12/1962 <citation fileId="1" extractId="def456" blockIds="1,2" /></p>
+
+Example input:
+<p><strong>ICD-10 Codes:</strong> {{QuickFill}}</p>
+
+Example output with ordered one citation and ordered list of values:
+<p>
+    All ICD-10 Codes: C23 <citation extractid="E1" fileid="1" blockids="7"></citation>
+    <ol>
+        <li>ICD 10 diagnoses: Malignant neoplasm of gallbladder (C23)</li>
+        <li>Unspecified asthma, uncomplicated (J45.909)</li>
+        <li>Type 2 diabetes mellitus without complications (E11.9)</li>
+        <li>Essential (primary) hypertension (I10)</li>
+        <li>Depression, unspecified (F32.A)</li>
+        <li>Bilateral primary osteoarthritis of hip (M16.0)</li>
+        <li>Hypothyroidism, unspecified (E03.9)</li>
+        <li>History of falling (Z91.81)</li>
+        <li>Long term (current) use of insulin (Z79.4)</li>
+        <li>Acquired absence of kidney (Z90.5)</li>
+    </ol>
+</p>
+
+Invalid outputs:
+- Adding extra HTML elements, unless the {{QuickFill}} replacement calls for an ordered or unordered list (<ol> or <ul>) with list items (<li>)
+- Changing template structure
+- Omitting citations
+- Free-form text outside template
+- Including information not specifically requested by the field label
+- Returning entire sections or paragraphs when only a specific data point is asked for`,
+      });
+
+      console.log(
+        "Received completed note content from LLM:",
+        typeof noteContent === "string"
+          ? noteContent.substring(0, 100) + "..."
+          : noteContent,
+      );
+
+      // Track FloNote creation event in PostHog
+      trackEvent('flonote_created', {
+        template_name: templateName,
+        has_pdfs: pdfs.length > 0,
+        pdf_count: pdfs.length,
+        has_template: isTemplateRun,
+      });
+
+      // Clean up the HTML - now we know we have a string from createAndWaitForNote
+      const cleanedHtml = sanitizeHtml(noteContent);
+      console.log("Cleaned HTML:", cleanedHtml.substring(0, 100) + "...");
+
+      // Force a complete content replacement
+      editor.commands.clearContent();
+      setTimeout(() => {
+        console.log("Setting editor content with cleaned response");
+        editor.commands.setContent(cleanedHtml);
+
+        // Validate content was set
+        setTimeout(() => {
+          const newContent = editor.getHTML();
+          console.log(
+            "New editor content after update:",
+            newContent.substring(0, 100) +
+              (newContent.length > 100 ? "..." : ""),
+          );
+
+          if (newContent.trim() === "" || newContent === "<p></p>") {
+            console.error(
+              "Editor content is still empty after update, falling back to direct HTML insertion",
+            );
+            // As a fallback, try a different approach to insert content
+            editor.commands.insertContent(cleanedHtml);
+          }
+        }, 100);
+      }, 100);
+    } catch (error) {
+      console.error("Error in handleRunAutoFill:", error);
+      // Show an error message to the user
+      alert(
+        `Error generating note: ${error instanceof Error ? error.message : "Unknown error"}`,
+      );
+    }
+  }, [editor, noteMutation, pdfs]);
```

## 10. Update Menu Bar to Use New Callbacks (components/shell/notes/components/menu/menu-bar.tsx)

```diff
@@ -12,7 +12,7 @@ import {
   DropdownMenuTrigger,
   DropdownMenuGroup,
 } from "../../../../../../neuron/react";
-import { useEditorCallbacks } from "../../hooks/use-editor-callbacks";
+import { useEditorCallbacks } from "../../hooks/use-editor-callbacks-updated";
 import { LoadingSpinner } from "../loading-spinner";

 interface MenuBarProps {
```

## 11. Update package.json to Add PostHog Dependency

```diff
@@ -54,6 +54,7 @@
     "framer-motion": "^12.0.6",
     "lodash": "^4.17.21",
+    "posthog-js": "^1.114.0",
     "react": "^18.3.1",
     "react-aria": "^3.37.0",
     "react-dom": "^18.3.1",
```

## 12. Deploy Script Updates (deploy-local.sh)

```diff
@@ -26,10 +27,16 @@ LOCALSTACK_ACCESS_KEY="000000000000"
 LOCALSTACK_SECRET_KEY="local-stack-accepts-anything-here"
 VITE_NOGGIN_HOST="http://localhost:20701"

+# PostHog configuration for local development
+VITE_POSTHOG_API_KEY="${VITE_POSTHOG_API_KEY:-ph_test_key_placeholder}"
+VITE_POSTHOG_HOST="${VITE_POSTHOG_HOST:-https://app.posthog.com}"
+VITE_POSTHOG_DEBUG="true"
+
 echo "Local deployment selected with Localstack endpoint: $LOCALSTACK_ENDPOINT"
 echo "Using VITE_NOGGIN_HOST: $VITE_NOGGIN_HOST"
+echo "Using PostHog settings:"
+echo "  VITE_POSTHOG_API_KEY: ${VITE_POSTHOG_API_KEY:0:5}... (truncated)"
+echo "  VITE_POSTHOG_HOST: $VITE_POSTHOG_HOST"
+echo "  VITE_POSTHOG_DEBUG: $VITE_POSTHOG_DEBUG"

 # ... More content follows ...
@@ -75,8 +82,14 @@ build_app() {
     # clean up the LOCAL build directory first
     echo "Cleaning previous build artifacts..."
     rm -rf dist node_modules/.vite
-
+
+    # Export environment variables for the build
     export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
+    export BASE_PATH="${BASE_PATH}"
+
+    # PostHog environment variables
+    export VITE_POSTHOG_API_KEY="${VITE_POSTHOG_API_KEY}"
+    export VITE_POSTHOG_HOST="${VITE_POSTHOG_HOST}"
+    export VITE_POSTHOG_DEBUG="${VITE_POSTHOG_DEBUG}"
```

## 13. Deploy Script Updates (deploy-aws.sh)

```diff
@@ -27,12 +27,21 @@ SSO_PROFILE="${1:-}" # TODO: add readme for getting an aws sso profile set up on
 BUCKET_NAME="${2:-}" # e.g. anterior-pr-3055-platform, even anterior-master-platform at some point
 VITE_NOGGIN_HOST="${3:-}" # e.g. https://labs-pr-3055-luhhr.anterior.app, https://labs.anterior.app at some point

+# PostHog configuration for production deployment
+VITE_POSTHOG_API_KEY="${VITE_POSTHOG_API_KEY:-ph_prod_key_placeholder}"
+VITE_POSTHOG_HOST="${VITE_POSTHOG_HOST:-https://app.posthog.com}"
+# Disable debug mode for production
+VITE_POSTHOG_DEBUG="false"
+
 if [ -z "$SSO_PROFILE" ] || [ -z "$BUCKET_NAME" ] || [ -z "$VITE_NOGGIN_HOST" ]; then
     echo "Error: Missing required parameters for AWS deployment"
     echo "Usage: ./deploy-aws.sh [sso_profile] [bucket_name] [noggin_host_url]"
     exit 1
 fi

 echo "AWS deployment selected with bucket: $BUCKET_NAME"
+echo "Using PostHog settings:"
+echo "  VITE_POSTHOG_API_KEY: ${VITE_POSTHOG_API_KEY:0:5}... (truncated)"
+echo "  VITE_POSTHOG_HOST: $VITE_POSTHOG_HOST"
+echo "  VITE_POSTHOG_DEBUG: $VITE_POSTHOG_DEBUG"

 # ... More content follows ...
@@ -68,8 +77,13 @@ build_app() {
     echo "Cleaning previous build artifacts..."
     rm -rf dist node_modules/.vite

-    # Set environment variables for the build
+    # Export environment variables for the build
     export VITE_NOGGIN_HOST="${VITE_NOGGIN_HOST}"
+    export BASE_PATH="${BASE_PATH}"
+
+    # PostHog environment variables
+    export VITE_POSTHOG_API_KEY="${VITE_POSTHOG_API_KEY}"
+    export VITE_POSTHOG_HOST="${VITE_POSTHOG_HOST}"
+    export VITE_POSTHOG_DEBUG="${VITE_POSTHOG_DEBUG}"
```
# PostHog Implementation Summary

This document summarizes the changes made to implement PostHog analytics in the FloNotes application.

## Files Created

1. **posthog.ts** - Core PostHog functionality:
   - Initialization functions
   - Event tracking utilities
   - Page view tracking hook
   - User identification helpers

2. **helpfulness-rating.tsx** - Thumbs up/down feedback component:
   - Provides user feedback on AI responses
   - Tracks ratings in PostHog

3. **csat-survey.tsx** - 5-point satisfaction survey:
   - Collects user satisfaction ratings
   - Includes optional text feedback
   - Tracks in PostHog

4. **POSTHOG.md** - Documentation of implementation approach

5. **POSTHOG_TEST_PLAN.md** - Detailed testing guidelines

## Files Modified

1. **env.ts** - Added PostHog environment variable definitions:
   - `VITE_POSTHOG_API_KEY`
   - `VITE_POSTHOG_HOST`
   - `VITE_POSTHOG_DEBUG`

2. **main.tsx** - Added PostHog initialization:
   - Imports and calls `initPostHog()`
   - Restructured app component for better organization

3. **router.tsx** - Added page tracking:
   - Created `PostHogTracker` component
   - Captures initial page views and route changes
   - Integrated with TanStack Router

4. **deploy-local.sh** - Added PostHog environment variables for local development

5. **deploy-aws.sh** - Added PostHog environment variables for production

6. **package.json** - Added PostHog dependency:
   - Added `posthog-js` to dependencies

7. **use-process-pdf.ts** (planned) - Added PDF tracking:
   - Tracks PDF uploads with extract counts

8. **editor.tsx** (planned) - Added template tracking attributes:
   - Adds data attributes to track template usage

9. **use-editor-callbacks.ts** (planned) - Enhanced with PostHog tracking:
   - Tracks template usage
   - Tracks FloNote creation

10. **menu-bar.tsx** (planned) - Updated import for tracking callbacks

## Events Being Tracked

1. **Page Views** - `$pageview`
   - Tracks route changes
   - Captures path and full URL

2. **Template Usage** - `template_reuse`
   - Tracks which templates are used
   - Includes PDF context

3. **Non-Template Queries** - `query_run_without_template`
   - Tracks when users create content without templates
   - Includes PDF context

4. **FloNote Creation** - `flonote_created`
   - Tracks when new FloNotes are created
   - Includes template and PDF context

5. **PDF Processing** - `flonote_pdf_added`
   - Tracks PDF uploads and processing
   - Captures extract counts

6. **Helpfulness Ratings** - `message_feedback`
   - Captures binary feedback on AI responses
   - Includes query context

7. **CSAT Ratings** - `csat_rating`
   - Captures 5-point satisfaction ratings
   - Includes optional text feedback

## Testing Guidance

The implementation includes comprehensive testing guidelines in `POSTHOG_TEST_PLAN.md`:

1. **Environment Setup**
   - Setting up test PostHog project
   - Configuring environment variables

2. **Test Cases**
   - Page visit tracking
   - Session recording
   - Template usage
   - FloNote creation
   - PDF processing
   - Helpfulness ratings
   - CSAT survey

3. **Verification**
   - Checking PostHog dashboard
   - Validating event properties
   - Reviewing session recordings

## Next Steps

1. **Integration Testing**
   - Test in local development environment
   - Verify events are captured correctly
   - Check session recordings

2. **Configuration**
   - Set up production PostHog project
   - Configure proper API keys for environments
   - Set up proper data retention policies

3. **Deployment**
   - Deploy changes to staging
   - Verify in staging environment
   - Deploy to production

4. **Analytics Usage**
   - Set up dashboards in PostHog
   - Configure funnels for key user flows
   - Establish baseline metrics
# PostHog Integration Test Plan

This document outlines the test plan for verifying the PostHog analytics integration in FloPilot and FloNotes applications.

## Prerequisites

1. Local development environment setup
2. PostHog testing project/instance (can use PostHog Cloud free tier for testing)
3. PostHog API key for the test project

## Environment Setup

1. Add the following environment variables to your `.env.local` file in both apps:

```bash
# FloPilot/.env.local
VITE_POSTHOG_API_KEY=<your_test_project_api_key>
VITE_POSTHOG_HOST=https://app.posthog.com
VITE_POSTHOG_DEBUG=true # Enable verbose logging during testing

# FloNotes/.env.local
VITE_POSTHOG_API_KEY=<your_test_project_api_key>
VITE_POSTHOG_HOST=https://app.posthog.com
VITE_POSTHOG_DEBUG=true # Enable verbose logging during testing
```

## Test Cases

### 1. Page Visit Tracking

**Test Steps:**
1. Navigate to different routes in the application
2. Check PostHog dashboard for `$pageview` events
3. Verify that URL and referrer information is correctly captured

**Expected Results:**
- Each page navigation should result in a `$pageview` event in PostHog
- URL path should be correctly captured in the event properties

### 2. Session Replay

**Test Steps:**
1. Enable session recordings in PostHog project settings (if not already enabled)
2. Perform various user actions in the application
3. Check PostHog dashboard for session recordings

**Expected Results:**
- Session recordings should be captured in PostHog
- User interactions should be visible in the recordings

### 3. Template Usage Tracking

**Test Steps:**
1. Navigate to FloNotes application
2. Apply a template from the template menu
3. Click "Run" to generate content based on the template
4. Check PostHog dashboard for `template_reuse` event

**Expected Results:**
- `template_reuse` event should be captured in PostHog
- Event properties should include:
  - `template_name`: Name of the used template
  - `has_pdfs`: Boolean indicating if PDFs were attached
  - `pdf_count`: Number of attached PDFs

### 4. FloNote Creation Tracking

**Test Steps:**
1. Create a new FloNote
2. Check PostHog dashboard for `flonote_created` event

**Expected Results:**
- `flonote_created` event should be captured in PostHog
- Event properties should include:
  - `template_name`: Name of the template used (or "custom" if none)
  - `has_pdfs`: Boolean indicating if PDFs were attached
  - `pdf_count`: Number of attached PDFs
  - `has_template`: Boolean indicating if a template was used

### 5. Non-Template Query Tracking

**Test Steps:**
1. Create a custom query outside of a template
2. Run the query
3. Check PostHog dashboard for `query_run_without_template` event

**Expected Results:**
- `query_run_without_template` event should be captured in PostHog
- Event properties should include:
  - `has_pdfs`: Boolean indicating if PDFs were attached
  - `pdf_count`: Number of attached PDFs

### 6. PDF Processing Tracking

**Test Steps:**
1. Upload a PDF to the FloNotes application
2. Check PostHog dashboard for `flonote_pdf_added` event

**Expected Results:**
- `flonote_pdf_added` event should be captured in PostHog
- Event properties should include:
  - `pdf_uid`: Unique identifier for the PDF
  - `stem_uid`: Stem identifier for the PDF
  - `extract_count`: Number of extracts from the PDF

### 7. Helpfulness Rating

**Test Steps:**
1. Find a message/response with the helpfulness rating component
2. Click the thumbs up or thumbs down button
3. Check PostHog dashboard for `message_feedback` event

**Expected Results:**
- `message_feedback` event should be captured in PostHog
- Event properties should include:
  - `message_id`: Identifier for the rated message
  - `rating`: Either "helpful" or "unhelpful"
  - `query`: The original query
  - `response_length`: Length of the response text

### 8. CSAT Survey

**Test Steps:**
1. Find a CSAT survey in the application
2. Select a rating (1-5)
3. Add optional feedback text
4. Submit the survey
5. Check PostHog dashboard for `csat_rating` event

**Expected Results:**
- `csat_rating` event should be captured in PostHog
- Event properties should include:
  - `session_id`: Identifier for the current session
  - `rating`: Numeric rating value (1-5)
  - `feedback`: Text feedback (if provided)

## Verification in PostHog Dashboard

For each test case:
1. Navigate to the Events tab in PostHog
2. Filter for the specific event
3. Check that all expected properties are present and correctly formatted
4. For session replays, verify that the recordings match the expected user interactions

## Notes for Production Deployment

- Disable debug mode by setting `VITE_POSTHOG_DEBUG=false` or removing it
- Ensure the appropriate API key is set for each environment (development, staging, production)
- Consider implementing sampling for high-volume events in production
- Review data retention settings in PostHog for compliance with data regulations
# Stripe Agent Toolkit MCP Server

This directory contains the Stripe Agent Toolkit MCP server configuration for Cline.

## Overview

The Stripe Agent Toolkit enables integration with Stripe APIs through function calling. It provides tools for working with Stripe resources such as customers, products, prices, payment links, invoices, and more.

## Configuration

The server is configured in the Cline MCP settings file at:
`/Users/hank/dotfiles/config/ai/cline/cline_mcp_settings.json`

### Required Environment Variables

To use the Stripe Agent Toolkit, you need to set your Stripe secret key in the MCP settings file:

```json
"env": {
  "STRIPE_SECRET_KEY": "sk_test_your_stripe_secret_key"
}
```

You can get your Stripe secret key from the [Stripe Dashboard](https://dashboard.stripe.com/apikeys).

## Supported API Methods

The Stripe Agent Toolkit supports the following API methods:

- Create a customer
- List all customers
- Create a product
- List all products
- Create a price
- List all prices
- Create a payment link
- Create an invoice
- Create an invoice item
- Finalize an invoice
- Retrieve balance
- Create a refund

## Usage Examples

Here are some examples of how to use the Stripe Agent Toolkit in Cline:

1. Create a customer:
   "Create a new Stripe customer with email john@example.com and name John Doe"

2. List products:
   "Show me all my Stripe products"

3. Create a payment link:
   "Create a Stripe payment link for my Basic Plan product"

4. Check balance:
   "What's my current Stripe balance?"

## Troubleshooting

If you encounter issues with the Stripe Agent Toolkit:

1. Ensure your Stripe secret key is correctly set in the MCP settings file
2. Check that the server is not disabled in the MCP settings
3. Restart Cline or reload the window after making changes to the MCP settings

## Resources

- [Stripe API Documentation](https://docs.stripe.com/api)
- [Stripe Agent Toolkit GitHub Repository](https://github.com/stripe/agent-toolkit)
# Advanced Prompting Techniques

This document outlines sophisticated AI prompting methods to achieve superior results for complex tasks. These techniques can be combined with task-specific templates for maximum effectiveness.

## Chain of Thought Prompting

### Description
Guide the AI through explicit reasoning steps before reaching a conclusion. This produces more accurate results for complex reasoning tasks.

### Implementation
```markdown
# Chain of Thought Request

I need you to solve this problem step-by-step:

[Problem description]

Please:
1. First, identify the key elements of this problem
2. Break down the approach into clear logical steps
3. Execute each step, showing your work
4. Evaluate the result for correctness
5. Summarize the final answer

Format each step clearly and explain your reasoning at each stage.
```

### When to Use
- Complex logical problems
- Multi-step calculations
- Debugging scenarios
- Complex decision-making with multiple factors

## Recursive Refinement

### Description
Use a staged approach where the AI first generates a draft, then iteratively improves it through self-critique and revision.

### Implementation
```markdown
# Recursive Refinement Request

I need a [type of content] about [subject]. Please approach this in three stages:

## Stage 1: Initial Draft
Create a first version that covers the core elements.

## Stage 2: Self-Critique
Review your draft and identify:
- Areas that need more detail or clarity
- Potential inaccuracies or omissions
- Structural improvements
- Stronger examples or evidence

## Stage 3: Refined Version
Create an improved version that addresses all the issues identified in your critique.

Present both the critique and the final refined version.
```

### When to Use
- Complex writing tasks
- System designs that require optimization
- Code that needs to balance multiple objectives
- Analyses where accuracy is critical

## Persona-Based Prompting

### Description
Direct the AI to adopt specific expert perspectives to leverage specialized knowledge and approaches.

### Implementation
```markdown
# Expert Perspective Request

I need analysis on [topic] from multiple expert perspectives.

Please approach this sequentially as the following experts:

1. As a senior systems architect with 15+ years of experience:
   [Specific questions/aspects to address]

2. As a security specialist focused on zero-trust implementations:
   [Specific questions/aspects to address]

3. As a DevOps engineer who prioritizes maintainability and observability:
   [Specific questions/aspects to address]

4. As a synthesis expert, integrate the perspectives above into final recommendations.

For each perspective, note key insights, blind spots, and specific recommendations.
```

### When to Use
- Getting well-rounded analysis of complex problems
- Identifying potential issues from different viewpoints
- Challenging initial assumptions
- Complex design decisions with multiple stakeholders

## Template Decomposition

### Description
Break complex outputs into parameterized templates with clear placeholders for the AI to fill in.

### Implementation
```markdown
# Template Decomposition Request

I need a [document type] with the following structure. For each section, I've provided guidance on what to include:

# [TITLE: Create a concise, descriptive title]

## Executive Summary
[EXEC_SUMMARY: Provide a 3-5 sentence overview that captures the key points]

## Problem Statement
[PROBLEM: Describe the current situation, challenges, and why this needs to be addressed]

## Proposed Approach
[APPROACH: Detail the recommended solution with justification]

## Implementation Steps
[STEPS: List 5-7 concrete actions in chronological order with owners and timeline]

## Success Metrics
[METRICS: Define 3-5 measurable indicators of success]

Please complete each section according to the instructions in brackets, then remove the bracketed instructions from the final output.
```

### When to Use
- Standardized document creation
- Complex outputs with consistent structure
- When you need to ensure all components are included
- Content that follows established patterns or protocols

## Comparative Analysis Framework

### Description
Structure a detailed comparison across multiple dimensions with explicit evaluation criteria.

### Implementation
```markdown
# Comparative Analysis Request

Please evaluate these options against the specified criteria:

## Options to Evaluate
- [Option 1]
- [Option 2]
- [Option 3]

## Evaluation Criteria
For each criterion, rate options on a scale of 1-10 and provide specific justification:

1. [Criterion 1] (Weight: X%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

2. [Criterion 2] (Weight: Y%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

3. [Criterion 3] (Weight: Z%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

## Output Format
- Provide individual criterion scores with justification
- Calculate weighted total scores
- Rank options from best to worst
- Explain key differentiators
- Include sensitivity analysis (how would rankings change if weights shifted)
```

### When to Use
- Technology selection decisions
- Architecture or design alternatives
- Investment or prioritization decisions
- Strategic option evaluation

## Decision Tree Construction

### Description
Map out a complex decision process with conditional branches based on different scenarios.

### Implementation
```markdown
# Decision Tree Request

Help me build a decision tree for [scenario/problem].

## Initial Conditions
- [Describe starting state and key variables]

## Primary Decision Points
For each primary decision point:
1. Identify the key question at this stage
2. List all possible choices (2-4 options)
3. For each choice:
   - Describe immediate consequences
   - Identify subsequent decision points
   - Note critical dependencies or assumptions

## Evaluation Metrics
At each terminal node (end state), evaluate:
- [Metric 1]
- [Metric 2]
- [Metric 3]

## Visual Representation
Create a text-based diagram of the decision tree showing:
- Decision nodes (questions)
- Option branches
- Terminal nodes (outcomes)
- Critical path highlighting
```

### When to Use
- Complex decision processes with multiple variables
- Scenario planning
- Risk assessment
- Troubleshooting guides

## System Interaction Modeling

### Description
Model complex interactions between system components, focusing on interfaces, dependencies, and data flows.

### Implementation
```markdown
# System Interaction Model Request

Help me understand interactions between components in [system name].

## System Components
- [Component 1]: [Brief description of responsibility]
- [Component 2]: [Brief description of responsibility]
- [Component 3]: [Brief description of responsibility]

## For Each Interface Between Components
1. Identify direction of dependency
2. Describe data/control flow
3. Document interface contract (API, events, etc.)
4. Note synchronicity (sync/async)
5. Identify potential failure modes
6. Document retry/resilience patterns

## Sequence Diagrams
Create text-based sequence diagrams for these key flows:
- [Flow 1]
- [Flow 2]
- [Flow 3]

## Critical Path Analysis
Identify and analyze:
- Performance bottlenecks
- Single points of failure
- Scalability constraints
- Data consistency challenges
```

### When to Use
- Complex distributed system design
- API design and documentation
- Troubleshooting integration issues
- Performance optimization

## Layered Explanation

### Description
Structure explanations across multiple levels of abstraction, from conceptual to implementation details.

### Implementation
```markdown
# Layered Explanation Request

Please explain [topic/concept] using a layered approach:

## Layer 1: Conceptual Overview (ELI5)
Explain the core concept as you would to a non-technical person, using analogies and simplified models.

## Layer 2: Functional Understanding
Explain how this works from a functional perspective - what it does, major components, and how they interact.

## Layer 3: Technical Implementation
Provide implementation details including:
- Specific technologies/methods used
- Important algorithms or patterns
- Code examples where appropriate
- Performance considerations

## Layer 4: Advanced Considerations
Discuss:
- Edge cases and how they're handled
- Optimization techniques
- Alternative approaches
- Common misconceptions
```

### When to Use
- Explaining complex technical concepts
- Educational content
- Technical documentation for diverse audiences
- Knowledge transfer

## Edge Case Exploration

### Description
Systematically identify and address uncommon but important scenarios that might be overlooked.

### Implementation
```markdown
# Edge Case Exploration Request

For this [system/code/process], help me identify and address edge cases:

## System Description
[Brief description of the system/code/process]

## Normal Operation
[Description of typical usage patterns and expected behavior]

## Edge Case Categories
For each category, identify specific edge cases and how to handle them:

1. Input Boundaries
   - Minimum/maximum values
   - Empty/null inputs
   - Malformed inputs
   - Extremely large inputs

2. Resource Constraints
   - Memory limitations
   - CPU constraints
   - Network issues
   - Timeouts

3. Concurrent Operations
   - Race conditions
   - Deadlocks
   - Contention scenarios

4. Integration Points
   - External system failures
   - API contract changes
   - Versioning issues

5. State Transitions
   - Interrupted operations
   - Invalid state sequences
   - Recovery scenarios

## For Each Edge Case
1. Describe the scenario
2. Explain potential impact
3. Recommend detection method
4. Provide handling strategy
5. Suggest testing approach
```

### When to Use
- Critical system design
- Security-sensitive applications
- High-reliability requirements
- Complex state management scenarios
# Developer Profile

## Core Identity
- Senior software engineer specializing in systems, AI infrastructure, and developer tooling
- Background in distributed systems, data engineering, and systems architecture
- Preference for intelligent, reproducible, and automated solutions
- Values high-quality implementations with minimal technical debt

## Technical Environment

### Hardware/OS
- Apple Silicon Mac (primary development environment)
- Multi-machine synchronization requirements
- Local-first architecture preferences

### Development Environment
- **Package Management**: Nix, Homebrew, uv (Python)
- **Configuration**: Git-tracked dotfiles, declarative configurations
- **Editor Ecosystem**: Neovim (LazyVim), Cursor IDE
- **Terminal**: Ghostty with ZSH
- **Window Management**: Raycast, Swish, Yabai, Hammerspoon

### Technical Stack
- **Primary Languages**:
  - TypeScript (eslint + prettier)
  - Python (ruff for linting)
  - Exposure to Rust, Go, and other systems languages
- **Tooling Preferences**:
  - Modern CLI utilities (ripgrep, fd, zoxide, atuin, starship)
  - AI-augmented development workflows
  - Keyboard-driven interfaces with minimal mouse usage

## Workflow Characteristics

### General Approach
- Preference for keyboard-driven, efficient workflows
- Strong Vim expertise and modal editing proficiency
- Value tools that reduce cognitive load and automate routine tasks
- Emphasis on reproducibility and declarative configuration

### Technical Values
- Intelligence/capability prioritized over resource usage
- Clear, maintainable code valued over cleverness
- Preference for functional programming patterns where appropriate
- Appreciation for elegant solutions that balance simplicity and power

### Learning Style
- Preference for bottom-up understanding with comprehensive examples
- Value explanations that balance theory and practical application
- Appreciate insights into underlying mechanisms and design decisions
- Learn best through hands-on implementation after conceptual understanding

## Communication Preferences

### Technical Documentation
- Structured, hierarchical organization with clear navigation
- Balanced detail: comprehensive without overwhelming
- Code examples that demonstrate real-world usage
- Visual aids for complex relationships and architectures

### Decision Processes
- Evidence-based recommendations with explicit reasoning
- Clear articulation of tradeoffs and constraints
- Preference for quantitative evaluation when possible
- Appreciation for multiple perspectives before conclusions

## AI Collaboration Context

### Interaction Mode
- Direct, efficient communication without unnecessary niceties
- Preference for precise, technical language
- Appreciation for both depth (when needed) and brevity (when appropriate)

### Output Preferences
- Structured, scannable content with clear section demarcation
- Visual representations for complex relationships
- Code that balances readability and efficiency
- Practical, implementable recommendations

### Follow-up Style
- Iterative refinement rather than extended initial responses
- Focused questions to clarify ambiguities
- Building on previous context rather than repetition
# AI Master Configuration

## System Overview

This document serves as the central configuration for all AI interactions. The complete system consists of:

1. **Master Configuration** (this file) - Core principles and system overview
2. **Developer Profile** - Your personal context and technical preferences
3. **Response Formatting** - Output structure and visual presentation guidelines
4. **Task Templates** - Purpose-specific prompt templates for different scenarios
5. **Advanced Techniques** - Advanced prompting methods for complex tasks
6. **Tool-Specific Configs** - Model-specific optimization guidelines

## Core Principles

1. **Maximum Context Efficiency**
   - Provide just enough context for optimal output
   - Use reference links to previous conversations when building on past work
   - Clearly separate instructions from content

2. **Structured Output by Default**
   - All responses should follow consistent structural patterns
   - Visual elements should be used where they enhance clarity
   - Complex information should be organized in digestible segments

3. **Progressive Enhancement**
   - Start with base prompts and add specific modules as needed
   - Build complex prompts by combining simpler templates
   - Refine outputs through iterative instructions

4. **Tool Specialization**
   - Use optimal AI model for each specific task
   - Leverage unique capabilities of different AI systems
   - Chain tools together for complex workflows

## Usage Methodology

1. **Task Analysis**
   - Determine primary goal of interaction
   - Identify relevant template modules
   - Consider required output formats

2. **Prompt Assembly**
   - Include developer profile (abbreviated if appropriate)
   - Add task-specific template
   - Specify response format requirements
   - Include any specialized techniques

3. **Response Management**
   - Evaluate output quality against requirements
   - Use follow-up prompts for refinement
   - Extract and store reusable components

4. **Continuous Improvement**
   - Note effective prompt patterns
   - Update templates based on successful interactions
   - Develop specialized templates for recurring tasks

## Integration Framework

These templates support integration with:

- **Claude Models** (Sonnet, Opus, Haiku)
- **OpenAI Models** (GPT-4, GPT-4o)
- **DeepSeek Models** (DeepSeek Coder)
- **Open Source Models** (via Ollama integration)
- **AI-powered Development Tools** (Cursor, GitHub Copilot)
- **Document Processing Tools** (for export/import)

## Template Evolution Strategy

For maximum effectiveness, treat these templates as living documents:

1. When an AI generates exceptionally good output, capture the prompt structure
2. When complex tasks are completed successfully, document the prompt chain
3. When outputs require significant refinement, note the refinement pattern
4. Regularly update templates with new capabilities as AI models evolve

## Quick Reference

| Task Type | Primary Template | Secondary Templates | Optimal AI Model |
|-----------|------------------|---------------------|------------------|
| Code Generation | `coding-primary.md` | `visual-code.md`, `advanced-reasoning.md` | DeepSeek Coder/Claude Sonnet |
| Code Review | `code-analysis.md` | `structured-feedback.md` | Claude Sonnet w/Extended Thinking |
| System Design | `system-design.md` | `visual-architecture.md` | Claude Opus/GPT-4 |
| Data Analysis | `data-analysis.md` | `visual-data.md` | Claude Sonnet |
| Technical Writing | `technical-docs.md` | `doc-formatting.md` | Claude Sonnet |
| Research | `research-synthesis.md` | `citation-format.md` | GPT-4o with browsing |
# Response Formatting Guidelines

## Document Structure

### Hierarchical Organization
- Use clear hierarchical headers (H1, H2, H3) for logical organization
- Ensure headers create a scannable outline of the content
- Maintain consistent heading levels for parallel concepts
- Keep header text concise and descriptive

### Content Blocks
- Separate distinct concepts with visual breaks
- Use bullet points for lists of related items
- Use numbered lists for sequential steps or ranked items
- Group related information in cohesive sections

### Progressive Disclosure
- Present most important information first
- Start with summary/overview before details
- Use expandable sections for supplementary details when appropriate
- Structure from general to specific

## Visual Elements

### Tables
For comparisons and structured data:
- Include 5-7 crucial criteria from modern best practices
- Score each option 1-10 for each criterion
- Apply appropriate weights when criteria importance varies
- Present options in descending order of overall score
- Include scoring rationale in the table or notes
- Add a clear explanation for the top choice selection

Example format:
```
| Option | Criterion 1 (w=0.3) | Criterion 2 (w=0.5) | Criterion 3 (w=0.2) | Weighted Score | Rationale |
|--------|---------------------|---------------------|---------------------|----------------|-----------|
| Opt A  | 9 (2.7)             | 7 (3.5)             | 8 (1.6)             | 7.8            | Excels at... |
```

### Diagrams
For complex relationships, include diagrams that show:
- System architecture with components and interfaces
- Workflow sequences with decision points
- Entity relationships with cardinality
- Data transformations and processing pipelines
- Algorithm mechanisms with step transitions

Diagram guidelines:
- Clear labels on all elements
- Logical flow direction (typically left-to-right or top-to-bottom)
- Concise legends when using symbols
- Visual hierarchy that emphasizes important elements

### Code Formatting
- Use appropriate syntax highlighting
- Include meaningful comments at logical breakpoints
- Add type annotations when helpful
- Demonstrate best practices in the code structure
- Include example usage when not obvious

## Content Quality

### Technical Precision
- Use correct terminology consistently
- Cite specific versions/tools when version-dependent
- Avoid vague generalizations
- Qualify claims with supporting evidence or reasoning

### Implementation Focus
- Prefer concrete examples over abstract concepts
- Include specific configuration samples
- Provide resource requirements and constraints
- Address practical implementation considerations

### Balanced Detail
- Provide comprehensive information without verbosity
- Emphasize distinctive or non-obvious aspects
- De-emphasize standard or expected behaviors
- Adjust detail level based on complexity and importance

## Special Content Types

### Process Descriptions
- Break down into clear sequential steps
- Indicate expected outcomes at key points
- Note potential failure points and mitigation
- Include validation methods for each stage

### Analysis Outputs
- Clearly state assumptions and methodology
- Present findings with supporting evidence
- Discuss limitations and confidence levels
- Include actionable conclusions

### Technical Evaluations
- Establish clear evaluation criteria
- Use consistent rating system
- Provide specific examples supporting ratings
- Include quantitative metrics when available

## Document Formatting

### Text Styling
- Use bold for emphasis of key terms and concepts
- Use italic for secondary emphasis or specialized terms
- Use code formatting for commands, variables, and technical identifiers
- Maintain consistent styling conventions throughout

### Lists and Indentation
- Use bulleted lists for unordered collections
- Use numbered lists for sequential steps
- Indent sublists consistently
- Keep list items parallel in structure

### Code Blocks
- Use triple backticks with language specification
- Format code with consistent indentation
- Break long lines appropriately
- Include sample output when helpful
# Task-Specific Templates

These templates provide specialized prompting patterns for common task types. They are designed to be combined with your Developer Profile and Response Format preferences.

## Code Generation Template

```markdown
# Code Generation Request

## Task Description
[Brief description of the code you need]

## Requirements
- Language: [Programming language and version]
- Framework/Libraries: [Any specific libraries or frameworks]
- Performance needs: [Time/space complexity requirements]
- Edge cases to handle: [List specific edge cases]

## Expected Functionality
[Detailed description of what the code should do]

## Output Format
- Provide clean, efficient implementation with type annotations
- Include relevant error handling
- Add comments explaining complex logic or design decisions
- Demonstrate usage with example inputs/outputs
- Discuss any performance considerations or tradeoffs

## Additional Context
[Any other relevant information or constraints]
```

## Code Review Template

```markdown
# Code Review Request

## Code to Review
```[language]
[Paste code here]
```

## Review Focus
- [Specific aspects to focus on, e.g., "performance", "security", "readability"]
- [Any specific concerns or questions]

## Expected Output
- Identify potential issues (bugs, performance problems, security vulnerabilities)
- Suggest improvements with specific code examples
- Highlight good practices already present
- Analyze overall code structure and organization
- Provide a table scoring the code on key dimensions (1-10):
  - Correctness, Performance, Maintainability, Security, Readability

## Additional Context
[Relevant background information about the code purpose, environment, etc.]
```

## System Design Template

```markdown
# System Design Request

## Design Task
[Brief description of the system to be designed]

## Requirements
- Functionality: [Key functional requirements]
- Scale: [Expected scale/load characteristics]
- Constraints: [Budget, technology, or other constraints]
- Quality attributes: [Reliability, performance, security needs]

## Expected Deliverables
- High-level architecture diagram
- Component breakdown with responsibilities
- Data model and flow
- API design (if applicable)
- Technology stack recommendations with rationale
- Scalability considerations
- Potential failure modes and mitigations

## Output Format Preferences
- Start with executive summary of the approach
- Include visual architecture diagram
- Provide component tables with responsibilities and interfaces
- Include sequence diagrams for key flows
- List technology choices with explicit rationale

## Additional Context
[Any background information or existing systems to consider]
```

## Data Analysis Template

```markdown
# Data Analysis Request

## Analysis Goal
[What insight or outcome you're seeking]

## Data Description
- Source: [Where the data comes from]
- Format: [CSV, JSON, etc.]
- Size: [Approximate size or dimensions]
- Key fields: [Important columns or attributes]
- Special characteristics: [Missing values, outliers, etc.]

## Analysis Requirements
- Specific questions to answer: [List questions]
- Metrics to calculate: [List metrics]
- Visualizations needed: [Types of charts/plots]
- Statistical methods to apply: [If applicable]

## Output Format Preferences
- Present key findings first as executive summary
- Include properly labeled visualizations
- Provide tables summarizing statistical results
- Document methodology and assumptions
- Include recommendations based on findings

## Additional Context
[Any relevant background information or hypotheses]
```

## Technical Documentation Template

```markdown
# Technical Documentation Request

## Documentation Purpose
[What this documentation will be used for and by whom]

## Subject Matter
[The system, code, or process to document]

## Documentation Requirements
- Target audience: [Who will read this]
- Technical depth: [Beginner, intermediate, expert]
- Sections to include: [List required sections]
- Examples needed: [Specific examples to include]

## Format Preferences
- Include table of contents
- Use hierarchical structure with clear navigation
- Provide code examples for key operations
- Include diagrams for complex concepts
- Add troubleshooting section for common issues

## Additional Context
[Any background information or existing documentation to reference]
```

## Research Synthesis Template

```markdown
# Research Synthesis Request

## Research Topic
[The subject to research]

## Research Objectives
- Key questions to answer: [List specific questions]
- Scope boundaries: [What to include/exclude]
- Required depth: [How comprehensive should this be]

## Source Preferences
- Preferred authorities: [Any specific sources to prioritize]
- Recency requirements: [How recent should sources be]
- Source types: [Academic papers, industry reports, etc.]

## Output Format
- Begin with executive summary of key findings
- Structure information hierarchically by subtopic
- Include comparison tables for competing approaches/views
- Provide citations for all significant claims
- End with synthesis of implications and next steps

## Additional Context
[Your current knowledge level, specific applications of the research, etc.]
```

## Technical Comparison Template

```markdown
# Technical Comparison Request

## Items to Compare
[List the technologies, methods, or tools to compare]

## Comparison Criteria
[List the 5-7 most important factors for evaluation]

## Usage Context
[How and where the solution will be used]

## Constraints
[Any limitations that would affect the decision]

## Output Format
- Start with brief overview of each option
- Create weighted comparison table (criteria weighted by importance)
- Score each option 1-10 on each criterion with clear rationale
- Include pros/cons section for each option
- Provide final recommendation with justification
- Address potential objections to the recommendation

## Additional Context
[Your experience level with these options, specific requirements, etc.]
```

## Troubleshooting Template

```markdown
# Troubleshooting Request

## Problem Description
[Detailed description of the issue]

## Environment Details
- Hardware: [Relevant hardware specifications]
- Software: [OS, application versions, etc.]
- Configuration: [Relevant configuration details]
- Recent changes: [Any recent changes that might be relevant]

## Observed Behavior
[What's actually happening]

## Expected Behavior
[What should be happening]

## Troubleshooting Already Attempted
[Steps you've already taken to diagnose or fix]

## Output Format
- Analyze possible root causes systematically
- Provide diagnostic steps in order of likelihood
- Include specific commands or code to run for diagnosis
- Suggest solutions with step-by-step implementation
- Explain preventative measures for the future

## Additional Context
[Error logs, screenshots, or other relevant information]
```

## Project Planning Template

```markdown
# Project Planning Request

## Project Description
[Brief description of the project]

## Project Goals
- Primary objectives: [List main goals]
- Success criteria: [How success will be measured]
- Constraints: [Time, budget, resource constraints]

## Scope
- In scope: [Features/components definitely included]
- Out of scope: [What's explicitly excluded]
- Potential extensions: [Nice-to-haves if time permits]

## Output Format
- Provide project breakdown structure with major components
- Create task dependency diagram
- Include risk assessment table with mitigation strategies
- Suggest timeline with milestones
- Recommend resource allocation approach
- Include key decision points and criteria

## Additional Context
[Team composition, expertise areas, relevant experience, etc.]
```
# AI Tool-Specific Configurations

This document provides optimized configurations for specific AI tools and models to maximize their unique capabilities and strengths.

## Claude (Anthropic)

### Claude 3.7 Sonnet with Extended Thinking

#### Optimal Use Cases
- Complex reasoning tasks
- Detailed technical analysis
- Nuanced explanations balancing depth and clarity
- Code review and improvement
- Technical writing with precise language

#### Configuration Optimizations
```markdown
# Claude 3.7 Sonnet Configuration

I'd like you to engage extended thinking mode and act as [specific role].

When analyzing this problem:
1. Think step-by-step through the reasoning process
2. Consider multiple approaches before selecting the optimal solution
3. Explicitly note assumptions and their implications
4. Evaluate tradeoffs quantitatively where possible

Please structure your response according to:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Enable extended thinking mode for complex reasoning
- Request explicit reasoning paths for verification
- Use for tasks requiring careful nuance and precision
- Excellent for generating well-structured, hierarchical content

### Claude 3 Opus

#### Optimal Use Cases
- System design and architecture
- Comprehensive technical evaluations
- Complex document generation
- In-depth research synthesis
- Detailed code explanations

#### Configuration Optimizations
```markdown
# Claude 3 Opus Configuration

I'd like you to act as [specific role] with extensive expertise in [domain].

For this complex task, please:
1. First map out the entire solution space
2. Identify key decision points and dependencies
3. Develop a comprehensive approach
4. Elaborate on implementation details

Please structure your response with:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Superior context handling for complex, multi-part problems
- Capability to generate comprehensive technical documents
- Handles nuanced, complex instructions effectively
- Excellent for multi-stage reasoning tasks

## OpenAI Models

### GPT-4o

#### Optimal Use Cases
- Interactive problem-solving
- Multi-modal inputs (text + images)
- Creative technical solutions
- Real-time web research integration
- Code generation with clear explanations

#### Configuration Optimizations
```markdown
# GPT-4o Configuration

You are an expert [specific role] assisting with [task type].

Consider this problem holistically:
1. Analyze the complete context, including any provided images or references
2. Formulate a structured approach
3. Generate concrete implementation details
4. Explain your reasoning transparently

Please follow this response structure:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Ability to process and reference images in reasoning
- Browsing capabilities for real-time research
- Strong code generation capabilities with context awareness
- Good at creative solutions to technical problems

### GPT-4o with Advanced Data Analysis

#### Optimal Use Cases
- Statistical analysis and data exploration
- Code execution and verification
- Data visualization suggestions
- Iterative problem-solving with code
- Mathematical proofs and calculations

#### Configuration Optimizations
```markdown
# GPT-4o with Advanced Data Analysis Configuration

You are an expert data scientist and programmer helping with [specific task].

For this analysis:
1. Explore the data to understand its structure and characteristics
2. Suggest appropriate analytical approaches
3. Implement solutions using executable code
4. Validate results and check for errors
5. Visualize key findings effectively

I need the response to include:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Code execution capabilities for instant verification
- Iterative data exploration and analysis
- Creation of visualizations from data
- Ability to process and analyze uploaded files
- Mathematical computation capabilities

## DeepSeek Models

### DeepSeek Coder

#### Optimal Use Cases
- Complex software engineering tasks
- Programming language-specific optimizations
- Implementation of algorithms and data structures
- Code optimization and refactoring
- Technical software documentation

#### Configuration Optimizations
```markdown
# DeepSeek Coder Configuration

You are an expert software engineer specializing in [language/framework].

For this coding task:
1. Analyze requirements with attention to edge cases
2. Design a solution prioritizing [efficiency/readability/maintainability]
3. Implement with idiomatic patterns and best practices
4. Include comprehensive documentation and usage examples
5. Highlight optimization opportunities

Structure your response as follows:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Exceptional code generation quality
- Strong understanding of software engineering principles
- Ability to work within existing codebases
- Effective at complex algorithm implementation
- Language-specific optimization knowledge

## Multi-Tool Workflows

### Research → Analysis → Implementation

This workflow combines multiple AI tools for comprehensive solutions.

#### Process Flow
1. **Research Phase** (GPT-4o with browsing)
   - Gather relevant information
   - Identify state-of-the-art approaches
   - Collect reference examples

2. **Analysis Phase** (Claude 3.7 Sonnet with Extended Thinking)
   - Evaluate approaches from research
   - Perform detailed tradeoff analysis
   - Develop conceptual design

3. **Implementation Phase** (DeepSeek Coder)
   - Convert design into concrete implementation
   - Optimize for target environment
   - Develop tests and documentation

#### Configuration Example
```markdown
# Multi-Tool Workflow: Research Phase

You are a research specialist gathering information on [topic].

Please find the most relevant and current information about:
- Core concepts and principles
- Leading approaches and methodologies
- Benchmark results and comparisons
- Implementation considerations

Format findings as a structured research brief with sections for each area and citations.

# Multi-Tool Workflow: Analysis Phase (For Claude)

Using the research provided, perform a comprehensive analysis:

1. Evaluate each approach against these criteria:
   - [List criteria with weights]
2. Identify key tradeoffs and dependencies
3. Recommend an optimal approach with justification
4. Outline a conceptual design

# Multi-Tool Workflow: Implementation Phase (For DeepSeek)

Based on the provided conceptual design, implement a solution that:
1. Follows the specified approach
2. Optimizes for [specific requirements]
3. Includes error handling and edge cases
4. Provides documentation and usage examples
```

### Design → Feedback → Refinement

This iterative workflow improves quality through multiple models.

#### Process Flow
1. **Initial Design** (GPT-4o or Claude Opus)
   - Generate comprehensive first draft
   - Include all required components
   - Document design decisions

2. **Critical Feedback** (Claude with Extended Thinking)
   - Analyze for weaknesses and gaps
   - Identify optimization opportunities
   - Suggest specific improvements

3. **Refinement** (Original or specialized model)
   - Incorporate feedback
   - Optimize critical components
   - Finalize the solution

#### Configuration Example
```markdown
# Initial Design Phase

Create a comprehensive design for [system/document/code].

Requirements:
- [List key requirements]
- [List constraints]
- [List quality attributes]

Provide a complete initial design including:
- [List expected deliverables]

# Feedback Phase (For Claude)

Review the provided design critically:

1. Evaluate against these criteria:
   - [List evaluation criteria]
2. Identify potential weaknesses, including:
   - Performance bottlenecks
   - Scalability issues
   - Security concerns
   - Maintenance challenges
3. Suggest specific improvements for each issue
4. Prioritize recommendations by impact

# Refinement Phase

Based on the feedback provided, refine the initial design:

1. Address each identified issue with specific changes
2. Optimize the critical components
3. Justify any feedback points not incorporated
4. Provide a complete, refined version
```

## Custom Configuration Templating

### Configuration Template Structure

Create your own tool-specific configurations using this template:

```markdown
# [Tool Name] Configuration for [Task Type]

## Context Setting
[Establish role, expertise level, and general approach]

## Task Instructions
[Specific task details and requirements]

## Processing Guidelines
[How the AI should approach the problem]
- Step 1: [Specific instruction]
- Step 2: [Specific instruction]
- etc.

## Output Format
[Response structure and formatting requirements]

## Additional Parameters
[Any special modes or settings to enable]
```

### Calibration Process

To optimize configurations for specific models:

1. **Baseline Testing**
   - Test the same prompt across different models
   - Note differences in output quality and approach
   - Identify relative strengths and weaknesses

2. **Parameter Tuning**
   - Adjust instructions based on model strengths
   - Modify level of detail in task breakdown
   - Calibrate output format specificity

3. **Iteration**
   - Refine based on output quality
   - Create model-specific instruction sets
   - Document effective patterns
# Visual Format Specifications

This document provides detailed specifications for visual elements in AI responses, ensuring consistent, high-quality visual outputs across different AI tools.

## Architecture Diagrams

### System Architecture Diagrams

#### Content Requirements
- **Components**: All significant system components clearly labeled
- **Interfaces**: Connections between components with interface type
- **Data Flow**: Directional indicators showing information flow
- **External Boundaries**: Clear system boundaries and external integrations
- **Layers**: Logical grouping of components by function/layer
- **Dependencies**: Clear indication of component dependencies

#### Notation Guidelines
```
[External System] ←→ [API Gateway] → [Service A] → [Database]
                                    ↓
                                [Service B] → [Cache]
                                    ↓
                               [Event Bus] → [Service C]
```

#### Example Specification
```markdown
Please create a system architecture diagram showing:
- Frontend components (UI, state management)
- Backend services (authentication, core logic, data access)
- Data storage systems (databases, caches)
- External integrations (third-party APIs, partner systems)
- Communication patterns (sync/async, events, direct calls)

Use distinct symbols for different component types and indicate data flow directions.
```

### Sequence Diagrams

#### Content Requirements
- **Actors**: All participants in the interaction
- **Time Flow**: Clear temporal sequence (typically top to bottom)
- **Messages**: Labeled arrows showing requests/responses
- **Activation**: Indication of when components are active
- **Conditional Paths**: Decision points and alternative flows
- **Failure Scenarios**: Error handling paths where relevant

#### Notation Guidelines
```
Actor     Service A     Service B     Database
  |           |             |            |
  |--Request->|             |            |
  |           |--Query----->|            |
  |           |             |--DB Call-->|
  |           |             |<--Result---|
  |           |<--Response--|            |
  |<--Reply---|             |            |
```

#### Example Specification
```markdown
Create a sequence diagram for user authentication flow showing:
- Initial request from client
- Authentication service validation
- Token generation
- Response to client
- Subsequent authenticated request
- Token validation
- Resource access

Include error paths for:
- Invalid credentials
- Expired tokens
- Insufficient permissions
```

## Data Visualization

### Comparison Tables

#### Content Requirements
- **Header Row**: Clear column headers with units if applicable
- **First Column**: Items being compared
- **Data Cells**: Consistent formatting of values
- **Scoring**: Numeric ratings (1-10) with visual indicators
- **Weighting**: Weight values for weighted calculations
- **Totals**: Weighted scores and rankings
- **Highlight**: Visual emphasis on optimal values

#### Format Example
```
| Option | Performance (w=0.3) | Security (w=0.4) | Maintainability (w=0.3) | Weighted Score | Rank |
|--------|---------------------|------------------|-------------------------|----------------|------|
| Opt A  | 9 ⭐⭐⭐ (2.7)        | 7 ⭐⭐ (2.8)       | 8 ⭐⭐⭐ (2.4)            | 7.9            | 1    |
| Opt B  | 7 ⭐⭐ (2.1)          | 8 ⭐⭐⭐ (3.2)      | 6 ⭐⭐ (1.8)              | 7.1            | 2    |
| Opt C  | 8 ⭐⭐⭐ (2.4)         | 6 ⭐⭐ (2.4)       | 7 ⭐⭐ (2.1)              | 6.9            | 3    |
```

#### Example Specification
```markdown
Create a comparison table for the options with these characteristics:
- Include all 5 options in rows
- Evaluate each on the 6 criteria specified
- Use a 1-10 scale for each criterion
- Apply the weights as indicated in parentheses
- Calculate weighted scores
- Rank options from highest to lowest score
- Add visual indicators (⭐) for scores:
  - 1-3: No stars
  - 4-6: ⭐
  - 7-8: ⭐⭐
  - 9-10: ⭐⭐⭐
- Highlight the best score in each column
```

### Decision Trees

#### Content Requirements
- **Decision Nodes**: Questions or decision points
- **Option Branches**: Paths representing choices
- **Leaf Nodes**: Outcomes or terminal states
- **Branch Labels**: Conditions for taking each path
- **Probabilities/Values**: Associated with paths or nodes (if applicable)
- **Critical Path**: Highlighted recommended path

#### Notation Guidelines
```
                         [Initial Decision]
                           /           \
                     [Yes]/             \[No]
                         /               \
            [Secondary Decision]     [Alternative Decision]
               /         \               /        \
          [Opt A]      [Opt B]      [Opt C]     [Opt D]
```

#### Example Specification
```markdown
Create a decision tree for the described scenario with:
- Root node representing the initial decision
- Branches for each option with conditions
- Secondary decision points where relevant
- Terminal nodes showing outcomes
- Annotations with key considerations at each step
- Highlight the recommended path based on specified criteria
```

### Process Flows

#### Content Requirements
- **Start/End**: Clear beginning and end points
- **Actions**: Process steps with descriptive labels
- **Decisions**: Conditional branching points
- **Flows**: Directional arrows connecting elements
- **Loops**: Clearly marked iteration points
- **Swimlanes**: Responsibility boundaries (if applicable)
- **Artifacts**: Inputs and outputs of steps

#### Notation Guidelines
```
(Start) → [Process Step] → <Decision?> → [Success Step] → (End)
                               |
                               ↓
                          [Failure Step] → [Recovery] ↩
```

#### Example Specification
```markdown
Create a process flow diagram showing:
- Initial request handling
- Validation steps with error paths
- Main processing sequence
- Decision points with criteria
- Success and failure paths
- Retry loops where applicable
- Final states and outputs

Use distinct symbols for:
- Start/end points (parentheses)
- Process steps [rectangles]
- Decision points <diamonds>
- Directional arrows connecting elements
```

## Code Visualization

### Code Structure Diagrams

#### Content Requirements
- **Modules**: Major code components
- **Classes/Functions**: Key implementation elements
- **Relationships**: Inheritance, composition, dependencies
- **Interfaces**: Public APIs and contracts
- **Access Patterns**: Usage and interaction flows
- **Data Structures**: Key data representations

#### Notation Guidelines
```
[Module A] ← implements ← [Interface X]
    ↑                         ↑
 inherits                  inherits
    |                         |
[Module B] → depends on → [Module C]
```

#### Example Specification
```markdown
Create a code structure diagram showing:
- Major components/modules
- Key classes with inheritance relationships
- Critical interfaces and implementations
- Dependencies between components
- Data flow between modules
- Extension points

Use UML-inspired notation with:
- Boxes for classes/modules
- Arrows for relationships
- Labels indicating relationship types
```

### Algorithm Flowcharts

#### Content Requirements
- **Initialization**: Starting state and setup
- **Steps**: Processing stages with operations
- **Branching**: Conditional logic
- **Loops**: Iteration constructs
- **Termination**: Exit conditions
- **Complexity**: Annotated time/space complexity
- **Examples**: Sample data state at key points

#### Notation Guidelines
```
(Initialize) → [Step 1] → [Step 2] → <Condition?> → [Step 3A] → (End)
                                         |
                                         ↓
                                     [Step 3B] → [Loop back to Step 2] ↩
```

#### Example Specification
```markdown
Create an algorithm flowchart showing:
- Initialization with input parameters
- Main processing steps in sequence
- Conditional branches with criteria
- Loop structures with entry/exit conditions
- Terminal states and return values
- Annotations with complexity analysis
- Example state progression with sample input

Use consistent notation:
- Rounded rectangles for start/end
- Rectangles for process steps
- Diamonds for decisions
- Arrows for flow direction
```

## Tabular Data

### Feature Comparison Matrices

#### Content Requirements
- **Features**: Comprehensive feature list in rows
- **Options**: Items being compared in columns
- **Support**: Clear indicators of feature support
- **Limitations**: Notes on partial implementations
- **Standouts**: Highlighting of exceptional capabilities
- **Categories**: Logical grouping of related features
- **Importance**: Indicators of feature significance

#### Format Example
```
| Feature Category | Feature | Option A | Option B | Option C |
|------------------|---------|----------|----------|----------|
| Performance      | Feature 1 | ✅ | ⚠️ Limited | ❌ |
|                  | Feature 2 | ✅ | ✅ | ⚠️ Planned |
| Security         | Feature 3 | ✅ Best | ✅ | ✅ |
|                  | Feature 4 | ❌ | ✅ | ✅ |
| Usability        | Feature 5 | ⚠️ Basic | ✅ | ✅ Best |
```

#### Example Specification
```markdown
Create a feature comparison matrix with:
- Features organized by category (rows)
- All options being evaluated (columns)
- Support indicators:
  - ✅ = Fully supported
  - ⚠️ = Partial/limited support (with note)
  - ❌ = Not supported
- "Best" notation for standout implementations
- "Planned" notation for upcoming features
- Bold text for critical features
```

### Decision Matrices

#### Content Requirements
- **Options**: Alternatives in rows
- **Criteria**: Evaluation factors in columns
- **Weights**: Importance weighting for criteria
- **Scores**: Raw scores (typically 1-10)
- **Weighted Scores**: Calculated weighted values
- **Totals**: Sum of weighted scores
- **Ranking**: Clear indication of final ranking

#### Format Example
```
| Option | Criterion 1 (w=0.2) | Criterion 2 (w=0.5) | Criterion 3 (w=0.3) | Total Score | Rank |
|--------|---------------------|---------------------|---------------------|-------------|------|
| Opt A  | 7 (1.4)             | 8 (4.0)             | 9 (2.7)             | 8.1         | 1    |
| Opt B  | 9 (1.8)             | 6 (3.0)             | 8 (2.4)             | 7.2         | 2    |
| Opt C  | 8 (1.6)             | 5 (2.5)             | 7 (2.1)             | 6.2         | 3    |
```

#### Example Specification
```markdown
Create a weighted decision matrix with:
- All options listed as rows
- Evaluation criteria as columns with weights
- Scores on a 1-10 scale for each option/criterion
- Calculation showing raw score and weighted contribution
- Total weighted score for each option
- Ranking based on total score
- Conditional formatting:
  - Bold the highest score in each column
  - Highlight the top-ranked option
```

## Chart Types

### Technical Chart Specifications

Specifications for common chart types in technical contexts:

#### Time Series
```markdown
Create a time series chart showing:
- X-axis: Time periods (specify increments)
- Y-axis: Metric values (specify units and scale)
- Multiple series for comparing trends
- Clear legend identifying each series
- Notable events or thresholds marked
- Trend lines where appropriate

Use text-based visualization with appropriate symbols and labels.
```

#### Comparison Bar Charts
```markdown
Create a bar chart comparing:
- X-axis: Categories/items being compared
- Y-axis: Value metric (specify units and scale)
- Grouped bars for multi-dimension comparison
- Labels showing exact values
- Sorted in descending order by primary metric
- Color coding to distinguish categories

Use text-based visualization with proportional bar lengths.
```

#### Technical Scatter Plots
```markdown
Create a scatter plot showing:
- X-axis: First variable (specify units and range)
- Y-axis: Second variable (specify units and range)
- Points representing individual data items
- Size variation for third dimension (if applicable)
- Color coding for categories
- Trend line showing correlation
- Labeled quadrants or regions of interest

Use text-based visualization with appropriate symbols and positioning.
```

## Documentation Diagrams

### API Interface Diagrams

#### Content Requirements
- **Endpoints**: All API endpoints grouped logically
- **Methods**: HTTP methods or function calls
- **Parameters**: Required and optional inputs
- **Responses**: Return types and structures
- **Authentication**: Security requirements
- **Dependencies**: Related services or components
- **Examples**: Sample calls and responses

#### Format Example
```
API: UserService
┌─────────────────────────────────────────────────────────┐
│ GET /users/{id}                                         │
├─────────────────────────────────────────────────────────┤
│ Parameters:                                             │
│   - id: string (required) - User identifier             │
├─────────────────────────────────────────────────────────┤
│ Responses:                                              │
│   - 200: User object                                    │
│   - 404: Not found error                                │
│   - 401: Authentication error                           │
├─────────────────────────────────────────────────────────┤
│ Authentication: Bearer token                            │
└─────────────────────────────────────────────────────────┘
```

#### Example Specification
```markdown
Create an API interface diagram documenting:
- All endpoints grouped by resource
- HTTP methods for each endpoint
- Path and query parameters with types
- Request body schema for POST/PUT methods
- Response codes and data structures
- Authentication requirements
- Rate limiting information
- Example request/response pairs

Use consistent box notation with clear sections for each component.
```

### Entity Relationship Diagrams

#### Content Requirements
- **Entities**: All major data entities
- **Attributes**: Key fields for each entity
- **Relationships**: Connections between entities
- **Cardinality**: Relationship multiplicity (1:1, 1:N, M:N)
- **Primary Keys**: Clearly marked identifiers
- **Foreign Keys**: Relationship implementation
- **Constraints**: Business rules and validations

#### Notation Guidelines
```
[User]                      [Order]
id (PK)    1 ------- * ┌─→ id (PK)
username               │   user_id (FK)
email                  │   total_amount
created_at             │   created_at
                       │
                       │   [OrderItem]
                       └── id (PK)
                           order_id (FK)
                           product_id (FK)
                           quantity
                           price
```

#### Example Specification
```markdown
Create an entity relationship diagram showing:
- All entities in the data model
- Primary key fields marked (PK)
- Foreign key relationships marked (FK)
- Essential attributes for each entity
- Relationship lines with cardinality:
  - One-to-one: 1 ──── 1
  - One-to-many: 1 ──── *
  - Many-to-many: * ──── *
- Optional relationships shown with dashed lines
```
# Workflow Integration Guide

This guide explains how to effectively integrate AI tools into your development, writing, and analysis workflows.

## Core Integration Principles

### 1. Task Decomposition
- Break complex tasks into discrete steps
- Identify which steps benefit most from AI assistance
- Create clear handoffs between human and AI work

### 2. Context Management
- Maintain consistent context across interactions
- Reference previous outputs explicitly
- Provide explicit updates when context changes

### 3. Output Refinement
- Use follow-up prompts to refine outputs
- Iterate based on specific feedback
- Save effective prompt+output combinations as templates

### 4. Version Control Integration
- Include AI prompts in version control
- Document AI-assisted components
- Attribute significant AI contributions appropriately

## Development Workflow Integration

### Code Generation Workflow

#### Initial Planning Phase
1. **Define Requirements** (Human)
   - Specify functionality, inputs, outputs
   - Identify constraints and edge cases
   - Determine performance requirements

2. **Generate Implementation Strategy** (AI)
   - Use the "Code Solution Template"
   - Request multiple approach options
   - Ask for tradeoff analysis

3. **Select and Refine Approach** (Human)
   - Choose preferred implementation approach
   - Identify areas needing customization
   - Note integration requirements

#### Implementation Phase
4. **Generate Initial Implementation** (AI)
   - Use "Code Generation Template" with selected approach
   - Request complete implementation with tests
   - Specify naming conventions and style preferences

5. **Review and Customize** (Human)
   - Evaluate generated code against requirements
   - Identify areas needing modification
   - Make necessary integrations with existing codebase

6. **Refine Implementation** (AI+Human)
   - Request specific optimizations
   - Address edge cases and error handling
   - Finalize documentation and examples

### Code Review Workflow

1. **Prepare Code for Review** (Human)
   - Select code segments for review
   - Provide context and requirements
   - Specify review focus areas

2. **Generate Review** (AI)
   - Use "Code Review Template"
   - Request specific feedback categories
   - Specify output format for actionable review

3. **Triage Feedback** (Human)
   - Categorize issues by severity
   - Identify patterns in feedback
   - Prioritize improvements

4. **Implement Improvements** (Human+AI)
   - Request specific optimizations based on review
   - Generate alternative implementations for problem areas
   - Use "Advanced Techniques" for complex refactoring

### System Design Workflow

1. **Define Requirements** (Human)
   - Specify functional requirements
   - Establish constraints and scale
   - Define quality attributes

2. **Generate Architecture** (AI)
   - Use "System Design Template"
   - Request component breakdown with responsibilities
   - Specify required diagrams

3. **Review and Refine** (Human)
   - Identify areas needing more detail
   - Question assumptions and tradeoffs
   - Note integration challenges

4. **Detailed Design** (AI+Human)
   - Generate detailed specifications for key components
   - Create interface definitions
   - Develop data models and flows

5. **Implementation Planning** (Human+AI)
   - Break down into implementation tasks
   - Identify dependencies and critical path
   - Create implementation timeline

## Writing and Documentation Workflow

### Technical Documentation Workflow

1. **Define Documentation Scope** (Human)
   - Identify target audience
   - Determine required sections and depth
   - Specify format and style requirements

2. **Generate Structure** (AI)
   - Use "Technical Documentation Template"
   - Request outline with section breakdown
   - Define content types for each section

3. **Generate Content** (AI)
   - Develop content for each section
   - Include required examples and diagrams
   - Format according to specifications

4. **Review and Refine** (Human)
   - Check technical accuracy
   - Ensure completeness and clarity
   - Note areas needing expansion or correction

5. **Finalize Documentation** (AI+Human)
   - Address specific feedback
   - Format for final delivery
   - Create supplementary materials if needed

### Research Synthesis Workflow

1. **Define Research Questions** (Human)
   - Specify primary questions to answer
   - Determine scope and boundaries
   - Identify key sources or data to consider

2. **Generate Research Plan** (AI)
   - Use "Research Synthesis Template"
   - Create structured approach to investigation
   - Define synthesis methodology

3. **Conduct Research** (AI+Human)
   - Gather and organize information
   - Analyze patterns and contradictions
   - Identify key findings

4. **Generate Synthesis** (AI)
   - Create comprehensive synthesis of findings
   - Highlight implications and applications
   - Suggest areas for further investigation

5. **Review and Extend** (Human)
   - Validate accuracy and completeness
   - Identify applications to current work
   - Determine next steps based on findings

## Analysis Workflow Integration

### Data Analysis Workflow

1. **Define Analysis Goals** (Human)
   - Specify questions to answer
   - Identify available data sources
   - Determine required outputs

2. **Generate Analysis Plan** (AI)
   - Use "Data Analysis Template"
   - Create structured analytical approach
   - Define visualization and presentation strategy

3. **Prepare and Analyze Data** (Human+AI)
   - Process and clean data
   - Perform initial analysis
   - Generate visualizations

4. **Interpret Results** (AI+Human)
   - Create narrative explaining findings
   - Highlight key insights and patterns
   - Connect to original questions

5. **Generate Recommendations** (AI)
   - Develop actionable recommendations
   - Support with specific findings
   - Address limitations and assumptions

### Decision Support Workflow

1. **Define Decision Context** (Human)
   - Specify decision to be made
   - Identify options and constraints
   - Determine decision criteria

2. **Generate Decision Framework** (AI)
   - Use "Comparative Analysis Framework"
   - Create evaluation methodology
   - Define scoring and weighting system

3. **Evaluate Options** (AI+Human)
   - Score options against criteria
   - Calculate weighted results
   - Perform sensitivity analysis

4. **Generate Recommendation** (AI)
   - Present ranked options with rationale
   - Highlight key differentiators
   - Address potential objections

5. **Implement Decision** (Human)
   - Make final selection
   - Document decision process
   - Create implementation plan

## Cross-Model Coordination

### Multi-Model Research Projects

For comprehensive research projects requiring multiple perspectives:

1. **Project Definition** (Human)
   - Define research scope and objectives
   - Identify required specializations
   - Create coordination framework

2. **Specialized Research** (Multiple AI models)
   - Assign different aspects to specialized models:
     - Claude for nuanced technical research
     - GPT-4o with browsing for current information
     - DeepSeek for code-heavy components

3. **Synthesis** (Claude with Extended Thinking)
   - Combine specialized inputs
   - Resolve contradictions
   - Create unified perspective

4. **Review and Refinement** (Human)
   - Identify gaps or inconsistencies
   - Request additional specialized input
   - Direct final synthesis

### Progressive Refinement Chain

For complex creative or technical work:

1. **Initial Generation** (GPT-4 or Claude)
   - Create first draft with broad structure
   - Establish key components and approach
   - Identify areas needing specialization

2. **Specialized Enhancement** (Specialized model)
   - Elaborate specific technical sections
   - Enhance code components
   - Develop specific visualizations

3. **Critical Review** (Claude with Extended Thinking)
   - Analyze for consistency and completeness
   - Identify logical flaws or gaps
   - Generate specific improvement recommendations

4. **Final Refinement** (Original or specialized model)
   - Implement recommended improvements
   - Harmonize style and structure
   - Finalize for delivery

## Tool Integration Examples

### IDE Integration Workflow

1. **Local Development Environment**
   - Configure editor extensions (Cursor, GitHub Copilot)
   - Create template snippets for common prompts
   - Develop keyboard shortcuts for AI interactions

2. **Version Control Integration**
   - Include prompt templates in project repositories
   - Document AI usage in contribution guidelines
   - Create commit hooks for AI-assisted contributions

3. **Continuous Integration**
   - Automate AI-powered code reviews
   - Generate test cases with AI assistance
   - Develop documentation from code automatically

### Document Management Integration

1. **Content Creation**
   - Use AI templates for consistent document generation
   - Create modular content blocks for reuse
   - Develop style guides for AI-assisted writing

2. **Review and Refinement**
   - Implement AI-powered editing workflows
   - Create checklist templates for human review
   - Establish quality standards for AI outputs

3. **Publication and Distribution**
   - Automate format conversions for different platforms
   - Generate supplementary materials automatically
   - Create presentation materials from documents

## Practical Integration Recipes

### Code Creation Recipe

```markdown
# Code Creation Workflow

## Step 1: Problem Definition
Use this prompt:
```
I need to implement [specific functionality].
Key requirements:
- [Requirement 1]
- [Requirement 2]
- [Requirement 3]

Technical constraints:
- Language: [Language]
- Framework: [Framework]
- Performance needs: [Specific requirements]

Please provide a high-level approach before implementation.
```

## Step 2: Review Proposed Approach
Evaluate the AI's suggested approach against:
- Alignment with requirements
- Technical feasibility
- Integration with existing systems
- Performance characteristics

## Step 3: Request Implementation
Use this prompt:
```
The approach looks good. Please implement a complete solution with:
1. Well-structured code following [style guide/conventions]
2. Error handling for these cases: [list cases]
3. Unit tests covering key functionality
4. Clear documentation and usage examples

[Include any specific implementation details based on the approach]
```

## Step 4: Integration and Testing
- Merge the code into your project
- Run tests and validate functionality
- Document any modifications made to the AI-generated code
```

### Technical Document Creation Recipe

```markdown
# Technical Documentation Workflow

## Step 1: Document Specification
Use this prompt:
```
I need to create [document type] about [subject].

Target audience:
- [Primary audience with technical level]
- [Secondary audience if applicable]

Required sections:
- [Section 1]
- [Section 2]
- [Section 3]

Key information to include:
- [Key point 1]
- [Key point 2]
- [Key point 3]

Please provide an outline before creating the full document.
```

## Step 2: Review and Approve Outline
Evaluate the AI's suggested outline against:
- Completeness
- Logical structure
- Alignment with audience needs
- Appropriate technical depth

## Step 3: Request Full Document
Use this prompt:
```
The outline looks good. Please generate the complete document with:
1. All sections fully developed
2. Code examples for key concepts
3. Visual diagrams for [specific complex concepts]
4. Tables comparing [specific items to compare]

Formatting requirements:
- Use markdown formatting
- Include syntax highlighting for code blocks
- Create clear hierarchical structure with headers
```

## Step 4: Review and Finalize
- Check technical accuracy
- Ensure all requirements are met
- Add any missing information
- Format for final delivery
```

### Decision Analysis Recipe

```markdown
# Decision Analysis Workflow

## Step 1: Frame the Decision
Use this prompt:
```
I need to make a decision about [decision context].

Options being considered:
- [Option 1]
- [Option 2]
- [Option 3]

Key decision criteria:
- [Criterion 1] (importance: high/medium/low)
- [Criterion 2] (importance: high/medium/low)
- [Criterion 3] (importance: high/medium/low)

Constraints:
- [Constraint 1]
- [Constraint 2]

Please create a decision analysis framework.
```

## Step 2: Review Framework
Evaluate the AI's suggested framework against:
- Completeness of criteria
- Appropriate weighting
- Consideration of constraints
- Evaluation methodology

## Step 3: Request Analysis
Use this prompt:
```
The framework looks good. Please perform a complete analysis:
1. Evaluate each option against all criteria
2. Create a weighted decision matrix
3. Perform sensitivity analysis on the weights
4. Provide a clear recommendation with rationale
5. Address potential weaknesses of the recommended option
```

## Step 4: Make and Document Decision
- Review analysis
- Consider any factors not in the model
- Document final decision with rationale
- Create implementation plan
```
{
    "$schema": "https://lnav.org/schemas/format-v1.schema.json",
    "openamdb_log": {
        "title": "OpenAM Debug Log",
        "description": "Debug logs for the OpenAM identity provider.",
        "url": "http://openam.forgerock.org",
        "regex": {
            "std": {
                "pattern": "^(?<module>[\\w]+):(?<timestamp>\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}:\\d{3} [AP]M \\w+): Thread\\[(?<thread>[^,]+,\\d+,[^,]+)\\]\\n?(?:\\*+|(?<body>.*))$"
            }
        },
        "sample": [
            {
                "line": "amMonitoring:06/09/2014 02:49:59:447 PM UTC: Thread[http-80-1,5,main]\n**********************************************"
            },
            {
                "line": "amLog:06/09/2014 04:08:22:515 PM UTC: Thread[http-80-8,5,main]\nERROR: LogMessageProviderBase.createLogRecord: unable to locate message ID object for ATTEMPT_GET_METAALIAS"
            }
        ]
    }
}
# Cline MCP Configuration

This directory contains configuration files for Cline MCP (Model Context Protocol) servers.

## Installed MCP Servers

### Time MCP Server

- **Server Name**: `github.com/modelcontextprotocol/servers/tree/main/src/time`
- **Installation Method**: Using `uvx` (uv execute)
- **Command**: `uvx mcp-server-time`
- **Source**: [GitHub Repository](https://github.com/modelcontextprotocol/servers/tree/main/src/time)

#### Available Tools

1. `get_current_time` - Get current time in a specific timezone
   - Required arguments:
     - `timezone` (string): IANA timezone name (e.g., 'America/New_York', 'Europe/London')

2. `convert_time` - Convert time between timezones
   - Required arguments:
     - `source_timezone` (string): Source IANA timezone name
     - `time` (string): Time in 24-hour format (HH:MM)
     - `target_timezone` (string): Target IANA timezone name

## Usage Examples

### Get Current Time

```json
{
  "name": "get_current_time",
  "arguments": {
    "timezone": "Europe/Warsaw"
  }
}
```

### Convert Time Between Timezones

```json
{
  "name": "convert_time",
  "arguments": {
    "source_timezone": "America/New_York",
    "time": "16:30",
    "target_timezone": "Asia/Tokyo"
  }
}
```

## Configuration

The MCP server configuration is stored in `cline_mcp_settings.json` in this directory. The Cursor settings file is configured to use this custom path for MCP settings.
# Advanced Prompting Techniques

This document outlines sophisticated AI prompting methods to achieve superior results for complex tasks. These techniques can be combined with task-specific templates for maximum effectiveness.

## Chain of Thought Prompting

### Description
Guide the AI through explicit reasoning steps before reaching a conclusion. This produces more accurate results for complex reasoning tasks.

### Implementation
```markdown
# Chain of Thought Request

I need you to solve this problem step-by-step:

[Problem description]

Please:
1. First, identify the key elements of this problem
2. Break down the approach into clear logical steps
3. Execute each step, showing your work
4. Evaluate the result for correctness
5. Summarize the final answer

Format each step clearly and explain your reasoning at each stage.
```

### When to Use
- Complex logical problems
- Multi-step calculations
- Debugging scenarios
- Complex decision-making with multiple factors

## Recursive Refinement

### Description
Use a staged approach where the AI first generates a draft, then iteratively improves it through self-critique and revision.

### Implementation
```markdown
# Recursive Refinement Request

I need a [type of content] about [subject]. Please approach this in three stages:

## Stage 1: Initial Draft
Create a first version that covers the core elements.

## Stage 2: Self-Critique
Review your draft and identify:
- Areas that need more detail or clarity
- Potential inaccuracies or omissions
- Structural improvements
- Stronger examples or evidence

## Stage 3: Refined Version
Create an improved version that addresses all the issues identified in your critique.

Present both the critique and the final refined version.
```

### When to Use
- Complex writing tasks
- System designs that require optimization
- Code that needs to balance multiple objectives
- Analyses where accuracy is critical

## Persona-Based Prompting

### Description
Direct the AI to adopt specific expert perspectives to leverage specialized knowledge and approaches.

### Implementation
```markdown
# Expert Perspective Request

I need analysis on [topic] from multiple expert perspectives.

Please approach this sequentially as the following experts:

1. As a senior systems architect with 15+ years of experience:
   [Specific questions/aspects to address]

2. As a security specialist focused on zero-trust implementations:
   [Specific questions/aspects to address]

3. As a DevOps engineer who prioritizes maintainability and observability:
   [Specific questions/aspects to address]

4. As a synthesis expert, integrate the perspectives above into final recommendations.

For each perspective, note key insights, blind spots, and specific recommendations.
```

### When to Use
- Getting well-rounded analysis of complex problems
- Identifying potential issues from different viewpoints
- Challenging initial assumptions
- Complex design decisions with multiple stakeholders

## Template Decomposition

### Description
Break complex outputs into parameterized templates with clear placeholders for the AI to fill in.

### Implementation
```markdown
# Template Decomposition Request

I need a [document type] with the following structure. For each section, I've provided guidance on what to include:

# [TITLE: Create a concise, descriptive title]

## Executive Summary
[EXEC_SUMMARY: Provide a 3-5 sentence overview that captures the key points]

## Problem Statement
[PROBLEM: Describe the current situation, challenges, and why this needs to be addressed]

## Proposed Approach
[APPROACH: Detail the recommended solution with justification]

## Implementation Steps
[STEPS: List 5-7 concrete actions in chronological order with owners and timeline]

## Success Metrics
[METRICS: Define 3-5 measurable indicators of success]

Please complete each section according to the instructions in brackets, then remove the bracketed instructions from the final output.
```

### When to Use
- Standardized document creation
- Complex outputs with consistent structure
- When you need to ensure all components are included
- Content that follows established patterns or protocols

## Comparative Analysis Framework

### Description
Structure a detailed comparison across multiple dimensions with explicit evaluation criteria.

### Implementation
```markdown
# Comparative Analysis Request

Please evaluate these options against the specified criteria:

## Options to Evaluate
- [Option 1]
- [Option 2]
- [Option 3]

## Evaluation Criteria
For each criterion, rate options on a scale of 1-10 and provide specific justification:

1. [Criterion 1] (Weight: X%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

2. [Criterion 2] (Weight: Y%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

3. [Criterion 3] (Weight: Z%)
   - What constitutes a score of 1-3, 4-7, and 8-10
   - Specific aspects to consider

## Output Format
- Provide individual criterion scores with justification
- Calculate weighted total scores
- Rank options from best to worst
- Explain key differentiators
- Include sensitivity analysis (how would rankings change if weights shifted)
```

### When to Use
- Technology selection decisions
- Architecture or design alternatives
- Investment or prioritization decisions
- Strategic option evaluation

## Decision Tree Construction

### Description
Map out a complex decision process with conditional branches based on different scenarios.

### Implementation
```markdown
# Decision Tree Request

Help me build a decision tree for [scenario/problem].

## Initial Conditions
- [Describe starting state and key variables]

## Primary Decision Points
For each primary decision point:
1. Identify the key question at this stage
2. List all possible choices (2-4 options)
3. For each choice:
   - Describe immediate consequences
   - Identify subsequent decision points
   - Note critical dependencies or assumptions

## Evaluation Metrics
At each terminal node (end state), evaluate:
- [Metric 1]
- [Metric 2]
- [Metric 3]

## Visual Representation
Create a text-based diagram of the decision tree showing:
- Decision nodes (questions)
- Option branches
- Terminal nodes (outcomes)
- Critical path highlighting
```

### When to Use
- Complex decision processes with multiple variables
- Scenario planning
- Risk assessment
- Troubleshooting guides

## System Interaction Modeling

### Description
Model complex interactions between system components, focusing on interfaces, dependencies, and data flows.

### Implementation
```markdown
# System Interaction Model Request

Help me understand interactions between components in [system name].

## System Components
- [Component 1]: [Brief description of responsibility]
- [Component 2]: [Brief description of responsibility]
- [Component 3]: [Brief description of responsibility]

## For Each Interface Between Components
1. Identify direction of dependency
2. Describe data/control flow
3. Document interface contract (API, events, etc.)
4. Note synchronicity (sync/async)
5. Identify potential failure modes
6. Document retry/resilience patterns

## Sequence Diagrams
Create text-based sequence diagrams for these key flows:
- [Flow 1]
- [Flow 2]
- [Flow 3]

## Critical Path Analysis
Identify and analyze:
- Performance bottlenecks
- Single points of failure
- Scalability constraints
- Data consistency challenges
```

### When to Use
- Complex distributed system design
- API design and documentation
- Troubleshooting integration issues
- Performance optimization

## Layered Explanation

### Description
Structure explanations across multiple levels of abstraction, from conceptual to implementation details.

### Implementation
```markdown
# Layered Explanation Request

Please explain [topic/concept] using a layered approach:

## Layer 1: Conceptual Overview (ELI5)
Explain the core concept as you would to a non-technical person, using analogies and simplified models.

## Layer 2: Functional Understanding
Explain how this works from a functional perspective - what it does, major components, and how they interact.

## Layer 3: Technical Implementation
Provide implementation details including:
- Specific technologies/methods used
- Important algorithms or patterns
- Code examples where appropriate
- Performance considerations

## Layer 4: Advanced Considerations
Discuss:
- Edge cases and how they're handled
- Optimization techniques
- Alternative approaches
- Common misconceptions
```

### When to Use
- Explaining complex technical concepts
- Educational content
- Technical documentation for diverse audiences
- Knowledge transfer

## Edge Case Exploration

### Description
Systematically identify and address uncommon but important scenarios that might be overlooked.

### Implementation
```markdown
# Edge Case Exploration Request

For this [system/code/process], help me identify and address edge cases:

## System Description
[Brief description of the system/code/process]

## Normal Operation
[Description of typical usage patterns and expected behavior]

## Edge Case Categories
For each category, identify specific edge cases and how to handle them:

1. Input Boundaries
   - Minimum/maximum values
   - Empty/null inputs
   - Malformed inputs
   - Extremely large inputs

2. Resource Constraints
   - Memory limitations
   - CPU constraints
   - Network issues
   - Timeouts

3. Concurrent Operations
   - Race conditions
   - Deadlocks
   - Contention scenarios

4. Integration Points
   - External system failures
   - API contract changes
   - Versioning issues

5. State Transitions
   - Interrupted operations
   - Invalid state sequences
   - Recovery scenarios

## For Each Edge Case
1. Describe the scenario
2. Explain potential impact
3. Recommend detection method
4. Provide handling strategy
5. Suggest testing approach
```

### When to Use
- Critical system design
- Security-sensitive applications
- High-reliability requirements
- Complex state management scenarios
# Developer Profile

## Core Identity
- Senior software engineer specializing in systems, AI infrastructure, and developer tooling
- Background in distributed systems, data engineering, and systems architecture
- Preference for intelligent, reproducible, and automated solutions
- Values high-quality implementations with minimal technical debt

## Technical Environment

### Hardware/OS
- Apple Silicon Mac (primary development environment)
- Multi-machine synchronization requirements
- Local-first architecture preferences

### Development Environment
- **Package Management**: Nix, Homebrew, uv (Python)
- **Configuration**: Git-tracked dotfiles, declarative configurations
- **Editor Ecosystem**: Neovim (LazyVim), Cursor IDE
- **Terminal**: Ghostty with ZSH
- **Window Management**: Raycast, Swish, Yabai, Hammerspoon

### Technical Stack
- **Primary Languages**:
  - TypeScript (eslint + prettier)
  - Python (ruff for linting)
  - Exposure to Rust, Go, and other systems languages
- **Tooling Preferences**:
  - Modern CLI utilities (ripgrep, fd, zoxide, atuin, starship)
  - AI-augmented development workflows
  - Keyboard-driven interfaces with minimal mouse usage

## Workflow Characteristics

### General Approach
- Preference for keyboard-driven, efficient workflows
- Strong Vim expertise and modal editing proficiency
- Value tools that reduce cognitive load and automate routine tasks
- Emphasis on reproducibility and declarative configuration

### Technical Values
- Intelligence/capability prioritized over resource usage
- Clear, maintainable code valued over cleverness
- Preference for functional programming patterns where appropriate
- Appreciation for elegant solutions that balance simplicity and power

### Learning Style
- Preference for bottom-up understanding with comprehensive examples
- Value explanations that balance theory and practical application
- Appreciate insights into underlying mechanisms and design decisions
- Learn best through hands-on implementation after conceptual understanding

## Communication Preferences

### Technical Documentation
- Structured, hierarchical organization with clear navigation
- Balanced detail: comprehensive without overwhelming
- Code examples that demonstrate real-world usage
- Visual aids for complex relationships and architectures

### Decision Processes
- Evidence-based recommendations with explicit reasoning
- Clear articulation of tradeoffs and constraints
- Preference for quantitative evaluation when possible
- Appreciation for multiple perspectives before conclusions

## AI Collaboration Context

### Interaction Mode
- Direct, efficient communication without unnecessary niceties
- Preference for precise, technical language
- Appreciation for both depth (when needed) and brevity (when appropriate)

### Output Preferences
- Structured, scannable content with clear section demarcation
- Visual representations for complex relationships
- Code that balances readability and efficiency
- Practical, implementable recommendations

### Follow-up Style
- Iterative refinement rather than extended initial responses
- Focused questions to clarify ambiguities
- Building on previous context rather than repetition
# AI Master Configuration

## System Overview

This document serves as the central configuration for all AI interactions. The complete system consists of:

1. **Master Configuration** (this file) - Core principles and system overview
2. **Developer Profile** - Your personal context and technical preferences
3. **Response Formatting** - Output structure and visual presentation guidelines
4. **Task Templates** - Purpose-specific prompt templates for different scenarios
5. **Advanced Techniques** - Advanced prompting methods for complex tasks
6. **Tool-Specific Configs** - Model-specific optimization guidelines

## Core Principles

1. **Maximum Context Efficiency**
   - Provide just enough context for optimal output
   - Use reference links to previous conversations when building on past work
   - Clearly separate instructions from content

2. **Structured Output by Default**
   - All responses should follow consistent structural patterns
   - Visual elements should be used where they enhance clarity
   - Complex information should be organized in digestible segments

3. **Progressive Enhancement**
   - Start with base prompts and add specific modules as needed
   - Build complex prompts by combining simpler templates
   - Refine outputs through iterative instructions

4. **Tool Specialization**
   - Use optimal AI model for each specific task
   - Leverage unique capabilities of different AI systems
   - Chain tools together for complex workflows

## Usage Methodology

1. **Task Analysis**
   - Determine primary goal of interaction
   - Identify relevant template modules
   - Consider required output formats

2. **Prompt Assembly**
   - Include developer profile (abbreviated if appropriate)
   - Add task-specific template
   - Specify response format requirements
   - Include any specialized techniques

3. **Response Management**
   - Evaluate output quality against requirements
   - Use follow-up prompts for refinement
   - Extract and store reusable components

4. **Continuous Improvement**
   - Note effective prompt patterns
   - Update templates based on successful interactions
   - Develop specialized templates for recurring tasks

## Integration Framework

These templates support integration with:

- **Claude Models** (Sonnet, Opus, Haiku)
- **OpenAI Models** (GPT-4, GPT-4o)
- **DeepSeek Models** (DeepSeek Coder)
- **Open Source Models** (via Ollama integration)
- **AI-powered Development Tools** (Cursor, GitHub Copilot)
- **Document Processing Tools** (for export/import)

## Template Evolution Strategy

For maximum effectiveness, treat these templates as living documents:

1. When an AI generates exceptionally good output, capture the prompt structure
2. When complex tasks are completed successfully, document the prompt chain
3. When outputs require significant refinement, note the refinement pattern
4. Regularly update templates with new capabilities as AI models evolve

## Quick Reference

| Task Type | Primary Template | Secondary Templates | Optimal AI Model |
|-----------|------------------|---------------------|------------------|
| Code Generation | `coding-primary.md` | `visual-code.md`, `advanced-reasoning.md` | DeepSeek Coder/Claude Sonnet |
| Code Review | `code-analysis.md` | `structured-feedback.md` | Claude Sonnet w/Extended Thinking |
| System Design | `system-design.md` | `visual-architecture.md` | Claude Opus/GPT-4 |
| Data Analysis | `data-analysis.md` | `visual-data.md` | Claude Sonnet |
| Technical Writing | `technical-docs.md` | `doc-formatting.md` | Claude Sonnet |
| Research | `research-synthesis.md` | `citation-format.md` | GPT-4o with browsing |
# Response Formatting Guidelines

## Document Structure

### Hierarchical Organization
- Use clear hierarchical headers (H1, H2, H3) for logical organization
- Ensure headers create a scannable outline of the content
- Maintain consistent heading levels for parallel concepts
- Keep header text concise and descriptive

### Content Blocks
- Separate distinct concepts with visual breaks
- Use bullet points for lists of related items
- Use numbered lists for sequential steps or ranked items
- Group related information in cohesive sections

### Progressive Disclosure
- Present most important information first
- Start with summary/overview before details
- Use expandable sections for supplementary details when appropriate
- Structure from general to specific

## Visual Elements

### Tables
For comparisons and structured data:
- Include 5-7 crucial criteria from modern best practices
- Score each option 1-10 for each criterion
- Apply appropriate weights when criteria importance varies
- Present options in descending order of overall score
- Include scoring rationale in the table or notes
- Add a clear explanation for the top choice selection

Example format:
```
| Option | Criterion 1 (w=0.3) | Criterion 2 (w=0.5) | Criterion 3 (w=0.2) | Weighted Score | Rationale |
|--------|---------------------|---------------------|---------------------|----------------|-----------|
| Opt A  | 9 (2.7)             | 7 (3.5)             | 8 (1.6)             | 7.8            | Excels at... |
```

### Diagrams
For complex relationships, include diagrams that show:
- System architecture with components and interfaces
- Workflow sequences with decision points
- Entity relationships with cardinality
- Data transformations and processing pipelines
- Algorithm mechanisms with step transitions

Diagram guidelines:
- Clear labels on all elements
- Logical flow direction (typically left-to-right or top-to-bottom)
- Concise legends when using symbols
- Visual hierarchy that emphasizes important elements

### Code Formatting
- Use appropriate syntax highlighting
- Include meaningful comments at logical breakpoints
- Add type annotations when helpful
- Demonstrate best practices in the code structure
- Include example usage when not obvious

## Content Quality

### Technical Precision
- Use correct terminology consistently
- Cite specific versions/tools when version-dependent
- Avoid vague generalizations
- Qualify claims with supporting evidence or reasoning

### Implementation Focus
- Prefer concrete examples over abstract concepts
- Include specific configuration samples
- Provide resource requirements and constraints
- Address practical implementation considerations

### Balanced Detail
- Provide comprehensive information without verbosity
- Emphasize distinctive or non-obvious aspects
- De-emphasize standard or expected behaviors
- Adjust detail level based on complexity and importance

## Special Content Types

### Process Descriptions
- Break down into clear sequential steps
- Indicate expected outcomes at key points
- Note potential failure points and mitigation
- Include validation methods for each stage

### Analysis Outputs
- Clearly state assumptions and methodology
- Present findings with supporting evidence
- Discuss limitations and confidence levels
- Include actionable conclusions

### Technical Evaluations
- Establish clear evaluation criteria
- Use consistent rating system
- Provide specific examples supporting ratings
- Include quantitative metrics when available

## Document Formatting

### Text Styling
- Use bold for emphasis of key terms and concepts
- Use italic for secondary emphasis or specialized terms
- Use code formatting for commands, variables, and technical identifiers
- Maintain consistent styling conventions throughout

### Lists and Indentation
- Use bulleted lists for unordered collections
- Use numbered lists for sequential steps
- Indent sublists consistently
- Keep list items parallel in structure

### Code Blocks
- Use triple backticks with language specification
- Format code with consistent indentation
- Break long lines appropriately
- Include sample output when helpful
# Task-Specific Templates

These templates provide specialized prompting patterns for common task types. They are designed to be combined with your Developer Profile and Response Format preferences.

## Code Generation Template

```markdown
# Code Generation Request

## Task Description
[Brief description of the code you need]

## Requirements
- Language: [Programming language and version]
- Framework/Libraries: [Any specific libraries or frameworks]
- Performance needs: [Time/space complexity requirements]
- Edge cases to handle: [List specific edge cases]

## Expected Functionality
[Detailed description of what the code should do]

## Output Format
- Provide clean, efficient implementation with type annotations
- Include relevant error handling
- Add comments explaining complex logic or design decisions
- Demonstrate usage with example inputs/outputs
- Discuss any performance considerations or tradeoffs

## Additional Context
[Any other relevant information or constraints]
```

## Code Review Template

```markdown
# Code Review Request

## Code to Review
```[language]
[Paste code here]
```

## Review Focus
- [Specific aspects to focus on, e.g., "performance", "security", "readability"]
- [Any specific concerns or questions]

## Expected Output
- Identify potential issues (bugs, performance problems, security vulnerabilities)
- Suggest improvements with specific code examples
- Highlight good practices already present
- Analyze overall code structure and organization
- Provide a table scoring the code on key dimensions (1-10):
  - Correctness, Performance, Maintainability, Security, Readability

## Additional Context
[Relevant background information about the code purpose, environment, etc.]
```

## System Design Template

```markdown
# System Design Request

## Design Task
[Brief description of the system to be designed]

## Requirements
- Functionality: [Key functional requirements]
- Scale: [Expected scale/load characteristics]
- Constraints: [Budget, technology, or other constraints]
- Quality attributes: [Reliability, performance, security needs]

## Expected Deliverables
- High-level architecture diagram
- Component breakdown with responsibilities
- Data model and flow
- API design (if applicable)
- Technology stack recommendations with rationale
- Scalability considerations
- Potential failure modes and mitigations

## Output Format Preferences
- Start with executive summary of the approach
- Include visual architecture diagram
- Provide component tables with responsibilities and interfaces
- Include sequence diagrams for key flows
- List technology choices with explicit rationale

## Additional Context
[Any background information or existing systems to consider]
```

## Data Analysis Template

```markdown
# Data Analysis Request

## Analysis Goal
[What insight or outcome you're seeking]

## Data Description
- Source: [Where the data comes from]
- Format: [CSV, JSON, etc.]
- Size: [Approximate size or dimensions]
- Key fields: [Important columns or attributes]
- Special characteristics: [Missing values, outliers, etc.]

## Analysis Requirements
- Specific questions to answer: [List questions]
- Metrics to calculate: [List metrics]
- Visualizations needed: [Types of charts/plots]
- Statistical methods to apply: [If applicable]

## Output Format Preferences
- Present key findings first as executive summary
- Include properly labeled visualizations
- Provide tables summarizing statistical results
- Document methodology and assumptions
- Include recommendations based on findings

## Additional Context
[Any relevant background information or hypotheses]
```

## Technical Documentation Template

```markdown
# Technical Documentation Request

## Documentation Purpose
[What this documentation will be used for and by whom]

## Subject Matter
[The system, code, or process to document]

## Documentation Requirements
- Target audience: [Who will read this]
- Technical depth: [Beginner, intermediate, expert]
- Sections to include: [List required sections]
- Examples needed: [Specific examples to include]

## Format Preferences
- Include table of contents
- Use hierarchical structure with clear navigation
- Provide code examples for key operations
- Include diagrams for complex concepts
- Add troubleshooting section for common issues

## Additional Context
[Any background information or existing documentation to reference]
```

## Research Synthesis Template

```markdown
# Research Synthesis Request

## Research Topic
[The subject to research]

## Research Objectives
- Key questions to answer: [List specific questions]
- Scope boundaries: [What to include/exclude]
- Required depth: [How comprehensive should this be]

## Source Preferences
- Preferred authorities: [Any specific sources to prioritize]
- Recency requirements: [How recent should sources be]
- Source types: [Academic papers, industry reports, etc.]

## Output Format
- Begin with executive summary of key findings
- Structure information hierarchically by subtopic
- Include comparison tables for competing approaches/views
- Provide citations for all significant claims
- End with synthesis of implications and next steps

## Additional Context
[Your current knowledge level, specific applications of the research, etc.]
```

## Technical Comparison Template

```markdown
# Technical Comparison Request

## Items to Compare
[List the technologies, methods, or tools to compare]

## Comparison Criteria
[List the 5-7 most important factors for evaluation]

## Usage Context
[How and where the solution will be used]

## Constraints
[Any limitations that would affect the decision]

## Output Format
- Start with brief overview of each option
- Create weighted comparison table (criteria weighted by importance)
- Score each option 1-10 on each criterion with clear rationale
- Include pros/cons section for each option
- Provide final recommendation with justification
- Address potential objections to the recommendation

## Additional Context
[Your experience level with these options, specific requirements, etc.]
```

## Troubleshooting Template

```markdown
# Troubleshooting Request

## Problem Description
[Detailed description of the issue]

## Environment Details
- Hardware: [Relevant hardware specifications]
- Software: [OS, application versions, etc.]
- Configuration: [Relevant configuration details]
- Recent changes: [Any recent changes that might be relevant]

## Observed Behavior
[What's actually happening]

## Expected Behavior
[What should be happening]

## Troubleshooting Already Attempted
[Steps you've already taken to diagnose or fix]

## Output Format
- Analyze possible root causes systematically
- Provide diagnostic steps in order of likelihood
- Include specific commands or code to run for diagnosis
- Suggest solutions with step-by-step implementation
- Explain preventative measures for the future

## Additional Context
[Error logs, screenshots, or other relevant information]
```

## Project Planning Template

```markdown
# Project Planning Request

## Project Description
[Brief description of the project]

## Project Goals
- Primary objectives: [List main goals]
- Success criteria: [How success will be measured]
- Constraints: [Time, budget, resource constraints]

## Scope
- In scope: [Features/components definitely included]
- Out of scope: [What's explicitly excluded]
- Potential extensions: [Nice-to-haves if time permits]

## Output Format
- Provide project breakdown structure with major components
- Create task dependency diagram
- Include risk assessment table with mitigation strategies
- Suggest timeline with milestones
- Recommend resource allocation approach
- Include key decision points and criteria

## Additional Context
[Team composition, expertise areas, relevant experience, etc.]
```
# AI Tool-Specific Configurations

This document provides optimized configurations for specific AI tools and models to maximize their unique capabilities and strengths.

## Claude (Anthropic)

### Claude 3.7 Sonnet with Extended Thinking

#### Optimal Use Cases
- Complex reasoning tasks
- Detailed technical analysis
- Nuanced explanations balancing depth and clarity
- Code review and improvement
- Technical writing with precise language

#### Configuration Optimizations
```markdown
# Claude 3.7 Sonnet Configuration

I'd like you to engage extended thinking mode and act as [specific role].

When analyzing this problem:
1. Think step-by-step through the reasoning process
2. Consider multiple approaches before selecting the optimal solution
3. Explicitly note assumptions and their implications
4. Evaluate tradeoffs quantitatively where possible

Please structure your response according to:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Enable extended thinking mode for complex reasoning
- Request explicit reasoning paths for verification
- Use for tasks requiring careful nuance and precision
- Excellent for generating well-structured, hierarchical content

### Claude 3 Opus

#### Optimal Use Cases
- System design and architecture
- Comprehensive technical evaluations
- Complex document generation
- In-depth research synthesis
- Detailed code explanations

#### Configuration Optimizations
```markdown
# Claude 3 Opus Configuration

I'd like you to act as [specific role] with extensive expertise in [domain].

For this complex task, please:
1. First map out the entire solution space
2. Identify key decision points and dependencies
3. Develop a comprehensive approach
4. Elaborate on implementation details

Please structure your response with:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Superior context handling for complex, multi-part problems
- Capability to generate comprehensive technical documents
- Handles nuanced, complex instructions effectively
- Excellent for multi-stage reasoning tasks

## OpenAI Models

### GPT-4o

#### Optimal Use Cases
- Interactive problem-solving
- Multi-modal inputs (text + images)
- Creative technical solutions
- Real-time web research integration
- Code generation with clear explanations

#### Configuration Optimizations
```markdown
# GPT-4o Configuration

You are an expert [specific role] assisting with [task type].

Consider this problem holistically:
1. Analyze the complete context, including any provided images or references
2. Formulate a structured approach
3. Generate concrete implementation details
4. Explain your reasoning transparently

Please follow this response structure:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Ability to process and reference images in reasoning
- Browsing capabilities for real-time research
- Strong code generation capabilities with context awareness
- Good at creative solutions to technical problems

### GPT-4o with Advanced Data Analysis

#### Optimal Use Cases
- Statistical analysis and data exploration
- Code execution and verification
- Data visualization suggestions
- Iterative problem-solving with code
- Mathematical proofs and calculations

#### Configuration Optimizations
```markdown
# GPT-4o with Advanced Data Analysis Configuration

You are an expert data scientist and programmer helping with [specific task].

For this analysis:
1. Explore the data to understand its structure and characteristics
2. Suggest appropriate analytical approaches
3. Implement solutions using executable code
4. Validate results and check for errors
5. Visualize key findings effectively

I need the response to include:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Code execution capabilities for instant verification
- Iterative data exploration and analysis
- Creation of visualizations from data
- Ability to process and analyze uploaded files
- Mathematical computation capabilities

## DeepSeek Models

### DeepSeek Coder

#### Optimal Use Cases
- Complex software engineering tasks
- Programming language-specific optimizations
- Implementation of algorithms and data structures
- Code optimization and refactoring
- Technical software documentation

#### Configuration Optimizations
```markdown
# DeepSeek Coder Configuration

You are an expert software engineer specializing in [language/framework].

For this coding task:
1. Analyze requirements with attention to edge cases
2. Design a solution prioritizing [efficiency/readability/maintainability]
3. Implement with idiomatic patterns and best practices
4. Include comprehensive documentation and usage examples
5. Highlight optimization opportunities

Structure your response as follows:
[Insert relevant response format preferences]

For this specific task, I need:
[Insert task-specific template]
```

#### Special Features to Leverage
- Exceptional code generation quality
- Strong understanding of software engineering principles
- Ability to work within existing codebases
- Effective at complex algorithm implementation
- Language-specific optimization knowledge

## Multi-Tool Workflows

### Research → Analysis → Implementation

This workflow combines multiple AI tools for comprehensive solutions.

#### Process Flow
1. **Research Phase** (GPT-4o with browsing)
   - Gather relevant information
   - Identify state-of-the-art approaches
   - Collect reference examples

2. **Analysis Phase** (Claude 3.7 Sonnet with Extended Thinking)
   - Evaluate approaches from research
   - Perform detailed tradeoff analysis
   - Develop conceptual design

3. **Implementation Phase** (DeepSeek Coder)
   - Convert design into concrete implementation
   - Optimize for target environment
   - Develop tests and documentation

#### Configuration Example
```markdown
# Multi-Tool Workflow: Research Phase

You are a research specialist gathering information on [topic].

Please find the most relevant and current information about:
- Core concepts and principles
- Leading approaches and methodologies
- Benchmark results and comparisons
- Implementation considerations

Format findings as a structured research brief with sections for each area and citations.

# Multi-Tool Workflow: Analysis Phase (For Claude)

Using the research provided, perform a comprehensive analysis:

1. Evaluate each approach against these criteria:
   - [List criteria with weights]
2. Identify key tradeoffs and dependencies
3. Recommend an optimal approach with justification
4. Outline a conceptual design

# Multi-Tool Workflow: Implementation Phase (For DeepSeek)

Based on the provided conceptual design, implement a solution that:
1. Follows the specified approach
2. Optimizes for [specific requirements]
3. Includes error handling and edge cases
4. Provides documentation and usage examples
```

### Design → Feedback → Refinement

This iterative workflow improves quality through multiple models.

#### Process Flow
1. **Initial Design** (GPT-4o or Claude Opus)
   - Generate comprehensive first draft
   - Include all required components
   - Document design decisions

2. **Critical Feedback** (Claude with Extended Thinking)
   - Analyze for weaknesses and gaps
   - Identify optimization opportunities
   - Suggest specific improvements

3. **Refinement** (Original or specialized model)
   - Incorporate feedback
   - Optimize critical components
   - Finalize the solution

#### Configuration Example
```markdown
# Initial Design Phase

Create a comprehensive design for [system/document/code].

Requirements:
- [List key requirements]
- [List constraints]
- [List quality attributes]

Provide a complete initial design including:
- [List expected deliverables]

# Feedback Phase (For Claude)

Review the provided design critically:

1. Evaluate against these criteria:
   - [List evaluation criteria]
2. Identify potential weaknesses, including:
   - Performance bottlenecks
   - Scalability issues
   - Security concerns
   - Maintenance challenges
3. Suggest specific improvements for each issue
4. Prioritize recommendations by impact

# Refinement Phase

Based on the feedback provided, refine the initial design:

1. Address each identified issue with specific changes
2. Optimize the critical components
3. Justify any feedback points not incorporated
4. Provide a complete, refined version
```

## Custom Configuration Templating

### Configuration Template Structure

Create your own tool-specific configurations using this template:

```markdown
# [Tool Name] Configuration for [Task Type]

## Context Setting
[Establish role, expertise level, and general approach]

## Task Instructions
[Specific task details and requirements]

## Processing Guidelines
[How the AI should approach the problem]
- Step 1: [Specific instruction]
- Step 2: [Specific instruction]
- etc.

## Output Format
[Response structure and formatting requirements]

## Additional Parameters
[Any special modes or settings to enable]
```

### Calibration Process

To optimize configurations for specific models:

1. **Baseline Testing**
   - Test the same prompt across different models
   - Note differences in output quality and approach
   - Identify relative strengths and weaknesses

2. **Parameter Tuning**
   - Adjust instructions based on model strengths
   - Modify level of detail in task breakdown
   - Calibrate output format specificity

3. **Iteration**
   - Refine based on output quality
   - Create model-specific instruction sets
   - Document effective patterns
# Visual Format Specifications

This document provides detailed specifications for visual elements in AI responses, ensuring consistent, high-quality visual outputs across different AI tools.

## Architecture Diagrams

### System Architecture Diagrams

#### Content Requirements
- **Components**: All significant system components clearly labeled
- **Interfaces**: Connections between components with interface type
- **Data Flow**: Directional indicators showing information flow
- **External Boundaries**: Clear system boundaries and external integrations
- **Layers**: Logical grouping of components by function/layer
- **Dependencies**: Clear indication of component dependencies

#### Notation Guidelines
```
[External System] ←→ [API Gateway] → [Service A] → [Database]
                                    ↓
                                [Service B] → [Cache]
                                    ↓
                               [Event Bus] → [Service C]
```

#### Example Specification
```markdown
Please create a system architecture diagram showing:
- Frontend components (UI, state management)
- Backend services (authentication, core logic, data access)
- Data storage systems (databases, caches)
- External integrations (third-party APIs, partner systems)
- Communication patterns (sync/async, events, direct calls)

Use distinct symbols for different component types and indicate data flow directions.
```

### Sequence Diagrams

#### Content Requirements
- **Actors**: All participants in the interaction
- **Time Flow**: Clear temporal sequence (typically top to bottom)
- **Messages**: Labeled arrows showing requests/responses
- **Activation**: Indication of when components are active
- **Conditional Paths**: Decision points and alternative flows
- **Failure Scenarios**: Error handling paths where relevant

#### Notation Guidelines
```
Actor     Service A     Service B     Database
  |           |             |            |
  |--Request->|             |            |
  |           |--Query----->|            |
  |           |             |--DB Call-->|
  |           |             |<--Result---|
  |           |<--Response--|            |
  |<--Reply---|             |            |
```

#### Example Specification
```markdown
Create a sequence diagram for user authentication flow showing:
- Initial request from client
- Authentication service validation
- Token generation
- Response to client
- Subsequent authenticated request
- Token validation
- Resource access

Include error paths for:
- Invalid credentials
- Expired tokens
- Insufficient permissions
```

## Data Visualization

### Comparison Tables

#### Content Requirements
- **Header Row**: Clear column headers with units if applicable
- **First Column**: Items being compared
- **Data Cells**: Consistent formatting of values
- **Scoring**: Numeric ratings (1-10) with visual indicators
- **Weighting**: Weight values for weighted calculations
- **Totals**: Weighted scores and rankings
- **Highlight**: Visual emphasis on optimal values

#### Format Example
```
| Option | Performance (w=0.3) | Security (w=0.4) | Maintainability (w=0.3) | Weighted Score | Rank |
|--------|---------------------|------------------|-------------------------|----------------|------|
| Opt A  | 9 ⭐⭐⭐ (2.7)        | 7 ⭐⭐ (2.8)       | 8 ⭐⭐⭐ (2.4)            | 7.9            | 1    |
| Opt B  | 7 ⭐⭐ (2.1)          | 8 ⭐⭐⭐ (3.2)      | 6 ⭐⭐ (1.8)              | 7.1            | 2    |
| Opt C  | 8 ⭐⭐⭐ (2.4)         | 6 ⭐⭐ (2.4)       | 7 ⭐⭐ (2.1)              | 6.9            | 3    |
```

#### Example Specification
```markdown
Create a comparison table for the options with these characteristics:
- Include all 5 options in rows
- Evaluate each on the 6 criteria specified
- Use a 1-10 scale for each criterion
- Apply the weights as indicated in parentheses
- Calculate weighted scores
- Rank options from highest to lowest score
- Add visual indicators (⭐) for scores:
  - 1-3: No stars
  - 4-6: ⭐
  - 7-8: ⭐⭐
  - 9-10: ⭐⭐⭐
- Highlight the best score in each column
```

### Decision Trees

#### Content Requirements
- **Decision Nodes**: Questions or decision points
- **Option Branches**: Paths representing choices
- **Leaf Nodes**: Outcomes or terminal states
- **Branch Labels**: Conditions for taking each path
- **Probabilities/Values**: Associated with paths or nodes (if applicable)
- **Critical Path**: Highlighted recommended path

#### Notation Guidelines
```
                         [Initial Decision]
                           /           \
                     [Yes]/             \[No]
                         /               \
            [Secondary Decision]     [Alternative Decision]
               /         \               /        \
          [Opt A]      [Opt B]      [Opt C]     [Opt D]
```

#### Example Specification
```markdown
Create a decision tree for the described scenario with:
- Root node representing the initial decision
- Branches for each option with conditions
- Secondary decision points where relevant
- Terminal nodes showing outcomes
- Annotations with key considerations at each step
- Highlight the recommended path based on specified criteria
```

### Process Flows

#### Content Requirements
- **Start/End**: Clear beginning and end points
- **Actions**: Process steps with descriptive labels
- **Decisions**: Conditional branching points
- **Flows**: Directional arrows connecting elements
- **Loops**: Clearly marked iteration points
- **Swimlanes**: Responsibility boundaries (if applicable)
- **Artifacts**: Inputs and outputs of steps

#### Notation Guidelines
```
(Start) → [Process Step] → <Decision?> → [Success Step] → (End)
                               |
                               ↓
                          [Failure Step] → [Recovery] ↩
```

#### Example Specification
```markdown
Create a process flow diagram showing:
- Initial request handling
- Validation steps with error paths
- Main processing sequence
- Decision points with criteria
- Success and failure paths
- Retry loops where applicable
- Final states and outputs

Use distinct symbols for:
- Start/end points (parentheses)
- Process steps [rectangles]
- Decision points <diamonds>
- Directional arrows connecting elements
```

## Code Visualization

### Code Structure Diagrams

#### Content Requirements
- **Modules**: Major code components
- **Classes/Functions**: Key implementation elements
- **Relationships**: Inheritance, composition, dependencies
- **Interfaces**: Public APIs and contracts
- **Access Patterns**: Usage and interaction flows
- **Data Structures**: Key data representations

#### Notation Guidelines
```
[Module A] ← implements ← [Interface X]
    ↑                         ↑
 inherits                  inherits
    |                         |
[Module B] → depends on → [Module C]
```

#### Example Specification
```markdown
Create a code structure diagram showing:
- Major components/modules
- Key classes with inheritance relationships
- Critical interfaces and implementations
- Dependencies between components
- Data flow between modules
- Extension points

Use UML-inspired notation with:
- Boxes for classes/modules
- Arrows for relationships
- Labels indicating relationship types
```

### Algorithm Flowcharts

#### Content Requirements
- **Initialization**: Starting state and setup
- **Steps**: Processing stages with operations
- **Branching**: Conditional logic
- **Loops**: Iteration constructs
- **Termination**: Exit conditions
- **Complexity**: Annotated time/space complexity
- **Examples**: Sample data state at key points

#### Notation Guidelines
```
(Initialize) → [Step 1] → [Step 2] → <Condition?> → [Step 3A] → (End)
                                         |
                                         ↓
                                     [Step 3B] → [Loop back to Step 2] ↩
```

#### Example Specification
```markdown
Create an algorithm flowchart showing:
- Initialization with input parameters
- Main processing steps in sequence
- Conditional branches with criteria
- Loop structures with entry/exit conditions
- Terminal states and return values
- Annotations with complexity analysis
- Example state progression with sample input

Use consistent notation:
- Rounded rectangles for start/end
- Rectangles for process steps
- Diamonds for decisions
- Arrows for flow direction
```

## Tabular Data

### Feature Comparison Matrices

#### Content Requirements
- **Features**: Comprehensive feature list in rows
- **Options**: Items being compared in columns
- **Support**: Clear indicators of feature support
- **Limitations**: Notes on partial implementations
- **Standouts**: Highlighting of exceptional capabilities
- **Categories**: Logical grouping of related features
- **Importance**: Indicators of feature significance

#### Format Example
```
| Feature Category | Feature | Option A | Option B | Option C |
|------------------|---------|----------|----------|----------|
| Performance      | Feature 1 | ✅ | ⚠️ Limited | ❌ |
|                  | Feature 2 | ✅ | ✅ | ⚠️ Planned |
| Security         | Feature 3 | ✅ Best | ✅ | ✅ |
|                  | Feature 4 | ❌ | ✅ | ✅ |
| Usability        | Feature 5 | ⚠️ Basic | ✅ | ✅ Best |
```

#### Example Specification
```markdown
Create a feature comparison matrix with:
- Features organized by category (rows)
- All options being evaluated (columns)
- Support indicators:
  - ✅ = Fully supported
  - ⚠️ = Partial/limited support (with note)
  - ❌ = Not supported
- "Best" notation for standout implementations
- "Planned" notation for upcoming features
- Bold text for critical features
```

### Decision Matrices

#### Content Requirements
- **Options**: Alternatives in rows
- **Criteria**: Evaluation factors in columns
- **Weights**: Importance weighting for criteria
- **Scores**: Raw scores (typically 1-10)
- **Weighted Scores**: Calculated weighted values
- **Totals**: Sum of weighted scores
- **Ranking**: Clear indication of final ranking

#### Format Example
```
| Option | Criterion 1 (w=0.2) | Criterion 2 (w=0.5) | Criterion 3 (w=0.3) | Total Score | Rank |
|--------|---------------------|---------------------|---------------------|-------------|------|
| Opt A  | 7 (1.4)             | 8 (4.0)             | 9 (2.7)             | 8.1         | 1    |
| Opt B  | 9 (1.8)             | 6 (3.0)             | 8 (2.4)             | 7.2         | 2    |
| Opt C  | 8 (1.6)             | 5 (2.5)             | 7 (2.1)             | 6.2         | 3    |
```

#### Example Specification
```markdown
Create a weighted decision matrix with:
- All options listed as rows
- Evaluation criteria as columns with weights
- Scores on a 1-10 scale for each option/criterion
- Calculation showing raw score and weighted contribution
- Total weighted score for each option
- Ranking based on total score
- Conditional formatting:
  - Bold the highest score in each column
  - Highlight the top-ranked option
```

## Chart Types

### Technical Chart Specifications

Specifications for common chart types in technical contexts:

#### Time Series
```markdown
Create a time series chart showing:
- X-axis: Time periods (specify increments)
- Y-axis: Metric values (specify units and scale)
- Multiple series for comparing trends
- Clear legend identifying each series
- Notable events or thresholds marked
- Trend lines where appropriate

Use text-based visualization with appropriate symbols and labels.
```

#### Comparison Bar Charts
```markdown
Create a bar chart comparing:
- X-axis: Categories/items being compared
- Y-axis: Value metric (specify units and scale)
- Grouped bars for multi-dimension comparison
- Labels showing exact values
- Sorted in descending order by primary metric
- Color coding to distinguish categories

Use text-based visualization with proportional bar lengths.
```

#### Technical Scatter Plots
```markdown
Create a scatter plot showing:
- X-axis: First variable (specify units and range)
- Y-axis: Second variable (specify units and range)
- Points representing individual data items
- Size variation for third dimension (if applicable)
- Color coding for categories
- Trend line showing correlation
- Labeled quadrants or regions of interest

Use text-based visualization with appropriate symbols and positioning.
```

## Documentation Diagrams

### API Interface Diagrams

#### Content Requirements
- **Endpoints**: All API endpoints grouped logically
- **Methods**: HTTP methods or function calls
- **Parameters**: Required and optional inputs
- **Responses**: Return types and structures
- **Authentication**: Security requirements
- **Dependencies**: Related services or components
- **Examples**: Sample calls and responses

#### Format Example
```
API: UserService
┌─────────────────────────────────────────────────────────┐
│ GET /users/{id}                                         │
├─────────────────────────────────────────────────────────┤
│ Parameters:                                             │
│   - id: string (required) - User identifier             │
├─────────────────────────────────────────────────────────┤
│ Responses:                                              │
│   - 200: User object                                    │
│   - 404: Not found error                                │
│   - 401: Authentication error                           │
├─────────────────────────────────────────────────────────┤
│ Authentication: Bearer token                            │
└─────────────────────────────────────────────────────────┘
```

#### Example Specification
```markdown
Create an API interface diagram documenting:
- All endpoints grouped by resource
- HTTP methods for each endpoint
- Path and query parameters with types
- Request body schema for POST/PUT methods
- Response codes and data structures
- Authentication requirements
- Rate limiting information
- Example request/response pairs

Use consistent box notation with clear sections for each component.
```

### Entity Relationship Diagrams

#### Content Requirements
- **Entities**: All major data entities
- **Attributes**: Key fields for each entity
- **Relationships**: Connections between entities
- **Cardinality**: Relationship multiplicity (1:1, 1:N, M:N)
- **Primary Keys**: Clearly marked identifiers
- **Foreign Keys**: Relationship implementation
- **Constraints**: Business rules and validations

#### Notation Guidelines
```
[User]                      [Order]
id (PK)    1 ------- * ┌─→ id (PK)
username               │   user_id (FK)
email                  │   total_amount
created_at             │   created_at
                       │
                       │   [OrderItem]
                       └── id (PK)
                           order_id (FK)
                           product_id (FK)
                           quantity
                           price
```

#### Example Specification
```markdown
Create an entity relationship diagram showing:
- All entities in the data model
- Primary key fields marked (PK)
- Foreign key relationships marked (FK)
- Essential attributes for each entity
- Relationship lines with cardinality:
  - One-to-one: 1 ──── 1
  - One-to-many: 1 ──── *
  - Many-to-many: * ──── *
- Optional relationships shown with dashed lines
```
# Workflow Integration Guide

This guide explains how to effectively integrate AI tools into your development, writing, and analysis workflows.

## Core Integration Principles

### 1. Task Decomposition
- Break complex tasks into discrete steps
- Identify which steps benefit most from AI assistance
- Create clear handoffs between human and AI work

### 2. Context Management
- Maintain consistent context across interactions
- Reference previous outputs explicitly
- Provide explicit updates when context changes

### 3. Output Refinement
- Use follow-up prompts to refine outputs
- Iterate based on specific feedback
- Save effective prompt+output combinations as templates

### 4. Version Control Integration
- Include AI prompts in version control
- Document AI-assisted components
- Attribute significant AI contributions appropriately

## Development Workflow Integration

### Code Generation Workflow

#### Initial Planning Phase
1. **Define Requirements** (Human)
   - Specify functionality, inputs, outputs
   - Identify constraints and edge cases
   - Determine performance requirements

2. **Generate Implementation Strategy** (AI)
   - Use the "Code Solution Template"
   - Request multiple approach options
   - Ask for tradeoff analysis

3. **Select and Refine Approach** (Human)
   - Choose preferred implementation approach
   - Identify areas needing customization
   - Note integration requirements

#### Implementation Phase
4. **Generate Initial Implementation** (AI)
   - Use "Code Generation Template" with selected approach
   - Request complete implementation with tests
   - Specify naming conventions and style preferences

5. **Review and Customize** (Human)
   - Evaluate generated code against requirements
   - Identify areas needing modification
   - Make necessary integrations with existing codebase

6. **Refine Implementation** (AI+Human)
   - Request specific optimizations
   - Address edge cases and error handling
   - Finalize documentation and examples

### Code Review Workflow

1. **Prepare Code for Review** (Human)
   - Select code segments for review
   - Provide context and requirements
   - Specify review focus areas

2. **Generate Review** (AI)
   - Use "Code Review Template"
   - Request specific feedback categories
   - Specify output format for actionable review

3. **Triage Feedback** (Human)
   - Categorize issues by severity
   - Identify patterns in feedback
   - Prioritize improvements

4. **Implement Improvements** (Human+AI)
   - Request specific optimizations based on review
   - Generate alternative implementations for problem areas
   - Use "Advanced Techniques" for complex refactoring

### System Design Workflow

1. **Define Requirements** (Human)
   - Specify functional requirements
   - Establish constraints and scale
   - Define quality attributes

2. **Generate Architecture** (AI)
   - Use "System Design Template"
   - Request component breakdown with responsibilities
   - Specify required diagrams

3. **Review and Refine** (Human)
   - Identify areas needing more detail
   - Question assumptions and tradeoffs
   - Note integration challenges

4. **Detailed Design** (AI+Human)
   - Generate detailed specifications for key components
   - Create interface definitions
   - Develop data models and flows

5. **Implementation Planning** (Human+AI)
   - Break down into implementation tasks
   - Identify dependencies and critical path
   - Create implementation timeline

## Writing and Documentation Workflow

### Technical Documentation Workflow

1. **Define Documentation Scope** (Human)
   - Identify target audience
   - Determine required sections and depth
   - Specify format and style requirements

2. **Generate Structure** (AI)
   - Use "Technical Documentation Template"
   - Request outline with section breakdown
   - Define content types for each section

3. **Generate Content** (AI)
   - Develop content for each section
   - Include required examples and diagrams
   - Format according to specifications

4. **Review and Refine** (Human)
   - Check technical accuracy
   - Ensure completeness and clarity
   - Note areas needing expansion or correction

5. **Finalize Documentation** (AI+Human)
   - Address specific feedback
   - Format for final delivery
   - Create supplementary materials if needed

### Research Synthesis Workflow

1. **Define Research Questions** (Human)
   - Specify primary questions to answer
   - Determine scope and boundaries
   - Identify key sources or data to consider

2. **Generate Research Plan** (AI)
   - Use "Research Synthesis Template"
   - Create structured approach to investigation
   - Define synthesis methodology

3. **Conduct Research** (AI+Human)
   - Gather and organize information
   - Analyze patterns and contradictions
   - Identify key findings

4. **Generate Synthesis** (AI)
   - Create comprehensive synthesis of findings
   - Highlight implications and applications
   - Suggest areas for further investigation

5. **Review and Extend** (Human)
   - Validate accuracy and completeness
   - Identify applications to current work
   - Determine next steps based on findings

## Analysis Workflow Integration

### Data Analysis Workflow

1. **Define Analysis Goals** (Human)
   - Specify questions to answer
   - Identify available data sources
   - Determine required outputs

2. **Generate Analysis Plan** (AI)
   - Use "Data Analysis Template"
   - Create structured analytical approach
   - Define visualization and presentation strategy

3. **Prepare and Analyze Data** (Human+AI)
   - Process and clean data
   - Perform initial analysis
   - Generate visualizations

4. **Interpret Results** (AI+Human)
   - Create narrative explaining findings
   - Highlight key insights and patterns
   - Connect to original questions

5. **Generate Recommendations** (AI)
   - Develop actionable recommendations
   - Support with specific findings
   - Address limitations and assumptions

### Decision Support Workflow

1. **Define Decision Context** (Human)
   - Specify decision to be made
   - Identify options and constraints
   - Determine decision criteria

2. **Generate Decision Framework** (AI)
   - Use "Comparative Analysis Framework"
   - Create evaluation methodology
   - Define scoring and weighting system

3. **Evaluate Options** (AI+Human)
   - Score options against criteria
   - Calculate weighted results
   - Perform sensitivity analysis

4. **Generate Recommendation** (AI)
   - Present ranked options with rationale
   - Highlight key differentiators
   - Address potential objections

5. **Implement Decision** (Human)
   - Make final selection
   - Document decision process
   - Create implementation plan

## Cross-Model Coordination

### Multi-Model Research Projects

For comprehensive research projects requiring multiple perspectives:

1. **Project Definition** (Human)
   - Define research scope and objectives
   - Identify required specializations
   - Create coordination framework

2. **Specialized Research** (Multiple AI models)
   - Assign different aspects to specialized models:
     - Claude for nuanced technical research
     - GPT-4o with browsing for current information
     - DeepSeek for code-heavy components

3. **Synthesis** (Claude with Extended Thinking)
   - Combine specialized inputs
   - Resolve contradictions
   - Create unified perspective

4. **Review and Refinement** (Human)
   - Identify gaps or inconsistencies
   - Request additional specialized input
   - Direct final synthesis

### Progressive Refinement Chain

For complex creative or technical work:

1. **Initial Generation** (GPT-4 or Claude)
   - Create first draft with broad structure
   - Establish key components and approach
   - Identify areas needing specialization

2. **Specialized Enhancement** (Specialized model)
   - Elaborate specific technical sections
   - Enhance code components
   - Develop specific visualizations

3. **Critical Review** (Claude with Extended Thinking)
   - Analyze for consistency and completeness
   - Identify logical flaws or gaps
   - Generate specific improvement recommendations

4. **Final Refinement** (Original or specialized model)
   - Implement recommended improvements
   - Harmonize style and structure
   - Finalize for delivery

## Tool Integration Examples

### IDE Integration Workflow

1. **Local Development Environment**
   - Configure editor extensions (Cursor, GitHub Copilot)
   - Create template snippets for common prompts
   - Develop keyboard shortcuts for AI interactions

2. **Version Control Integration**
   - Include prompt templates in project repositories
   - Document AI usage in contribution guidelines
   - Create commit hooks for AI-assisted contributions

3. **Continuous Integration**
   - Automate AI-powered code reviews
   - Generate test cases with AI assistance
   - Develop documentation from code automatically

### Document Management Integration

1. **Content Creation**
   - Use AI templates for consistent document generation
   - Create modular content blocks for reuse
   - Develop style guides for AI-assisted writing

2. **Review and Refinement**
   - Implement AI-powered editing workflows
   - Create checklist templates for human review
   - Establish quality standards for AI outputs

3. **Publication and Distribution**
   - Automate format conversions for different platforms
   - Generate supplementary materials automatically
   - Create presentation materials from documents

## Practical Integration Recipes

### Code Creation Recipe

```markdown
# Code Creation Workflow

## Step 1: Problem Definition
Use this prompt:
```
I need to implement [specific functionality].
Key requirements:
- [Requirement 1]
- [Requirement 2]
- [Requirement 3]

Technical constraints:
- Language: [Language]
- Framework: [Framework]
- Performance needs: [Specific requirements]

Please provide a high-level approach before implementation.
```

## Step 2: Review Proposed Approach
Evaluate the AI's suggested approach against:
- Alignment with requirements
- Technical feasibility
- Integration with existing systems
- Performance characteristics

## Step 3: Request Implementation
Use this prompt:
```
The approach looks good. Please implement a complete solution with:
1. Well-structured code following [style guide/conventions]
2. Error handling for these cases: [list cases]
3. Unit tests covering key functionality
4. Clear documentation and usage examples

[Include any specific implementation details based on the approach]
```

## Step 4: Integration and Testing
- Merge the code into your project
- Run tests and validate functionality
- Document any modifications made to the AI-generated code
```

### Technical Document Creation Recipe

```markdown
# Technical Documentation Workflow

## Step 1: Document Specification
Use this prompt:
```
I need to create [document type] about [subject].

Target audience:
- [Primary audience with technical level]
- [Secondary audience if applicable]

Required sections:
- [Section 1]
- [Section 2]
- [Section 3]

Key information to include:
- [Key point 1]
- [Key point 2]
- [Key point 3]

Please provide an outline before creating the full document.
```

## Step 2: Review and Approve Outline
Evaluate the AI's suggested outline against:
- Completeness
- Logical structure
- Alignment with audience needs
- Appropriate technical depth

## Step 3: Request Full Document
Use this prompt:
```
The outline looks good. Please generate the complete document with:
1. All sections fully developed
2. Code examples for key concepts
3. Visual diagrams for [specific complex concepts]
4. Tables comparing [specific items to compare]

Formatting requirements:
- Use markdown formatting
- Include syntax highlighting for code blocks
- Create clear hierarchical structure with headers
```

## Step 4: Review and Finalize
- Check technical accuracy
- Ensure all requirements are met
- Add any missing information
- Format for final delivery
```

### Decision Analysis Recipe

```markdown
# Decision Analysis Workflow

## Step 1: Frame the Decision
Use this prompt:
```
I need to make a decision about [decision context].

Options being considered:
- [Option 1]
- [Option 2]
- [Option 3]

Key decision criteria:
- [Criterion 1] (importance: high/medium/low)
- [Criterion 2] (importance: high/medium/low)
- [Criterion 3] (importance: high/medium/low)

Constraints:
- [Constraint 1]
- [Constraint 2]

Please create a decision analysis framework.
```

## Step 2: Review Framework
Evaluate the AI's suggested framework against:
- Completeness of criteria
- Appropriate weighting
- Consideration of constraints
- Evaluation methodology

## Step 3: Request Analysis
Use this prompt:
```
The framework looks good. Please perform a complete analysis:
1. Evaluate each option against all criteria
2. Create a weighted decision matrix
3. Perform sensitivity analysis on the weights
4. Provide a clear recommendation with rationale
5. Address potential weaknesses of the recommended option
```

## Step 4: Make and Document Decision
- Review analysis
- Consider any factors not in the model
- Document final decision with rationale
- Create implementation plan
```
# Core Prompting

This directory contains fundamental prompting techniques, base templates, and system roles that form the foundation of effective AI interactions.

## Structure

- **Base_Templates/**: Generic templates that can be adapted for various purposes
- **System_Roles/**: Definitions that shape AI behavior and capabilities
- **Best_Practices/**: Guidelines and principles for effective prompt engineering

## Key Components

- Base system messages for different AI interactions
- Universal prompt structures that work across various models
- Core prompt engineering principles and techniques
- Fundamental strategies for context management and output control

## Usage

These base components are designed to be combined with more specific templates from other directories. They provide the foundational structure that can be extended with domain-specific content.

Example workflow:
1. Start with an appropriate system role from `System_Roles/`
2. Add a base template structure from `Base_Templates/`
3. Incorporate domain-specific content from the `Use_Cases/` directory
4. Add any provider-specific optimizations if needed

## Best Practices

- Keep system roles concise but comprehensive
- Use consistent formatting and structure across all prompts
- Maintain clear separation between instructions and content
- Balance specificity with flexibility to allow the AI appropriate latitude
# Task Master

### by [@eyaltoledano](https://x.com/eyaltoledano)

A task management system for AI-driven development with Claude, designed to work seamlessly with Cursor AI.

## Requirements

- Node.js 14.0.0 or higher
- Anthropic API key (Claude API)
- Anthropic SDK version 0.39.0 or higher
- OpenAI SDK (for Perplexity API integration, optional)

## Configuration

The script can be configured through environment variables in a `.env` file at the root of the project:

### Required Configuration

- `ANTHROPIC_API_KEY`: Your Anthropic API key for Claude

### Optional Configuration

- `MODEL`: Specify which Claude model to use (default: "claude-3-7-sonnet-20250219")
- `MAX_TOKENS`: Maximum tokens for model responses (default: 4000)
- `TEMPERATURE`: Temperature for model responses (default: 0.7)
- `PERPLEXITY_API_KEY`: Your Perplexity API key for research-backed subtask generation
- `PERPLEXITY_MODEL`: Specify which Perplexity model to use (default: "sonar-medium-online")
- `DEBUG`: Enable debug logging (default: false)
- `LOG_LEVEL`: Log level - debug, info, warn, error (default: info)
- `DEFAULT_SUBTASKS`: Default number of subtasks when expanding (default: 3)
- `DEFAULT_PRIORITY`: Default priority for generated tasks (default: medium)
- `PROJECT_NAME`: Override default project name in tasks.json
- `PROJECT_VERSION`: Override default version in tasks.json

## Installation

```bash
# Install globally
npm install -g task-master-ai

# OR install locally within your project
npm install task-master-ai
```

### Initialize a new project

```bash
# If installed globally
task-master init

# If installed locally
npx task-master-init
```

This will prompt you for project details and set up a new project with the necessary files and structure.

### Important Notes

1. **ES Modules Configuration:**

   - This project uses ES Modules (ESM) instead of CommonJS.
   - This is set via `"type": "module"` in your package.json.
   - Use `import/export` syntax instead of `require()`.
   - Files should use `.js` or `.mjs` extensions.
   - To use a CommonJS module, either:
     - Rename it with `.cjs` extension
     - Use `await import()` for dynamic imports
   - If you need CommonJS throughout your project, remove `"type": "module"` from package.json, but Task Master scripts expect ESM.

2. The Anthropic SDK version should be 0.39.0 or higher.

## Quick Start with Global Commands

After installing the package globally, you can use these CLI commands from any directory:

```bash
# Initialize a new project
task-master init

# Parse a PRD and generate tasks
task-master parse-prd your-prd.txt

# List all tasks
task-master list

# Show the next task to work on
task-master next

# Generate task files
task-master generate
```

## Troubleshooting

### If `task-master init` doesn't respond:

Try running it with Node directly:

```bash
node node_modules/claude-task-master/scripts/init.js
```

Or clone the repository and run:

```bash
git clone https://github.com/eyaltoledano/claude-task-master.git
cd claude-task-master
node scripts/init.js
```

## Task Structure

Tasks in tasks.json have the following structure:

- `id`: Unique identifier for the task (Example: `1`)
- `title`: Brief, descriptive title of the task (Example: `"Initialize Repo"`)
- `description`: Concise description of what the task involves (Example: `"Create a new repository, set up initial structure."`)
- `status`: Current state of the task (Example: `"pending"`, `"done"`, `"deferred"`)
- `dependencies`: IDs of tasks that must be completed before this task (Example: `[1, 2]`)
  - Dependencies are displayed with status indicators (✅ for completed, ⏱️ for pending)
  - This helps quickly identify which prerequisite tasks are blocking work
- `priority`: Importance level of the task (Example: `"high"`, `"medium"`, `"low"`)
- `details`: In-depth implementation instructions (Example: `"Use GitHub client ID/secret, handle callback, set session token."`)
- `testStrategy`: Verification approach (Example: `"Deploy and call endpoint to confirm 'Hello World' response."`)
- `subtasks`: List of smaller, more specific tasks that make up the main task (Example: `[{"id": 1, "title": "Configure OAuth", ...}]`)

## Integrating with Cursor AI

Claude Task Master is designed to work seamlessly with [Cursor AI](https://www.cursor.so/), providing a structured workflow for AI-driven development.

### Setup with Cursor

1. After initializing your project, open it in Cursor
2. The `.cursor/rules/dev_workflow.mdc` file is automatically loaded by Cursor, providing the AI with knowledge about the task management system
3. Place your PRD document in the `scripts/` directory (e.g., `scripts/prd.txt`)
4. Open Cursor's AI chat and switch to Agent mode

### Setting up MCP in Cursor

To enable enhanced task management capabilities directly within Cursor using the Model Control Protocol (MCP):

1. Go to Cursor settings
2. Navigate to the MCP section
3. Click on "Add New MCP Server"
4. Configure with the following details:
   - Name: "Task Master"
   - Type: "Command"
   - Command: "npx -y task-master-ai"
5. Save the settings

Once configured, you can interact with Task Master's task management commands directly through Cursor's interface, providing a more integrated experience.

### Initial Task Generation

In Cursor's AI chat, instruct the agent to generate tasks from your PRD:

```
Please use the task-master parse-prd command to generate tasks from my PRD. The PRD is located at scripts/prd.txt.
```

The agent will execute:

```bash
task-master parse-prd scripts/prd.txt
```

This will:

- Parse your PRD document
- Generate a structured `tasks.json` file with tasks, dependencies, priorities, and test strategies
- The agent will understand this process due to the Cursor rules

### Generate Individual Task Files

Next, ask the agent to generate individual task files:

```
Please generate individual task files from tasks.json
```

The agent will execute:

```bash
task-master generate
```

This creates individual task files in the `tasks/` directory (e.g., `task_001.txt`, `task_002.txt`), making it easier to reference specific tasks.

## AI-Driven Development Workflow

The Cursor agent is pre-configured (via the rules file) to follow this workflow:

### 1. Task Discovery and Selection

Ask the agent to list available tasks:

```
What tasks are available to work on next?
```

The agent will:

- Run `task-master list` to see all tasks
- Run `task-master next` to determine the next task to work on
- Analyze dependencies to determine which tasks are ready to be worked on
- Prioritize tasks based on priority level and ID order
- Suggest the next task(s) to implement

### 2. Task Implementation

When implementing a task, the agent will:

- Reference the task's details section for implementation specifics
- Consider dependencies on previous tasks
- Follow the project's coding standards
- Create appropriate tests based on the task's testStrategy

You can ask:

```
Let's implement task 3. What does it involve?
```

### 3. Task Verification

Before marking a task as complete, verify it according to:

- The task's specified testStrategy
- Any automated tests in the codebase
- Manual verification if required

### 4. Task Completion

When a task is completed, tell the agent:

```
Task 3 is now complete. Please update its status.
```

The agent will execute:

```bash
task-master set-status --id=3 --status=done
```

### 5. Handling Implementation Drift

If during implementation, you discover that:

- The current approach differs significantly from what was planned
- Future tasks need to be modified due to current implementation choices
- New dependencies or requirements have emerged

Tell the agent:

```
We've changed our approach. We're now using Express instead of Fastify. Please update all future tasks to reflect this change.
```

The agent will execute:

```bash
task-master update --from=4 --prompt="Now we are using Express instead of Fastify."
```

This will rewrite or re-scope subsequent tasks in tasks.json while preserving completed work.

### 6. Breaking Down Complex Tasks

For complex tasks that need more granularity:

```
Task 5 seems complex. Can you break it down into subtasks?
```

The agent will execute:

```bash
task-master expand --id=5 --num=3
```

You can provide additional context:

```
Please break down task 5 with a focus on security considerations.
```

The agent will execute:

```bash
task-master expand --id=5 --prompt="Focus on security aspects"
```

You can also expand all pending tasks:

```
Please break down all pending tasks into subtasks.
```

The agent will execute:

```bash
task-master expand --all
```

For research-backed subtask generation using Perplexity AI:

```
Please break down task 5 using research-backed generation.
```

The agent will execute:

```bash
task-master expand --id=5 --research
```

## Command Reference

Here's a comprehensive reference of all available commands:

### Parse PRD

```bash
# Parse a PRD file and generate tasks
task-master parse-prd <prd-file.txt>

# Limit the number of tasks generated
task-master parse-prd <prd-file.txt> --num-tasks=10
```

### List Tasks

```bash
# List all tasks
task-master list

# List tasks with a specific status
task-master list --status=<status>

# List tasks with subtasks
task-master list --with-subtasks

# List tasks with a specific status and include subtasks
task-master list --status=<status> --with-subtasks
```

### Show Next Task

```bash
# Show the next task to work on based on dependencies and status
task-master next
```

### Show Specific Task

```bash
# Show details of a specific task
task-master show <id>
# or
task-master show --id=<id>

# View a specific subtask (e.g., subtask 2 of task 1)
task-master show 1.2
```

### Update Tasks

```bash
# Update tasks from a specific ID and provide context
task-master update --from=<id> --prompt="<prompt>"
```

### Generate Task Files

```bash
# Generate individual task files from tasks.json
task-master generate
```

### Set Task Status

```bash
# Set status of a single task
task-master set-status --id=<id> --status=<status>

# Set status for multiple tasks
task-master set-status --id=1,2,3 --status=<status>

# Set status for subtasks
task-master set-status --id=1.1,1.2 --status=<status>
```

When marking a task as "done", all of its subtasks will automatically be marked as "done" as well.

### Expand Tasks

```bash
# Expand a specific task with subtasks
task-master expand --id=<id> --num=<number>

# Expand with additional context
task-master expand --id=<id> --prompt="<context>"

# Expand all pending tasks
task-master expand --all

# Force regeneration of subtasks for tasks that already have them
task-master expand --all --force

# Research-backed subtask generation for a specific task
task-master expand --id=<id> --research

# Research-backed generation for all tasks
task-master expand --all --research
```

### Clear Subtasks

```bash
# Clear subtasks from a specific task
task-master clear-subtasks --id=<id>

# Clear subtasks from multiple tasks
task-master clear-subtasks --id=1,2,3

# Clear subtasks from all tasks
task-master clear-subtasks --all
```

### Analyze Task Complexity

```bash
# Analyze complexity of all tasks
task-master analyze-complexity

# Save report to a custom location
task-master analyze-complexity --output=my-report.json

# Use a specific LLM model
task-master analyze-complexity --model=claude-3-opus-20240229

# Set a custom complexity threshold (1-10)
task-master analyze-complexity --threshold=6

# Use an alternative tasks file
task-master analyze-complexity --file=custom-tasks.json

# Use Perplexity AI for research-backed complexity analysis
task-master analyze-complexity --research
```

### View Complexity Report

```bash
# Display the task complexity analysis report
task-master complexity-report

# View a report at a custom location
task-master complexity-report --file=my-report.json
```

### Managing Task Dependencies

```bash
# Add a dependency to a task
task-master add-dependency --id=<id> --depends-on=<id>

# Remove a dependency from a task
task-master remove-dependency --id=<id> --depends-on=<id>

# Validate dependencies without fixing them
task-master validate-dependencies

# Find and fix invalid dependencies automatically
task-master fix-dependencies
```

### Add a New Task

```bash
# Add a new task using AI
task-master add-task --prompt="Description of the new task"

# Add a task with dependencies
task-master add-task --prompt="Description" --dependencies=1,2,3

# Add a task with priority
task-master add-task --prompt="Description" --priority=high
```

## Feature Details

### Analyzing Task Complexity

The `analyze-complexity` command:

- Analyzes each task using AI to assess its complexity on a scale of 1-10
- Recommends optimal number of subtasks based on configured DEFAULT_SUBTASKS
- Generates tailored prompts for expanding each task
- Creates a comprehensive JSON report with ready-to-use commands
- Saves the report to scripts/task-complexity-report.json by default

The generated report contains:

- Complexity analysis for each task (scored 1-10)
- Recommended number of subtasks based on complexity
- AI-generated expansion prompts customized for each task
- Ready-to-run expansion commands directly within each task analysis

### Viewing Complexity Report

The `complexity-report` command:

- Displays a formatted, easy-to-read version of the complexity analysis report
- Shows tasks organized by complexity score (highest to lowest)
- Provides complexity distribution statistics (low, medium, high)
- Highlights tasks recommended for expansion based on threshold score
- Includes ready-to-use expansion commands for each complex task
- If no report exists, offers to generate one on the spot

### Smart Task Expansion

The `expand` command automatically checks for and uses the complexity report:

When a complexity report exists:

- Tasks are automatically expanded using the recommended subtask count and prompts
- When expanding all tasks, they're processed in order of complexity (highest first)
- Research-backed generation is preserved from the complexity analysis
- You can still override recommendations with explicit command-line options

Example workflow:

```bash
# Generate the complexity analysis report with research capabilities
task-master analyze-complexity --research

# Review the report in a readable format
task-master complexity-report

# Expand tasks using the optimized recommendations
task-master expand --id=8
# or expand all tasks
task-master expand --all
```

### Finding the Next Task

The `next` command:

- Identifies tasks that are pending/in-progress and have all dependencies satisfied
- Prioritizes tasks by priority level, dependency count, and task ID
- Displays comprehensive information about the selected task:
  - Basic task details (ID, title, priority, dependencies)
  - Implementation details
  - Subtasks (if they exist)
- Provides contextual suggested actions:
  - Command to mark the task as in-progress
  - Command to mark the task as done
  - Commands for working with subtasks

### Viewing Specific Task Details

The `show` command:

- Displays comprehensive details about a specific task or subtask
- Shows task status, priority, dependencies, and detailed implementation notes
- For parent tasks, displays all subtasks and their status
- For subtasks, shows parent task relationship
- Provides contextual action suggestions based on the task's state
- Works with both regular tasks and subtasks (using the format taskId.subtaskId)

## Best Practices for AI-Driven Development

1. **Start with a detailed PRD**: The more detailed your PRD, the better the generated tasks will be.

2. **Review generated tasks**: After parsing the PRD, review the tasks to ensure they make sense and have appropriate dependencies.

3. **Analyze task complexity**: Use the complexity analysis feature to identify which tasks should be broken down further.

4. **Follow the dependency chain**: Always respect task dependencies - the Cursor agent will help with this.

5. **Update as you go**: If your implementation diverges from the plan, use the update command to keep future tasks aligned with your current approach.

6. **Break down complex tasks**: Use the expand command to break down complex tasks into manageable subtasks.

7. **Regenerate task files**: After any updates to tasks.json, regenerate the task files to keep them in sync.

8. **Communicate context to the agent**: When asking the Cursor agent to help with a task, provide context about what you're trying to achieve.

9. **Validate dependencies**: Periodically run the validate-dependencies command to check for invalid or circular dependencies.

## Example Cursor AI Interactions

### Starting a new project

```
I've just initialized a new project with Claude Task Master. I have a PRD at scripts/prd.txt.
Can you help me parse it and set up the initial tasks?
```

### Working on tasks

```
What's the next task I should work on? Please consider dependencies and priorities.
```

### Implementing a specific task

```
I'd like to implement task 4. Can you help me understand what needs to be done and how to approach it?
```

### Managing subtasks

```
I need to regenerate the subtasks for task 3 with a different approach. Can you help me clear and regenerate them?
```

### Handling changes

```
We've decided to use MongoDB instead of PostgreSQL. Can you update all future tasks to reflect this change?
```

### Completing work

```
I've finished implementing the authentication system described in task 2. All tests are passing.
Please mark it as complete and tell me what I should work on next.
```

### Analyzing complexity

```
Can you analyze the complexity of our tasks to help me understand which ones need to be broken down further?
```

### Viewing complexity report

```
Can you show me the complexity report in a more readable format?
```
