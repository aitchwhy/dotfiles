# AI Core Configuration
# Central configuration for AI integration components
# Version: 1.0.0 (May 2025)

# System-wide settings
system:
  version: "1.0.0"
  default_provider: "anthropic"
  logging:
    enabled: true
    level: "info"
    path: "${HOME}/.logs/ai"
  temp_directory: "${HOME}/.cache/ai"
  history_directory: "${HOME}/.config/ai/history"

# Model configurations
models:
  # Anthropic models
  anthropic:
    claude-3-opus:
      id: "claude-3-opus-20240229"
      version: "20240229"
      context_window: 200000
      token_limit: 4000
      cost_per_1k_input: 15.00
      cost_per_1k_output: 75.00
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "embeddings", "vision"]
      roles: ["system", "user", "assistant"]
      tunable: false
    claude-3-sonnet:
      id: "claude-3-7-sonnet-20250219"
      version: "20250219"
      context_window: 200000
      token_limit: 4000
      cost_per_1k_input: 3.00
      cost_per_1k_output: 15.00
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "embeddings", "vision"]
      roles: ["system", "user", "assistant"]
      tunable: false
    claude-3-haiku:
      id: "claude-3-haiku-20240307"
      version: "20240307"
      context_window: 200000
      token_limit: 4000
      cost_per_1k_input: 0.25
      cost_per_1k_output: 1.25
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "embeddings", "vision"]
      roles: ["system", "user", "assistant"]
      tunable: false

  # OpenAI models
  openai:
    gpt-4-turbo:
      id: "gpt-4-turbo-2024-04-09"
      version: "2024-04-09"
      context_window: 128000
      token_limit: 4096
      cost_per_1k_input: 10.00
      cost_per_1k_output: 30.00
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "embeddings", "vision", "tools", "function_calling"]
      roles: ["system", "user", "assistant", "tool"]
      tunable: true
    gpt-4:
      id: "gpt-4-0125-preview"
      version: "0125-preview"
      context_window: 8192
      token_limit: 4096
      cost_per_1k_input: 10.00
      cost_per_1k_output: 30.00
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "embeddings", "vision", "tools", "function_calling"]
      roles: ["system", "user", "assistant", "tool"]
      tunable: true
    gpt-3.5-turbo:
      id: "gpt-3.5-turbo-0125"
      version: "0125"
      context_window: 16385
      token_limit: 4096
      cost_per_1k_input: 0.50
      cost_per_1k_output: 1.50
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "embeddings", "function_calling"]
      roles: ["system", "user", "assistant", "tool"]
      tunable: true

  # Google models
  google:
    gemini-pro:
      id: "gemini-pro"
      version: "2023-12"
      context_window: 32768
      token_limit: 2048
      cost_per_1k_input: 0.50
      cost_per_1k_output: 1.50
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "vision"]
      roles: ["system", "user", "model"]
      tunable: false
    gemini-ultra:
      id: "gemini-1.5-pro"
      version: "2024-05"
      context_window: 1000000
      token_limit: 8192
      cost_per_1k_input: 2.50
      cost_per_1k_output: 7.50
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn", "vision"]
      roles: ["system", "user", "model"]
      tunable: false

  # Local models
  local:
    mixtral:
      id: "mixtral-8x7b-32768"
      version: "latest"
      context_window: 32768
      token_limit: 2048
      cost_per_1k_input: 0.00
      cost_per_1k_output: 0.00
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn"]
      roles: ["system", "user", "assistant"]
      tunable: false
      endpoint: "http://localhost:11434/api/generate"
    llama-3:
      id: "llama-3-70b-instruct"
      version: "latest"
      context_window: 8192
      token_limit: 2048
      cost_per_1k_input: 0.00
      cost_per_1k_output: 0.00
      streaming: true
      capabilities: ["reasoning", "code", "multi-turn"]
      roles: ["system", "user", "assistant"]
      tunable: false
      endpoint: "http://localhost:11434/api/generate"

# Provider-specific API configurations
providers:
  anthropic:
    api_endpoint: "https://api.anthropic.com/v1/messages"
    auth_type: "bearer"
    env_var: "ANTHROPIC_API_KEY"
    api_version: "2023-06-01"
    headers:
      "anthropic-version": "2023-06-01"
      "content-type": "application/json"
    rate_limit:
      requests_per_minute: 50
      tokens_per_minute: 100000
    retry:
      max_attempts: 3
      initial_delay_ms: 500
      max_delay_ms: 5000
      backoff_factor: 2.0

  openai:
    api_endpoint: "https://api.openai.com/v1/chat/completions"
    auth_type: "bearer" 
    env_var: "OPENAI_API_KEY"
    headers:
      "content-type": "application/json"
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 150000
    retry:
      max_attempts: 3
      initial_delay_ms: 500
      max_delay_ms: 5000
      backoff_factor: 2.0

  google:
    api_endpoint: "https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent"
    auth_type: "apikey"
    env_var: "GOOGLE_API_KEY" 
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 120000
    retry:
      max_attempts: 3
      initial_delay_ms: 500
      max_delay_ms: 5000
      backoff_factor: 2.0

# Task-specific model recommendations
tasks:
  code_generation:
    default: "anthropic.claude-3-sonnet"
    alternatives:
      - "openai.gpt-4-turbo"
      - "anthropic.claude-3-opus"
    local_option: "local.llama-3"
    parameters:
      temperature: 0.2
      max_tokens: 2000
      top_p: 0.95
    
  code_review:
    default: "anthropic.claude-3-opus"
    alternatives:
      - "openai.gpt-4-turbo"
      - "anthropic.claude-3-sonnet"
    local_option: "local.mixtral"
    parameters:
      temperature: 0.1
      max_tokens: 2500
      top_p: 0.9

  documentation:
    default: "anthropic.claude-3-sonnet"
    alternatives:
      - "google.gemini-ultra"
      - "anthropic.claude-3-opus"
    local_option: "local.llama-3"
    parameters:
      temperature: 0.4
      max_tokens: 3000
      top_p: 0.9

  commit_messages:
    default: "anthropic.claude-3-haiku"
    alternatives:
      - "openai.gpt-3.5-turbo"
      - "local.mixtral"
    local_option: "local.mixtral"
    parameters:
      temperature: 0.2
      max_tokens: 200
      top_p: 0.95

  api_design:
    default: "anthropic.claude-3-opus"
    alternatives:
      - "openai.gpt-4-turbo"
      - "anthropic.claude-3-sonnet"
    local_option: null
    parameters:
      temperature: 0.1
      max_tokens: 4000
      top_p: 0.9

  quick_assist:
    default: "anthropic.claude-3-haiku"
    alternatives:
      - "openai.gpt-3.5-turbo" 
      - "local.mixtral"
    local_option: "local.mixtral"
    parameters:
      temperature: 0.5
      max_tokens: 1000
      top_p: 0.9

  complex_reasoning:
    default: "anthropic.claude-3-opus"
    alternatives:
      - "openai.gpt-4-turbo"
      - "google.gemini-ultra"
    local_option: null
    parameters:
      temperature: 0.2
      max_tokens: 4000
      top_p: 0.9

# Tool integrations
integrations:
  git:
    enable_commit_hook: true
    hook_path: ".git/hooks/prepare-commit-msg"
    commit_message_format: "conventional"
    allow_ai_generation: true
    allow_ai_suggestions: true
    check_sensitive_info: true
    
  ide:
    vscode:
      extension_id: "anthropic.claude-vscode"
      auto_suggest: true
      inline_completion: true
    neovim:
      enable_completion: true
      keybindings:
        generate_code: "<leader>ag"
        explain_code: "<leader>ae"
        refactor_code: "<leader>ar"
    cursor:
      config_path: ".cursor/settings.json"
      prompt_path: ".cursor/prompts/"
  
  api:
    openapi:
      validator: "spectral"
      linter_config: ".spectral.yaml"
      mock_server: "prism"
      generation_tool: "openapi-generator-cli"
      default_output: "typescript-fetch"
    
    codegen:
      api_client_format: "typescript-fetch"
      python_client_format: "python"
      enable_tanstack_query: true
      generate_hooks: true

# Default parameters
defaults:
  parameters:
    temperature: 0.3
    max_tokens: 2000
    top_p: 0.95
    frequency_penalty: 0.0
    presence_penalty: 0.0
    stop_sequences: []
    timeout_ms: 60000

# Script integration
scripts:
  bash_integration: true
  zsh_integration: true
  bash_source_path: "${HOME}/.config/ai/utils/ai_bash.sh"
  shell_aliases:
    ai: "just ai"
    aicode: "just ai code"
    aicommit: "just ai commit-msg"
    aiexplain: "just ai explain"
    aiapi: "just api"