<repomix>This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where content has been formatted for parsing in xml style.<file_summary>This section contains a summary of this file.<purpose>This file contains a packed representation of the entire repository&apos;s contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.</purpose><file_format>The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file</file_format><usage_guidelines>- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.</usage_guidelines><notes>- Some files may have been excluded based on .gitignore rules and Repomix&apos;s configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.ts
- Files matching these patterns are excluded: tmp/, *.log, *tsconfig*, *build*, *node_modules*
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been formatted for parsing in xml style
- Files are sorted by Git change count (files with more changes are at the bottom)</notes><additional_info><user_provided_header>Custom header text</user_provided_header></additional_info></file_summary><directory_structure>src/
  @anterior/
    models/
      workflows/
        flonotes/
          process_flonotes_llm/
            process_flonotes_llm_egress.ts
            process_flonotes_llm_ingress.ts
        pdfs/
          process_pdf_standalone/
            process_pdf_standalone_egress.ts
            process_pdf_standalone_ingress.ts
  art-store/
    handlers/
      handle-query.ts
    scripts/
      init-s3.ts
    utils/
      list-all-blob-keys.ts
      parse-query-params.ts
    app.ts
    constants.ts
    infra.ts
    main.ts
    platform.ts
  auth/
    app.ts
    jwt.ts
    main.ts
  chat/
    orchestrator/
      tools/
        mnr.ts
      anthropic.ts
      index.ts
      types.ts
    app.ts
    infra.ts
    main.ts
    platform.ts
  flonotes/
    test-e2e/
      flonotes.spec.ts
    app.ts
    infra.ts
    main.ts
    platform.ts
  flopilot/
    app.ts
    infra.ts
    main.ts
    platform.ts
  health/
    app.ts
    main.ts
  hello-world/
    test-e2e/
      greet.spec.ts
      health.spec.ts
      latest.spec.ts
    test-integration/
      hello.test.ts
    app.ts
    hello.test.ts
    hello.ts
  notes/
    handlers/
      notes.ts
    schemas/
      api.ts
      flows.ts
      models.ts
      types.ts
    app.ts
    infra.ts
    main.ts
    platform.ts
  pdfs/
    models/
      flows.ts
      schemas.ts
      types.ts
    app.ts
    infra.ts
    main.ts
    platform.ts
  shims/
    Array.fromAsync.ts
  stems/
    handlers/
      attach-clinicals.ts
      attach-criteria.ts
      attach-services.ts
      create-stem.ts
      rollup.ts
    schemas/
      common.ts
      event-create-stem.ts
    app.ts
    infra.ts
    main.ts
    platform.ts
  tasks/
    handlers/
      run-mnr-raw.ts
      run-mnr-stem.ts
      validate-clinicals-stem.ts
    app.ts
    infra.ts
    main.ts
    platform.ts
    schemas.ts
  app.test.ts
  app.ts
  assoc.contract.ts
  assoc.test.ts
  assoc.ts
  async.ts
  auth.test.ts
  auth.ts
  blob.contract.ts
  blob.test.ts
  blob.ts
  config.test.ts
  config.ts
  cors.test.ts
  cors.ts
  errors.ts
  events.contract.ts
  events.test.ts
  events.ts
  flows.test.ts
  flows.ts
  ids.test.ts
  ids.ts
  in-memory.test.ts
  in-memory.ts
  index.ts
  invariant.test.ts
  invariant.ts
  json.test.ts
  json.ts
  jwt.ts
  kv.contract.ts
  kv.test.ts
  kv.ts
  log.ts
  migrate.ts
  object.test.ts
  object.ts
  queue.ts
  schema.test.ts
  schema.ts
  secrets.test.ts
  secrets.ts
  serializer.ts
  tasks.ts
  test.ts
  types.ts
  utils.ts
  worker.ts
  zod.ts
vitest.config.ts</directory_structure><files>This section contains the contents of the repository&apos;s files.<file path="src/@anterior/models/workflows/flonotes/process_flonotes_llm/process_flonotes_llm_egress.ts">import { z } from &quot;zod&quot;

export const processFlonotesLlmEgress = z.object({ &quot;result&quot;: z.string() })
export type ProcessFlonotesLlmEgress = z.infer&lt;typeof processFlonotesLlmEgress&gt;</file><file path="src/@anterior/models/workflows/flonotes/process_flonotes_llm/process_flonotes_llm_ingress.ts">import { z } from &quot;zod&quot;

export const processFlonotesLlmIngress = z.object({ &quot;flowName&quot;: z.literal(&quot;process-flonotes-llm/dev&quot;), &quot;s3InputKey&quot;: z.string(), &quot;s3OutputKey&quot;: z.string() })
export type ProcessFlonotesLlmIngress = z.infer&lt;typeof processFlonotesLlmIngress&gt;</file><file path="src/@anterior/models/workflows/pdfs/process_pdf_standalone/process_pdf_standalone_egress.ts">import { z } from &quot;zod&quot;

export const processPdfStandaloneEgress = z.object({ &quot;result&quot;: z.string() })
export type ProcessPdfStandaloneEgress = z.infer&lt;typeof processPdfStandaloneEgress&gt;</file><file path="src/@anterior/models/workflows/pdfs/process_pdf_standalone/process_pdf_standalone_ingress.ts">import { z } from &quot;zod&quot;

export const processPdfStandaloneIngress = z.object({ &quot;flowName&quot;: z.literal(&quot;process-pdf-standalone/dev&quot;), &quot;s3InputKey&quot;: z.string(), &quot;s3OutputKey&quot;: z.string() })
export type ProcessPdfStandaloneIngress = z.infer&lt;typeof processPdfStandaloneIngress&gt;</file><file path="src/art-store/handlers/handle-query.ts">import { dirname } from &quot;node:path&quot;;
import { platform } from &quot;../infra.ts&quot;;
import { scheme } from &quot;../platform.ts&quot;;
import { listAllBlobKeys } from &quot;../utils/list-all-blob-keys.ts&quot;;

type ParsedArtQuery = {
	enterpriseUid: string | undefined;
	workspaceUid: string | undefined;
	category?: string | undefined;
	sourceId?: string | undefined;
	artifactId?: string | undefined;
	queryParams: Record&lt;string, string[]&gt;;
};

export async function handleQuery(artQuery: ParsedArtQuery): Promise&lt;string[]&gt; {
	const prefix = [
		artQuery.workspaceUid,
		artQuery.category,
		artQuery.sourceId,
		artQuery.artifactId,
	]
		.filter(Boolean)
		.join(&quot;/&quot;);
	const fileTypes = artQuery.queryParams[&quot;type&quot;] || [];
	const blobKeys = (await listAllBlobKeys(prefix)).map((it) =&gt; `${scheme}${it}`);
	if (!fileTypes.length) {
		if (artQuery.sourceId) {
			return blobKeys;
		}
		return [...new Set(blobKeys.map((it) =&gt; dirname(`${it}`)))];
	}
	return blobKeys.filter((it) =&gt; {
		for (const type of fileTypes) {
			if (it.endsWith(type)) {
				return true;
			}
		}
		return false;
	});
}

export async function handleBlobQuery(artQuery: ParsedArtQuery): Promise&lt;Blob&gt; {
	const prefix = [
		artQuery.workspaceUid,
		artQuery.category,
		artQuery.sourceId,
		artQuery.artifactId,
	]
		.filter(Boolean)
		.join(&quot;/&quot;);
	const keys = await listAllBlobKeys(prefix);
	if (keys.length !== 1 || !keys[0]) {
		throw new Error(&quot;No blob found&quot;);
	}
	return await platform.blobs.fetchBlob(keys[0]);
}</file><file path="src/art-store/scripts/init-s3.ts">/**
 * Ad-hoc script to initialize the S3 bucket with some test files.
 */
import { MIME_TYPES, toBlob } from &quot;@anterior/lib-platform/blob&quot;;
import { join } from &quot;path&quot;;
import { Category } from &quot;../constants.ts&quot;;
import { platform } from &quot;../infra.ts&quot;;

const enterpriseUid = &quot;optum&quot;;
const workspaceUid = &quot;optum-workspace-1&quot;;

const files = [
	{
		category: Category.CLINICAL,
		sourceId: &quot;artifact_abc_123&quot;,
		artifactId: &quot;artifact_abc_123.raw.pdf&quot;,
		content: &quot;Sample PDF content&quot;,
		extension: &quot;pdf&quot;,
	},
	{
		category: Category.CLINICAL,
		sourceId: &quot;artifact_abc_123&quot;,
		artifactId: &quot;artifact_abc_123.extr_v1.json&quot;,
		content: JSON.stringify({ example: &quot;data&quot; }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.CLINICAL,
		sourceId: &quot;artifact_def_231&quot;,
		artifactId: &quot;artifact_def_231.raw.pdf&quot;,
		content: &quot;Sample PDF content&quot;,
		extension: &quot;pdf&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_abc_123&quot;,
		artifactId: &quot;IQ_abc_123.orig.xml&quot;,
		content: &quot;&lt;root&gt;&lt;/root&gt;&quot;,
		extension: &quot;xml&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_abc_123&quot;,
		artifactId: &quot;IQ_abc_123-a.gdt_v1.json&quot;,
		content: JSON.stringify({ a: 1 }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_abc_123&quot;,
		artifactId: &quot;IQ_abc_123-b.gdt_v1.json&quot;,
		content: JSON.stringify({ b: 2 }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_abc_123&quot;,
		artifactId: &quot;IQ_abc_123-c.gdt_v1.json&quot;,
		content: JSON.stringify({ c: 3 }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_def_491&quot;,
		artifactId: &quot;IQ_def_491.orig.xml&quot;,
		content: &quot;&lt;root&gt;&lt;/root&gt;&quot;,
		extension: &quot;xml&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_def_491&quot;,
		artifactId: &quot;IQ_def_491-a.gdt_v1.json&quot;,
		content: JSON.stringify({ a: 1 }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.CRITERIA,
		sourceId: &quot;IQ_def_567&quot;,
		artifactId: &quot;IQ_def_567.orig.xml&quot;,
		content: &quot;&lt;root&gt;&lt;/root&gt;&quot;,
		extension: &quot;xml&quot;,
	},
	{
		category: Category.CONFIG,
		sourceId: &quot;traps&quot;,
		artifactId: &quot;traps.json&quot;,
		content: JSON.stringify({ traps: true }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.CONFIG,
		sourceId: &quot;guideline&quot;,
		artifactId: &quot;guideline_adapters.json&quot;,
		content: JSON.stringify({ adapter: &quot;sample&quot; }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.TIPS,
		sourceId: &quot;anterior&quot;,
		artifactId: &quot;anterior.base.json&quot;,
		content: JSON.stringify({ tip: &quot;base&quot; }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.TIPS,
		sourceId: &quot;optum&quot;,
		artifactId: &quot;optum.overrides.json&quot;,
		content: JSON.stringify({ override: true }),
		extension: &quot;json&quot;,
	},
	{
		category: Category.DETERMINATIONS,
		sourceId: &quot;determination_bde_123&quot;,
		artifactId: &quot;determination_bde_123.det_v1.json&quot;,
		content: JSON.stringify({ decision: &quot;approved&quot; }),
		extension: &quot;json&quot;,
	},
] as const;

await Promise.all(
	files.map(async ({ category, sourceId, artifactId, content, extension }) =&gt; {
		const mimeType = MIME_TYPES[extension];
		const blob = await toBlob(content, mimeType);
		const fullPath = join(workspaceUid, category, sourceId, artifactId);
		await platform.blobs.storeBlob(fullPath, blob, mimeType);
	})
);

process.exit(0);</file><file path="src/art-store/utils/list-all-blob-keys.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { ListObjectsV2Command, S3Client } from &quot;@aws-sdk/client-s3&quot;;

const s3 = new S3Client({
	...(config.ANT_S3_FORCE_PATH_STYLE ? { forcePathStyle: true } : undefined),
});

const bucket = config.ANT_S3_BUCKET;

/**
 * Lists all blob keys in the S3 bucket under an optional prefix.
 * Ad-hoc implementation as `lib-infra` does not support this.
 *
 * @param prefix optional prefix to filter by, e.g. &quot;optum_wkpc_id/criteria/&quot;
 */
export async function listAllBlobKeys(prefix?: string): Promise&lt;string[]&gt; {
	let continuationToken: string | undefined = undefined;
	const keys: string[] = [];
	do {
		const command: ListObjectsV2Command = new ListObjectsV2Command({
			Bucket: bucket,
			Prefix: prefix,
			ContinuationToken: continuationToken,
		});
		const response = await s3.send(command);
		const contents = response.Contents ?? [];
		for (const obj of contents) {
			if (obj.Key) {
				keys.push(obj.Key);
			}
		}
		continuationToken = response.IsTruncated ? response.NextContinuationToken : undefined;
	} while (continuationToken);

	return keys;
}</file><file path="src/art-store/utils/parse-query-params.ts">/**
 * Parses the query parameters from a given URL and returns them as an object.
 * Platform doesn&apos;t support multiple values for the same key :( so we do it ourselves
 */
export function parseQueryParams(url: string): Record&lt;string, string[]&gt; {
	const result: Record&lt;string, string[]&gt; = {};
	const parsedUrl = new URL(url);
	const params = new URLSearchParams(parsedUrl.search);
	for (const [key, value] of params.entries()) {
		if (!result[key]) {
			result[key] = [];
		}
		result[key].push(value);
	}
	return result;
}</file><file path="src/art-store/app.ts">import { App, type AppOptions, type EndpointSignature } from &quot;@anterior/lib-platform/app&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import { z } from &quot;zod&quot;;
import { handleBlobQuery, handleQuery } from &quot;./handlers/handle-query.ts&quot;;
import type { Platform } from &quot;./platform.ts&quot;;
import { parseQueryParams } from &quot;./utils/parse-query-params.ts&quot;;

export interface ArtStoreAppContext {
	logger: Logger;
	platform: Platform;
}

export interface ArtStoreAppService {
	GET: {
		&quot;/&quot;: EndpointSignature&lt;{}, string[]&gt;;
		&quot;/:category&quot;: EndpointSignature&lt;
			{
				cateogory: string;
			},
			string[]
		&gt;;
		&quot;/:category/:sourceId&quot;: EndpointSignature&lt;
			{
				cateogory: string;
				sourceId: string;
			},
			string[]
		&gt;;
		&quot;/:category/:sourceId/:artifactId&quot;: EndpointSignature&lt;
			{
				cateogory: string;
				sourceId: string;
				artifactId: string;
			},
			Response
		&gt;;
	};
	POST: {
		&quot;/&quot;: EndpointSignature&lt;{}, string[]&gt;;
	};
}

export type ArtStoreApp = App&lt;ArtStoreAppService, ArtStoreAppContext&gt;;

export function createArtStoreApp(baseCtx: ArtStoreAppContext, options: AppOptions): ArtStoreApp {
	const app: ArtStoreApp = new App(baseCtx, options);

	// TODO: Will be inferred from JWT
	const enterpriseUid = &quot;optum&quot;;
	const workspaceUid = &quot;optum-workspace-1&quot;;

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/&quot;,
		response: z.array(z.string()),
		handler: async ({ req: { url } }) =&gt; {
			return await handleQuery({
				enterpriseUid,
				workspaceUid,
				queryParams: parseQueryParams(url),
			});
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/:category&quot;,
		response: z.array(z.string()),
		handler: async ({ params: { category }, req: { url } }) =&gt; {
			return await handleQuery({
				enterpriseUid,
				workspaceUid,
				category,
				queryParams: parseQueryParams(url),
			});
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/:category/:sourceId&quot;,
		response: z.array(z.string()),
		handler: async ({ params: { category, sourceId }, req: { url } }) =&gt; {
			return await handleQuery({
				enterpriseUid,
				workspaceUid,
				category,
				sourceId,
				queryParams: parseQueryParams(url),
			});
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/:category/:sourceId/:artifactId&quot;,
		response: z.instanceof(Response),
		handler: async ({ params: { category, sourceId, artifactId }, req: { url } }) =&gt; {
			const blob = await handleBlobQuery({
				enterpriseUid,
				workspaceUid,
				category,
				sourceId,
				artifactId,
				queryParams: parseQueryParams(url),
			});
			return new Response(await blob.arrayBuffer(), {
				headers: {
					&quot;Content-Type&quot;: blob.type,
					&quot;Content-Disposition&quot;: `attachment; filename=&quot;${artifactId}&quot;`,
				},
			});
		},
	});
	return app;
}</file><file path="src/art-store/constants.ts">/**
 * Ad-hoc categories for the art store.
 */
export enum Category {
	CLINICAL = &quot;clinicals&quot;,
	CRITERIA = &quot;criteria&quot;,
	DETERMINATIONS = &quot;determinations&quot;,
	CONFIG = &quot;config&quot;,
	TIPS = &quot;tips&quot;,
}</file><file path="src/art-store/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

/**
 * Platform configuration for the art-store application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for note payloads and metadata
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
};
export const platform = Platform.fromInfra(infra);</file><file path="src/art-store/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createArtStoreApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createArtStoreApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;art-store&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		// TODO: enable CSRF ASAP, when client is ready
		// the frontend apps will need this implemented
		useCSRF: false,
		noLogging: false,
		// jwtConfig: {
		// 	jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
		// 	tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
		// },
	}
);</file><file path="src/art-store/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import { InMemoryBlobStoreBackend } from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;

export const scheme = &quot;art://&quot;;

/**
 * Infrastructure interface defining the external resources needed by the platform.
 *
 * The platform can be deployed in different environments:
 * - Production: Resources managed by CDK/Terraform
 * - Docker/CI: Resources managed by Docker Compose
 * - Local/Testing: Resources represented by in-memory implementations
 *
 * This abstraction allows for environment-specific implementations
 * while maintaining a consistent interface for the application.
 */
export interface Infra {
	logger: Logger;
	blobs: BlobStoreBackend;
}

/**
 * Creates an in-memory implementation of the infrastructure.
 *
 * This is not meant for production use! The in-memory implementation
 * is useful for:
 * - Unit testing: Fast, isolated test environment
 * - Local development: Quick setup without external dependencies
 *
 * Note that all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		blobs: new InMemoryBlobStoreBackend(),
	};
}

/**
 * Application layer interface to the resource backends.
 *
 * This interface provides type-safe access to:
 * - Blob storage: For storing note payloads and metadata
 *
 * The interface ensures that all platform operations are properly
 * typed and validated against their schemas.
 */
export interface Stores {
	readonly blobs: BlobStore;
}

/**
 * Core platform implementation for the art-store application.
 *
 * The platform provides:
 * 1. Factory methods for creating platform instances
 * 2. Type-safe access to storage and flow services
 * 3. Environment-specific resource management
 */
export class Platform {
	/**
	 * Creates an in-memory platform instance for testing.
	 *
	 * This method allows for partial infrastructure overrides,
	 * making it easy to mock specific components while using
	 * real implementations for others.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	/**
	 * Creates a platform instance from infrastructure configuration.
	 *
	 * This method:
	 * 1. Sets up the message queue for flow scheduling
	 * 2. Initializes blob storage for note data
	 * 3. Configures proper serialization and validation
	 */
	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	/**
	 * Type-safe access to the blob storage service.
	 * Used for storing and retrieving note data.
	 */
	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/auth/app.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { S3Bucket } from &quot;@anterior/lib-infra/s3&quot;;
import {
	App,
	type AppOptions,
	type EndpointSignature,
	createStaticApp,
} from &quot;@anterior/lib-platform/app&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { KeyMissingError } from &quot;@anterior/lib-platform/errors&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import * as z from &quot;@anterior/lib-platform/zod&quot;;
import type { ExecutionContext } from &quot;hono&quot;;

const PATH_PREFIX = &quot;/auth&quot;;

// this doesn&apos;t match what the API sets
// for the access token expiry...it&apos;s much shorter
// revisit, and use the same source of truth
const ACCESS_TOKEN_MAX_AGE = 900; // 15 minutes

// refresh is not yet functional...
// but we need it so that users aren&apos;t
// logged out
const REFRESH_TOKEN_MAX_AGE = 43200; // 30 days

// we should shore this up...was testing various cookie options for
// authenticating the frontend app hosted by noggin/src/flonotes
const COOKIE_OPTIONS = {
	ACCESS_TOKEN: (token: string) =&gt;
		`access_token=${token}; Path=/; HttpOnly; SameSite=Lax; Max-Age=${ACCESS_TOKEN_MAX_AGE}`,
	REFRESH_TOKEN: (token: string) =&gt;
		`refresh_token=${token}; Path=/; HttpOnly; SameSite=Lax; Max-Age=${REFRESH_TOKEN_MAX_AGE}`,
	AUTH_INDICATOR: `auth_indicator=true; Path=/; SameSite=Lax; Max-Age=${ACCESS_TOKEN_MAX_AGE}`,
	CLEAR_ACCESS_TOKEN: &quot;access_token=; Path=/; HttpOnly; SameSite=Lax; Max-Age=0&quot;,
	CLEAR_REFRESH_TOKEN: &quot;refresh_token=; Path=/; HttpOnly; SameSite=Lax; Max-Age=0&quot;,
	CLEAR_AUTH_INDICATOR: &quot;auth_indicator=; Path=/; SameSite=Lax; Max-Age=0&quot;,
};

export class Auth {
	constructor(private apiBaseUrl: string) {}
	async createOtp(body: { email: string; password: string }) {
		const res = await fetch(`${this.apiBaseUrl}/login`, {
			method: &quot;POST&quot;,
			headers: {
				&quot;Content-Type&quot;: &quot;application/json&quot;,
			},
			body: JSON.stringify(body),
		});
		if (!res.ok) {
			throw new Error(&quot;Authentication failed&quot;);
		}
	}
	async verifyOtp({ email, otp }: { email: string; otp: string }): Promise&lt;AuthResponse&gt; {
		const authResponse = await fetch(
			`${this.apiBaseUrl}/otp/verify?email=${encodeURIComponent(email)}&amp;otp=${otp}`,
			{ method: &quot;POST&quot; }
		);
		if (!authResponse.ok) {
			throw new Error(&quot;Authentication failed&quot;);
		}
		const authResponseData = await authResponse.json();
		return authResponseSchema.parse(authResponseData);
	}
	// NB: not yet implemented below as createOtp and verifyOtp are
	async refreshToken(refreshToken: string): Promise&lt;AuthResponse&gt; {
		const res = await fetch(`${this.apiBaseUrl}/token/refresh`, {
			method: &quot;POST&quot;,
			headers: {
				&quot;Content-Type&quot;: &quot;application/json&quot;,
				Cookie: `refresh_token=${refreshToken}`,
			},
		});
		if (!res.ok) {
			throw new Error(&quot;Failed to refresh token&quot;);
		}
		const responseData = await res.json();
		return authResponseSchema.parse(responseData);
	}
}

export interface AuthAppContext {
	logger: Logger;
	auth: Auth;
}

export interface AuthResponse {
	access_token: string;
	refresh_token: string;
	token_type: &quot;bearer&quot;;
}

export const authResponseSchema = z.object({
	access_token: z.string(),
	refresh_token: z.string(),
	token_type: z.literal(&quot;bearer&quot;),
}) satisfies z.ZodType&lt;AuthResponse&gt;;

export interface AuthAppService {
	GET: {
		&quot;/login.html&quot;: EndpointSignature&lt;null, string&gt;;
		&quot;/otp-verify.html&quot;: EndpointSignature&lt;null, string&gt;;
		&quot;/&quot;: EndpointSignature&lt;null, void&gt;;
		// NB: this we could ideally do away with @ajit
		&quot;/status&quot;: EndpointSignature&lt;null, { authenticated: boolean; user?: { uid: string } }&gt;;
	};
	POST: {
		&quot;/login&quot;: EndpointSignature&lt;{ email: string; password: string }, null&gt;;
		&quot;/otp/verify&quot;: EndpointSignature&lt;{ email: string; otp: string }, AuthResponse&gt;;
		&quot;/login.html&quot;: EndpointSignature&lt;{ email: string; password: string }, string&gt;;
		&quot;/otp-verify.html&quot;: EndpointSignature&lt;{ email: string; otp: string }, string&gt;;
		&quot;/refresh&quot;: EndpointSignature&lt;null, AuthResponse&gt;;
	};
}

export type AuthApp = App&lt;AuthAppService, AuthAppContext&gt;;

export function createAuthApp(baseCtx: AuthAppContext, options: AppOptions): AuthApp {
	const app: AuthApp = new App(baseCtx, options);

	const setAuthCookies = (
		response: Response,
		tokens: { access_token: string; refresh_token: string }
	) =&gt; {
		response.headers.append(&quot;Set-Cookie&quot;, COOKIE_OPTIONS.ACCESS_TOKEN(tokens.access_token));
		response.headers.append(&quot;Set-Cookie&quot;, COOKIE_OPTIONS.AUTH_INDICATOR);
		response.headers.append(&quot;Set-Cookie&quot;, COOKIE_OPTIONS.REFRESH_TOKEN(tokens.refresh_token));
		return response;
	};

	// default to redirecting to the FloNotes frontend app proxied from s3 by noggin/src/flonotes
	const getFormValue = (formData: FormData, key: string) =&gt; formData.get(key)?.toString() || &quot;&quot;;

	app.endpoint({
		method: &quot;POST&quot;,
		body: z.object({ email: z.string(), password: z.string() }),
		route: &quot;/login&quot;,
		async handler(ctx) {
			await ctx.auth.createOtp(ctx.body);
		},
	});

	app.endpoint({
		method: &quot;POST&quot;,
		body: z.object({ email: z.string(), otp: z.string() }),
		route: &quot;/otp/verify&quot;,
		async handler(ctx) {
			const result = await ctx.auth.verifyOtp(ctx.body);

			const response = new Response(JSON.stringify(result), {
				headers: {
					&quot;Content-Type&quot;: &quot;application/json&quot;,
				},
			});

			setAuthCookies(response, result);

			return response;
		},
	});

	const s3 = new S3Bucket(config.ANT_S3_BUCKET, &quot;vibes/flonotes/&quot;);
	const blobs = BlobStore.createReadonly(logger, s3);

	const staticApp = createStaticApp({
		prefix: &quot;&quot;, // no prefix
		blobs,
		index: &quot;login.html&quot;,
		shouldGenerateCSP: true,
	});

	// serve static files from s3 by default
	app.hono.use(&quot;*&quot;, async (c, next) =&gt; {
		if (c.req.method === &quot;GET&quot;) {
			try {
				baseCtx.logger.info({
					msg: &quot;Attempting to serve static file&quot;,
					path: c.req.path,
					method: c.req.method,
					url: c.req.url,
				});

				const response = await staticApp.fetch(c.req.raw, {}, {} as ExecutionContext);

				baseCtx.logger.info({
					msg: &quot;Successfully served static file&quot;,
					path: c.req.path,
					status: response.status,
				});

				return response;
			} catch (error) {
				if (error instanceof KeyMissingError) {
					baseCtx.logger.info({
						msg: &quot;Static file not found, continuing to next handler&quot;,
						path: c.req.path,
					});
					return await next();
				}

				baseCtx.logger.error({
					msg: &quot;Error serving static file&quot;,
					path: c.req.path,
					error: error instanceof Error ? error.message : String(error),
					stack: error instanceof Error ? error.stack : undefined,
					url: c.req.url,
					headers: Object.fromEntries(c.req.raw.headers.entries()),
				});

				throw error;
			}
		}
		return await next();
	});

	app.hono.post(&quot;/login.html&quot;, async (c) =&gt; {
		try {
			const formData = await c.req.formData();
			const email = getFormValue(formData, &quot;email&quot;);
			const password = getFormValue(formData, &quot;password&quot;);

			const redirectTo = `${config.ANT_NOGGIN_BASE_URL}/flonotes`;

			logger.info({ msg: &quot;Login form submitted&quot;, email, redirectTo });

			try {
				await baseCtx.auth.createOtp({ email, password });
				logger.info({
					msg: &quot;OTP created successfully, redirecting to verification page&quot;,
					email,
				});

				const searchParams = new URLSearchParams();
				searchParams.set(&quot;email&quot;, email);
				searchParams.set(&quot;redirectTo&quot;, redirectTo);

				return c.redirect(`${PATH_PREFIX}/otp-verify.html?${searchParams.toString()}`);
			} catch (error) {
				const errorMsg = `We couldn&apos;t authenticate you with the provided credentials.
				Please check your email and password and try again.`;

				const searchParams = new URLSearchParams();
				searchParams.set(&quot;email&quot;, email);
				searchParams.set(&quot;redirectTo&quot;, redirectTo);
				searchParams.set(&quot;error&quot;, errorMsg);

				return c.redirect(`${PATH_PREFIX}/login.html?${searchParams.toString()}`);
			}
		} catch (err) {
			logger.error({ msg: &quot;Error processing login form&quot;, error: err });
			return c.text(&quot;Error processing login&quot;, 500);
		}
	});

	app.hono.post(&quot;/otp/verify.html&quot;, async (c) =&gt; {
		try {
			const formData = await c.req.formData();
			const email = getFormValue(formData, &quot;email&quot;);
			const otp = getFormValue(formData, &quot;otp&quot;);

			const redirectTo = `${config.ANT_NOGGIN_BASE_URL}/flonotes`;

			logger.info({ msg: &quot;OTP verification submitted&quot;, email, redirectTo });

			try {
				const tokens = await baseCtx.auth.verifyOtp({ email, otp });
				const response = c.redirect(redirectTo);
				setAuthCookies(response, tokens);

				logger.info({ msg: &quot;Authentication successful, redirecting&quot;, redirectTo });

				return response;
			} catch (error) {
				logger.error({ msg: &quot;OTP verification failed&quot;, error });
				const errorMsg =
					&quot;The verification code is invalid or has expired. Please try again.&quot;;

				const searchParams = new URLSearchParams();
				searchParams.set(&quot;email&quot;, email);
				searchParams.set(&quot;redirectTo&quot;, redirectTo);
				searchParams.set(&quot;error&quot;, errorMsg);

				return c.redirect(`${PATH_PREFIX}/otp-verify.html?${searchParams.toString()}`);
			}
		} catch (err) {
			logger.error({ msg: &quot;Error processing OTP verification&quot;, error: err });
			return c.text(&quot;Error processing OTP verification&quot;, 500);
		}
	});

	return app;
}</file><file path="src/auth/jwt.ts">/**
 * Authentication utilities for JWT handling
 *
 * Provides functions to extract enterprise and user information from JWT payload
 */

import type { BaseContext } from &quot;@anterior/lib-platform/app&quot;;
import { HTTPException } from &quot;@anterior/lib-platform/app&quot;;

export type TokenContext = {
	userUid: string;
	enterpriseUid: string;
	role: string;
	workspaceUid: string;
};

/**
 * Extracts the enterprise ID and user ID from the JWT payload
 *
 * Expects payload.entr to be an object with enterprise IDs as keys
 * and payload.sub to contain the user ID
 *
 * TODO: introduce support for workspaces, and multiple enterprise IDs per user!
 *
 * @throws HTTPException if JWT payload is missing or invalid
 */
export function parseEnterpriseUserFromJwt(ctx: BaseContext): TokenContext {
	if (!ctx.jwtPayload?.entr) {
		throw new HTTPException(401, { message: &quot;Authentication required&quot; });
	}

	// Get enterprise and user uids from the token.
	let enterpriseUid = undefined;
	let role = undefined;

	// Most tokens will have a single enterprise.
	if (Object.keys(ctx.jwtPayload.entr).length === 1) {
		enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
		role = ctx.jwtPayload.entr[enterpriseUid];
	} else {
		// If token contains multiple enterprises.
		// First check for the X-Anterior-Enterprise-Id header.
		const entHeader = ctx.req.headers.get(&quot;X-Anterior-Enterprise-Id&quot;);
		if (entHeader &amp;&amp; ctx.jwtPayload.entr[entHeader]) {
			enterpriseUid = ctx.jwtPayload.entr[entHeader];
			role = ctx.jwtPayload.entr[enterpriseUid];
		} else {
			// Use the first enterprise in the map (random).
			enterpriseUid = Object.keys(ctx.jwtPayload.entr)[0] as string;
			role = ctx.jwtPayload.entr[enterpriseUid];
		}
	}
	if (!enterpriseUid || !role) {
		throw new HTTPException(401, { message: &quot;Failed to get enterprise from request&quot; });
	}

	// Get workspaceUid from claims or header if present.
	let workspaceUid: string = &quot;unassigned&quot;;
	const wksp = ctx.jwtPayload.wksp;
	if (Object.keys(wksp).length === 1) {
		// Token has a single workspace.
		Object.entries(wksp).forEach(([wkId, entId]) =&gt; {
			if (entId !== enterpriseUid) {
				throw new HTTPException(401, {
					message: &quot;Workspace in token does not match enterprise&quot;,
				});
			}
			workspaceUid = wkId;
		});
	} else {
		// Token claims contain multiple workspaces. Look for
		// the X-Anterior-Workspace-Id header.
		const wkspHeader = ctx.req.headers.get(&quot;X-Anterior-Workspace-Id&quot;);
		if (wkspHeader) {
			const entId = wksp[wkspHeader];
			if (!entId) {
				throw new HTTPException(401, {
					message: &quot;Workspace from header not found in token&quot;,
				});
			}
			if (entId !== enterpriseUid) {
				throw new HTTPException(401, {
					message: &quot;Workspace in header does not match enterprise&quot;,
				});
			}
			workspaceUid = wkspHeader;
		}
	}

	return {
		enterpriseUid,
		role,
		workspaceUid,
		userUid: ctx.jwtPayload.sub,
	};
}</file><file path="src/auth/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Auth, createAuthApp } from &quot;./app.ts&quot;;

export default createAuthApp(
	{
		logger,
		auth: new Auth(config.ANT_API_BASE_URL),
	},
	{
		name: &quot;auth&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		// TODO: enable CSRF ASAP, when client is ready
		// the frontend apps will need this implemented
		useCSRF: false,
		noLogging: false,
	}
);</file><file path="src/chat/orchestrator/tools/mnr.ts">import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type { Messages } from &quot;@anthropic-ai/sdk/resources/messages&quot;;
import type { Platform } from &quot;../../platform.ts&quot;;

export interface MnrToolInput {
	clinicals_uri: string;
	guideline_uri: string;
	requested_service_description?: string;
	indication_description?: string;
	cpt_code?: string;
	icd_10_code?: string;
	hcpcs_code?: string;
}

export class MnrTool {
	private logger: Logger;
	private platform: Platform;

	constructor(logger: Logger, platform: Platform) {
		this.logger = logger;
		this.platform = platform;
	}

	getToolDefinition(): Messages.Tool {
		return {
			name: &quot;perform_mnr&quot;,
			description: &quot;Performs Medical Necessity Review on clinical documents&quot;,
			input_schema: {
				type: &quot;object&quot;,
				properties: {
					clinicals_uri: {
						type: &quot;string&quot;,
						description: &quot;URI to the clinical document that needs to be reviewed&quot;,
					},
					guideline_uri: {
						type: &quot;string&quot;,
						description: &quot;URI to the guideline document to use for the review&quot;,
					},
					requested_service_description: {
						type: &quot;string&quot;,
						description: &quot;Description of the requested medical service&quot;,
					},
					indication_description: {
						type: &quot;string&quot;,
						description: &quot;Description of the medical indication&quot;,
					},
					cpt_code: {
						type: &quot;string&quot;,
						description: &quot;CPT code for the procedure&quot;,
					},
					icd_10_code: {
						type: &quot;string&quot;,
						description: &quot;ICD-10 diagnosis code&quot;,
					},
					hcpcs_code: {
						type: &quot;string&quot;,
						description: &quot;HCPCS code for the procedure or service&quot;,
					},
				},
				required: [&quot;clinicals_uri&quot;, &quot;guideline_uri&quot;],
			},
		};
	}

	async execute(input: MnrToolInput): Promise&lt;string&gt; {
		this.logger.info({
			msg: &quot;Executing perform_mnr tool&quot;,
			clinicalsUri: input.clinicals_uri,
			guidelineUri: input.guideline_uri,
		});

		const mockResult = {
			assessment: &quot;APPROVED&quot;,
			reason: &quot;The clinical documentation supports medical necessity based on guideline criteria.&quot;,
			matching_criteria: [
				&quot;Patient age and condition meets guideline requirements&quot;,
				&quot;Documented previous treatments align with step therapy protocols&quot;,
				&quot;Clinical evidence supports requested procedure&quot;,
			],
			clinical_summary:
				&quot;Patient shows appropriate indications for the requested service based on the clinical documentation.&quot;,
		};

		return JSON.stringify(mockResult);
	}
}</file><file path="src/chat/orchestrator/anthropic.ts">import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type Anthropic from &quot;@anthropic-ai/sdk&quot;;
import type { Messages, ToolUseBlock } from &quot;@anthropic-ai/sdk/resources/messages&quot;;
import type { Platform } from &quot;../platform.ts&quot;;
import { MnrTool, type MnrToolInput } from &quot;./tools/mnr.ts&quot;;
import type { IOrchestrator, OrchestratorContext } from &quot;./types.ts&quot;;

export interface AnthropicOrchestratorContext extends OrchestratorContext {
	anthropic: Anthropic;
}

export const MODEL_NAME = &quot;claude-3-7-sonnet-latest&quot;;

export class AnthropicOrchestrator implements IOrchestrator {
	private logger: Logger;
	private platform: Platform;
	private anthropic: Anthropic;
	private mnrTool: MnrTool;

	constructor(context: AnthropicOrchestratorContext) {
		this.logger = context.logger;
		this.platform = context.platform;
		this.anthropic = context.anthropic;
		this.mnrTool = new MnrTool(this.logger, this.platform);

		this.logger.info({
			msg: &quot;Initializing AnthropicOrchestrator&quot;,
		});
	}

	private getTools(): Messages.Tool[] {
		return [this.mnrTool.getToolDefinition()];
	}

	private async handleToolCalls(toolCall: Messages.ToolUseBlock): Promise&lt;string&gt; {
		if (toolCall.name === &quot;perform_mnr&quot;) {
			try {
				const input = toolCall.input as MnrToolInput;
				return await this.mnrTool.execute(input);
			} catch (error) {
				this.logger.error({
					msg: &quot;Error executing perform_mnr tool&quot;,
					error,
				});
				return JSON.stringify({ error: &quot;Failed to perform medical necessity review&quot; });
			}
		}

		return JSON.stringify({ error: &quot;Unknown tool&quot; });
	}

	async processChat(
		system: string | undefined,
		messages: Array&lt;{ role: &quot;user&quot; | &quot;assistant&quot;; content: string }&gt;
	): Promise&lt;string&gt; {
		const tools = this.getTools();

		const initialResponse = await this.anthropic.messages.create({
			model: MODEL_NAME,
			max_tokens: 1024,
			system: system ?? &quot;&quot;,
			messages: messages,
			tools: tools,
		});

		let toolUseBlock: ToolUseBlock | undefined;
		let fullResponse = &quot;&quot;;

		for (const block of initialResponse.content) {
			if (block.type === &quot;text&quot;) {
				fullResponse += block.text;
			} else if (block.type === &quot;tool_use&quot;) {
				// Assuming there&apos;s always only one tool use block for now
				toolUseBlock = block;
			}
		}

		if (toolUseBlock) {
			const toolResults = await this.handleToolCalls(toolUseBlock);

			const continuationResponse = await this.anthropic.messages.create({
				model: MODEL_NAME,
				max_tokens: 1024,
				system: system ?? &quot;&quot;,
				messages: [
					...messages,
					{ role: &quot;assistant&quot;, content: initialResponse.content },
					{
						role: &quot;user&quot;,
						content: [
							{
								type: &quot;tool_result&quot;,
								tool_use_id: toolUseBlock.id,
								content: toolResults,
							},
						],
					},
				],
			});

			fullResponse = &quot;&quot;;
			for (const block of continuationResponse.content) {
				if (block.type === &quot;text&quot;) {
					fullResponse += block.text;
				}
			}
		}

		return fullResponse;
	}
}</file><file path="src/chat/orchestrator/index.ts">export * from &quot;./anthropic.ts&quot;;
export * from &quot;./tools/mnr.ts&quot;;
export * from &quot;./types.ts&quot;;</file><file path="src/chat/orchestrator/types.ts">import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type { Platform } from &quot;../platform.ts&quot;;

export interface OrchestratorContext {
	logger: Logger;
	platform: Platform;
}

export interface IOrchestrator {
	processChat(
		system: string | undefined,
		messages: Array&lt;{ role: &quot;user&quot; | &quot;assistant&quot;; content: string }&gt;
	): Promise&lt;string&gt;;
}</file><file path="src/chat/app.ts">import { App, type AppOptions, type EndpointSignature } from &quot;@anterior/lib-platform/app&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type { SchemaLike } from &quot;@anterior/lib-platform/schema&quot;;
import * as z from &quot;@anterior/lib-platform/zod&quot;;
import type Anthropic from &quot;@anthropic-ai/sdk&quot;;
import { AnthropicOrchestrator } from &quot;./orchestrator/index.ts&quot;;
import type { Platform } from &quot;./platform.ts&quot;;

export interface ChatAppContext {
	logger: Logger;
	platform: Platform;
	anthropic: Anthropic;
}

type ChatRequestBody = {
	system?: string | undefined;
	messages: Array&lt;{ role: &quot;user&quot; | &quot;assistant&quot;; content: string }&gt;;
};

export interface ChatAppService {
	POST: {
		&quot;/&quot;: EndpointSignature&lt;ChatRequestBody, Response&gt;;
	};
}

export type ChatApp = App&lt;ChatAppService, ChatAppContext&gt;;

export function createChatApp(ctx: ChatAppContext, options: AppOptions): ChatApp {
	ctx.logger.info({
		msg: &quot;Creating Chat App&quot;,
		contextKeys: Object.keys(ctx),
	});

	const orchestrator = new AnthropicOrchestrator(ctx);

	const app: ChatApp = new App(ctx, options);

	app.endpoint({
		method: &quot;POST&quot;,
		body: z.object({
			system: z.string().optional(),
			messages: z
				.array(
					z.object({
						role: z.enum([&quot;user&quot;, &quot;assistant&quot;]),
						content: z.string(),
					})
				)
				.min(1, &quot;At least one message is required&quot;),
		}) satisfies SchemaLike&lt;ChatRequestBody&gt;,
		route: &quot;/&quot;,
		async handler(ctx) {
			const response = await orchestrator.processChat(ctx.body.system, ctx.body.messages);

			const readableStream = new ReadableStream({
				start(controller) {
					controller.enqueue(response);
					controller.close();
				},
			});

			return new Response(readableStream, {
				headers: {
					&quot;Content-Type&quot;: &quot;text/event-stream&quot;,
					&quot;Cache-Control&quot;: &quot;no-cache&quot;,
					Connection: &quot;keep-alive&quot;,
				},
			});
		},
	});

	return app;
}</file><file path="src/chat/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import config from &quot;@anterior/lib-infra/config&quot;;
import { S3Bucket } from &quot;@anterior/lib-infra/s3&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

const BUCKET_NAME = config.ANT_S3_BUCKET;
const chatStorageS3 = new S3Bucket(BUCKET_NAME);
export const chatStorageBlobs = new BlobStore(logger, chatStorageS3);

// todo:
// set filepaths for relevant stem/chat storage

/**
 * Platform configuration for the chat application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for chat data and file uploads
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
};

export const platform = Platform.fromInfra(infra);</file><file path="src/chat/main.ts">import secrets from &quot;@anterior/lib-infra/secrets&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import Anthropic from &quot;@anthropic-ai/sdk&quot;;
import config from &quot;../config.ts&quot;;
import { createChatApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createChatApp(
	{
		logger,
		platform,
		anthropic: new Anthropic({
			apiKey: config[&quot;ANTHROPIC_API_KEY&quot;],
		}),
	},
	{
		name: &quot;chat&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		useCSRF: true,
		noLogging: false,
		jwtConfig: {
			jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
			tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
		},
	}
);</file><file path="src/chat/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import { InMemoryBlobStoreBackend } from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;

// NB: for when we want to store chats similar to how we store
// pdfs and notes within noggin/src/notes (formerly llms)
// @ajit we&apos;ll likely want to store everything flopilot related
// in the same place within the same stem
//
// every new &quot;chat&quot; could comprise a new stem...we may have
// different types of stems over time, starting with &quot;chat&quot;
// the various elements of the chat: input pdfs, chat history,
// artifacts, the workflows that create them, could be stored
// as part of that stem
//
// note that we do not have any events for any of the noggin
// implementations yet...that&apos;s to come

export interface Infra {
	logger: Logger;
	blobs: BlobStoreBackend;
}

export function getInMemoryInfra(): Infra {
	return {
		logger,
		blobs: new InMemoryBlobStoreBackend(),
	};
}

export interface Stores {
	readonly blobs: BlobStore;
}

export class Platform {
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/flonotes/test-e2e/flonotes.spec.ts">import test, { expect } from &quot;@playwright/test&quot;;

const baseUrl = process.env[&quot;ANT_NOGGIN_BASE_URL&quot;];

/**
 * E2E Tests for Flonotes Noggin Application
 *
 * These tests validate the authentication flow and API functionality of the Flonotes Noggin app.
 */

test(&quot;Unauthenticated navigation to protected resources redirects to login page&quot;, async () =&gt; {
	const origPath = &quot;flonotes&quot;;
	const fullUrl = `${baseUrl}/${origPath}`;
	// Attempt to access API without authentication
	const response = await fetch(fullUrl, {
		redirect: &quot;manual&quot;, // Don&apos;t follow redirects automatically
	});

	// Should redirect to login
	expect(response.status).toBe(302);

	// Validate redirect URL
	const location = response.headers.get(&quot;Location&quot;);
	expect(location).not.toBeNull();
	expect(location).toContain(&quot;/auth/login.html&quot;);

	// TODO: Verify it includes the redirectTo parameter
});</file><file path="src/flonotes/app.ts">import {
	App,
	createStaticApp,
	type AppOptions,
	type EndpointSignature,
} from &quot;@anterior/lib-platform/app&quot;;
import { KeyMissingError } from &quot;@anterior/lib-platform/errors&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type { ExecutionContext } from &quot;hono&quot;;
import { flonotesFeBlobs } from &quot;./infra.ts&quot;;

import type { Platform } from &quot;./platform.ts&quot;;

export interface FlonotesAppContext {
	logger: Logger;
	platform: Platform;
}

export interface FlonotesAppService {
	GET: {
		&quot;/&quot;: EndpointSignature&lt;{}, object&gt;;
	};
}

export type FlonotesApp = App&lt;FlonotesAppService, FlonotesAppContext&gt;;

export function createFlonotesApp(baseCtx: FlonotesAppContext, options: AppOptions): FlonotesApp {
	const app: FlonotesApp = new App(baseCtx, options);

	// serves static files from s3, including assets
	// see createStaticApp for more details, in lib/ts/lib-platform/app.ts
	const staticApp = createStaticApp({
		blobs: flonotesFeBlobs,
		prefix: &quot;&quot;,
		index: &quot;index.html&quot;,
		shouldGenerateCSP: true,
	});

	// serve static files from s3 by default
	app.hono.use(&quot;*&quot;, async (c, next) =&gt; {
		if (c.req.method === &quot;GET&quot;) {
			try {
				baseCtx.logger.info({
					msg: &quot;Attempting to serve static file&quot;,
					path: c.req.path,
					method: c.req.method,
					url: c.req.url,
				});

				const response = await staticApp.fetch(c.req.raw, {}, {} as ExecutionContext);

				baseCtx.logger.info({
					msg: &quot;Successfully served static file&quot;,
					path: c.req.path,
					status: response.status,
				});

				return response;
			} catch (error) {
				if (error instanceof KeyMissingError) {
					baseCtx.logger.info({
						msg: &quot;Static file not found, continuing to next handler&quot;,
						path: c.req.path,
					});
					return await next();
				}

				baseCtx.logger.error({
					msg: &quot;Error serving static file&quot;,
					path: c.req.path,
					error: error instanceof Error ? error.message : String(error),
					stack: error instanceof Error ? error.stack : undefined,
					url: c.req.url,
					headers: Object.fromEntries(c.req.raw.headers.entries()),
				});

				throw error;
			}
		}
		return await next();
	});

	return app;
}</file><file path="src/flonotes/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import config from &quot;@anterior/lib-infra/config&quot;;
import { S3Bucket } from &quot;@anterior/lib-infra/s3&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

// DANGER! do not change this path, or risk overwriting
// production S3 data in other paths. We need to overhaul
// our bucket policy to restrict bucket path access to
// certain roles to prevent accidental deletion.
//
// NB: this is only used to pull the frontend app from s3
// and serve it to the user from noggin
export const FLONOTES_FE_S3_PATH = &quot;vibes/flonotes/&quot;;
const flonotesFeS3 = new S3Bucket(config.ANT_S3_BUCKET, FLONOTES_FE_S3_PATH);
export const flonotesFeBlobs = BlobStore.createReadonly(logger, flonotesFeS3);

/**
 * Platform configuration for the Flonotes application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for note payloads and metadata
 * - Flows: LLM-based note generation pipeline
 *
 * Each service is configured with specific settings for the Flonotes
 * use case, ensuring proper isolation and resource management.
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
	flowsQueue: infra_.workflowQueue,
};
export const platform = Platform.fromInfra(infra);</file><file path="src/flonotes/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import secrets from &quot;@anterior/lib-infra/secrets&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createFlonotesApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createFlonotesApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;flonotes&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		useCSRF: true,
		noLogging: true,
		jwtConfig: {
			jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
			tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
			redirectUrl: &quot;/auth/login.html&quot;,
		},
	}
);</file><file path="src/flonotes/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import { InMemoryBlobStoreBackend } from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;

/**
 * Infrastructure interface defining the external resources needed by the platform.
 *
 * The platform can be deployed in different environments:
 * - Production: Resources managed by CDK/Terraform
 * - Docker/CI: Resources managed by Docker Compose
 * - Local/Testing: Resources represented by in-memory implementations
 *
 * This abstraction allows for environment-specific implementations
 * while maintaining a consistent interface for the application.
 */
export interface Infra {
	logger: Logger;
	blobs: BlobStoreBackend;
}

/**
 * Creates an in-memory implementation of the infrastructure.
 *
 * This is not meant for production use! The in-memory implementation
 * is useful for:
 * - Unit testing: Fast, isolated test environment
 * - Local development: Quick setup without external dependencies
 *
 * Note that all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		blobs: new InMemoryBlobStoreBackend(),
	};
}

/**
 * Application layer interface to the resource backends.
 *
 * This interface provides type-safe access to:
 * - Blob storage: For storing note payloads and metadata
 *
 * The interface ensures that all platform operations are properly
 * typed and validated against their schemas.
 */
export interface Stores {
	readonly blobs: BlobStore;
}

/**
 * Core platform implementation for the Flonotes application.
 *
 * The platform provides:
 * 1. Factory methods for creating platform instances
 * 2. Type-safe access to storage and flow services
 * 3. Environment-specific resource management
 */
export class Platform {
	/**
	 * Creates an in-memory platform instance for testing.
	 *
	 * This method allows for partial infrastructure overrides,
	 * making it easy to mock specific components while using
	 * real implementations for others.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	/**
	 * Creates a platform instance from infrastructure configuration.
	 *
	 * This method:
	 * 1. Sets up the message queue for flow scheduling
	 * 2. Initializes blob storage for note data
	 * 3. Configures proper serialization and validation
	 */
	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	/**
	 * Type-safe access to the blob storage service.
	 * Used for storing and retrieving note data.
	 */
	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/flopilot/app.ts">import {
	App,
	createStaticApp,
	type AppOptions,
	type EndpointSignature,
} from &quot;@anterior/lib-platform/app&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import { flopilotFeBlobs } from &quot;./infra.ts&quot;;
import type { Platform } from &quot;./platform.ts&quot;;

export interface FlopilotAppContext {
	logger: Logger;
	platform: Platform;
}

export interface FlopilotAppService {
	GET: {
		&quot;/&quot;: EndpointSignature&lt;{}, object&gt;;
	};
}

export type FlopilotApp = App&lt;FlopilotAppService, FlopilotAppContext&gt;;

export function createFlopilotApp(baseCtx: FlopilotAppContext, options: AppOptions): FlopilotApp {
	const app: FlopilotApp = new App(baseCtx, options);

	// serves static files from s3, including assets
	// see createStaticApp for more details, in lib/ts/lib-platform/app.ts
	const staticApp = createStaticApp({
		blobs: flopilotFeBlobs,
		prefix: &quot;&quot;,
		index: &quot;index.html&quot;,
		shouldGenerateCSP: true,
	});

	// 🚨 Using `hono` outside `lib-platform` is a smell!
	app.hono.route(&quot;/&quot;, staticApp);

	return app;
}</file><file path="src/flopilot/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import config from &quot;@anterior/lib-infra/config&quot;;
import { S3Bucket } from &quot;@anterior/lib-infra/s3&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

// 🚨 DANGER! do not change this path, or risk overwriting
// production S3 data in other paths. We need to overhaul
// our bucket policy to restrict bucket path access to
// certain roles to prevent accidental deletion.
//
// NB: this is only used to pull the frontend app from s3
// and serve it to the user from noggin
export const FLOPILOT_FE_S3_PATH = &quot;vibes/flopilot/&quot;;
const flopilotFeS3 = new S3Bucket(config.ANT_S3_BUCKET, FLOPILOT_FE_S3_PATH);
export const flopilotFeBlobs = BlobStore.createReadonly(logger, flopilotFeS3);

/**
 * Platform configuration for the Flopilot application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for note payloads and metadata
 * - Flows: LLM-based note generation pipeline
 *
 * Each service is configured with specific settings for the Flopilot
 * use case, ensuring proper isolation and resource management.
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
	flowsQueue: infra_.workflowQueue,
};
export const platform = Platform.fromInfra(infra);</file><file path="src/flopilot/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import secrets from &quot;@anterior/lib-infra/secrets&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createFlopilotApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createFlopilotApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;flopilot&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		useCSRF: true,
		noLogging: true,
		jwtConfig: {
			jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
			tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
			redirectUrl: &quot;/auth/login.html&quot;,
		},
	}
);</file><file path="src/flopilot/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import { InMemoryBlobStoreBackend } from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;

/**
 * Infrastructure interface defining the external resources needed by the platform.
 *
 * The platform can be deployed in different environments:
 * - Production: Resources managed by CDK/Terraform
 * - Docker/CI: Resources managed by Docker Compose
 * - Local/Testing: Resources represented by in-memory implementations
 *
 * This abstraction allows for environment-specific implementations
 * while maintaining a consistent interface for the application.
 */
export interface Infra {
	logger: Logger;
	blobs: BlobStoreBackend;
}

/**
 * Creates an in-memory implementation of the infrastructure.
 *
 * This is not meant for production use! The in-memory implementation
 * is useful for:
 * - Unit testing: Fast, isolated test environment
 * - Local development: Quick setup without external dependencies
 *
 * Note that all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		blobs: new InMemoryBlobStoreBackend(),
	};
}

/**
 * Application layer interface to the resource backends.
 *
 * This interface provides type-safe access to:
 * - Blob storage: For storing note payloads and metadata
 *
 * The interface ensures that all platform operations are properly
 * typed and validated against their schemas.
 */
export interface Stores {
	readonly blobs: BlobStore;
}

/**
 * Core platform implementation for the Flopilot application.
 *
 * The platform provides:
 * 1. Factory methods for creating platform instances
 * 2. Type-safe access to storage and flow services
 * 3. Environment-specific resource management
 */
export class Platform {
	/**
	 * Creates an in-memory platform instance for testing.
	 *
	 * This method allows for partial infrastructure overrides,
	 * making it easy to mock specific components while using
	 * real implementations for others.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	/**
	 * Creates a platform instance from infrastructure configuration.
	 *
	 * This method:
	 * 1. Sets up the message queue for flow scheduling
	 * 2. Initializes blob storage for note data
	 * 3. Configures proper serialization and validation
	 */
	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	/**
	 * Type-safe access to the blob storage service.
	 * Used for storing and retrieving note data.
	 */
	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/health/app.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { App, type EndpointSignature } from &quot;@anterior/lib-platform/app&quot;;
import { logger, type Logger } from &quot;@anterior/lib-platform/log&quot;;

interface HealthAppService {
	GET: {
		&quot;/&quot;: EndpointSignature&lt;{}, string&gt;;
	};
}
export function createHealthApp() {
	const healthApp: App&lt;HealthAppService, { logger: Logger }&gt; = new App&lt;
		HealthAppService,
		{ logger: Logger }
	&gt;(
		{ logger },
		{
			name: &quot;health&quot;,
			port: parseInt(config.ANT_LISTEN_PORT),
			allowedHeaders: [],
			allowedOrigins: [],
			useCSRF: false,
		}
	);
	healthApp.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/&quot;,
		handler: async (ctx) =&gt; {
			return new Response(&quot;OK&quot;, { status: 200 });
		},
	});
	return healthApp;
}</file><file path="src/health/main.ts">import { createHealthApp } from &quot;./app.ts&quot;;

const healthApp = createHealthApp();
export default healthApp;</file><file path="src/hello-world/test-e2e/greet.spec.ts">import test, { expect } from &quot;@playwright/test&quot;;

const baseUrl = process.env[&quot;ANT_NOGGIN_BASE_URL&quot;];

test(&quot;Get a greeting&quot;, async () =&gt; {
	const response = await fetch(
		`${process.env[&quot;ANT_NOGGIN_BASE_URL&quot;]}/hello-world/greet?greetee=world`
	);
	expect(response.status).toBe(200);
	await expect(response.json()).resolves.toEqual({ message: &quot;Hello, world!&quot; });
});

test(&quot;Get a greeting with missing greetee throws&quot;, async () =&gt; {
	const response = await fetch(`${process.env[&quot;ANT_NOGGIN_BASE_URL&quot;]}/hello-world/greet`);
	expect(response.status).toBe(400);
});</file><file path="src/hello-world/test-e2e/health.spec.ts">import test, { expect } from &quot;@playwright/test&quot;;

test(&quot;Health check&quot;, async () =&gt; {
	const response = await fetch(`${process.env[&quot;ANT_NOGGIN_BASE_URL&quot;]}/hello-world/health`);

	expect(response.status).toBe(200);
	await expect(response.text()).resolves.toBe(&quot;OK&quot;);
});</file><file path="src/hello-world/test-e2e/latest.spec.ts">import test, { expect } from &quot;@playwright/test&quot;;

test(&quot;Post a greet then retrieve the latest greet&quot;, async () =&gt; {
	const greet = await fetch(`${process.env[&quot;ANT_NOGGIN_BASE_URL&quot;]}/hello-world/greet`, {
		method: &quot;POST&quot;,
		headers: { &quot;Content-Type&quot;: &quot;application/json&quot; },
		body: JSON.stringify({ greetee: &quot;e2e&quot; }),
	});

	expect(greet.status).toBe(204);

	const latest = await fetch(`${process.env[&quot;ANT_NOGGIN_BASE_URL&quot;]}/hello-world/latest`);

	expect(latest.status).toBe(200);
	await expect(latest.text()).resolves.toEqual(&quot;Hello, e2e!&quot;);
});</file><file path="src/hello-world/test-integration/hello.test.ts">import { expect, test } from &quot;vitest&quot;;
import { getLatestGreetee, setLatestGreetee } from &quot;../hello.ts&quot;;

test(&quot;Set and get latest greetee&quot;, async () =&gt; {
	await setLatestGreetee(&quot;greetee-1&quot;);
	await expect(getLatestGreetee()).resolves.toBe(&quot;greetee-1&quot;);
	await setLatestGreetee(&quot;greetee-2&quot;);
	await setLatestGreetee(&quot;greetee-3&quot;);
	await expect(getLatestGreetee()).resolves.toBe(&quot;greetee-3&quot;);
});</file><file path="src/hello-world/app.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { FSDirectory } from &quot;@anterior/lib-infra/fs&quot;;
import { App, createStaticApp, type EndpointSignature } from &quot;@anterior/lib-platform/app&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { logger, type Logger } from &quot;@anterior/lib-platform/log&quot;;
import * as z from &quot;@anterior/lib-platform/zod&quot;;
import { dirname, join } from &quot;path&quot;;
import { fileURLToPath } from &quot;url&quot;;
import { getGreeting, getLatestGreetee, setLatestGreetee } from &quot;./hello.ts&quot;;

interface AppEnv {
	readonly ANT_LISTEN_PORT: string;
	readonly ANT_ALLOWED_ORIGINS: string;
}

/**
 * TODO this is _not_ how things should work. Config throws for missing env vars, and we should not
 * be throwing at import time. For now, to get directory apps working, we do this until we figure
 * out how to ensure type safety for the dynamic imports
 */
const env: AppEnv = config;

const app: App&lt;
	{
		GET: {
			&quot;/health&quot;: EndpointSignature&lt;null, string&gt;;
			&quot;/latest&quot;: EndpointSignature&lt;null, string&gt;;
			&quot;/greet&quot;: EndpointSignature&lt;null, { message: string }&gt;;
		};
		POST: {
			&quot;/greet&quot;: EndpointSignature&lt;{ greetee: string }, null&gt;;
		};
	},
	{
		logger: Logger;
	}
&gt; = new App(
	{
		logger,
	},
	{
		name: &quot;hello-world&quot;,
		port: parseInt(env.ANT_LISTEN_PORT),
		allowedHeaders: [],
		useCSRF: false,
		allowedOrigins: env.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		noLogging: false,
	}
);

app.endpoint({
	method: &quot;GET&quot;,
	route: &quot;/health&quot;,
	async handler() {
		return &quot;OK&quot;;
	},
});

app.endpoint({
	method: &quot;POST&quot;,
	route: &quot;/greet&quot;,
	body: z.object({
		greetee: z.string(),
	}),
	async handler({ body }) {
		await setLatestGreetee(body.greetee);
	},
});

app.endpoint({
	method: &quot;GET&quot;,
	route: &quot;/latest&quot;,
	async handler() {
		return getGreeting(await getLatestGreetee());
	},
});

app.endpoint({
	method: &quot;GET&quot;,
	route: &quot;/greet&quot;,
	query: z.object({
		greetee: z.string(),
	}),
	response: z.object({
		message: z.string(),
	}),
	async handler({ query }) {
		return {
			message: getGreeting(query.greetee),
		};
	},
});

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);
const staticDir = new FSDirectory(join(__dirname, &quot;static&quot;));
const blobs = new BlobStore(logger, staticDir);
// TODO this doesn&apos;t currently work, will look into it later when preparing the hello world for a
// show and tell (if that ever happens) - jkz 2025-04
app.hono.route(
	&quot;/static&quot;,
	createStaticApp({
		blobs,
		prefix: &quot;/hello-world/static/&quot;,
		index: &quot;index.html&quot;,
		env: {
			ANT_NOGGIN_BASE_URL: config.ANT_NOGGIN_BASE_URL + &quot;/hello-world&quot;,
		},
	})
);

export default app;</file><file path="src/hello-world/hello.test.ts">import { expect, test } from &quot;vitest&quot;;
import { getGreeting } from &quot;./hello.ts&quot;;

test(&quot;Create a greeting&quot;, () =&gt; {
	expect(getGreeting(&quot;world&quot;)).toBe(&quot;Hello, world!&quot;);
});</file><file path="src/hello-world/hello.ts">import infra from &quot;@anterior/lib-infra&quot;;
import { KVStore } from &quot;@anterior/lib-platform/kv&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { toSchema } from &quot;@anterior/lib-platform/schema&quot;;
import { typedIdentitySerializer } from &quot;@anterior/lib-platform/serializer&quot;;
import * as z from &quot;@anterior/lib-platform/zod&quot;;

// TODO This is 100% not where this should live;
// It is a contrived way to set up an integration test requirement
const greeteeStore = new KVStore&lt;string&gt;(
	logger,
	infra.s3Bucket,
	typedIdentitySerializer&lt;string&gt;(),
	toSchema(z.string()),
	1000, // get timeout ms
	1000 // set timeout ms
);

export function getGreeting(greetee: string): string {
	return `Hello, ${greetee}!`;
}

export async function getLatestGreetee(): Promise&lt;string&gt; {
	return await greeteeStore.get(&quot;latest-greetee&quot;);
}

export async function setLatestGreetee(greetee: string): Promise&lt;void&gt; {
	await greeteeStore.set(&quot;latest-greetee&quot;, greetee);
}</file><file path="src/notes/handlers/notes.ts">import { HTTPException, type BaseContext } from &quot;@anterior/lib-platform/app&quot;;
import { Blob } from &quot;@anterior/lib-platform/blob&quot;;
import { KeyMissingError } from &quot;@anterior/lib-platform/errors&quot;;
import { generateRandomId } from &quot;@anterior/lib-platform/ids&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import { NoSuchKey } from &quot;@aws-sdk/client-s3&quot;;
import { parseEnterpriseUserFromJwt } from &quot;../../auth/jwt.ts&quot;;
import { buildS3NotePath, stemStorageBlobs } from &quot;../infra.ts&quot;;
import type { Platform } from &quot;../platform.ts&quot;;
import { flonotesFlowName, flonotesFlowResultSchema } from &quot;../schemas/flows.ts&quot;;
import type {
	CreateNoteInput,
	CreateNoteOutput,
	NoteOutput,
	ProcessingStatus,
} from &quot;../schemas/types.ts&quot;;

export function handleStorageError(error: unknown, logger: Logger, context: string): never {
	logger.error({
		msg: `Storage error in ${context}`,
		error: error instanceof Error ? error.message : String(error),
		stack: error instanceof Error ? error.stack : undefined,
	});

	if (error instanceof NoSuchKey || error instanceof KeyMissingError) {
		throw new HTTPException(404, { message: &quot;Requested resource not found&quot; });
	}

	if (error instanceof SyntaxError) {
		throw new HTTPException(400, { message: &quot;Invalid data format&quot; });
	}

	throw new HTTPException(500, {
		message: &quot;Internal server error while processing your request&quot;,
	});
}

interface NoteContext extends BaseContext {
	platform: Platform;
	logger: Logger;
}

/**
 * Handle POST /notes endpoint
 * Creates a new note by submitting user message to the LLM service
 */
export async function createNote(
	ctx: NoteContext,
	body: CreateNoteInput
): Promise&lt;CreateNoteOutput&gt; {
	try {
		const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
		const noteUid = generateRandomId(&quot;nte&quot;);
		const stemUid = body.stemUid;
		const paths = buildS3NotePath(enterpriseUid, stemUid, noteUid);

		await stemStorageBlobs.storeBlob(
			paths.payloadPath,
			new Blob([JSON.stringify(body)], { type: &quot;application/json&quot; }),
			&quot;application/json&quot;
		);

		const flowParams = {
			// Ensure flowName matches exactly what the schema expects
			// the `as const` prevents accidental changes to the workflow name
			flowName: flonotesFlowName,
			s3InputKey: paths.payloadPath,
			s3OutputKey: paths.outputPath,
		};

		await ctx.platform.flows.schedule(flowParams);

		return {
			noteUid: noteUid,
			stemUid: stemUid,
		};
	} catch (error) {
		if (error instanceof HTTPException) {
			throw error;
		}

		throw new HTTPException(500, {
			message: &quot;Internal server error processing note&quot;,
		});
	}
}

/**
 * Handle GET /notes-status/:stemUid/:noteUid endpoint
 * Checks the processing status of a note
 */
export async function getNoteStatus(
	ctx: NoteContext,
	params: { stemUid: string; noteUid: string }
): Promise&lt;{ status: ProcessingStatus; message?: string }&gt; {
	try {
		const { stemUid, noteUid } = params;
		const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
		const paths = buildS3NotePath(enterpriseUid, stemUid, noteUid);

		try {
			await stemStorageBlobs.fetchBlob(paths.outputPath);

			ctx.logger.info({
				msg: &quot;Note output file found - status ready&quot;,
				stemUid,
				noteUid,
				outputPath: paths.outputPath,
			});

			return { status: &quot;ready&quot; };
		} catch (error) {
			// if the file isn&apos;t found, the note is still processing
			// nb: we aren&apos;t doing anything on the workflow level to mark
			// the jobs status as &quot;error&quot;
			if (error instanceof NoSuchKey || error instanceof KeyMissingError) {
				ctx.logger.info({
					msg: &quot;Note output file not found - status processing&quot;,
					stemUid,
					noteUid,
					outputPath: paths.outputPath,
				});

				return { status: &quot;processing&quot; };
			}

			ctx.logger.warn({
				msg: &quot;Error checking note status&quot;,
				stemUid,
				noteUid,
				outputPath: paths.outputPath,
				error: error instanceof Error ? error.message : String(error),
			});

			return {
				status: &quot;error&quot;,
				message:
					error instanceof Error ? error.message : &quot;Unknown error checking note status&quot;,
			};
		}
	} catch (error) {
		ctx.logger.error({
			msg: &quot;Error checking note status&quot;,
			error: error instanceof Error ? error.message : String(error),
			stack: error instanceof Error ? error.stack : undefined,
		});

		if (error instanceof HTTPException) {
			throw error;
		}

		throw new HTTPException(500, {
			message: &quot;Internal server error checking note status&quot;,
		});
	}
}

/**
 * Handle GET /notes/:stemUid/:noteUid endpoint
 * Retrieves a note by its ID after processing is complete
 */
export async function getNote(
	ctx: NoteContext,
	params: { stemUid: string; noteUid: string }
): Promise&lt;NoteOutput&gt; {
	try {
		const { stemUid, noteUid } = params;
		const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
		const paths = buildS3NotePath(enterpriseUid, stemUid, noteUid);

		const outputBlob = await stemStorageBlobs.fetchBlob(paths.outputPath);
		const outputText = await outputBlob.text();

		ctx.logger.info({
			msg: &quot;Successfully retrieved note output file&quot;,
			stemUid,
			noteUid,
			outputPath: paths.outputPath,
			contentLength: outputText.length,
		});

		const parsedOutput = flonotesFlowResultSchema.parse(JSON.parse(outputText));

		return {
			noteUid: noteUid,
			stemUid,
			status: &quot;ready&quot;,
			content: parsedOutput.result,
		};
	} catch (error) {
		ctx.logger.error({
			msg: &quot;Error retrieving note&quot;,
			params,
			error: error instanceof Error ? error.message : String(error),
			stack: error instanceof Error ? error.stack : undefined,
		});

		throw handleStorageError(error, ctx.logger, &quot;getNote&quot;);
	}
}</file><file path="src/notes/schemas/api.ts">import * as z from &quot;@anterior/lib-platform/zod&quot;;
import { clinicalSchema, noteMessageSchema } from &quot;./models.ts&quot;;

/**
 * API-specific schemas for routing and request/response handling
 */

const STEM_ID_REGEX = /^stm_[a-fA-F0-9_]+$/;
const NOTE_ID_REGEX = /^nte_[a-fA-F0-9_]+$/;

export const stemNoteUidSchema = z.object({
	stemUid: z.string().regex(STEM_ID_REGEX, &quot;Invalid stem ID format&quot;),
	noteUid: z.string().regex(NOTE_ID_REGEX, &quot;Invalid note ID format&quot;),
});

export const createNoteRequestSchema = z.object({
	stemUid: z.string().regex(STEM_ID_REGEX, &quot;Invalid stem ID format&quot;),
	clinicals: z.array(clinicalSchema).min(1, &quot;At least one clinical record is required&quot;),
	userMessage: z.string().min(1, &quot;Message cannot be empty&quot;),
	messages: z.array(noteMessageSchema).optional(),
});

export const createNoteResponseSchema = z.object({
	noteUid: z.string().regex(NOTE_ID_REGEX, &quot;Invalid note ID format&quot;),
	stemUid: z.string().regex(STEM_ID_REGEX, &quot;Invalid stem ID format&quot;),
});

export const getNoteStatusResponseSchema = z.object({
	status: z.enum([&quot;processing&quot;, &quot;ready&quot;, &quot;error&quot;]),
	message: z.string().optional(),
});

export const getNoteResponseSchema = z.object({
	noteUid: z.string().regex(NOTE_ID_REGEX, &quot;Invalid note ID format&quot;),
	stemUid: z.string().regex(STEM_ID_REGEX, &quot;Invalid stem ID format&quot;),
	status: z.enum([&quot;processing&quot;, &quot;ready&quot;, &quot;error&quot;]),
	content: z.string().optional(),
	createdAt: z.string().optional(),
	completedAt: z.string().optional(),
});</file><file path="src/notes/schemas/flows.ts">import * as z from &quot;@anterior/lib-platform/zod&quot;;
import { processFlonotesLlmEgress } from &quot;../../@anterior/models/workflows/flonotes/process_flonotes_llm/process_flonotes_llm_egress.ts&quot;;
import { processFlonotesLlmIngress } from &quot;../../@anterior/models/workflows/flonotes/process_flonotes_llm/process_flonotes_llm_ingress.ts&quot;;

export const flonotesFlowName = &quot;process-flonotes-llm/dev&quot; as const;

/**
 * Schema for validating workflow parameters when scheduling the process-flonotes-llm/dev flow
 */
export const flonotesFlowParamsSchema = processFlonotesLlmIngress.merge(
	z.object({
		flowName: z.literal(flonotesFlowName),
	})
);

export type FlonotesFlowParams = z.infer&lt;typeof flonotesFlowParamsSchema&gt;;

/**
 * Schema for validating workflow outputs from the process-flonotes-llm/dev flow
 */
export const flonotesFlowResultSchema = processFlonotesLlmEgress;
export type FlonotesFlowResult = z.infer&lt;typeof flonotesFlowResultSchema&gt;;</file><file path="src/notes/schemas/models.ts">import { z } from &quot;@anterior/lib-platform/zod&quot;;

/**
 * Core data models used across the application
 */

export const clinicalSchema = z.object({
	id: z.string(),
	extracts: z.array(z.any()),
});

export const documentSchema = z.object({
	url: z.string(),
	pdf_id: z.string(),
});

export const noteMessageSchema = z.object({
	role: z.enum([&quot;user&quot;, &quot;assistant&quot;]),
	content: z.string(),
});</file><file path="src/notes/schemas/types.ts">// TODO: as we revisit PDF processing, include proper schemas
// for the (a) PDF extracts, (b) the entire LLM response, and (c) any
// schemas we expect the LLM to generate

import type { z } from &quot;@anterior/lib-platform/zod&quot;;
import { createNoteRequestSchema, createNoteResponseSchema, getNoteResponseSchema } from &quot;./api.ts&quot;;
import { clinicalSchema, documentSchema, noteMessageSchema } from &quot;./models.ts&quot;;

export type ProcessingStatus = &quot;processing&quot; | &quot;ready&quot; | &quot;error&quot;;

export type Clinical = z.infer&lt;typeof clinicalSchema&gt;;
export type Document = z.infer&lt;typeof documentSchema&gt;;
export type NoteMessage = z.infer&lt;typeof noteMessageSchema&gt;;
export type CreateNoteInput = z.infer&lt;typeof createNoteRequestSchema&gt;;
export type CreateNoteOutput = z.infer&lt;typeof createNoteResponseSchema&gt;;
export type NoteOutput = z.infer&lt;typeof getNoteResponseSchema&gt;;</file><file path="src/notes/app.ts">import { App, type AppOptions, type EndpointSignature } from &quot;@anterior/lib-platform/app&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type { Platform } from &quot;./platform.ts&quot;;
import type { CreateNoteInput, CreateNoteOutput } from &quot;./schemas/types.ts&quot;;

import {
	createNoteRequestSchema,
	createNoteResponseSchema,
	getNoteResponseSchema,
	getNoteStatusResponseSchema,
	stemNoteUidSchema,
} from &quot;./schemas/api.ts&quot;;

import { createNote, getNote, getNoteStatus } from &quot;./handlers/notes.ts&quot;;

export interface LLMAppContext {
	logger: Logger;
	platform: Platform;
}

export interface LLMAppService {
	GET: {
		&quot;/notes/:stemUid/:noteUid&quot;: EndpointSignature&lt;
			{ stemUid: string; noteUid: string },
			Response
		&gt;;
		&quot;/notes-status/:stemUid/:noteUid&quot;: EndpointSignature&lt;
			{ stemUid: string; noteUid: string },
			Response
		&gt;;
	};
	POST: {
		&quot;/notes&quot;: EndpointSignature&lt;CreateNoteInput, CreateNoteOutput&gt;;
	};
}

export type LLMApp = App&lt;LLMAppService, LLMAppContext&gt;;

export function createNotesApp(ctx: LLMAppContext, options: AppOptions): LLMApp {
	ctx.logger.info({
		msg: &quot;Creating LLM App&quot;,
		contextKeys: Object.keys(ctx),
	});

	const app: LLMApp = new App(ctx, options);

	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/notes&quot;,
		body: createNoteRequestSchema,
		response: createNoteResponseSchema,
		async handler(ctx) {
			return createNote(ctx, ctx.body);
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/notes-status/:stemUid/:noteUid&quot;,
		params: stemNoteUidSchema,
		response: getNoteStatusResponseSchema,
		async handler(ctx) {
			return getNoteStatus(ctx, ctx.params);
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/notes/:stemUid/:noteUid&quot;,
		params: stemNoteUidSchema,
		response: getNoteResponseSchema,
		async handler(ctx) {
			return getNote(ctx, ctx.params);
		},
	});

	return app;
}</file><file path="src/notes/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import config from &quot;@anterior/lib-infra/config&quot;;
import { S3Bucket } from &quot;@anterior/lib-infra/s3&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

const BUCKET_NAME = config.ANT_S3_BUCKET;
const stemStorageS3 = new S3Bucket(BUCKET_NAME);
export const stemStorageBlobs = new BlobStore(logger, stemStorageS3);

// we can generalize this for llm outputs beyond notes
export function buildS3NotePath(
	enterpriseUid: string,
	stemUid: string,
	noteUid: string
): {
	payloadPath: string;
	outputPath: string;
} {
	const basePath = `stems/${enterpriseUid}/${stemUid}/${noteUid}`;

	return {
		payloadPath: `${basePath}/payload.json`,
		outputPath: `${basePath}/output.json`,
	};
}

/**
 * Platform configuration for the client-agnostic, generic LLM application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for note payloads and metadata
 * - Flows: Workflows for various LLM-based tasks (e.g. process-flonotes-llm/dev)
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
	flowsQueue: infra_.workflowQueue,
};

export const platform = Platform.fromInfra(infra);</file><file path="src/notes/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import secrets from &quot;@anterior/lib-infra/secrets&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createNotesApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createNotesApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;llms&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		useCSRF: true,
		noLogging: false,
		jwtConfig: {
			jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
			tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
		},
	}
);</file><file path="src/notes/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import { type FlowEnvelope, FlowScheduler, toFlowSchema } from &quot;@anterior/lib-platform/flows&quot;;
import {
	InMemoryBlobStoreBackend,
	InMemoryMessageQueueBackend,
} from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;
import { type MessageQueueBackend, MessageQueueImpl } from &quot;@anterior/lib-platform/queue&quot;;
import { jsonSerializer } from &quot;@anterior/lib-platform/serializer&quot;;

// import all types of workflows here
import { type FlonotesFlowParams, flonotesFlowParamsSchema } from &quot;./schemas/flows.ts&quot;;

/**
 * Infrastructure interface defining the external resources needed by the platform.
 *
 * The platform can be deployed in different environments:
 * - Production: Resources managed by CDK/Terraform
 * - Docker/CI: Resources managed by Docker Compose
 * - Local/Testing: Resources represented by in-memory implementations
 *
 * This abstraction allows for environment-specific implementations
 * while maintaining a consistent interface for the application.
 */
export interface Infra {
	logger: Logger;
	flowsQueue: MessageQueueBackend&lt;string&gt;;
	blobs: BlobStoreBackend;
}

/**
 * Creates an in-memory implementation of the infrastructure.
 *
 * This is not meant for production use! The in-memory implementation
 * is useful for:
 * - Unit testing: Fast, isolated test environment
 * - Local development: Quick setup without external dependencies
 *
 * Note that all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		flowsQueue: new InMemoryMessageQueueBackend(),
		blobs: new InMemoryBlobStoreBackend(),
	};
}

/**
 * Application layer interface to the resource backends.
 *
 * This interface provides type-safe access to:
 * - Blob storage: For storing note payloads and metadata
 * - Flow scheduling: For managing LLM-based note generation
 *
 * The interface ensures that all platform operations are properly
 * typed and validated against their schemas.
 */
export interface Stores {
	readonly blobs: BlobStore;
	readonly flows: FlowScheduler&lt;FlonotesFlowParams&gt;;
}

/**
 * Core platform implementation for the Flonotes application.
 *
 * The platform provides:
 * 1. Factory methods for creating platform instances
 * 2. Type-safe access to storage and flow services
 * 3. Environment-specific resource management
 */
export class Platform {
	/**
	 * Creates an in-memory platform instance for testing.
	 *
	 * This method allows for partial infrastructure overrides,
	 * making it easy to mock specific components while using
	 * real implementations for others.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	/**
	 * Creates a platform instance from infrastructure configuration.
	 *
	 * This method:
	 * 1. Sets up the message queue for flow scheduling
	 * 2. Initializes blob storage for note data
	 * 3. Configures proper serialization and validation
	 */
	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			flows: new FlowScheduler&lt;FlonotesFlowParams&gt;(
				infra.logger,
				new MessageQueueImpl&lt;FlowEnvelope&lt;FlonotesFlowParams&gt;, string&gt;(
					logger,
					infra.flowsQueue,
					jsonSerializer,
					toFlowSchema(flonotesFlowParamsSchema)
				),
				flonotesFlowParamsSchema
			),
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	/**
	 * Type-safe access to the flow scheduling service.
	 * Used for managing LLM-based note generation tasks.
	 */
	get flows(): FlowScheduler&lt;FlonotesFlowParams&gt; {
		return this.stores.flows;
	}

	/**
	 * Type-safe access to the blob storage service.
	 * Used for storing and retrieving note data.
	 */
	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/pdfs/models/flows.ts">import * as z from &quot;@anterior/lib-platform/zod&quot;;
import { processPdfStandaloneEgress } from &quot;../../@anterior/models/workflows/pdfs/process_pdf_standalone/process_pdf_standalone_egress.ts&quot;;
import { processPdfStandaloneIngress } from &quot;../../@anterior/models/workflows/pdfs/process_pdf_standalone/process_pdf_standalone_ingress.ts&quot;;

export const processPdfStandaloneFlowName = &quot;process-pdf-standalone/dev&quot; as const;
/**
 * Schema for validating workflow parameters when scheduling the process-pdf-standalone/dev flow
 */
export const pdfFlowParamsSchema = processPdfStandaloneIngress.merge(
	z.object({
		flowName: z.literal(processPdfStandaloneFlowName),
	})
);

export type PdfFlowParams = z.infer&lt;typeof pdfFlowParamsSchema&gt;;

/**
 * Schema for validating workflow outputs from the process-pdf-standalone/dev flow
 */
export const pdfFlowResultSchema = processPdfStandaloneEgress;
export type PdfFlowResult = z.infer&lt;typeof pdfFlowResultSchema&gt;;</file><file path="src/pdfs/models/schemas.ts">import { z } from &quot;@anterior/lib-platform/zod&quot;;

export const createIdValidator = (prefix: string, errorMessage?: string) =&gt;
	z
		.string()
		.regex(
			new RegExp(`^${prefix}_[a-fA-F0-9_]+$`),
			errorMessage || `Invalid ${prefix} ID format`
		);

export const stemPdfProcessingStatusSchema = z.enum([&quot;processing&quot;, &quot;ready&quot;, &quot;error&quot;]);

export const stemPdfUidSchema = z.object({
	stemUid: createIdValidator(&quot;stm&quot;),
	pdfUid: createIdValidator(&quot;pdf&quot;),
});

export const processStemPdfSchema = z.object({
	stemUid: createIdValidator(&quot;stm&quot;).optional(),
	file: z.instanceof(Blob).refine((blob) =&gt; blob.type === &quot;application/pdf&quot;, {
		message: &quot;File must be a PDF document&quot;,
	}),
});

export const stemPdfUidResponseSchema = z.object({
	stemUid: z.string(),
	pdfUid: z.string(),
});

export const stemPdfStatusResponseSchema = z.object({
	status: stemPdfProcessingStatusSchema,
	message: z.string().optional(),
});</file><file path="src/pdfs/models/types.ts">import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import type { z } from &quot;@anterior/lib-platform/zod&quot;;
import type { Platform } from &quot;../platform.ts&quot;;
import { stemPdfStatusResponseSchema, stemPdfUidResponseSchema } from &quot;./schemas.ts&quot;;

export interface PdfAppContext {
	logger: Logger;
	platform: Platform;
}

export type StemPdfStatusResponse = z.infer&lt;typeof stemPdfStatusResponseSchema&gt;;
export type StemPdfUidResponse = z.infer&lt;typeof stemPdfUidResponseSchema&gt;;</file><file path="src/pdfs/app.ts">import { NoSuchKey } from &quot;@aws-sdk/client-s3&quot;;

import {
	App,
	getHonoContext,
	HTTPException,
	type AppOptions,
	type EndpointSignature,
} from &quot;@anterior/lib-platform/app&quot;;
import { KeyMissingError } from &quot;@anterior/lib-platform/errors&quot;;
import { generateRandomId } from &quot;@anterior/lib-platform/ids&quot;;
import * as z from &quot;@anterior/lib-platform/zod&quot;;

import { parseEnterpriseUserFromJwt } from &quot;../auth/jwt.ts&quot;;

import {
	processStemPdfSchema,
	stemPdfStatusResponseSchema,
	stemPdfUidResponseSchema,
	stemPdfUidSchema,
} from &quot;./models/schemas.ts&quot;;
import type { PdfAppContext, StemPdfStatusResponse, StemPdfUidResponse } from &quot;./models/types.ts&quot;;

import {
	processPdfStandaloneFlowName,
	type PdfFlowParams,
	type PdfFlowResult,
} from &quot;./models/flows.ts&quot;;

export type PdfUidParams = z.infer&lt;typeof stemPdfUidSchema&gt;;
export type PdfUploadBody = z.infer&lt;typeof processStemPdfSchema&gt;;
export type PdfApp = App&lt;PdfAppService, PdfAppContext&gt;;

export interface PdfAppService {
	GET: {
		&quot;/pdf-extracts/:stemUid/:pdfUid&quot;: EndpointSignature&lt;PdfUidParams, PdfFlowResult&gt;;
		&quot;/pdf-status/:stemUid/:pdfUid&quot;: EndpointSignature&lt;PdfUidParams, StemPdfStatusResponse&gt;;
	};
	POST: {
		&quot;/process-pdf&quot;: EndpointSignature&lt;PdfUploadBody, StemPdfUidResponse&gt;;
	};
}

interface BuildS3PdfPathArgs {
	enterpriseUid: string;
	stemUid: string;
	pdfUid: string;
	filename: string;
}

function buildS3PdfPath({ enterpriseUid, stemUid, pdfUid, filename }: BuildS3PdfPathArgs): string {
	const basePath = `stems/${enterpriseUid}/${stemUid}/${pdfUid}`;
	return `${basePath}/${filename}`;
}

/**
 * Handles S3-related errors with appropriate HTTP responses
 * Maps common S3 errors to user-friendly messages
 */
function handleS3Error(error: unknown): never {
	if (error instanceof NoSuchKey || error instanceof KeyMissingError) {
		throw new HTTPException(404, { message: &quot;PDF is still being processed or not found&quot; });
	}

	if (error instanceof SyntaxError) {
		throw new HTTPException(400, { message: &quot;Invalid JSON format in extracts&quot; });
	}

	throw new HTTPException(500, {
		message: &quot;Error retrieving PDF data. The processing service may be unavailable.&quot;,
	});
}

export function createPdfApp(ctx: PdfAppContext, options: AppOptions): PdfApp {
	const app: PdfApp = new App(ctx, options);

	app.endpoint({
		method: &quot;POST&quot;,
		body: processStemPdfSchema,
		response: stemPdfUidResponseSchema,
		route: &quot;/process-pdf&quot;,
		async handler(ctx) {
			const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);

			const pdfUid = generateRandomId(&quot;pdf&quot;);
			const blob = ctx.body.file;

			/**
			 * Use provided stemUid or generate a new one if missing
			 *
			 * A stem is used to combine multiple PDFs into a single
			 * element for processing and inference.
			 */
			const stemUid = ctx.body.stemUid || generateRandomId(&quot;stm&quot;);

			const s3InputKey = buildS3PdfPath({
				enterpriseUid,
				stemUid,
				pdfUid,
				filename: &quot;raw.pdf&quot;,
			});
			const s3OutputKey = buildS3PdfPath({
				enterpriseUid,
				stemUid,
				pdfUid,
				filename: &quot;extracts.json&quot;,
			});

			try {
				await ctx.platform.blobs.storeBlob(s3InputKey, blob, &quot;application/pdf&quot;);

				const flowParams: PdfFlowParams = {
					flowName: processPdfStandaloneFlowName,
					s3InputKey,
					s3OutputKey,
				};

				await ctx.platform.flows.schedule(flowParams);

				return { stemUid, pdfUid };
			} catch (error) {
				ctx.logger.error({
					msg: &quot;PDF processing failed&quot;,
					stemUid,
					pdfUid,
					error:
						error instanceof Error
							? {
									message: error.message,
									stack: error.stack,
									name: error.name,
								}
							: String(error),
				});

				throw new HTTPException(500, {
					message: &quot;Failed to process PDF. Please try again later.&quot;,
				});
			}
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/pdf-extracts/:stemUid/:pdfUid&quot;,
		params: stemPdfUidSchema,
		async handler(ctx) {
			const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
			const { stemUid, pdfUid } = ctx.params;
			const s3Key = buildS3PdfPath({
				enterpriseUid,
				stemUid,
				pdfUid,
				filename: &quot;extracts.json&quot;,
			});

			try {
				const blob = await ctx.platform.blobs.fetchBlob(s3Key);
				const text = await blob.text();

				try {
					const result = JSON.parse(text);
					const flowResult: PdfFlowResult = { result };
					return getHonoContext(ctx).json(flowResult);
				} catch (error) {
					throw new HTTPException(500, { message: &quot;Invalid JSON format in extracts&quot; });
				}
			} catch (error) {
				handleS3Error(error);
			}
		},
	});

	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/pdf-status/:stemUid/:pdfUid&quot;,
		params: stemPdfUidSchema,
		response: stemPdfStatusResponseSchema,
		async handler(ctx) {
			const { enterpriseUid } = parseEnterpriseUserFromJwt(ctx);
			const { stemUid, pdfUid } = ctx.params;
			const s3Key = buildS3PdfPath({
				enterpriseUid,
				stemUid,
				pdfUid,
				filename: &quot;extracts.json&quot;,
			});

			try {
				await ctx.platform.blobs.fetchBlob(s3Key);
				return { status: &quot;ready&quot; as const };
			} catch (error) {
				if (error instanceof NoSuchKey || error instanceof KeyMissingError) {
					return { status: &quot;processing&quot; as const };
				}

				ctx.logger.error({
					msg: &quot;Error checking PDF status&quot;,
					stemUid,
					pdfUid,
					error,
				});

				return {
					status: &quot;error&quot; as const,
					message:
						error instanceof Error
							? error.message
							: &quot;Unknown error checking PDF status&quot;,
				};
			}
		},
	});

	return app;
}</file><file path="src/pdfs/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

export const infra = {
	logger,
	blobs: infra_.s3Bucket,
	flowsQueue: infra_.workflowQueue,
};

export const platform = Platform.fromInfra(infra);</file><file path="src/pdfs/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import secrets from &quot;@anterior/lib-infra/secrets&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createPdfApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createPdfApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;pdf&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		// TODO: enable CSRF ASAP, when client is ready
		// the frontend apps will need this implemented
		useCSRF: false,
		noLogging: false,
		jwtConfig: {
			jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
			tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
		},
	}
);</file><file path="src/pdfs/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import { type FlowEnvelope, FlowScheduler, toFlowSchema } from &quot;@anterior/lib-platform/flows&quot;;
import {
	InMemoryBlobStoreBackend,
	InMemoryMessageQueueBackend,
} from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;
import { type MessageQueueBackend, MessageQueueImpl } from &quot;@anterior/lib-platform/queue&quot;;
import { jsonSerializer } from &quot;@anterior/lib-platform/serializer&quot;;
import { type PdfFlowParams, pdfFlowParamsSchema } from &quot;./models/flows.ts&quot;;

/**
 * The backends for external resources that the platform needs to interact with.
 * In prod, these are managed by cdkrf/terraform
 * In docker/CI, these are managed by docker compose services
 * In local dev and unit tests, these can be represented by in-memory implementations
 */
export interface Infra {
	logger: Logger;
	flowsQueue: MessageQueueBackend&lt;string&gt;;
	blobs: BlobStoreBackend;
}

/**
 * This is not meant for production use!
 *
 * The in-memory infra is useful for unit testing and local development,
 * but all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		flowsQueue: new InMemoryMessageQueueBackend(),
		blobs: new InMemoryBlobStoreBackend(),
	};
}

/**
 * The application layer interface to the resource backends.
 */
export interface Stores {
	readonly blobs: BlobStore;
	readonly flows: FlowScheduler&lt;PdfFlowParams&gt;;
}

// NB: this derives from src/mnr/service/platform.ts
// taking only what it needs
export class Platform {
	/**
	 * This returns a fledged in-memory implementation of the platform.
	 * Useful for testing.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}
	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			flows: new FlowScheduler(
				infra.logger,
				new MessageQueueImpl&lt;FlowEnvelope&lt;PdfFlowParams&gt;, string&gt;(
					logger,
					infra.flowsQueue,
					jsonSerializer,
					toFlowSchema(pdfFlowParamsSchema)
				),
				pdfFlowParamsSchema
			),
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	get flows(): FlowScheduler&lt;PdfFlowParams&gt; {
		return this.stores.flows;
	}

	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/stems/handlers/attach-clinicals.ts">export const attachClinicalsToStem = async (stemId: string, artQueries: string[]) =&gt; {
	//TBD - @asim
	//attach this event to the stem in the event store
};</file><file path="src/stems/handlers/attach-criteria.ts">export const attachCriteriaToStem = async (
	stemId: string,
	codes: Array&lt;{ type: string; value: string }&gt;
) =&gt; {
	//TBD - @asim
	//read the workspace from the stem ID
	//push the codes into the &quot;atlas&quot; and get back atlas queries
	//attach atlas queries to this stem
};</file><file path="src/stems/handlers/attach-services.ts">export interface ServiceRequest {
	code: {
		type: &quot;cpt&quot; | &quot;icd10&quot; | &quot;hcpcs&quot; | &quot;protocol&quot;;
		value: string;
	};
	description: string;
}

export const attachServicesToStem = async (stemId: string, services: Array&lt;ServiceRequest&gt;) =&gt; {
	//TBD - @asim
	//convert these services requested into Fhir ServiceRequest objects
	//attach them to the stem
};</file><file path="src/stems/handlers/create-stem.ts">import { EventStream, type EventStore } from &quot;@anterior/lib-platform/events&quot;;
import { generateRandomId } from &quot;@anterior/lib-platform/ids&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import type { StemMeta } from &quot;../schemas/common.ts&quot;;
import type { EventCreateStem, EventCreateStemPayload } from &quot;../schemas/event-create-stem.ts&quot;;

export const createStem = async (
	workspaceUid: string,
	enterpriseUid: string,
	userUid: string,
	eventStore: EventStore&lt;EventCreateStem&gt;
) =&gt; {
	const stemId = generateRandomId(&quot;stm&quot;);

	const meta: StemMeta = {
		enterprise_uid: enterpriseUid,
		workspace_uid: workspaceUid,
		user_uid: userUid,
		case_uid: stemId,
		stem_id: stemId,
		meta_type: &quot;stem_meta_v1_1&quot;,
	};

	const payload: EventCreateStemPayload = {
		type: &quot;event_create_stem_v1_1&quot;,
	};

	const stream = new EventStream(logger, &quot;write-only&quot;, eventStore, stemId, meta);
	await stream.save(payload);
	return stemId;
};</file><file path="src/stems/handlers/rollup.ts">export const rollupEventsForStem = async (stemId: string, filter: string) =&gt; {
	//TBD - @asim
	//read the event store for this stemId
	//if a filter is not provided, do a complete rollup
	//if a filter is provided, simply return all the events matching that filter
	//sort the events by oldest first

	return [
		{ type: filter, createdAt: new Date(), data: `testing a log message` },
		{ type: filter, createdAt: new Date(), data: `testing anoter log message` },
	];
};</file><file path="src/stems/schemas/common.ts">import { idSchema } from &quot;@anterior/lib-platform/ids&quot;;
import { z } from &quot;zod&quot;;

export interface StemMeta {
	enterprise_uid: string;
	workspace_uid: string;
	user_uid: string;
	case_uid: string;
	meta_type: string;
	stem_id: string;
}

//TODO: @asim, move these to a common place later on
const userIdSchema = idSchema(&quot;usr&quot;);
const entIdSchema = idSchema(&quot;ent&quot;);
const wksIdSchema = idSchema(&quot;wks&quot;);
const stmIdSchema = idSchema(&quot;stm&quot;);

export const stemMetaSchema = z.object({
	enterprise_uid: entIdSchema,
	workspace_uid: wksIdSchema,
	user_uid: userIdSchema,
	case_uid: stmIdSchema,
	stem_id: stmIdSchema,
	meta_type: z.literal(&quot;stem_meta_v1_1&quot;),
});</file><file path="src/stems/schemas/event-create-stem.ts">import { toEventPayloadSchema, toEventSchema, type Event } from &quot;@anterior/lib-platform/events&quot;;
import { typedIdentitySerializer } from &quot;@anterior/lib-platform/serializer&quot;;
import { z } from &quot;zod&quot;;
import { stemMetaSchema, type StemMeta } from &quot;./common.ts&quot;;

export interface EventCreateStemPayload {
	type: &quot;event_create_stem_v1_1&quot;;
}

export type EventCreateStem = Event&lt;EventCreateStemPayload, StemMeta&gt;;

const pSchema = toEventPayloadSchema&lt;EventCreateStemPayload&gt;({
	event_create_stem_v1_1: z.object({
		type: z.literal(&quot;event_create_stem_v1_1&quot;),
	}),
});

export const eventCreateStemSchema = toEventSchema&lt;
	EventCreateStemPayload,
	StemMeta,
	EventCreateStem
&gt;(pSchema, stemMetaSchema);

export const serializer = typedIdentitySerializer&lt;EventCreateStem&gt;();</file><file path="src/stems/app.ts">import { App, type AppOptions, type EndpointSignature } from &quot;@anterior/lib-platform/app&quot;;
import { type Logger } from &quot;@anterior/lib-platform/log&quot;;
import { z } from &quot;zod&quot;;
import type { Platform } from &quot;./platform.ts&quot;;

import { parseEnterpriseUserFromJwt } from &quot;../auth/jwt.ts&quot;;
import { attachClinicalsToStem } from &quot;./handlers/attach-clinicals.ts&quot;;
import { attachCriteriaToStem } from &quot;./handlers/attach-criteria.ts&quot;;
import { attachServicesToStem } from &quot;./handlers/attach-services.ts&quot;;
import { createStem } from &quot;./handlers/create-stem.ts&quot;;
import { rollupEventsForStem } from &quot;./handlers/rollup.ts&quot;;
export interface StemsAppContext {
	logger: Logger;
	platform: Platform;
}

export interface StemsAppService {
	GET: {
		&quot;/:stemId&quot;: EndpointSignature&lt;
			{},
			Array&lt;{
				message: string;
				event_type: string;
			}&gt;
		&gt;;
	};

	POST: {
		&quot;/&quot;: EndpointSignature&lt;{}, { stemId: string }&gt;;
		&quot;/:stemId/clinicals&quot;: EndpointSignature&lt;{}, { success: boolean }&gt;;
		&quot;/:stemId/criteria&quot;: EndpointSignature&lt;{}, { success: boolean }&gt;;
		&quot;/:stemId/services&quot;: EndpointSignature&lt;{}, { success: boolean }&gt;;
	};
}

export type StemsApp = App&lt;StemsAppService, StemsAppContext&gt;;

export function createStemsApp(baseCtx: StemsAppContext, options: AppOptions): StemsApp {
	const app: StemsApp = new App(baseCtx, options);

	/*******************************************************************************************
	 * Rollup a stem
	 ********************************************************************************************/
	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/:stemId&quot;,
		response: z.array(
			z.object({
				type: z.string(),
				createdAt: z.date(),
				data: z.string().optional(),
			})
		),
		handler: async (ctx) =&gt; {
			const filter = (ctx.query as { filter: string }).filter || &quot;&quot;;
			const stemId = ctx.params.stemId;
			return await rollupEventsForStem(stemId, filter);
		},
	});

	/*******************************************************************************************
	 *	Create a new stem
	 ********************************************************************************************/
	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/&quot;,
		response: z.object({
			stemId: z.string(),
		}),
		handler: async (ctx) =&gt; {
			const { userUid, enterpriseUid, workspaceUid } = await parseEnterpriseUserFromJwt(ctx);
			return {
				stemId: await createStem(
					workspaceUid,
					enterpriseUid,
					userUid,
					ctx.platform.stores.events
				),
			};
		},
	});

	/********************************************************************************************
	 * attach clinicals (art queries) to the specified stem
	 ********************************************************************************************/
	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/:stemId/clinicals&quot;,
		body: z.object({
			artQueries: z.array(z.string()),
		}),
		handler: async ({ params: { stemId }, body }) =&gt; {
			await attachClinicalsToStem(stemId, body.artQueries);
		},
	});

	/*********************************************************************************************
	 * attach codes (criteria) to a given stem
	 ********************************************************************************************/
	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/:stemId/criteria&quot;,
		body: z.object({
			codes: z.array(
				z.object({
					type: z.string(),
					value: z.string(),
				})
			),
		}),
		handler: async ({ params: { stemId }, body }) =&gt; {
			await attachCriteriaToStem(stemId, body.codes);
		},
	});

	/*********************************************************************************************
	 * attach service requests to a specified stem
	 ********************************************************************************************/
	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/:stemId/services&quot;,
		body: z.object({
			services: z.array(
				z.object({
					code: z.object({
						type: z.enum([&quot;cpt&quot;, &quot;icd10&quot;, &quot;hcpcs&quot;, &quot;protocol&quot;]),
						value: z.string(),
					}),
					description: z.string(),
				})
			),
		}),
		handler: async ({ params: { stemId }, body }) =&gt; {
			await attachServicesToStem(stemId, body.services);
		},
	});

	return app;
}</file><file path="src/stems/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

/**
 * Platform configuration for the stems application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for note payloads and metadata
 * - Flows: LLM-based note generation pipeline
 *
 * Each service is configured with specific settings for the stems
 * use case, ensuring proper isolation and resource management.
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
	flowsQueue: infra_.workflowQueue,
	events: infra_.dynamoEventsTable,
};

export const platform = Platform.fromInfra(infra);</file><file path="src/stems/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createStemsApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createStemsApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;stems&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		// TODO: enable CSRF ASAP, when client is ready
		// the frontend apps will need this implemented
		useCSRF: false,
		noLogging: false,
		// jwtConfig: {
		// 	jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
		// 	tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
		// },
	}
);</file><file path="src/stems/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import type { EventStoreBackend } from &quot;@anterior/lib-platform/events&quot;;
import { EventStore } from &quot;@anterior/lib-platform/events&quot;;
import {
	InMemoryBlobStoreBackend,
	InMemoryEventStoreBackend,
} from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;
import {
	type EventCreateStem,
	eventCreateStemSchema,
	serializer,
} from &quot;./schemas/event-create-stem.ts&quot;;

/**
 * Infrastructure interface defining the external resources needed by the platform.
 *
 * The platform can be deployed in different environments:
 * - Production: Resources managed by CDK/Terraform
 * - Docker/CI: Resources managed by Docker Compose
 * - Local/Testing: Resources represented by in-memory implementations
 *
 * This abstraction allows for environment-specific implementations
 * while maintaining a consistent interface for the application.
 */
export interface Infra {
	logger: Logger;
	blobs: BlobStoreBackend;
	events: EventStoreBackend;
}

/**
 * Creates an in-memory implementation of the infrastructure.
 *
 * This is not meant for production use! The in-memory implementation
 * is useful for:
 * - Unit testing: Fast, isolated test environment
 * - Local development: Quick setup without external dependencies
 *
 * Note that all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		blobs: new InMemoryBlobStoreBackend(),
		events: new InMemoryEventStoreBackend(),
	};
}

/**
 * Application layer interface to the resource backends.
 *
 * This interface provides type-safe access to:
 * - Blob storage: For storing note payloads and metadata
 *
 * The interface ensures that all platform operations are properly
 * typed and validated against their schemas.
 */
export interface Stores {
	readonly blobs: BlobStore;
	events: EventStore&lt;EventCreateStem&gt;;
}

/**
 * Core platform implementation for the stems application.
 *
 * The platform provides:
 * 1. Factory methods for creating platform instances
 * 2. Type-safe access to storage and flow services
 * 3. Environment-specific resource management
 */
export class Platform {
	/**
	 * Creates an in-memory platform instance for testing.
	 *
	 * This method allows for partial infrastructure overrides,
	 * making it easy to mock specific components while using
	 * real implementations for others.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	/**
	 * Creates a platform instance from infrastructure configuration.
	 *
	 * This method:
	 * 1. Sets up the message queue for flow scheduling
	 * 2. Initializes blob storage for note data
	 * 3. Configures proper serialization and validation
	 */
	public static fromInfra(infra: Infra): Platform {
		// wrap this into a read/write stream
		const stores: Stores = {
			blobs: new BlobStore(infra.logger, infra.blobs),
			events: new EventStore&lt;EventCreateStem&gt;(
				infra.logger,
				infra.events,
				serializer,
				eventCreateStemSchema
			),
		};
		return new Platform(stores);
	}

	constructor(public readonly stores: Stores) {}

	/**
	 * Type-safe access to the blob storage service.
	 * Used for storing and retrieving note data.
	 */
	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/tasks/handlers/run-mnr-raw.ts">import { randomUUID } from &quot;crypto&quot;;
import type { ServiceRequest } from &quot;../../stems/handlers/attach-services&quot;;
import type { Platform } from &quot;../platform.ts&quot;;

// TODO: stub
export const runMnrRaw = async (
	platform: Platform,
	artQueries: {
		member: string;
		criteria: string;
		clinicals: string[];
	},
	serviceRequest: ServiceRequest
) =&gt; {
	const stemUid = randomUUID().toString();
	const traceId = randomUUID().toString();
	await platform.tasks.schedule({
		task_name: &quot;mnr_workflo&quot;,
		config: {
			workflo_meta: {
				name: &quot;mnr_workflo&quot;,
				idempotency_key: &quot;mnr_workflo&quot;,
				scheduled_at: new Date().toISOString(),
			},
			source_meta: {
				stem_uid: stemUid,
				user_uid: &quot;test_user_uid&quot;,
				enterprise_uid: &quot;test_enterprise_uid&quot;,
				workspace_uid: &quot;test_workspace_uid&quot;,
				workstream_id: &quot;test_workstream_id&quot;,
				product_id: &quot;test_product_id&quot;,
				trace_id: traceId,
			},
			workspace_config: {
				trap_door_codes: [&quot;10927&quot;],
			},
		},
		request: {
			type: &quot;HHRequest&quot;,
			protocol_code: serviceRequest.code.value,
		},
		member_query: artQueries.member,
		clinicals_query: artQueries.clinicals,
	});
	// TODO: poll for result from events, here or elsewhere
	// TODO: return something sensible
	return stemUid;
};</file><file path="src/tasks/handlers/run-mnr-stem.ts">export const runMnrOnStemId = async (stemId: string) =&gt; {
	//TBD - @asim
	//extract these fields from the stem:
	// - vault queries -&gt; clinicals
	// - atlas queries -&gt; criteria as [GDT]
	// - requested services -&gt; FHIR service
	// then invoke a brrr task with all this stuff
};</file><file path="src/tasks/handlers/validate-clinicals-stem.ts">export async function validateClinicalsOnStemId(stemId: string) {
	// TODO - @shunueda
}</file><file path="src/tasks/app.ts">import {
	App,
	createStaticApp,
	type AppOptions,
	type EndpointSignature,
} from &quot;@anterior/lib-platform/app&quot;;
import type { Logger } from &quot;@anterior/lib-platform/log&quot;;
import z from &quot;zod&quot;;
import { runMnrRaw } from &quot;./handlers/run-mnr-raw.ts&quot;;
import { runMnrOnStemId } from &quot;./handlers/run-mnr-stem.ts&quot;;
import { validateClinicalsOnStemId } from &quot;./handlers/validate-clinicals-stem.ts&quot;;
import { tasksBlob } from &quot;./infra.ts&quot;;
import type { Platform } from &quot;./platform.ts&quot;;

export interface TasksAppContext {
	logger: Logger;
	platform: Platform;
}

export interface TasksAppService {
	GET: {
		&quot;/&quot;: EndpointSignature&lt;{}, object&gt;;
	};

	POST: {
		&quot;/mnr/actions/run&quot;: EndpointSignature&lt;{}, void&gt;;
		&quot;/mnr/actions/run/:stemId&quot;: EndpointSignature&lt;{}, void&gt;;
		&quot;/validate-clinicals/:stemId&quot;: EndpointSignature&lt;{}, void&gt;;
	};
}

export type TasksApp = App&lt;TasksAppService, TasksAppContext&gt;;

export function createTasksApp(baseCtx: TasksAppContext, options: AppOptions): TasksApp {
	const app: TasksApp = new App(baseCtx, options);

	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/mnr/actions/run&quot;,
		body: z.object({
			artQueries: z.object({
				member: z.string(),
				criteria: z.string(),
				clinicals: z.array(z.string()),
			}),
			serviceRequest: z.object({
				code: z.object({
					type: z.enum([&quot;cpt&quot;, &quot;icd10&quot;, &quot;hcpcs&quot;, &quot;protocol&quot;]),
					value: z.string(),
				}),
				description: z.string(),
			}),
		}),
		handler: async ({ body, platform }) =&gt; {
			const { artQueries, serviceRequest } = body;
			const stemUid = await runMnrRaw(platform, artQueries, serviceRequest);
			return new Response(`MNR started for stem ${stemUid}`, {
				status: 200,
			});
		},
	});

	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/mnr/actions/run/:stemId&quot;,
		body: z.object({}),
		handler: async ({ params: { stemId } }) =&gt; {
			await runMnrOnStemId(stemId);
			return new Response(`MNR started for stem ${stemId}`, {
				status: 200,
			});
		},
	});

	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/validate-clinicals/:stemId&quot;,
		body: z.object({}),
		handler: async ({ params: { stemId } }) =&gt; {
			await validateClinicalsOnStemId(stemId);
		},
	});

	// serves static files from s3, including assets
	// see createStaticApp for more details, in lib/ts/lib-platform/app.ts
	const staticApp = createStaticApp({
		blobs: tasksBlob,
		prefix: &quot;&quot;,
		index: &quot;index.html&quot;,
		shouldGenerateCSP: true,
	});

	app.hono.route(&quot;/&quot;, staticApp);

	return app;
}</file><file path="src/tasks/infra.ts">import infra_ from &quot;@anterior/lib-infra&quot;;
import config from &quot;@anterior/lib-infra/config&quot;;
import { S3Bucket } from &quot;@anterior/lib-infra/s3&quot;;
import { BlobStore } from &quot;@anterior/lib-platform/blob&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { Platform } from &quot;./platform.ts&quot;;

// 🚨 DANGER! do not change this path, or risk overwriting
// production S3 data in other paths. We need to overhaul
// our bucket policy to restrict bucket path access to
// certain roles to prevent accidental deletion.
//
// NB: this is only used to pull the frontend app from s3
// and serve it to the user from noggin
export const TASKS_S3_PATH = &quot;vibes/tasks/&quot;;
const tasksS3 = new S3Bucket(config.ANT_S3_BUCKET, TASKS_S3_PATH);
export const tasksBlob = BlobStore.createReadonly(logger, tasksS3);

/**
 * Platform configuration for the Tasks application.
 *
 * The platform provides access to core services:
 * - Storage: S3-based storage for note payloads and metadata
 * - Flows: LLM-based note generation pipeline
 *
 * Each service is configured with specific settings for the Tasks
 * use case, ensuring proper isolation and resource management.
 */
export const infra = {
	logger,
	blobs: infra_.s3Bucket,
	taskQueue: infra_.taskQueue,
};
export const platform = Platform.fromInfra(infra);</file><file path="src/tasks/main.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;
import { createTasksApp } from &quot;./app.ts&quot;;
import { platform } from &quot;./infra.ts&quot;;

export default createTasksApp(
	{
		logger,
		platform,
	},
	{
		name: &quot;tasks&quot;,
		port: parseInt(config.ANT_LISTEN_PORT),
		allowedOrigins: config.ANT_ALLOWED_ORIGINS.split(&quot;,&quot;),
		allowedHeaders: [&quot;X-API-Key&quot;, &quot;X-Anterior-Enterprise-Id&quot;],
		useCSRF: true,
		noLogging: true,
		// jwtConfig: {
		// 	jwtSecret: secrets.ANT_JWT_SECRET.getValue(),
		// 	tokenRefreshEndpoint: config.ANT_TOKEN_REFRESH_ENDPOINT,
		// 	redirectUrl: &quot;/auth/login.html&quot;,
		// },
	}
);</file><file path="src/tasks/platform.ts">import { BlobStore, type BlobStoreBackend } from &quot;@anterior/lib-platform/blob&quot;;
import {
	InMemoryBlobStoreBackend,
	InMemoryMessageQueueBackend,
} from &quot;@anterior/lib-platform/in-memory&quot;;
import { Logger, logger } from &quot;@anterior/lib-platform/log&quot;;
import type { MessageQueue } from &quot;@anterior/lib-platform/queue&quot;;
import { toSchema } from &quot;@anterior/lib-platform/schema&quot;;
import { TaskScheduler } from &quot;@anterior/lib-platform/tasks&quot;;
import type { POJO } from &quot;@anterior/lib-platform/types&quot;;

import { mnr_workflo, type MNRWorkflo } from &quot;./schemas.ts&quot;;

/**
 * Infrastructure interface defining the external resources needed by the platform.
 *
 * The platform can be deployed in different environments:
 * - Production: Resources managed by CDK/Terraform
 * - Docker/CI: Resources managed by Docker Compose
 * - Local/Testing: Resources represented by in-memory implementations
 *
 * This abstraction allows for environment-specific implementations
 * while maintaining a consistent interface for the application.
 */
export interface Infra {
	readonly logger: Logger;
	readonly blobs: BlobStoreBackend;
	readonly taskQueue: MessageQueue&lt;POJO&gt;;
}

/**
 * Creates an in-memory implementation of the infrastructure.
 *
 * This is not meant for production use! The in-memory implementation
 * is useful for:
 * - Unit testing: Fast, isolated test environment
 * - Local development: Quick setup without external dependencies
 *
 * Note that all data is lost when the process exits.
 */
export function getInMemoryInfra(): Infra {
	return {
		logger,
		blobs: new InMemoryBlobStoreBackend(),
		taskQueue: new InMemoryMessageQueueBackend(),
	};
}

/**
 * Application layer interface to the resource backends.
 *
 * This interface provides type-safe access to:
 * - Blob storage: For storing note payloads and metadata
 *
 * The interface ensures that all platform operations are properly
 * typed and validated against their schemas.
 */
export interface Stores {
	readonly blobs: BlobStore;
}

export function getTaskScheduler(infra: {
	logger: Logger;
	taskQueue: MessageQueue&lt;POJO&gt;;
}): TaskScheduler&lt;MNRWorkflo&gt; {
	return new TaskScheduler&lt;MNRWorkflo&gt;(
		infra.logger,
		infra.taskQueue,
		toSchema(mnr_workflo),
		(task: MNRWorkflo) =&gt; ({
			// TODO: enforce typing instead of POJO
			workflow_name: &quot;mnr_workflo&quot;,
			parameters: task,
		})
	);
}

/**
 * Core platform implementation for the Tasks application.
 *
 * The platform provides:
 * 1. Factory methods for creating platform instances
 * 2. Type-safe access to storage and flow services
 * 3. Environment-specific resource management
 */
export class Platform {
	/**
	 * Creates an in-memory platform instance for testing.
	 *
	 * This method allows for partial infrastructure overrides,
	 * making it easy to mock specific components while using
	 * real implementations for others.
	 */
	public static inMemory(infra: Partial&lt;Infra&gt; = {}): Platform {
		return Platform.fromInfra({ ...getInMemoryInfra(), ...infra });
	}

	/**
	 * Creates a platform instance from infrastructure configuration.
	 *
	 * This method:
	 * 1. Sets up the message queue for flow scheduling
	 * 2. Initializes blob storage for note data
	 * 3. Configures proper serialization and validation
	 */
	public static fromInfra(infra: Infra): Platform {
		const stores: Stores = {
			blobs: new BlobStore(infra.logger, infra.blobs),
		};
		return new Platform(stores, getTaskScheduler(infra));
	}

	constructor(
		public readonly stores: Stores,
		public readonly tasks: TaskScheduler&lt;MNRWorkflo&gt;
	) {}

	/**
	 * Type-safe access to the blob storage service.
	 * Used for storing and retrieving note data.
	 */
	get blobs(): BlobStore {
		return this.stores.blobs;
	}
}</file><file path="src/tasks/schemas.ts">import { type SchemaLike } from &quot;@anterior/lib-platform/schema&quot;;
import { z } from &quot;@anterior/lib-platform/zod&quot;;

// Tasks

export interface MNRWorkflo {
	readonly config: {
		readonly workflo_meta: {
			readonly name: string;
			readonly idempotency_key: string;
			readonly scheduled_at: string; // TODO: datetime
		};
		readonly source_meta: {
			readonly stem_uid: string;
			readonly user_uid: string;
			readonly enterprise_uid: string;
			readonly workspace_uid: string;
			readonly workstream_id: string;
			readonly product_id: string;
			readonly trace_id: string;
		};
		readonly workspace_config: {
			readonly trap_door_codes: string[];
		};
	};
	readonly request: {
		// TODO: union with optum request
		readonly type: &quot;HHRequest&quot;;
		readonly protocol_code: string;
	};
	readonly member_query: string;
	readonly clinicals_query: string[];
}

export const mnr_workflo = z.object({
	config: z.object({
		workflo_meta: z.object({
			name: z.string(),
			idempotency_key: z.string(),
			scheduled_at: z.string(), // TODO: datetime
		}),
		source_meta: z.object({
			stem_uid: z.string(),
			user_uid: z.string(),
			enterprise_uid: z.string(),
			workspace_uid: z.string(),
			workstream_id: z.string(),
			product_id: z.string(),
			trace_id: z.string(),
		}),
		workspace_config: z.object({
			trap_door_codes: z.array(z.string()),
		}),
	}),
	request: z.object({
		type: z.literal(&quot;HHRequest&quot;),
		protocol_code: z.string(),
	}),
	member_query: z.string(),
	clinicals_query: z.array(z.string()),
}) satisfies SchemaLike&lt;MNRWorkflo&gt;;</file><file path="src/config.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { getConfig } from &quot;@anterior/lib-platform/config&quot;;

export default {
	...config,
	...getConfig({
		ANT_TOKEN_REFRESH_ENDPOINT: undefined,
		ANTHROPIC_API_KEY: undefined,
	}),
};</file><file path="src/config.ts">import config from &quot;@anterior/lib-infra/config&quot;;
import { getConfig } from &quot;@anterior/lib-platform/config&quot;;

export default {
	...config,
	...getConfig({
		ANT_TOKEN_REFRESH_ENDPOINT: undefined,
		ANTHROPIC_API_KEY: undefined,
	}),
};</file><file path="src/index.ts">#!/usr/bin/env node

import config from &quot;@anterior/lib-infra/config&quot;;
import { Router } from &quot;@anterior/lib-platform/app&quot;;
import { logger } from &quot;@anterior/lib-platform/log&quot;;

import artStoreApp from &quot;./art-store/main.ts&quot;;
import authApp from &quot;./auth/main.ts&quot;;
import chatApp from &quot;./chat/main.ts&quot;;
import flonotesApp from &quot;./flonotes/main.ts&quot;;
import flopilotApp from &quot;./flopilot/main.ts&quot;;
import healthApp from &quot;./health/main.ts&quot;;
import helloWorldApp from &quot;./hello-world/app.ts&quot;;
import notesApp from &quot;./notes/main.ts&quot;;
import pdfApp from &quot;./pdfs/main.ts&quot;;
import stemsApp from &quot;./stems/main.ts&quot;;
import tasksApp from &quot;./tasks/main.ts&quot;;

const router = new Router({ logger });

router.mount(&quot;/health&quot;, healthApp);
router.mount(&quot;/auth&quot;, authApp);
router.mount(&quot;/pdfs&quot;, pdfApp);
router.mount(&quot;/notes&quot;, notesApp);
router.mount(&quot;/chat&quot;, chatApp);
router.mount(&quot;/flonotes&quot;, flonotesApp);
router.mount(&quot;/flopilot&quot;, flopilotApp);
router.mount(&quot;/hello-world&quot;, helloWorldApp);
router.mount(&quot;/stems&quot;, stemsApp);
router.mount(&quot;/tasks&quot;, tasksApp);
router.mount(&quot;/art-store&quot;, artStoreApp);

router.serve(parseInt(config.ANT_LISTEN_PORT));</file><file path="src/migrate.ts">#!/usr/bin/env node

function migrate() {
	console.log(&quot;noggin: nothing to migrate&quot;);
}

migrate();</file><file path="vitest.config.ts">import * as process from &quot;process&quot;;
import { defineConfig } from &quot;vitest/config&quot;;

export default defineConfig(({ mode }) =&gt; ({
	// This ensures vitest cache is always in a writable directory which is important for building and
	// running tests in Nix.  However it does of course mean tests now lose cache every machine reboot
	// for local devs: if that becomes a problem we can add a better heuristic here to only use TMP in
	// ephemeral build contexts.
	cacheDir: (process.env.TMPDIR || &quot;.&quot;) + &quot;/vitest-cache&quot;,
}));</file><file path="vitest.config.ts">import * as process from &quot;process&quot;;
import { defineConfig } from &quot;vitest/config&quot;;

export default defineConfig(({ mode }) =&gt; ({
	// This ensures vitest cache is always in a writable directory which is important for building and
	// running tests in Nix.  However it does of course mean tests now lose cache every machine reboot
	// for local devs: if that becomes a problem we can add a better heuristic here to only use TMP in
	// ephemeral build contexts.
	cacheDir: (process.env.TMPDIR || &quot;.&quot;) + &quot;/vitest-cache&quot;,
}));</file><file path="src/shims/Array.fromAsync.ts">/**
 * Shim for `Array.fromAsync` method, a proposal that has been implemented in
 * Bun, and some browsers, but not in Nodejs, yet.
 */
if (!(&quot;fromAsync&quot; in Array)) {
	(Array as any).fromAsync = async &lt;T&gt;(iterable: AsyncIterable&lt;T&gt;): Promise&lt;T[]&gt; =&gt; {
		const result = [];
		for await (const item of iterable) {
			result.push(item);
		}
		return result;
	};
}</file><file path="src/app.test.ts">import { describe, expect, test } from &quot;vitest&quot;;
import {
	App,
	createPathParamsExtractor,
	createStaticApp,
	extractBody,
	extractBodyFormData,
	extractQuery,
	extractQueryParam,
	type EndpointSignature,
	type ExtractParams,
	type Procedure,
	type ServiceImplementation,
} from &quot;./app.ts&quot;;
import { BlobStore, toBlob } from &quot;./blob.ts&quot;;
import { DuplicateKeyError, ParseError } from &quot;./errors.ts&quot;;
import { InMemoryBlobStoreBackend, InMemoryLogger } from &quot;./in-memory.ts&quot;;
import type { Parser } from &quot;./types.ts&quot;;
import { z } from &quot;./zod.ts&quot;;

describe(&quot;pathExtractor&quot;, () =&gt; {
	const extractPath = createPathParamsExtractor(&quot;/foo/:bar/baz/:qux&quot;);

	function typeAssert&lt;T extends never&gt;() {}
	type TypeEqualityGuard&lt;A, B&gt; = Exclude&lt;A, B&gt; | Exclude&lt;B, A&gt;;
	typeAssert&lt;TypeEqualityGuard&lt;ExtractParams&lt;&quot;/foo&quot;&gt;, {}&gt;&gt;();
	typeAssert&lt;
		TypeEqualityGuard&lt;ExtractParams&lt;&quot;/foo/:bar/baz/:qux&quot;&gt;, { bar: string; qux: string }&gt;
	&gt;();
	typeAssert&lt;
		// @ts-expect-error
		TypeEqualityGuard&lt;
			ExtractParams&lt;&quot;/foo/:bar/baz/:qux&quot;&gt;,
			{ bar: string; qux: string; zoop: 1 }
		&gt;
	&gt;();
	typeAssert&lt;
		// @ts-expect-error
		TypeEqualityGuard&lt;
			ExtractParams&lt;&quot;/foo/:bar/baz/:qux/zap/:zoop&quot;&gt;,
			{ bar: string; qux: string }
		&gt;
	&gt;();

	test(&quot;parses&quot;, () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456&quot;);
		expect(extractPath(req)).toEqual({ bar: &quot;123&quot;, qux: &quot;456&quot; });
	});
	test(&quot;parses with prefix&quot;, () =&gt; {
		const req = new Request(&quot;http://example.com/prefix/foo/123/baz/456&quot;);
		expect(extractPath(req)).toEqual({ bar: &quot;123&quot;, qux: &quot;456&quot; });
	});

	test(&quot;throws&quot;, () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz&quot;);
		expect(() =&gt; extractPath(req)).toThrowError(ParseError);
	});
	test(&quot;throws suffix&quot;, () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456/suffix&quot;);
		expect(() =&gt; extractPath(req)).toThrowError(ParseError);
	});
});

describe(&quot;bodyParser&quot;, () =&gt; {
	test(&quot;parses&quot;, async () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456&quot;, {
			method: &quot;POST&quot;,
			body: JSON.stringify({ foo: &quot;bar&quot; }),
			headers: { &quot;content-type&quot;: &quot;application/json&quot; },
		});
		await expect(extractBody(req)).resolves.toEqual({ foo: &quot;bar&quot; });
	});

	test(&quot;returns null for empty body&quot;, async () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456&quot;);
		await expect(extractBody(req)).resolves.toBeNull();
	});

	test(&quot;throws for invalid JSON&quot;, async () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456&quot;, {
			method: &quot;POST&quot;,
			body: &apos;{&quot;foo&quot;: &quot;bar&quot;&apos;,
			headers: { &quot;content-type&quot;: &quot;application/json&quot; },
		});
		await expect(extractBody(req)).rejects.toThrow(ParseError);
	});
});

describe(&quot;queryParser&quot;, () =&gt; {
	test(&quot;parses&quot;, async () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456?foo=bar&amp;bar=123&quot;);
		expect(extractQuery(req)).toEqual({ foo: &quot;bar&quot;, bar: &quot;123&quot; });
	});

	test(&quot;parses empty&quot;, async () =&gt; {
		const req = new Request(&quot;http://example.com/foo/123/baz/456&quot;);
		expect(extractQuery(req)).toEqual({});
	});
});

describe(&quot;extractQueryParam&quot;, () =&gt; {
	test(&quot;extracts query params&quot;, () =&gt; {
		const url = &quot;http://example.com/foo/123/baz/456?qux=789&quot;;
		expect(extractQueryParam(url, &quot;qux&quot;)).toEqual(&quot;789&quot;);
	});
	test(&quot;extracts the first for duplicate params&quot;, () =&gt; {
		const url = &quot;http://example.com/foo/123/baz/456?qux=789&amp;qux=abc&quot;;
		expect(extractQueryParam(url, &quot;qux&quot;)).toEqual(&quot;789&quot;);
	});
	test(&quot;returns null for missing query params&quot;, () =&gt; {
		const url = &quot;http://example.com/foo/123/baz/456?qux=789&quot;;
		expect(extractQueryParam(url, &quot;missing&quot;)).toBeNull();
	});
});

describe(&quot;extractBodyFormData&quot;, () =&gt; {
	// ── scalar key ➜ last value wins
	test(&quot;extracts scalar key&quot;, async () =&gt; {
		const fd = new FormData();
		fd.append(&quot;foo&quot;, &quot;one&quot;);
		fd.append(&quot;foo&quot;, &quot;two&quot;);
		const req = new Request(&quot;https://example.com&quot;, { method: &quot;POST&quot;, body: fd });
		await expect(extractBodyFormData(req)).resolves.toEqual({ foo: &quot;two&quot; });
	});

	// ── array key ➜ values append
	test(&quot;extracts array key&quot;, async () =&gt; {
		const fd = new FormData();
		fd.append(&quot;foo[]&quot;, &quot;bar&quot;);
		fd.append(&quot;foo[]&quot;, &quot;baz&quot;);
		const req = new Request(&quot;https://example.com&quot;, { method: &quot;POST&quot;, body: fd });
		await expect(extractBodyFormData(req)).resolves.toEqual({ foo: [&quot;bar&quot;, &quot;baz&quot;] });
	});

	// ── scalar ⇢ array  (scalar folded into array)
	test(&quot;folds earlier scalar into later array key&quot;, async () =&gt; {
		const fd = new FormData();
		fd.append(&quot;foo&quot;, &quot;first&quot;);
		fd.append(&quot;foo[]&quot;, &quot;second&quot;);
		const req = new Request(&quot;https://example.com&quot;, { method: &quot;POST&quot;, body: fd });
		await expect(extractBodyFormData(req)).resolves.toEqual({ foo: [&quot;first&quot;, &quot;second&quot;] });
	});

	// ── array ⇢ scalar  (scalar overwrites array)
	test(&quot;overwrites array with final scalar key&quot;, async () =&gt; {
		const fd = new FormData();
		fd.append(&quot;foo[]&quot;, &quot;a&quot;);
		fd.append(&quot;foo[]&quot;, &quot;b&quot;);
		fd.append(&quot;foo&quot;, &quot;last&quot;);
		const req = new Request(&quot;https://example.com&quot;, { method: &quot;POST&quot;, body: fd });
		await expect(extractBodyFormData(req)).resolves.toEqual({ foo: &quot;last&quot; });
	});

	// ── File objects survive intact
	test(&quot;handles File objects&quot;, async () =&gt; {
		// Create test files
		const f1 = new File([&quot;x&quot;], &quot;f1.txt&quot;);
		const f2 = new File([&quot;y&quot;], &quot;f2.txt&quot;);

		// Prepare form data with single and array file fields
		const fd = new FormData();
		fd.append(&quot;doc&quot;, f1);
		fd.append(&quot;docs[]&quot;, f1);
		fd.append(&quot;docs[]&quot;, f2);

		const req = new Request(&quot;https://example.com&quot;, { method: &quot;POST&quot;, body: fd });
		const result = await extractBodyFormData(req);

		// Type-safe validation using structure matching
		expect(result).toEqual(
			expect.objectContaining({
				doc: expect.any(File),
				docs: expect.arrayContaining([expect.any(File), expect.any(File)]),
			})
		);

		// Type-safe array validation
		const formData = result as Record&lt;string, any&gt;;
		const docsArray = formData[&quot;docs&quot;];
		if (Array.isArray(docsArray)) {
			expect(docsArray.length).toBe(2);
		}
	});

	// ── error propagation
	test(&quot;throws ParseError on failure&quot;, async () =&gt; {
		const badReq = { formData: () =&gt; Promise.reject(&quot;boom&quot;) } as unknown as Request;
		await expect(extractBodyFormData(badReq)).rejects.toBeInstanceOf(ParseError);
	});

	// ── Test for mixed key formats
	test(&quot;throws DuplicateKeyError when mixing keys&quot;, async () =&gt; {
		const fd = new FormData();
		fd.append(&quot;foo[]&quot;, &quot;a&quot;);
		fd.append(&quot;foo&quot;, &quot;b&quot;);
		const req = new Request(&quot;https://example.com&quot;, { method: &quot;POST&quot;, body: fd });
		await expect(extractBodyFormData(req)).rejects.toThrow(DuplicateKeyError);
		await expect(extractBodyFormData(req)).rejects.toThrow(
			&apos;Ambiguous use of both &quot;foo&quot; and &quot;foo[]&quot; keys is not allowed&apos;
		);
	});
});

describe(&quot;app&quot;, async () =&gt; {
	interface PostHelloRequest {
		foo: number;
	}
	interface TestService {
		GET: {
			&quot;/hello/:greetee&quot;: EndpointSignature&lt;null, { bar: string }&gt;;
		};
		POST: {
			&quot;/hello/:greetee&quot;: EndpointSignature&lt;PostHelloRequest, { bar: string }&gt;;
		};
		query: {
			getGreetings: Procedure&lt;null, { greetings: string[] }&gt;;
		};
	}
	const app: App&lt;TestService, {}&gt; = new App(
		{},
		{
			name: &quot;lib-platform/test&quot;,
			port: 8080,
			allowedOrigins: [&quot;http://example.com&quot;],
			allowedHeaders: [],
			useCSRF: false,
			logger: InMemoryLogger(),
		}
	);
	app.endpoint({
		method: &quot;GET&quot;,
		route: &quot;/hello/:greetee&quot;,
		params: z.object({ greetee: z.string() }),
		response: z.object({ bar: z.string() }),
		handler: async ({ params: { greetee } }) =&gt; {
			return { bar: greetee };
		},
	});
	app.endpoint({
		method: &quot;POST&quot;,
		route: &quot;/hello/:greetee&quot;,
		params: z.object({ greetee: z.string() }) satisfies Parser&lt;{ greetee: string }&gt;,
		body: z.object({ foo: z.number() }) satisfies Parser&lt;PostHelloRequest&gt;,
		response: z.object({ bar: z.string() }),
		handler: async ({ params, body }) =&gt; {
			return { bar: Array(body.foo).fill(params.greetee).join(&quot;&quot;) };
		},
	});
	app.useTRPCRouter({
		router: {
			getGreetings: app.trpc.procedure.query(async () =&gt; {
				return { greetings: [&quot;hello&quot;, &quot;howdy&quot;] };
			}),
			createGreeting: app.trpc.procedure
				.input(z.object({ greeting: z.string() }))
				.mutation(async ({ input }) =&gt; {
					return { greetings: [&quot;hello&quot;, &quot;howdy&quot;, input.greeting] };
				}),
		},
	});
	app satisfies ServiceImplementation&lt;TestService&gt;;

	test(&quot;GET&quot;, async () =&gt; {
		const foo = await app.fetch(new Request(&quot;http://example.com/hello/world&quot;), {}, {} as any);
		await expect(foo.json()).resolves.toEqual({ bar: &quot;world&quot; });
	});

	test(&quot;POST&quot;, async () =&gt; {
		const foo = await app.fetch(
			new Request(&quot;http://example.com/hello/world&quot;, {
				method: &quot;POST&quot;,
				body: JSON.stringify({ foo: 3 }),
				headers: { &quot;content-type&quot;: &quot;application/json&quot; },
			}),
			{},
			{} as any
		);
		await expect(foo.json()).resolves.toEqual({ bar: &quot;worldworldworld&quot; });
	});

	test(&quot;query&quot;, async () =&gt; {
		const foo = await app.fetch(
			new Request(&quot;http://example.com/trpc/getGreetings&quot;),
			{},
			{} as any
		);
		await expect(foo.json()).resolves.toEqual({
			result: {
				data: {
					json: {
						greetings: [&quot;hello&quot;, &quot;howdy&quot;],
					},
				},
			},
		});
	});

	test(&quot;mutation&quot;, async () =&gt; {
		const foo = await app.fetch(
			new Request(`http://example.com/trpc/createGreeting`, {
				method: &quot;POST&quot;,
				headers: { &quot;content-type&quot;: &quot;application/json&quot; },
				body: JSON.stringify({ json: { greeting: &quot;yoyo&quot; } }),
			}),
			{},
			{} as any
		);
		await expect(foo.json()).resolves.toEqual({
			result: {
				data: {
					json: {
						greetings: [&quot;hello&quot;, &quot;howdy&quot;, &quot;yoyo&quot;],
					},
				},
			},
		});
	});

	test(&quot;useExtraContext&quot;, async () =&gt; {
		const app: App&lt;{}, { foo: number }&gt; = new App(
			{
				foo: 123,
			},
			{
				name: &quot;lib-platform/test&quot;,
				port: 8080,
				allowedOrigins: [&quot;http://example.com&quot;],
				allowedHeaders: [],
				useCSRF: false,
				logger: InMemoryLogger(),
				noLogging: true,
			}
		);

		type TestServiceWithExtraContext = {
			GET: {
				&quot;/test-use&quot;: EndpointSignature&lt;null, { foo: number; bar: string }&gt;;
			};
		};

		const appWithBar: App&lt;TestServiceWithExtraContext, { foo: number; bar: string }&gt; =
			app.useExtraContext(async () =&gt; {
				return {
					bar: &quot;bar&quot;,
				};
			});
		appWithBar.endpoint({
			method: &quot;GET&quot;,
			route: &quot;/test-use&quot;,
			response: z.object({ foo: z.number(), bar: z.string() }),
			handler: async ({ foo, bar }) =&gt; {
				return { foo, bar };
			},
		});

		appWithBar satisfies ServiceImplementation&lt;TestServiceWithExtraContext&gt;;

		await expect(
			app
				.fetch(new Request(&quot;http://example.com/test-use&quot;), {}, {} as any)
				.then((res) =&gt; res.json())
		).resolves.toEqual({ foo: 123, bar: &quot;bar&quot; });
	});
});

describe(&quot;createStaticApp&quot;, async () =&gt; {
	test(&quot;works&quot;, async () =&gt; {
		const logger = InMemoryLogger();
		const backend = new InMemoryBlobStoreBackend();
		const blob = await toBlob(&quot;Hello World&quot;, &quot;text/plain&quot;);
		await backend.storeBlob(&quot;testBlob&quot;, blob);

		const blobs = BlobStore.createReadonly(logger, backend);

		const app = createStaticApp({
			prefix: &quot;/static/&quot;,
			blobs,
		});

		const res = await app.fetch(
			new Request(&quot;http://example.com/static/testBlob&quot;),
			{},
			{} as any
		);
		expect(res.status).toBe(200);
		expect(res.headers.get(&quot;content-type&quot;)).toBe(&quot;text/html&quot;);
		expect(await res.text()).toBe(&quot;Hello World&quot;);
	});
});</file><file path="src/app.ts">import { initTRPC } from &quot;@trpc/server&quot;;
import fs from &quot;fs&quot;;
import { type Env, type ExecutionContext, Hono, type Context as HonoContext } from &quot;hono&quot;;
import { logger as pinoLogger } from &quot;hono-pino&quot;;
import { cors } from &quot;hono/cors&quot;;
import { csrf } from &quot;hono/csrf&quot;;
import { HTTPException } from &quot;hono/http-exception&quot;;
import { requestId } from &quot;hono/request-id&quot;;
import { secureHeaders } from &quot;hono/secure-headers&quot;;
import { timing } from &quot;hono/timing&quot;;
import path from &quot;path&quot;;
import superjson from &quot;superjson&quot;;
import { URL } from &quot;url&quot;;
import { z } from &quot;./zod.ts&quot;;

import { allowOrigins } from &quot;./cors.ts&quot;;

import { fetchRequestHandler } from &quot;@trpc/server/adapters/fetch&quot;;
import type { Blob, ReadOnlyBlobStore } from &quot;./blob.ts&quot;;
import { DuplicateKeyError, KeyMissingError, ParseError, Unauthorized } from &quot;./errors.ts&quot;;
import { type JWTConfig, jwtMiddleware, type JWTPayload } from &quot;./jwt.ts&quot;;
import { logger, Logger } from &quot;./log.ts&quot;;
import { fromEach } from &quot;./object.ts&quot;;
import { type Schema, type SchemaLike, toSchema } from &quot;./schema.ts&quot;;
import type { POJO } from &quot;./types.ts&quot;;

import { serve } from &quot;@hono/node-server&quot;;

export { serve } from &quot;@hono/node-server&quot;;
export { HTTPException } from &quot;hono/http-exception&quot;;

// To make it less attractive to use the hono specific context. Right now it&apos;s helping us return a stream response somewhere
const HONO_CONTEXT = Symbol(&quot;HONO_CONTEXT&quot;);
export function getHonoContext(ctx: any): HonoContext {
	if (!ctx[HONO_CONTEXT]) {
		throw new Error(&quot;Hono context not found&quot;);
	}
	return ctx[HONO_CONTEXT];
}

/**
 * This type turns this path string
 *
 * 	&quot;/foo/:bar/baz/:qux&quot;
 *
 * into this object type
 *
 * 	{ bar: string, qux: string }
 */
export type ExtractParams&lt;T extends string&gt; =
	T extends `${infer _Start}/:${infer Param}/${infer Rest}`
		? { [K in Param | keyof ExtractParams&lt;`/${Rest}`&gt;]: string }
		: T extends `${infer _Start}/:${infer Param}`
			? { [K in Param]: string }
			: {};

/**
 * This function creates a parser function that can extract parameter values from a path
 */
export function createPathParamsExtractor&lt;T extends Path&gt;(template: T) {
	// Convert the template into a regex pattern that matches the placeholders
	const paramNames: string[] = [];

	// `/\/:([^\/]+)/g` regex captures route params in a URL pattern, specifically:
	//  segments that start with a colon (:)
	//  and continue until the next slash (/)
	const regexPattern = template.replace(/\/:([^\/]+)/g, (_, paramName) =&gt; {
		paramNames.push(paramName);
		return &quot;/([^/]+)&quot;; // Match anything except &apos;/&apos;
	});

	// Create a regex with end anchors to match the path; there may be any number of prefix segments
	const regex = new RegExp(`${regexPattern}$`);

	// Return a parser function that can match and extract values from a path
	return function extractPathParams(req: Request): ExtractParams&lt;T&gt; | null {
		const path = new URL(req.url).pathname;
		const match = path.match(regex);

		if (!match) {
			throw new ParseError(`Path ${path} does not match template ${template}`);
		}

		// Create an object with parameter names as keys and matched values as values
		// The first match is the full path, so start at index 1
		return Object.fromEntries(
			paramNames.map((name, index) =&gt; [name, match[index + 1]])
		) as ExtractParams&lt;T&gt;;
	};
}

/**
 * Only supports one value per key for now
 */
export async function extractBodyFormData(req: Request): Promise&lt;POJO&gt; {
	try {
		const formData = await req.formData();
		return fromEach(formData);
	} catch (e) {
		throw new ParseError(e);
	}
}

export async function extractBodyJSON(req: Request): Promise&lt;POJO&gt; {
	try {
		// should we raise if content-length 0?
		return (await req.json()) as POJO;
	} catch (e) {
		throw new ParseError(e);
	}
}

/**
 * Only support application/x-www-form-urlencoded and application/json for now
 * TODO support other content types, like text/plain, etc.
 */
export async function extractBody(req: Request): Promise&lt;POJO | null&gt; {
	if (req.headers.get(&quot;content-type&quot;)?.startsWith(&quot;application/x-www-form-urlencoded&quot;)) {
		return await extractBodyFormData(req);
	} else if (req.headers.get(&quot;content-type&quot;)?.startsWith(&quot;multipart/form-data&quot;)) {
		return await extractBodyFormData(req);
	} else if (req.headers.get(&quot;content-type&quot;)?.startsWith(&quot;application/json&quot;)) {
		return await extractBodyJSON(req);
	} else {
		return null;
	}
}

/**
 * When you pass in a Zod schema, make sure to use coerce on number fields
 * TODO: Only uses the last occurence for duplicates for now.
 * ?foo[1]=bar&amp;foo[2]=baz -&gt; { &quot;foo[1]&quot;: &quot;bar&quot;, &quot;foo[2]&quot;: &quot;baz&quot; }
 * ?foo=bar&amp;foo=baz -&gt; { &quot;foo&quot;: &quot;baz&quot; }
 */
export function extractQuery(req: Request): POJO {
	try {
		const url = new URL(req.url);
		return fromEach(url.searchParams);
	} catch (e) {
		throw new ParseError(e);
	}
}

export function extractHeaders(req: Request): POJO {
	try {
		return req.headers ? fromEach(req.headers) : {};
	} catch (e) {
		console.error(e);
		throw new ParseError(e);
	}
}

// Realistically these could be the same type

export type EndpointSignature&lt;TRequest, TResponse&gt; = (arg: {
	body: TRequest;
}) =&gt; Promise&lt;TResponse&gt;;
export interface Procedure&lt;TInput, TOutput&gt; {
	input: TInput;
	output: TOutput;
}

/**
 * Extract a query parameter from a URL
 */
export const extractQueryParam = (url: string, key: string): string | null =&gt;
	new URL(url).searchParams.get(key);

// This path type is most likely a placeholder
export type Path = &quot;/&quot; | `/${string}`;

// TODO support other methods
type Method = &quot;GET&quot; | &quot;POST&quot;;

type Operation = &quot;query&quot; | &quot;mutate&quot;;

/**
 * This type helps define the shape of a http service map that is easy to read.
 * We can also use it for type type assertions in methods on the app class.
 */
type EndpointMap = {
	[K in Method]?: {
		[route: Path]: EndpointSignature&lt;any, any&gt;;
	};
};

/**
 * This type helps define the shape of a trpc service map that is easy to read.
 * We can also use it for type type assertions in methods on the app class.
 */
type ProcedureMap = {
	[K in Operation]?: {
		[procedure: string]: Procedure&lt;any, any&gt;;
	};
};

type ServiceMap = EndpointMap &amp; ProcedureMap;

export type ServiceImplementation&lt;T extends EndpointMap&gt; = {
	[TMethod in keyof Omit&lt;T, Operation&gt;]: {
		// TODO also type check operations
		[TRoute in keyof T[TMethod]]: T[TMethod][TRoute];
	};
};

export interface AppOptions {
	readonly name: string;
	readonly port: number;
	readonly allowedHeaders: string[];
	readonly allowedOrigins: string[];
	readonly useCSRF: boolean;
	/**
	 * When true, the app will not log anything. Useful to keep the test output clean.
	 * Altertatively, you can pass in an in-memory or noop logger.
	 */
	readonly noLogging?: boolean;
	/**
	 * Defaults to the global logger
	 */
	readonly logger?: Logger;
	/**
	 * If this value is set, the app will use JWT middleware
	 */
	readonly jwtConfig?: JWTConfig;
}

export type BaseContext = {
	/**
	 * The raw request object, we try to use this as much as possible to minimize indirection
	 */
	req: Request;
	/**
	 * This is a bit of non-generic context that we support out of the box. Only set to a value when relevant
	 */
	jwtPayload?: JWTPayload;
	auth?: {
		userUid: string;
		enterpriseUid: string;
	};
} &amp; {
	/**
	 * This sneaky property is used to pass the hono context around as an escape hatch
	 */
	[key in typeof HONO_CONTEXT]: HonoContext;
};

export type RequestContext&lt;
	TAppContext,
	TRoute extends Path,
	TParams = unknown,
	TQuery = unknown,
	THeaders = unknown,
	TBody = unknown,
&gt; = {
	/**
	 * The route is the path template that the request was matched against
	 */
	route: TRoute;
	/**
	 * Parameters found in the url path, e.g. /foo/:bar/baz/:qux
	 */
	params: TParams;
	/**
	 * Query parameters found in the url, e.g. /foo?bar=baz
	 */
	query: TQuery;
	/**
	 * Headers found in the request
	 */
	headers: THeaders;
	/**
	 * The body of the request in an appropriate format, i.e. json and form data as POJO
	 */
	body: TBody;
} &amp; TAppContext &amp;
	BaseContext;

/**
 * This is what the application author provides when defining an endpoint, it&apos;s a bit more flexible
 * than the Endpoint type, but it&apos;s converted to that type before being used.
 */
export interface EndpointConfig&lt;
	TAppContext,
	TMethod extends Method,
	TRoute extends Path,
	TParams extends ExtractParams&lt;TRoute&gt; | undefined,
	TQuery,
	THeaders,
	TBody,
	TResponse,
	TRequest extends RequestContext&lt;
		TAppContext,
		TRoute,
		TParams,
		TQuery,
		THeaders,
		TBody
	&gt; = RequestContext&lt;TAppContext, TRoute, TParams, TQuery, THeaders, TBody&gt;,
&gt; {
	method: TMethod;
	route: TRoute;
	params?: SchemaLike&lt;TParams&gt;;
	query?: SchemaLike&lt;TQuery&gt;;
	body?: SchemaLike&lt;TBody&gt;;
	headers?: SchemaLike&lt;THeaders&gt;;
	response?: SchemaLike&lt;TResponse&gt;;
	handler: (ctx: TRequest) =&gt; Promise&lt;TResponse&gt;;
}

/**
 * Endpoints contain all information for a single API endpoint to parse a request and return a response
 */
export interface Endpoint&lt;
	TAppContext,
	TMethod extends Method,
	TRoute extends Path,
	TParams extends ExtractParams&lt;TRoute&gt;,
	TQuery,
	THeaders,
	TBody,
	TResponse,
	TRequest extends RequestContext&lt;
		TAppContext,
		TRoute,
		TParams,
		TQuery,
		THeaders,
		TBody
	&gt; = RequestContext&lt;TAppContext, TRoute, TParams, TQuery, THeaders, TBody&gt;,
&gt; {
	method: TMethod;
	route: TRoute;
	params: Schema&lt;TParams&gt; | undefined;
	query: Schema&lt;TQuery&gt; | undefined;
	headers: Schema&lt;THeaders&gt; | undefined;
	body: Schema&lt;TBody&gt; | undefined;
	response: Schema&lt;TResponse&gt; | undefined;
	handler(ctx: TRequest): Promise&lt;TResponse&gt;;
}

function createEndpoint&lt;
	TAppContext,
	TMethod extends Method,
	TRoute extends Path,
	TParams extends ExtractParams&lt;TRoute&gt;,
	TQuery,
	THeaders,
	TBody,
	TResponse,
	TRequest extends RequestContext&lt;
		TAppContext,
		TRoute,
		TParams,
		TQuery,
		THeaders,
		TBody
	&gt; = RequestContext&lt;TAppContext, TRoute, TParams, TQuery, THeaders, TBody&gt;,
&gt;(
	config: EndpointConfig&lt;
		TAppContext,
		TMethod,
		TRoute,
		TParams,
		TQuery,
		THeaders,
		TBody,
		TResponse,
		TRequest
	&gt;
): Endpoint&lt;TAppContext, TMethod, TRoute, TParams, TQuery, THeaders, TBody, TResponse, TRequest&gt; {
	return {
		method: config.method,
		route: config.route,
		params: config.params ? toSchema(config.params) : undefined,
		query: config.query ? toSchema(config.query) : undefined,
		headers: config.headers ? toSchema(config.headers) : undefined,
		body: config.body ? toSchema(config.body) : undefined,
		response: config.response ? toSchema(config.response) : undefined,
		handler: config.handler,
	};
}

const PASSTHROUGH = z.object({}).passthrough();

async function parseRequestContext&lt;
	TAppContext,
	TMethod extends Method,
	TRoute extends Path,
	TParams extends ExtractParams&lt;TRoute&gt;,
	TQuery,
	THeaders,
	TBody,
	TResponse,
	TRequest extends RequestContext&lt;
		TAppContext,
		TRoute,
		TParams,
		TQuery,
		THeaders,
		TBody
	&gt; = RequestContext&lt;TAppContext, TRoute, TParams, TQuery, THeaders, TBody&gt;,
&gt;(
	endpoint: Endpoint&lt;
		TAppContext,
		TMethod,
		TRoute,
		TParams,
		TQuery,
		THeaders,
		TBody,
		TResponse,
		TRequest
	&gt;,
	baseCtx: TAppContext &amp; BaseContext
): Promise&lt;TRequest&gt; {
	const req = baseCtx.req;

	// TODO we could set these up to be lazy
	// TODO we could precreate this extractor and keep it somewhere.
	const params = createPathParamsExtractor(endpoint.route)(req);
	const query = extractQuery(req);
	const headers = extractHeaders(req);
	const body = await extractBody(req);

	return {
		...baseCtx,
		route: endpoint.route,
		params: endpoint.params?.parse(params) ?? PASSTHROUGH.parse(params),
		query: endpoint.query?.parse(query) ?? PASSTHROUGH.parse(query),
		headers: endpoint.headers?.parse(headers) ?? PASSTHROUGH.parse(headers),
		body: body &amp;&amp; (endpoint.body?.parse(body) ?? PASSTHROUGH.parse(body)),
	} as TRequest;
}

// TODO more typing here?
async function handleEndpoint&lt;TAppContext&gt;(
	endpoint: Endpoint&lt;TAppContext, any, any, any, any, any, any, any, any&gt;,
	baseCtx: TAppContext &amp; BaseContext,
	// For now we use the HonoContext, but ideally this takes just a bare Request object
	// We&apos;re close though, we&apos;re only using hono for the responses.
	c: HonoContext
): Promise&lt;Response&gt; {
	const context = await parseRequestContext(endpoint, baseCtx);
	const resp = await endpoint.handler(context);

	if (resp instanceof Response) {
		return resp;
	} else if (typeof resp === &quot;string&quot;) {
		return c.html(resp);
	} else if (endpoint.response) {
		// TODO should we even allow not specifying the response?
		const pojo = endpoint.response?.parse(resp) ?? resp;
		return c.json(pojo);
	} else {
		c.status(204);
		return c.body(null);
	}
}

/**
 * An App is a wrapper around a Hono instance that provides a more structured way to define API endpoints
 * It focuses on type safety and schema adherence. It also provides a way to define trpc endpoints.
 */
export class App&lt;TServiceMap extends ServiceMap, TAppContext extends object&gt; {
	public readonly hono: Hono&lt;{ Variables: TAppContext &amp; BaseContext }&gt;;
	public readonly logger: Logger;

	// TODO make this optional? Maybe as a &quot;cached propery&quot;?
	public readonly trpc = initTRPC
		.context&lt;RequestContext&lt;TAppContext, any, any, any, any, any&gt;&gt;()
		.create({
			transformer: superjson,
		});

	/**
	 * Use this to define trpc procedures into a router
	 */
	get procedure() {
		return this.trpc.procedure;
	}

	/**
	 * This ({port, fetch}) is the actual entrypoint for the app, as a default export
	 * As a neat convention we can export this as such:
	 *
	 * export default app satisfies ServiceImplementation&lt;MyService&gt;
	 **/
	public readonly port: number;
	public readonly fetch: (request: Request, env: Env, ctx: ExecutionContext) =&gt; Promise&lt;Response&gt;;

	constructor(
		public readonly baseCtx: TAppContext,
		public readonly options: AppOptions,

		// for getting auth
		public readonly auth?: (
			req: Request
		) =&gt; Promise&lt;{ userUid: string; enterpriseUid: string } | Response&gt;
	) {
		this.logger = options.logger ?? logger.child({ name: options.name });
		this.port = options.port;
		this.hono = new Hono();
		this.fetch = async (...args) =&gt; this.hono.fetch(...args);
		this.hono.use(&quot;*&quot;, async (c, next): Promise&lt;Response | void&gt; =&gt; {
			await next();
			if (c.res &amp;&amp; c.res.status == 301) {
				return c.redirect(c.res.headers.get(&quot;Location&quot;) ?? &quot;/&quot;);
			}
			if (c.error) {
				const e = c.error;

				// TODO Error handling. There are many ways we could go about this, to name a few:
				// - Error middleware
				// - Error EndpointConfig by type
				// - Errors that are responses
				// Caution: We should not leak stack traces and other sensitive information here to users
				if (e instanceof HTTPException) {
					c.res = new Response(e.name, {
						status: e.status,
					});
				} else if (e instanceof ParseError) {
					// TODO: pass a safe error message to the user.
					c.res = new Response(&quot;Bad request&quot;, {
						status: 400,
					});
				} else if (e instanceof DuplicateKeyError) {
					c.res = new Response(e.message, {
						status: 400,
					});
				} else if (e instanceof Unauthorized) {
					c.res = new Response(&quot;Unauthorized&quot;, {
						status: 403,
					});
				} else {
					this.logger.error({ msg: &quot;Uncaught error in request handler&quot;, err: c.error });
					c.res = new Response(&quot;Internal server error&quot;, {
						status: 500,
					});
				}
			}
		});
		this.hono.use(
			&quot;*&quot;,
			cors({
				origin: allowOrigins(options.allowedOrigins),
				credentials: true,
				allowHeaders: [
					&quot;Authorization&quot;,
					&quot;Accept&quot;,
					&quot;Content-type&quot;,
					&quot;Cookie&quot;,
					&quot;Origin&quot;,
					&quot;Referer&quot;,
					&quot;Request-Id&quot;,
					&quot;User-Agent&quot;,
					...(options.allowedHeaders ?? []),
				],
			})
		);
		if (options.useCSRF) {
			this.hono.use(&quot;*&quot;, csrf({ origin: options.allowedOrigins.join(&quot;,&quot;) }));
		}

		// we may want to have this implemented in an app-specific context
		// for now, it&apos;s the same for pdfs, llms,
		if (options.jwtConfig) {
			this.hono.use(&quot;*&quot;, jwtMiddleware(options.jwtConfig));
		}
		this.hono.use(secureHeaders());
		this.hono.use(&quot;*&quot;, requestId());
		this.hono.use(timing());
		if (!options.noLogging) {
			this.hono.use(pinoLogger());
		}
		this.hono.use(async (c, next): Promise&lt;Response | void&gt; =&gt; {
			// Typescript doesn&apos;t like us assigning to the context here
			c.set(&quot;req&quot; as any, c.req.raw);
			c.set(HONO_CONTEXT as any, c);
			for (const key in this.baseCtx) {
				c.set(key as any, this.baseCtx[key]);
			}

			if (this.auth) {
				const auth = await this.auth(c.req.raw);
				if (auth instanceof Response) {
					return auth;
				}
				c.set(&quot;auth&quot;, auth as any);
			}
			await next();
		});
	}

	useExtraContext&lt;T&gt;(
		middleware: (ctx: TAppContext) =&gt; Promise&lt;T&gt;
	): App&lt;TServiceMap, T &amp; TAppContext&gt; {
		this.hono.use(async (c, next) =&gt; {
			const vars = await middleware(c.var as TAppContext);
			for (const key in vars) {
				c.set(key as any, vars[key]);
			}
			await next();
		});
		return this as any;
	}

	/**
	 * This method does a bunch of type spaghetti to lock down the api endpoints to a spec,
	 * translate our desired interface to hono, and hide hono from the application developer
	 */
	endpoint&lt;
		TMethod extends Method,
		TRoute extends keyof TServiceMap[TMethod] &amp; Path,
		TParams extends ExtractParams&lt;TRoute&gt;,
		TQuery,
		THeaders,
		TBody,
		TResponse,
		TContext extends RequestContext&lt;
			TAppContext,
			TRoute,
			TParams,
			TQuery,
			THeaders,
			TBody
		&gt; = RequestContext&lt;TAppContext, TRoute, TParams, TQuery, THeaders, TBody&gt;,
	&gt;(
		config: EndpointConfig&lt;
			TAppContext,
			TMethod,
			TRoute,
			TParams,
			TQuery,
			THeaders,
			TBody,
			TResponse,
			TContext
		&gt;
	): asserts this is this &amp; {
		[k1 in TMethod]: { [k2 in TRoute]: EndpointSignature&lt;TBody, TResponse&gt; };
	} {
		const endpoint = createEndpoint(config);
		this.hono[config.method.toLowerCase() as Lowercase&lt;TMethod&gt;](config.route, async (c) =&gt;
			handleEndpoint(endpoint, c.var as TAppContext &amp; BaseContext, c)
		);
	}

	useTRPCRouter(options: TRPCOptions) {
		// Hacky way to check whether the router is a trpc router
		const router = options.router._def ? options.router : this.trpc.router(options.router);

		// This is a &quot;fork&quot; of @hono/trpc-server
		// Started with a copy paste from https://github.com/honojs/middleware/blob/main/packages/trpc-server/src/index.ts
		this.hono.use(&quot;/trpc/*&quot;, async (c) =&gt; {
			const ctx = {
				...c.var,
				// For now we don&apos;t support path params for trpc; paths are just function names and all params
				// come through the input schema. So the route and the pathname are the same
				route: new URL(c.req.raw.url).pathname,
				req: c.req.raw,
				headers: extractHeaders(c.req.raw),
				// Hide the request properties; trpc takes care of them here (the raw request is still available)
				params: {},
				query: {},
				body: null,
			} as RequestContext&lt;TAppContext, any&gt;;

			const trpcResp = await fetchRequestHandler({
				createContext: () =&gt; ctx,
				endpoint: &quot;/trpc&quot;,
				req: c.req.raw,
				router,
				onError:
					options.onError ??
					((opts) =&gt; {
						this.logger.error({ msg: &quot;TRPC ERROR&quot;, err: opts.error });
					}),
			});

			// Fix for type compatibility issue with body
			const responseBody = trpcResp.body || &quot;&quot;;
			const honoResp = c.body(responseBody, trpcResp);
			return honoResp;
		});
	}
}

interface RouterOptions {
	logger: Logger;
}

/**
 * A router is a simple mounting point for multiple apps. Each app has has a path prefix and
 * is responsible for all its middleware and context (barring hono defaults)
 */
export class Router {
	public readonly hono: Hono;
	public readonly logger: Logger;

	constructor({ logger }: RouterOptions) {
		this.hono = new Hono();
		this.logger = logger;
	}

	mount(path: Path, app: App&lt;any, any&gt;) {
		this.hono.mount(path, app.fetch);
	}

	serve(port: number) {
		serve({
			fetch: this.hono.fetch.bind(this.hono),
			port,
		});
	}
}

interface TRPCOptions {
	/**
	 * TODO: actually type this
	 */
	router: any;
	/**
	 * Not sure how to get hold of the actual type here.
	 *
	 * import { OnErrorFunction } from &quot;@trpc/server/src/internals/types&quot;;
	 *
	 * (opts: {
	 *		error: TRPCError;
	 *		type: ProcedureType | &quot;unknown&quot;;
	 *		path: string | undefined;
	 *		req: Request;
	 *		input: unknown;
	 *		ctx: any;
	 *}) =&gt; void
	 */
	onError?: any;
}

/**
 * Recursively load all hono apps from app.ts files in a directory and mount them on a rootApp.
 * File access is sync. This is only run once at startup and we want to crash hard if it fails.
 *
 * This failes hard
 */
export async function importAndMountNestedDirectoryApps(
	logger: Logger,
	app: Hono,
	basePath: string,
	baseRoute: string = &quot;&quot;
) {
	let dirPath = basePath;
	let routePath = baseRoute;
	try {
		const entries = fs.readdirSync(basePath, { withFileTypes: true });

		for (const entry of entries) {
			if (!entry.isDirectory()) {
				continue;
			}

			dirPath = path.join(basePath, entry.name);
			routePath = path.join(baseRoute, entry.name);

			// Depth first search to match more specific routes first
			await importAndMountNestedDirectoryApps(logger, app, dirPath, routePath);

			const appFilePath = path.join(dirPath, &quot;app.ts&quot;);
			try {
				fs.accessSync(appFilePath, fs.constants.R_OK);
			} catch (err) {
				continue;
			}

			fs.accessSync(appFilePath);

			const appModule = await import(`file://${appFilePath}`);
			if (!appModule.default || !appModule.default.hono) {
				logger.warn(`Module at ${appFilePath} does not export a Hono app as default`);
				continue;
			}

			logger.info(`Mounting /${routePath}`);
			app.route(`/${routePath}`, appModule.default.hono);
		}
	} catch (err) {
		logger.error({ msg: `Error loading apps from ${dirPath}:`, err });
		throw err;
	}
}

export async function createDirectoryApp({
	dirname,
	logger,
}: {
	dirname: string;
	logger: Logger;
}): Promise&lt;Hono&gt; {
	const app = new Hono();
	await importAndMountNestedDirectoryApps(logger, app, dirname);
	return app;
}

// This is needed to allow for experimental frontend apps like FloNotes
// to be proxied from S3 through Noggin, with access to assets. Otherwise,
// we&apos;ll get errors like:
//
// Refused to apply inline style because it violates the following Content Security Policy directive:
// &quot;default-src &apos;self&apos; anterior.app *.anterior.app&quot;. Either the &apos;unsafe-inline&apos; keyword, a hash
// (&apos;sha256-FgMOc4cb0X+ALfcQ2j4p2XO7TnNrN7BF4xQInZThVKQ=&apos;), or a nonce (&apos;nonce-...&apos;)
// is required to enable inline execution. Note also that &apos;style-src&apos; was not explicitly set,
// so &apos;default-src&apos; is used as a fallback.

// Define types for CSP directives
type CSPDirectiveValue = string;
type CSPDirectives = Record&lt;string, CSPDirectiveValue&gt;;

/**
 * Simplified Content Security Policy configuration
 * Focused on what&apos;s actually needed while maintaining security
 */
export const cspConfig = {
	// Core directives that apply to all contexts
	coreDirectives: {
		defaultSrc: &quot;&apos;self&apos;&quot;,
		// Allow unsafe-inline for styles to support dynamic style injection (TipTap, etc.)
		styleSrc: &quot;&apos;self&apos; &apos;unsafe-inline&apos;&quot;,
		// Media and resource handling
		imgSrc: &quot;&apos;self&apos; data: blob:&quot;,
		fontSrc: &quot;&apos;self&apos; data:&quot;,
		// For PDF previews and other functionality
		connectSrc: &quot;&apos;self&apos; blob:&quot;,
		workerSrc: &quot;&apos;self&apos; blob:&quot;,
		frameSrc: &quot;&apos;self&apos;&quot;,
		objectSrc: &quot;&apos;self&apos; blob:&quot;,
		// Script handling - no nonces needed for external scripts
		scriptSrc: &quot;&apos;self&apos;&quot;,
		scriptSrcElem: &quot;&apos;self&apos; blob:&quot;,
		// Additional security headers
		frameAncestors: &quot;&apos;none&apos;&quot;,
		baseUri: &quot;&apos;self&apos;&quot;,
		formAction: &quot;&apos;self&apos;&quot;,
	} as CSPDirectives,
};

/**
 * Generates a simple CSP string without unnecessary complexity
 *
 * @param additionalDirectives - Any additional CSP directives to include
 * @returns A CSP string with all directives joined with semicolons
 */
export const generateCSP = (additionalDirectives: Record&lt;string, string&gt; = {}) =&gt; {
	const directives = { ...cspConfig.coreDirectives, ...additionalDirectives };

	// convert directives to CSP format
	const cspParts = Object.entries(directives).map(([key, value]) =&gt; {
		const directive = key.replace(/([A-Z])/g, &quot;-$1&quot;).toLowerCase();
		return `${directive} ${value}`;
	});

	return cspParts.join(&quot;; &quot;);
};

/**
 * Returns a Hono app that serves static files from a blob store
 * The prefix parameter is the path prefix that the app is serving files from
 * which is stripped from the request path before looking up the blob.
 *
 * E.g. if the prefix is &quot;/static/&quot; and the request path is &quot;/static/foo/bar.js&quot;,
 * the blob store will look for the key &quot;foo/bar.js&quot;.
 *
 * The index parameter is the key to use when the request path is just the prefix.
 *
 * Handles both text and binary files with appropriate content types based on file extension.
 */
export function createStaticApp({
	prefix,
	blobs,
	index = &quot;index.html&quot;,
	env,
	shouldGenerateCSP = false,
}: {
	prefix: Path | &quot;&quot;; // Allow empty string as a valid prefix
	blobs: ReadOnlyBlobStore;
	index?: string;
	env?: Record&lt;string, string&gt;;
	shouldGenerateCSP?: boolean;
}): Hono {
	const app = new Hono();

	app.get(&quot;*&quot;, async (c) =&gt; {
		const url = new URL(c.req.url);
		let path = url.pathname;

		if (prefix &amp;&amp; !path.startsWith(prefix)) {
			c.status(404);
			return c.text(&quot;Not found&quot;);
		}

		const effectivePath = prefix ? path.slice(prefix.length) : path;

		const normalizedPath = effectivePath.startsWith(&quot;/&quot;)
			? effectivePath.slice(1)
			: effectivePath;

		const key = normalizedPath === &quot;&quot; ? index : normalizedPath;

		let blob: Blob;
		try {
			blob = await blobs.fetchBlob(key);
		} catch (e) {
			if (e instanceof KeyMissingError) {
				c.status(404);
				return c.text(&quot;Not found&quot;);
			}

			throw e;
		}

		c.status(200);

		// Determine content type based on file extension
		const ext = key.split(&quot;.&quot;).slice(1).pop()?.toLowerCase();
		let contentType = &quot;text/html&quot;;

		switch (ext) {
			case &quot;json&quot;:
				contentType = &quot;application/json&quot;;
				break;
			case &quot;js&quot;:
				contentType = &quot;application/javascript&quot;;
				break;
			case &quot;mjs&quot;:
				contentType = &quot;application/javascript&quot;;
				break;
			case &quot;css&quot;:
				contentType = &quot;text/css&quot;;
				break;
			case &quot;svg&quot;:
				contentType = &quot;image/svg+xml&quot;;
				break;
			case &quot;png&quot;:
				contentType = &quot;image/png&quot;;
				break;
			case &quot;jpg&quot;:
			case &quot;jpeg&quot;:
				contentType = &quot;image/jpeg&quot;;
				break;
			case &quot;gif&quot;:
				contentType = &quot;image/gif&quot;;
				break;
			case &quot;woff&quot;:
				contentType = &quot;font/woff&quot;;
				break;
			case &quot;woff2&quot;:
				contentType = &quot;font/woff2&quot;;
				break;
			case &quot;ttf&quot;:
				contentType = &quot;font/ttf&quot;;
				break;
			case &quot;eot&quot;:
				contentType = &quot;application/vnd.ms-fontobject&quot;;
				break;
			case &quot;otf&quot;:
				contentType = &quot;font/otf&quot;;
				break;
			case &quot;pdf&quot;:
				contentType = &quot;application/pdf&quot;;
				break;
			case &quot;ico&quot;:
				contentType = &quot;image/x-icon&quot;;
				break;
			case &quot;webp&quot;:
				contentType = &quot;image/webp&quot;;
				break;
			case &quot;html&quot;:
			default:
				contentType = &quot;text/html&quot;;
				break;
		}

		c.header(&quot;Content-Type&quot;, contentType);

		if (shouldGenerateCSP) {
			c.header(&quot;Content-Security-Policy&quot;, generateCSP());
		}

		// Handle binary vs text content appropriately
		const isBinary = contentType.startsWith(&quot;image/&quot;) || contentType.startsWith(&quot;font/&quot;);

		if (isBinary) {
			// For binary data, return the raw blob data
			const arrayBuffer = await blob.arrayBuffer();
			return c.body(arrayBuffer);
		} else {
			// For text data, apply environment variable substitution if needed
			let text = await blob.text();

			// Apply environment variable substitution if env is provided
			if (env) {
				for (const [key, value] of Object.entries(env)) {
					text = text.replaceAll(`$${key}`, value);
				}
			}

			return c.body(text);
		}
	});

	// just for logging/debugging requests
	app.use(&quot;*&quot;, async (c, next) =&gt; {
		logger.info({
			msg: &quot;Request received&quot;,
			path: c.req.path,
			method: c.req.method,
			referer: c.req.header(&quot;referer&quot;),
		});

		return await next();
	});

	return app;
}</file><file path="src/assoc.contract.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import type { AssocStoreBackend } from &quot;./assoc.ts&quot;;
import { generateRandomId } from &quot;./ids.ts&quot;;
import { beforeAll, describe, expect, test } from &quot;./test.ts&quot;;
import type { Pagination } from &quot;./types.ts&quot;;

export function asAssocStoreBackend(
	assocs: AssocStoreBackend,
	beforeHandler?: () =&gt; Promise&lt;void&gt;
) {
	if (beforeHandler) {
		beforeAll(beforeHandler);
	}

	test(&quot;returns empty for missing&quot;, async () =&gt; {
		const uid = generateRandomId(&quot;usr&quot;);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, uid, &quot;forward&quot;))
		).resolves.toMatchObject([]);
	});

	test(&quot;returns edges in forward order&quot;, async () =&gt; {
		const ent = generateRandomId(&quot;ent&quot;);
		const usr1 = generateRandomId(&quot;usr&quot;);
		const usr2 = generateRandomId(&quot;usr&quot;);
		const e1 = { uid: &quot;e1&quot;, value: 1 };
		const e2 = { uid: &quot;e2&quot;, value: 2 };
		await assocs.addAssoc(&quot;case&quot;, [ent, usr1], e1.uid, e1);
		await assocs.addAssoc(&quot;case&quot;, [ent, usr2], e2.uid, e2);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, ent, &quot;forward&quot;))
		).resolves.toMatchObject([e1, e2]);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, usr1, &quot;forward&quot;))
		).resolves.toMatchObject([e1]);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, usr2, &quot;forward&quot;))
		).resolves.toMatchObject([e2]);
	});

	test(&quot;returns events in backward order&quot;, async () =&gt; {
		const ent = generateRandomId(&quot;ent&quot;);
		const usr1 = generateRandomId(&quot;usr&quot;);
		const usr2 = generateRandomId(&quot;usr&quot;);
		const e1 = { uid: &quot;e1&quot;, value: 1 };
		const e2 = { uid: &quot;e2&quot;, value: 2 };
		await assocs.addAssoc(&quot;case&quot;, [ent, usr1], e1.uid, e1);
		await assocs.addAssoc(&quot;case&quot;, [ent, usr2], e2.uid, e2);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, ent, &quot;backward&quot;))
		).resolves.toMatchObject([e2, e1]);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, usr1, &quot;backward&quot;))
		).resolves.toMatchObject([e1]);
		await expect(
			Array.fromAsync(assocs.readAssocs(&quot;case&quot;, usr2, &quot;backward&quot;))
		).resolves.toMatchObject([e2]);
	});

	describe(&quot;paginates&quot;, async () =&gt; {
		const usr = generateRandomId(&quot;usr&quot;);
		const e1 = { uid: &quot;e1&quot;, value: 1 };
		const e2 = { uid: &quot;e2&quot;, value: 2 };
		const e3 = { uid: &quot;e3&quot;, value: 3 };
		const e4 = { uid: &quot;e4&quot;, value: 4 };
		const e5 = { uid: &quot;e5&quot;, value: 5 };

		beforeAll(async () =&gt; {
			await assocs.addAssoc(&quot;case&quot;, [usr], e1.uid, e1);
			await assocs.addAssoc(&quot;case&quot;, [usr], e2.uid, e2);
			await assocs.addAssoc(&quot;case&quot;, [usr], e3.uid, e3);
			await assocs.addAssoc(&quot;case&quot;, [usr], e4.uid, e4);
			await assocs.addAssoc(&quot;case&quot;, [usr], e5.uid, e5);
		});

		test(&quot;paginates forward&quot;, async () =&gt; {
			let cursor: string | undefined = undefined;
			let page: Pagination&lt;any&gt; | undefined = undefined;

			({ cursor, ...page } = await assocs.paginateAssocs(&quot;case&quot;, usr, 2, &quot;forward&quot;, cursor));
			expect(page).toMatchObject({ items: [e1, e2], count: 2 });
			({ cursor, ...page } = await assocs.paginateAssocs(&quot;case&quot;, usr, 2, &quot;forward&quot;, cursor));

			expect(page).toMatchObject({ items: [e3, e4], count: 2 });
			({ cursor, ...page } = await assocs.paginateAssocs(&quot;case&quot;, usr, 2, &quot;forward&quot;, cursor));

			expect(page).toMatchObject({ items: [e5], count: 1 });
		});

		test(&quot;paginates backward&quot;, async () =&gt; {
			let cursor: string | undefined = undefined;
			let page: Pagination&lt;any&gt; | undefined = undefined;

			({ cursor, ...page } = await assocs.paginateAssocs(&quot;case&quot;, usr, 2, &quot;backward&quot;, cursor));

			expect(page).toMatchObject({ items: [e5, e4], count: 2 });
			({ cursor, ...page } = await assocs.paginateAssocs(&quot;case&quot;, usr, 2, &quot;backward&quot;, cursor));

			expect(page).toMatchObject({ items: [e3, e2], count: 2 });
			({ cursor, ...page } = await assocs.paginateAssocs(&quot;case&quot;, usr, 2, &quot;backward&quot;, cursor));

			expect(page).toMatchObject({ items: [e1], count: 1 });
		});
	});
}</file><file path="src/assoc.test.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import { describe } from &quot;vitest&quot;;
import { asAssocStoreBackend } from &quot;./assoc.contract.ts&quot;;
import { InMemoryAssocStoreBackend } from &quot;./in-memory.ts&quot;;

describe(&quot;EntityStore&quot;, () =&gt; {
	const backend = new InMemoryAssocStoreBackend();

	asAssocStoreBackend(backend);
});</file><file path="src/assoc.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import { ParseError, SerializerError, timeout } from &quot;./errors.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;
import type { Schema } from &quot;./schema.ts&quot;;
import type { Serializer } from &quot;./serializer.ts&quot;;
import type { Pagination, POJO, SortDirection } from &quot;./types.ts&quot;;

// Went with Entity but could also go with &quot;EdgeStore&quot; or &quot;RelationsStore&quot;

/**
 * This table is designed with dynamodb in mind to look up related entities to a single group id
 * The group id itself represents a single entity
 */

// -- ASSOC STORE ----------------------------------------------------------------

export interface ReadAssocStoreBackend {
	readAssocs(name: string, pk: string, direction: SortDirection): AsyncGenerator&lt;POJO&gt;;
	paginateAssocs(
		name: string,
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string
	): Promise&lt;Pagination&lt;POJO&gt;&gt;;
}
export interface WriteAssocStoreBackend {
	addAssoc(name: string, pks: string[], sk: string, e: POJO): Promise&lt;void&gt;;
}

/**
 * The actual operations a backend must implement to be considered an event storage.
 * Our first implementation is DynamoDB, but we could easily swap out for another storage backend.
 */
export interface AssocStoreBackend extends ReadAssocStoreBackend, WriteAssocStoreBackend {}

export interface AssocStoreWriter&lt;E&gt; {
	saveAssoc(pks: string[], sk: string, e: E): Promise&lt;void&gt;;
}

export interface AssocStoreReader&lt;E&gt; {
	readAssocs(pk: string, direction: SortDirection): AsyncGenerator&lt;E&gt;;
	paginateAssocs(
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string
	): Promise&lt;Pagination&lt;E&gt;&gt;;
}

export interface AssocStore&lt;E&gt; extends AssocStoreReader&lt;E&gt;, AssocStoreWriter&lt;E&gt; {
	readonly schema: Schema&lt;E&gt;;
	readonly serializer: Serializer&lt;E, POJO&gt;;
}

export class AssocStore&lt;E&gt; {
	constructor(
		public readonly assocName: string,
		public readonly logger: Logger,
		private readonly backend: AssocStoreBackend,
		public readonly serializer: Serializer&lt;E, POJO&gt;,
		public readonly schema: Schema&lt;E&gt;,
		public readonly saveAssocTimeoutMs: number = 1000,
		public readonly paginateAssocsTimeoutMs: number = 2000
	) {}

	async *readAssocs(pk: string, direction: SortDirection): AsyncGenerator&lt;E&gt; {
		for await (const stored of this.backend.readAssocs(this.assocName, pk, direction)) {
			const deserialized = this.serializer.fromStore(stored);
			const parsed = this.schema.parse(deserialized);
			yield parsed;
		}
	}

	@timeout
	async paginateAssocs(
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string
	): Promise&lt;Pagination&lt;E&gt;&gt; {
		const page = await this.backend.paginateAssocs(
			this.assocName,
			pk,
			limit,
			direction,
			cursor
		);
		const items = page.items.map(this.serializer.fromStore);
		return {
			items: items.map(this.schema.parse),
			count: page.count,
			...(page.cursor ? { cursor: page.cursor } : undefined),
		};
	}

	@timeout
	async saveAssoc(pks: string[], sk: string, event: E = {} as E /** TODO */): Promise&lt;void&gt; {
		try {
			const created = this.schema.create(event);
			const storable = this.serializer.toStore(created);
			await this.backend.addAssoc(this.assocName, pks, sk, storable);
		} catch (error) {
			const msg =
				error instanceof ParseError
					? &quot;Failed to create event&quot;
					: error instanceof SerializerError
						? &quot;Failed to serialize event&quot;
						: &quot;Failed to save event&quot;;
			this.logger.error({ msg, method: &quot;save&quot;, error, pks, event });
			throw error;
		}

		this.logger.info({ msg: &quot;Entity saved&quot;, pks, event });
	}
}</file><file path="src/async.ts">export async function sleep(seconds: number) {
	return new Promise((resolve) =&gt; setTimeout(resolve, 1000));
}

export async function poll&lt;T&gt;(args: {
	intervalMs: number;
	timeoutMs: number;
	fn: () =&gt; Promise&lt;T&gt;;
}): Promise&lt;T&gt; {
	return new Promise((resolve, reject) =&gt; {
		const interval = setInterval(async () =&gt; {
			try {
				const result = await args.fn();
				clear();
				resolve(result);
			} catch (e) {
				// pass
			}
		}, args.intervalMs);
		const timer = setTimeout(() =&gt; {
			clear();
			reject(new Error(&quot;Timeout&quot;));
		}, args.timeoutMs);
		const clear = () =&gt; {
			clearInterval(interval);
			clearTimeout(timer);
		};
	});
}</file><file path="src/auth.test.ts">import { describe, expect, test } from &quot;vitest&quot;;
import { AuthorizationError, authorize } from &quot;./auth.ts&quot;;
import type { RequestVars } from &quot;./types.ts&quot;;

const ALLOW = () =&gt; true;
const DENY = () =&gt; false;

const CONTEXT: RequestVars = {
	idempotencyKey: &quot;test-idempotency-key&quot;,
	traceId: &quot;test-trade-id&quot;,
};

describe(&quot;authorize&quot;, () =&gt; {
	test(&quot;allows&quot;, () =&gt; {
		expect(() =&gt; authorize(CONTEXT, [ALLOW])).not.toThrow();
	});
	test(&quot;allows empty&quot;, () =&gt; {
		expect(() =&gt; authorize(CONTEXT, [])).not.toThrow();
	});
	test(&quot;allows undefined&quot;, () =&gt; {
		expect(() =&gt; authorize(CONTEXT, undefined)).not.toThrow();
	});
	test(&quot;denies&quot;, () =&gt; {
		expect(() =&gt; authorize(CONTEXT, [DENY])).toThrowError(AuthorizationError);
	});
	test(&quot;allows multiple&quot;, () =&gt; {
		expect(() =&gt; authorize(CONTEXT, [ALLOW, ALLOW])).not.toThrow();
	});
	test(&quot;denies multiple&quot;, () =&gt; {
		expect(() =&gt; authorize(CONTEXT, [ALLOW, DENY])).toThrowError(AuthorizationError);
	});
});</file><file path="src/auth.ts">import { PublicError } from &quot;./errors.ts&quot;;

export class AuthorizationError extends PublicError {
	constructor() {
		super(&quot;Authorization Error&quot;);
	}
}

export interface Permission&lt;TVars&gt; {
	(ctx: TVars): boolean;
}

// TODO should we validate the context here?
export function authorize&lt;TVars&gt;(context: TVars, permissions?: Permission&lt;TVars&gt;[]) {
	if (!permissions) {
		return;
	}
	for (const perm of permissions) {
		if (!perm(context)) {
			throw new AuthorizationError();
		}
	}
}</file><file path="src/blob.contract.ts">import { describe, expect, test } from &quot;./test.ts&quot;;

import { Blob, type BlobStoreBackend, type ReadonlyBlobStoreBackend, toBlob } from &quot;./blob.ts&quot;;
import { generateRandomId } from &quot;./ids.ts&quot;;

/**
 * To test readonly stores, we expect one specific key to be present:
 *
 * the-readonly-blobstore/test-key
 *
 * Which should be a blob with the text:
 *
 * This value exists just to satisfy the readonly blob contract.
 *
 * (Contrary to the read-write blobstore, we can&apos;t properly write random blobs here to read.)
 */
export function asReadonlyBlobStoreBackend(blobs: ReadonlyBlobStoreBackend) {
	describe(&quot;as ReadOnlyBlobStoreBackend&quot;, async () =&gt; {
		test(&quot;ignores missing keys&quot;, async () =&gt; {
			await expect(
				blobs.fetchBlob(&quot;not-the-readonly-blobstore/test-key&quot;)
			).resolves.toBeUndefined();
		});
		test(&quot;returns the expected key&quot;, async () =&gt; {
			const key = &quot;the-readonly-blobstore/test-key&quot;;
			const expectedValue = &quot;This value exists just to satisfy the readonly blob contract.\n&quot;;
			const blob = await blobs.fetchBlob(key);
			await expect(blob?.text()).resolves.toEqual(expectedValue);
		});
	});
}

export function asBlobStoreBackend(blobs: BlobStoreBackend) {
	// TODO add tests for mimetypes

	describe(&quot;as BlobStoreBackend&quot;, async () =&gt; {
		test(&quot;ignores missing keys&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			await expect(blobs.fetchBlob(key)).resolves.toBeUndefined();
		});

		test(&quot;returns a url&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			const blob = await toBlob(key, &quot;text/plain&quot;);
			const url = await blobs.storeBlob(key, blob);
			expect(url).toBeTypeOf(&quot;string&quot;);
			// TODO check if the url works.
		});

		test(&quot;writes and reads Blobs&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			const inputBlob = await toBlob(&quot;blob1&quot;, &quot;text/plain&quot;);
			await blobs.storeBlob(key, inputBlob);
			const blob = await blobs.fetchBlob(key);
			expect(blob).toBeInstanceOf(Blob);
			await expect(blob?.text()).resolves.toBe(&quot;blob1&quot;);
		});

		test(&quot;deletes blobs&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			const blob = await toBlob(&quot;hello&quot;, &quot;text/plain&quot;);
			await blobs.storeBlob(key, blob);
			await expect(blobs.fetchBlob(key)?.then((b) =&gt; b?.text())).resolves.toEqual(&quot;hello&quot;);
			await blobs.deleteBlob(key);
			await expect(blobs.fetchBlob(key)).resolves.toBeUndefined();
		});
	});
}</file><file path="src/blob.test.ts">import { afterEach, describe, expect, test } from &quot;vitest&quot;;
import { asBlobStoreBackend, asReadonlyBlobStoreBackend } from &quot;./blob.contract.ts&quot;;
import {
	Blob,
	blobFromArrayBuffer,
	blobFromBase64,
	blobFromBuffer,
	blobFromPlainText,
	BlobStore,
	toBlob,
	toBlobResponse,
} from &quot;./blob.ts&quot;;
import { KeyMissingError, ParseError, Timeout } from &quot;./errors.ts&quot;;
import { generateRandomId } from &quot;./ids.ts&quot;;
import { InMemoryBlobStoreBackend, InMemoryLogger } from &quot;./in-memory.ts&quot;;
import * as log from &quot;./log.ts&quot;;

describe(&quot;blobFrom&quot;, () =&gt; {
	test(&quot;blobFromPlainText&quot;, async () =&gt; {
		const blob = blobFromPlainText(&quot;hello&quot;, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});
	test(&quot;plainText toBlob&quot;, async () =&gt; {
		const blob = await toBlob(&quot;hello&quot;, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});

	test(&quot;blobFromBuffer&quot;, async () =&gt; {
		const blob = blobFromBuffer(Buffer.from(&quot;hello&quot;), &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});
	test(&quot;Buffer toBlob&quot;, async () =&gt; {
		const blob = await toBlob(Buffer.from(&quot;hello&quot;), &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});

	test(&quot;is ArrayBuffer&quot;, async () =&gt; {
		const ab = new TextEncoder().encode(&quot;hello&quot;).buffer;
		expect(ab).toBeInstanceOf(ArrayBuffer);
	});
	test(&quot;blobFromArrayBuffer&quot;, async () =&gt; {
		const ab = new TextEncoder().encode(&quot;hello&quot;).buffer;
		const blob = blobFromArrayBuffer(ab as ArrayBuffer, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});
	test(&quot;ArrayBuffer toBlob&quot;, async () =&gt; {
		const ab = new TextEncoder().encode(&quot;hello&quot;).buffer;
		const blob = await toBlob(ab as ArrayBuffer, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});

	test(&quot;Response toBlob&quot;, async () =&gt; {
		const res = new Response(&quot;hello&quot;, { headers: { &quot;Content-Type&quot;: &quot;text/plain&quot; } });
		const blob = await toBlob(res, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});

	test(&quot;blobFromBase64&quot;, async () =&gt; {
		const base64 = btoa(&quot;hello&quot;);
		const blob = blobFromBase64(base64, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.toEqual(&quot;hello&quot;);
	});
	test(&quot;Base64 toBlob results in a base64 encoded text message&quot;, async () =&gt; {
		const base64 = btoa(&quot;hello&quot;);
		const blob = await toBlob(base64, &quot;text/plain&quot;);
		await expect(blob.text()).resolves.not.toEqual(&quot;hello&quot;);
		await expect(blob.text()).resolves.toEqual(btoa(&quot;hello&quot;));
	});
});

describe(&quot;toBlobResponse&quot;, () =&gt; {
	test(&quot;works&quot;, async () =&gt; {
		const blob = await toBlob(Buffer.from(&quot;hello&quot;), &quot;text/plain&quot;);
		const response = toBlobResponse(blob);
		const responsedBlob = await response.blob();
		await expect(responsedBlob.text()).resolves.toEqual(&quot;hello&quot;);
	});
});

describe(&quot;blobs&quot;, () =&gt; {
	const logger = InMemoryLogger();
	const backend = new InMemoryBlobStoreBackend();
	const blobs = new BlobStore(logger, backend);

	afterEach(() =&gt; {
		logger.logs.length = 0;
	});

	asBlobStoreBackend(backend);

	describe(&quot;as ReadonlyBlobStore&quot;, async () =&gt; {
		const blob = await toBlob(
			&quot;This value exists just to satisfy the readonly blob contract.\n&quot;,
			&quot;text/plain&quot;
		);
		await backend.storeBlob(&quot;the-readonly-blobstore/test-key&quot;, blob);

		asReadonlyBlobStoreBackend(backend);
	});

	describe(&quot;as BlobStore&quot;, async () =&gt; {
		test(&quot;throws KeyMissingError&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			await expect(blobs.fetchBlob(key)).rejects.toThrow(KeyMissingError);
			expect(logger.logs[0]).toMatchObject({ level: log.ERROR, msg: &quot;Blob not found&quot; });
		});
		test(&quot;throws ParseError&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			await expect(blobs.storeBlob(key, undefined!, &quot;text/plain&quot;)).rejects.toThrow(
				ParseError
			);
			expect(logger.logs[0]).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to validate blob&quot;,
			});
		});
		test(&quot;throws Timeout&quot;, async () =&gt; {
			const logger = InMemoryLogger();
			const backend = new InMemoryBlobStoreBackend({ delayMs: 2 });
			const blobs = new BlobStore(logger, backend, &quot;prefix/&quot;, 1, 1, 1);
			const key = generateRandomId(&quot;test-blob&quot;);
			await expect(blobs.storeBlob(key, &quot;blob1&quot;, &quot;text/plain&quot;)).rejects.toThrow(Timeout);
			expect(logger.logs[0]).toMatchObject({
				level: log.ERROR,
				msg: &quot;Timeout&quot;,
				method: &quot;storeBlob&quot;,
			});
		});

		for (const [name, bloblike] of [
			[&quot;plainText&quot;, &quot;hello&quot;] as const,
			[&quot;buffer&quot;, Buffer.from(&quot;hello&quot;)] as const,
			[&quot;arrayBuffer&quot;, new TextEncoder().encode(&quot;hello&quot;).buffer] as const,
		]) {
			test(`writes reads and deletes ${name} blobs`, async () =&gt; {
				const key = generateRandomId(&quot;test-blob&quot;);

				await blobs.storeBlob(key, bloblike, &quot;text/plain&quot;);
				expect(logger.logs[0]).toMatchObject({ level: log.INFO, msg: &quot;Blob written&quot; });

				const blob = await blobs.fetchBlob(key);
				expect(blob).toBeInstanceOf(Blob);

				await expect(blob.text()).resolves.toBe(&quot;hello&quot;);

				await blobs.deleteBlob(key);
				expect(logger.logs[1]).toMatchObject({ level: log.INFO, msg: &quot;Blob deleted&quot; });

				await expect(blobs.fetchBlob(key)).rejects.toThrow(KeyMissingError);
			});
		}

		test(&quot;subStores work&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-blob&quot;);
			const subStore = blobs.getSubStore(&quot;sub/&quot;);
			await subStore.storeBlob(key, &quot;hello&quot;, &quot;text/plain&quot;);

			const subBlob = await subStore.fetchBlob(key);
			expect(subBlob).toBeInstanceOf(Blob);
			await expect(subBlob.text()).resolves.toBe(&quot;hello&quot;);

			const superBlob = await blobs.fetchBlob(`sub/${key}`);
			expect(superBlob).toBeInstanceOf(Blob);
			await expect(superBlob.text()).resolves.toBe(&quot;hello&quot;);
		});
	});
});</file><file path="src/blob.ts">import { Blob } from &quot;buffer&quot;;
import type { BodyInit } from &quot;node-fetch&quot;;
import { isArrayBuffer } from &quot;util/types&quot;;
import { KeyMissingError, ParseError, SerializerError, timeout } from &quot;./errors.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;

/**
 * There is some confusion about the Blob type. There is a browser version
 * and a node version. We are using the node version.
 */
export { Blob } from &quot;buffer&quot;;

/**
 * We don&apos;t have a great type story yet for readonly vs read-write stores.
 * This is a first stab. - jkz 2025-03
 */
export interface ReadOnlyBlobStore {
	/**
	 * Retrieve a blob from the storage backend.
	 *
	 * Throws:
	 * - Timeout
	 * - KeyNotFound
	 * - ParseError
	 */
	fetchBlob(key: string): Promise&lt;Blob&gt;;
}

/**
 * Blob store provides the following features:
 * - [x] Large size (let&apos;s say &quot;Megabytes&quot; for starters)
 * - [x] Bytes are streamed (in-and-out)
 * - [x] Content type is available but not enforced
 * - [ ] We support public urls that can fetch the blob
 * - [x] Keys are file-system like paths, with / separators
 *
 * Blob trade-offs
 * - Slow read latency (&gt;200ms)
 * - No schema validation
 *
 * Features we may want:
 * - Versioned values
 * - Content addressable keys
 * - Content type enforcement
 */
export interface BlobStore extends ReadOnlyBlobStore {
	/**
	 * Returns a substore, semantically equivalent to a subdirectory.
	 * The joining logic is a simple &quot;+&quot;. For directory semantics, bring your own separator.
	 */
	getSubStore(segment: string): BlobStore;

	/**
	 * Returns a URL that lets the user upload a blob directly to the storage backend.
	 */
	getSignedUrl(key: string): Promise&lt;SignedPostUrlInfo&gt;;

	/**
	 * Store a blob in the storage backend.
	 *
	 * Throws:
	 * - Timeout
	 * - ParseError
	 * - SerializationError
	 */
	// TODO indicate when to use which
	storeBlob(key: string, plainText: string, type: MIMETypelike): Promise&lt;string&gt;;
	storeBlob(key: string, arrayBuffer: ArrayBuffer, type: MIMETypelike): Promise&lt;string&gt;;
	storeBlob(key: string, buffer: Buffer, type: MIMETypelike): Promise&lt;string&gt;;
	storeBlob(key: string, blob: Blob, type: MIMETypelike): Promise&lt;string&gt;;

	/**
	 * Write a base64 string to the storage backend.
	 */
	writeBase64(key: string, base64: string, type: MIMETypelike): Promise&lt;string&gt;;

	/**
	 * Delete a blob from the storage backend. Does nothing if the blob does not exist.
	 *
	 * Throws:
	 * - Timeout
	 */
	deleteBlob(key: string): Promise&lt;void&gt;;
}

export class BlobStore implements ReadOnlyBlobStore {
	public readonly logger: Logger;
	constructor(
		logger: Logger,
		private readonly backend: BlobStoreBackend,
		public readonly prefix: string = &quot;&quot;,
		public readonly fetchBlobTimeoutMs: number = 200,
		public readonly storeBlobTimeoutMs: number = 10000,
		public readonly deleteBlobTimeoutMs: number = 200,
		public readonly getSignedUrlTimeoutMs: number = 200
	) {
		this.logger = logger.child({ cls: &quot;BlobStore&quot;, prefix });
	}

	static createReadonly(
		logger: Logger,
		backend: ReadonlyBlobStoreBackend,
		prefix?: string,
		fetchBlobTimeoutMs?: number
	): ReadOnlyBlobStore {
		return new BlobStore(
			logger,
			// We&apos;re cheating here! We&apos;re casting a readonly backend to a read-write backend.
			// But it&apos;s okay, because the returned BlobStore is readonly and should never have any write methods called.
			backend as BlobStoreBackend,
			prefix,
			fetchBlobTimeoutMs
		);
	}

	getSubStore(addition: string): BlobStore {
		return new BlobStore(
			this.logger,
			this.backend,
			this.prefix + addition,
			this.fetchBlobTimeoutMs,
			this.storeBlobTimeoutMs,
			this.deleteBlobTimeoutMs,
			this.getSignedUrlTimeoutMs
		);
	}

	@timeout
	async getSignedUrl(key: string): Promise&lt;SignedPostUrlInfo&gt; {
		return this.backend.getSignedUrl(this.prefix + key);
	}

	@timeout
	// TODO: return explicit errors here to catch more easily
	async fetchBlob(key: string): Promise&lt;Blob&gt; {
		let blob;

		try {
			blob = await this.backend.fetchBlob(this.prefix + key);
		} catch (err) {
			this.logger.error({ msg: &quot;Failed to read blob&quot;, method: &quot;readBlob&quot;, key, err });
			throw err;
		}

		if (blob === undefined) {
			this.logger.error({ msg: &quot;Blob not found&quot;, method: &quot;readBlob&quot;, key });
			throw new KeyMissingError();
		}

		try {
			validateBlob(blob);
		} catch (err) {
			this.logger.error({ msg: &quot;Failed to validate blob&quot;, method: &quot;readBlob&quot;, key, err });
			throw err;
		}

		return blob;
	}

	@timeout
	async storeBlob(key: string, value: Bloblike, type: MIMETypelike): Promise&lt;string&gt; {
		try {
			validateBloblike(value);
		} catch (err) {
			this.logger.error({
				msg: &quot;Failed to validate blob&quot;,
				method: &quot;writeBlob&quot;,
				key,
				type,
				err,
			});
			throw err;
		}

		let blob;

		try {
			blob = await toBlob(value, type);
		} catch (err) {
			this.logger.error({
				msg: &quot;Failed to convert bloblike to blob&quot;,
				method: &quot;writeBlob&quot;,
				key,
				type,
				error: err,
			});
			throw err;
		}

		let url;

		try {
			url = await this.backend.storeBlob(this.prefix + key, blob);
		} catch (err) {
			this.logger.error({
				msg: &quot;Failed to write blob&quot;,
				method: &quot;writeBlob&quot;,
				key,
				type,
				err,
			});
			throw err;
		}

		this.logger.info({ msg: &quot;Blob written&quot;, key, type, url });

		return url;
	}

	async writeBase64(key: string, base64: string, type: MIMETypelike): Promise&lt;string&gt; {
		return await this.storeBlob(key, atob(base64), type);
	}

	@timeout
	async deleteBlob(key: string): Promise&lt;void&gt; {
		try {
			await this.backend.deleteBlob(this.prefix + key);
		} catch (err) {
			this.logger.error({ msg: &quot;Failed to delete blob&quot;, method: &quot;deleteBlob&quot;, key, err });
			throw err;
		}

		this.logger.info({ msg: &quot;Blob deleted&quot;, key, method: &quot;deleteBlob&quot; });
	}
}

/**
 * String blobs are UTF-8 encoded strings
 * Buffer blobs are raw bytes.
 */
export type Bloblike = string | Blob | ReadableStream | Buffer | ArrayBufferLike | Response;

/**
 * This URL lets users upload blobs directly to the storage backend.
 */
export interface SignedPostUrlInfo {
	url: string;
	/**
	 * The fields are the form fields that the user must include in the POST request.
	 */
	fields: { [key: string]: string };
}

// GPT:
async function blobFromReadableStream(readableStream: ReadableStream, type: MIMEType) {
	// Create an empty array to store the chunks of data
	const chunks: any[] = [];

	// Get a reader from the ReadableStream
	const reader = readableStream.getReader();

	// Read the stream chunk by chunk
	while (true) {
		const { done, value } = await reader.read();

		// If done is true, we&apos;re finished reading the stream
		if (done) break;

		// Push the chunk to the array
		chunks.push(value);
	}

	// Convert the chunks to a Blob
	return new Blob(chunks, { type });
}

export interface ReadonlyBlobStoreBackend {
	fetchBlob(key: string): Promise&lt;Blob | undefined&gt;;
}

/**
 * For now, the BlobType is symmetrical between gets and sets.
 */
export interface BlobStoreBackend extends ReadonlyBlobStoreBackend {
	getSignedUrl(key: string): Promise&lt;SignedPostUrlInfo&gt;;
	fetchBlob(key: string): Promise&lt;Blob | undefined&gt;;
	storeBlob(key: string, val: Blob): Promise&lt;string&gt;;
	deleteBlob(key: string): Promise&lt;void&gt;;
}

export function toReadableStream(value: string): ReadableStream {
	return new ReadableStream({
		start(controller) {
			controller.enqueue(new TextEncoder().encode(value));
			controller.close();
		},
	});
}

// This is rudimentary mimetype handling, we don&apos;t take into account characters sets etc

export type MIMEType =
	| &quot;application/pdf&quot;
	| &quot;application/json&quot;
	| &quot;text/plain&quot;
	| &quot;application/octet-stream&quot;
	| &quot;application/xml&quot;;

export const MIME_TYPES = {
	&quot;application/octet-stream&quot;: &quot;application/octet-stream&quot;,
	&quot;application/pdf&quot;: &quot;application/pdf&quot;,
	&quot;application/json&quot;: &quot;application/json&quot;,
	&quot;text/plain&quot;: &quot;text/plain&quot;,
	&quot;application/xml&quot;: &quot;application/xml&quot;,
	pdf: &quot;application/pdf&quot;,
	json: &quot;application/json&quot;,
	txt: &quot;text/plain&quot;,
	xml: &quot;application/xml&quot;,

	// TODO proper values for these
	img: &quot;application/octet-stream&quot;,
	audio: &quot;application/octet-stream&quot;,
} as const;

export type MIMETypelike = keyof typeof MIME_TYPES;

export function blobFromPlainText(plainText: string, type: MIMEType) {
	return new Blob([plainText], { type });
}
export function blobFromBuffer(buffer: Buffer, type: MIMEType) {
	// TODO Sort out this type violation? Seems to work, despite it
	return new Blob([buffer as any], { type });
}
export function blobFromArrayBuffer(arrayBuffer: ArrayBuffer, type: MIMEType) {
	return new Blob([arrayBuffer], { type });
}
export function blobFromBase64(base64: string, type: MIMEType) {
	return blobFromBuffer(Buffer.from(base64, &quot;base64&quot;), type);
}
export async function blobFromResponse(response: Response): Promise&lt;Blob&gt; {
	// TODO get rid of the type error here, for the mismatch of Response.Blob and buffer.Blob
	return await response.blob();
}
// TODO improve test coverage for this
export async function toBlob(value: Bloblike, mimeType: MIMETypelike): Promise&lt;Blob&gt; {
	const type = MIME_TYPES[mimeType];
	if (type === undefined) {
		throw new SerializerError(`Unsupported mimetype: ${mimeType}`);
	}
	if (value instanceof Blob) {
		return value;
	} else if (value instanceof Response) {
		return blobFromResponse(value);
	} else if (value instanceof Buffer) {
		return blobFromBuffer(value, type);
	} else if (isArrayBuffer(value)) {
		return blobFromArrayBuffer(value, type);
	} else if (value instanceof ReadableStream) {
		return blobFromReadableStream(value, type);
	} else if (typeof value === &quot;string&quot;) {
		return blobFromPlainText(value, type);
	} else {
		throw new SerializerError(&quot;Unsupported bloblike type&quot;);
	}
}

export function toMIMEType(value: string): MIMEType {
	const type = MIME_TYPES[value as MIMETypelike];
	if (type === undefined) {
		throw new SerializerError(`Unsupported mimetype: ${value}`);
	}
	return type;
}

export function validateBlob(val: unknown): asserts val is Blob {
	if (!(val instanceof Blob)) {
		throw new ParseError(`not a Blob: ${val}`);
	}
}

export function validateBloblike(val: unknown): asserts val is Bloblike {
	if (
		typeof val !== &quot;string&quot; &amp;&amp;
		!(val instanceof Blob) &amp;&amp;
		!(val instanceof ReadableStream) &amp;&amp;
		!(val instanceof Buffer) &amp;&amp;
		!isArrayBuffer(val)
	) {
		throw new ParseError(`not a string, Blob, ReadableStream, or Buffer`);
	}
}

/**
 * This is a hack to get around the fact that the Response constructor
 * says it doesn&apos;t like the Node Blob we&apos;re passing, even though it
 * seems to handle it just fine.
 */
function thisBlobIsBodyInitCompatible(blob: Blob | unknown): asserts blob is BodyInit {
	if (blob instanceof Blob) {
		return;
	}
	throw new Error(`Not a Blob: ${blob}`);
}

// TODO include a file name?
export function toBlobResponse(blob: Blob): Response {
	thisBlobIsBodyInitCompatible(blob);

	return new Response(blob, {
		status: 200,
		headers: {
			&quot;Content-Type&quot;: blob.type,
		},
	});
}</file><file path="src/config.test.ts">import { describe, expect, test } from &quot;vitest&quot;;
import { getConfig, getFlags } from &quot;./config.ts&quot;;

describe(&quot;getConfig&quot;, () =&gt; {
	test(&quot;works&quot;, () =&gt; {
		const vars = {
			FOO: &quot;foo&quot;,
			BAR: &quot;bar&quot;,
		};
		expect(getConfig(vars)).toEqual(vars);
	});
	test(&quot;throws&quot;, () =&gt; {
		const vars = {
			FOO: &quot;foo&quot;,
			BAR: undefined,
		};
		expect(() =&gt; getConfig(vars)).toThrow();
	});
});

describe(&quot;getFlags&quot;, () =&gt; {
	test(&quot;works&quot;, () =&gt; {
		const flags = [&quot;FOO&quot;, &quot;BAR&quot;];
		process.env[&quot;FOO&quot;] = &quot;true&quot;;
		expect(getFlags(flags)).toEqual({ FOO: true, BAR: false });
	});
	test(&quot;throws for false&quot;, () =&gt; {
		const flags = [&quot;FOO&quot;];
		process.env[&quot;FOO&quot;] = &quot;false&quot;;
		expect(() =&gt; getFlags(flags)).toThrow();
	});
	test(&quot;throws for missing&quot;, () =&gt; {
		const flags = [&quot;FOO&quot;, &quot;BAR&quot;];
		process.env[&quot;FOO&quot;] = &quot;true&quot;;
		process.env[&quot;BAR&quot;] = &quot;false&quot;;
		expect(() =&gt; getFlags(flags)).toThrow();
	});
	test(&quot;throws for anything other than true&quot;, () =&gt; {
		const flags = [&quot;FOO&quot;, &quot;BAR&quot;];
		process.env[&quot;FOO&quot;] = &quot;true&quot;;
		process.env[&quot;BAR&quot;] = &quot;false&quot;;
		expect(() =&gt; getFlags(flags)).toThrow();
	});
});</file><file path="src/config.ts">export function getConfig&lt;K extends string&gt;(vars: { [key in K]: string | undefined }): {
	[key in K]: string;
} {
	// TODO proper errors or use zod or something off the shelf
	const entries = Object.entries(vars).map(([key, val]) =&gt; [key, process.env[key] || val]);
	const missing = entries.filter(([_, val]) =&gt; val === undefined).map(([key, _]) =&gt; key);
	if (missing.length &gt; 0) {
		throw new Error(`Missing env vars: ${missing.join(&quot;, &quot;)}`);
	}
	return Object.fromEntries(entries);
}

/**
 * All flags are false by default, and can be set to true by setting the env var to &quot;true&quot;
 * Any other value will crash the program
 */
export function getFlags&lt;K extends string&gt;(flags: K[]): { [key in K]: boolean } {
	const invalid = flags.filter(
		(key) =&gt; process.env[key] !== undefined &amp;&amp; process.env[key] !== &quot;true&quot;
	);
	if (invalid.length &gt; 0) {
		throw new Error(
			`Invalid flags: ${invalid.map((key) =&gt; `${key}=${process.env[key]}`).join(&quot;, &quot;)}`
		);
	}
	return Object.fromEntries(flags.map((key) =&gt; [key, process.env[key] === &quot;true&quot;])) as {
		[key in K]: boolean;
	};
}</file><file path="src/config.ts">export function getConfig&lt;K extends string&gt;(vars: { [key in K]: string | undefined }): {
	[key in K]: string;
} {
	// TODO proper errors or use zod or something off the shelf
	const entries = Object.entries(vars).map(([key, val]) =&gt; [key, process.env[key] || val]);
	const missing = entries.filter(([_, val]) =&gt; val === undefined).map(([key, _]) =&gt; key);
	if (missing.length &gt; 0) {
		throw new Error(`Missing env vars: ${missing.join(&quot;, &quot;)}`);
	}
	return Object.fromEntries(entries);
}

/**
 * All flags are false by default, and can be set to true by setting the env var to &quot;true&quot;
 * Any other value will crash the program
 */
export function getFlags&lt;K extends string&gt;(flags: K[]): { [key in K]: boolean } {
	const invalid = flags.filter(
		(key) =&gt; process.env[key] !== undefined &amp;&amp; process.env[key] !== &quot;true&quot;
	);
	if (invalid.length &gt; 0) {
		throw new Error(
			`Invalid flags: ${invalid.map((key) =&gt; `${key}=${process.env[key]}`).join(&quot;, &quot;)}`
		);
	}
	return Object.fromEntries(flags.map((key) =&gt; [key, process.env[key] === &quot;true&quot;])) as {
		[key in K]: boolean;
	};
}</file><file path="src/cors.test.ts">import { expect, test } from &quot;vitest&quot;;
import { allowOrigins } from &quot;./cors.ts&quot;;

test(&quot;allow origins&quot;, () =&gt; {
	expect(
		allowOrigins([&quot;https://api.anterior.app&quot;, &quot;https://*.anterior.app&quot;])(
			&quot;https://foo.anterior.app&quot;
		)
	).toBe(&quot;https://foo.anterior.app&quot;);
	expect(
		allowOrigins([&quot;https://api.anterior.app&quot;, &quot;https://bar.anterior.app&quot;])(
			&quot;https://foo.anterior.app&quot;
		)
	).toBe(&quot;https://api.anterior.app&quot;);
	expect(
		allowOrigins([&quot;https://api.anterior.app&quot;, &quot;https://.*.anterior.app&quot;])(
			&quot;http://foo.anterior.app&quot;
		)
	).toBe(&quot;https://api.anterior.app&quot;);
	expect(
		allowOrigins([&quot;https://foo.anterior.app&quot;, &quot;https://.*.anterior.bar&quot;])(
			&quot;https://foo.anterior.app&quot;
		)
	).toBe(&quot;https://foo.anterior.app&quot;);
});</file><file path="src/cors.ts">export const allowOrigins = function (allowOrigins: string[]): (o: string) =&gt; string {
	return (origin: string): string =&gt; {
		for (let i = 0; i &lt; allowOrigins.length; i++) {
			const allowedOrigin = allowOrigins[i] as string;
			if (origin === allowedOrigin) {
				return origin;
			} else {
				const i: number = allowedOrigin.indexOf(&quot;://*.&quot;);
				if (i !== -1) {
					const prefix = allowedOrigin.substring(0, i + 3);
					const suffix = allowedOrigin.substring(i + 4);
					if (origin.startsWith(prefix) &amp;&amp; origin.endsWith(suffix)) {
						return origin;
					}
				}
			}
		}
		return allowOrigins[0] as string;
	};
};</file><file path="src/errors.ts">import { Logger } from &quot;./log.ts&quot;;

export class DeliberatelyUnimplementedError extends Error {
	constructor() {
		super(&quot;This method has not been implemented yet.&quot;);
	}
}

/**
 * Public exceptions are visible to users, their messages relayed in responses.
 */
// TODO actually use this as intended or scrap it
export class PublicError extends Error {}

export class ErrorWrapper extends Error {
	constructor(cause: unknown) {
		// TODO Some kind of message?
		super(undefined, { cause });
	}

	// Might be getting ahead of myself here, but if we use them often in many places this could be
	// nice.
	static invoke&lt;T&gt;(callback: () =&gt; T): T {
		try {
			return callback();
		} catch (e) {
			throw new this(e);
		}
	}

	static async await&lt;T&gt;(promise: Promise&lt;T&gt;): Promise&lt;T&gt; {
		try {
			return await promise;
		} catch (e) {
			throw new this(e);
		}
	}
}

// TODO Some of these are user errors (400s) others are server errors (500s). We want to make that
// explicit
export class KeyMissingError extends Error {}
export class KeyAlreadyExistsError extends Error {}

type AsyncMethod&lt;T extends any[], R, This&gt; = (this: This, ...args: T) =&gt; Promise&lt;R&gt;;

/**
 * A timeout error is thrown when a promise takes longer than a certain amount of time to resolve.
 */
export class Timeout extends Error {
	constructor(
		public readonly ms: number,
		message: string
	) {
		super(message);
	}
	wrap&lt;T&gt;(promise: Promise&lt;T&gt;): Promise&lt;T&gt; {
		const timeout = new Promise&lt;never&gt;((_, reject) =&gt; {
			setTimeout(() =&gt; {
				reject(this);
			}, this.ms);
		});
		return Promise.race&lt;T&gt;([promise, timeout]);
	}
}

export function timeout&lt;
	Prop extends string,
	A extends any[],
	T,
	C extends { [K in `${Prop}TimeoutMs`]: number } &amp; { logger: Logger },
	M extends AsyncMethod&lt;A, T, C&gt;,
&gt;(target: C, prop: Prop, descriptor: TypedPropertyDescriptor&lt;M&gt;) {
	// This check failed in the vcr test running with vitest, but not with bun:test.
	// I don&apos;t know why and this will come to bite us at some point @jkz 2025-02
	if (!descriptor) {
		console.error(&quot;Timeout decorator can only be applied to methods&quot;);
	}
	const method: M = descriptor.value!;
	descriptor.value = async function (this: C, ...args: A) {
		const ms = this[`${prop}TimeoutMs`] as number;
		const logger: Logger = this.logger;
		const timeout = new Timeout(ms, `Timed out after ${ms}ms`);
		const start = +new Date();
		try {
			return await timeout.wrap(method.apply(this, args));
		} catch (err) {
			const end = +new Date();
			if (err == timeout) {
				logger.error({
					msg: &quot;Timeout&quot;,
					method: prop,
					expectedMs: ms,
					actualMs: end - start,
				});
			}
			throw err;
		}
	} as M;
}

/**
 * Thrown as a result of failed parsing. This would indicate some sort of malformed data provided by
 * the user and result in a 400 response
 *
 * NOTE These could theoretically be thrown for mismatched versions of data which wouldn&apos;t be the
 * user&apos;s fault. It is up to the parsing context to cause a 500.
 */
export class ParseError extends ErrorWrapper {}

/**
 * Thrown when both key and key[] forms are used together, which is ambiguous
 * and potentially error-prone.
 */
export class DuplicateKeyError extends ErrorWrapper {
	constructor(key: string) {
		super(`Ambiguous use of both &quot;${key}&quot; and &quot;${key}[]&quot; keys is not allowed`);
	}
}

/**
 * Thrown as a result of failed (de)serialization. This would indicate some sort of malformed data
 * in our systems that past parsing, and wouldn&apos;t be the user&apos;s fault. Should generally produce a
 * 500 response.
 */
export class SerializerError extends ErrorWrapper {}

/**
 * Thrown as a result of a failed permission check. Should generally produce a 403 response.
 */
export class Unauthorized extends ErrorWrapper {}</file><file path="src/events.contract.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import type { EventStoreBackend } from &quot;./events.ts&quot;;
import { generateRandomId } from &quot;./ids.ts&quot;;
import { beforeAll, describe, expect, test } from &quot;./test.ts&quot;;
import type { Pagination } from &quot;./types.ts&quot;;

export function asEventStoreBackend(
	events: EventStoreBackend,
	beforeHandler?: () =&gt; Promise&lt;void&gt;
) {
	if (beforeHandler) {
		beforeAll(beforeHandler);
	}

	test(&quot;returns empty for missing&quot;, async () =&gt; {
		const key = generateRandomId(&quot;test-events&quot;);
		await expect(Array.fromAsync(events.readEvents(key, &quot;forward&quot;))).resolves.toEqual([]);
	});

	test(&quot;returns events in forward order&quot;, async () =&gt; {
		const key = generateRandomId(&quot;test-events&quot;);
		const e1 = { value: 1, k: key };
		const e2 = { value: 2, k: key };

		await events.addEvent(key, e1);
		await events.addEvent(key, e2);
		await expect(Array.fromAsync(events.readEvents(key, &quot;forward&quot;))).resolves.toEqual([e1, e2]);
	});

	test(&quot;returns events in backward order&quot;, async () =&gt; {
		const key = generateRandomId(&quot;test-events&quot;);
		const e1 = { value: 1, k: key };
		const e2 = { value: 2, k: key };

		await events.addEvent(key, e1);
		await events.addEvent(key, e2);
		await expect(Array.fromAsync(events.readEvents(key, &quot;backward&quot;))).resolves.toEqual([
			e2,
			e1,
		]);
	});

	test(&quot;returns events by pk&quot;, async () =&gt; {
		const k1 = generateRandomId(&quot;test-events&quot;);
		const k2 = generateRandomId(&quot;test-events&quot;);
		const k3 = generateRandomId(&quot;test-events&quot;);
		const e1 = { value: 1, k: k1 };
		const e2 = { value: 2, k: k2 };

		await events.addEvent(k1, e1);
		await events.addEvent(k2, e2);

		await expect(Array.fromAsync(events.readEvents(k1, &quot;forward&quot;))).resolves.toEqual([e1]);
		await expect(Array.fromAsync(events.readEvents(k2, &quot;forward&quot;))).resolves.toEqual([e2]);
		await expect(Array.fromAsync(events.readEvents(k3, &quot;forward&quot;))).resolves.toEqual([]);
	});

	describe(&quot;paginates&quot;, async () =&gt; {
		const key = generateRandomId(&quot;test-events&quot;);
		const e1 = { value: 1, k: key };
		const e2 = { value: 2, k: key };
		const e3 = { value: 3, k: key };
		const e4 = { value: 4, k: key };
		const e5 = { value: 5, k: key };

		beforeAll(async () =&gt; {
			await events.addEvent(key, e1);
			await events.addEvent(key, e2);
			await events.addEvent(key, e3);
			await events.addEvent(key, e4);
			await events.addEvent(key, e5);
		});

		test(&quot;paginates forward&quot;, async () =&gt; {
			let cursor: string | undefined = undefined;
			let page: Pagination&lt;any&gt; | undefined = undefined;

			({ cursor, ...page } = await events.paginateEvents(key, 2, &quot;forward&quot;, cursor));

			expect(page).toEqual({ items: [e1, e2], count: 2 });
			({ cursor, ...page } = await events.paginateEvents(key, 2, &quot;forward&quot;, cursor));

			expect(page).toEqual({ items: [e3, e4], count: 2 });
			({ cursor, ...page } = await events.paginateEvents(key, 2, &quot;forward&quot;, cursor));

			expect(page).toEqual({ items: [e5], count: 1 });
		});

		test(&quot;paginates forward&quot;, async () =&gt; {
			let cursor: string | undefined = undefined;
			let page: Pagination&lt;any&gt; | undefined = undefined;

			({ cursor, ...page } = await events.paginateEvents(key, 2, &quot;backward&quot;, cursor));

			expect(page).toEqual({ items: [e5, e4], count: 2 });
			({ cursor, ...page } = await events.paginateEvents(key, 2, &quot;backward&quot;, cursor));

			expect(page).toEqual({ items: [e3, e2], count: 2 });
			({ cursor, ...page } = await events.paginateEvents(key, 2, &quot;backward&quot;, cursor));

			expect(page).toEqual({ items: [e1], count: 1 });
		});
	});
}</file><file path="src/events.test.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import { beforeEach, describe, expect, test } from &quot;vitest&quot;;
import { z } from &quot;zod&quot;;
import { ParseError, SerializerError, Timeout } from &quot;./errors.ts&quot;;
import { asEventStoreBackend } from &quot;./events.contract.ts&quot;;
import {
	type Event,
	type Projectors,
	AFTER_EACH_HANDLER,
	EMPTY,
	EventStore,
	EventStream,
	SCHEMA,
	toEventPayloadSchema,
	toEventSchema,
} from &quot;./events.ts&quot;;
import { generateRandomId } from &quot;./ids.ts&quot;;
import { InMemoryEventStoreBackend, InMemoryLogger } from &quot;./in-memory.ts&quot;;
import * as log from &quot;./log.ts&quot;;
import { neverSchema } from &quot;./schema.ts&quot;;
import { camelSnakeSerializer, neverSerializer } from &quot;./serializer.ts&quot;;

interface Create {
	readonly type: &quot;create&quot;;
}
interface Update {
	readonly type: &quot;update&quot;;
	readonly value: string;
}

interface State {
	value: string;
	count: number;
	createdBy: string;
}

interface Meta {
	readonly pk: string;
	readonly user: string;
	readonly createdAt: bigint;
}

type EventPayload = Create | Update;
type TestEvent = Event&lt;EventPayload, Meta&gt;;

const projectorsWithInitializer: Projectors&lt;State, EventPayload, Meta, &quot;create&quot;, TestEvent&gt; = {
	[EMPTY]: [&quot;create&quot;],
	[AFTER_EACH_HANDLER]: (state) =&gt; {
		state.count++;
	},
	[SCHEMA]: z.object({
		value: z.string(),
		count: z.number(),
		createdBy: z.string(),
	}),
	create: (e) =&gt; ({
		value: &quot;new&quot;,
		count: 1,
		createdBy: e.user,
	}),
	update: (state, { data }) =&gt; {
		state.value = data.value;
	},
};
const metaSchema = z.object({
	pk: z.string(),
	user: z.string(),
	createdAt: z.coerce.bigint(),
});

const payloadSchema = toEventPayloadSchema&lt;EventPayload&gt;({
	create: z.object({
		type: z.literal(&quot;create&quot;),
	}),
	update: z.object({
		type: z.literal(&quot;update&quot;),
		value: z.string(),
	}),
});

const projectorsWithEmptyState: Projectors&lt;State, EventPayload, Meta, null, TestEvent&gt; = {
	...projectorsWithInitializer,
	[EMPTY]: () =&gt; ({
		value: &quot;&quot;,
		count: 0,
		createdBy: &quot;&quot;,
	}),
	create: (s, m) =&gt; {
		s.value = &quot;new&quot;;
		s.count = 0;
		s.createdBy = m.user;
	},
};

// TODO zod is not the best tool for this. We need to validate, not parse the object here.
const schema = toEventSchema&lt;EventPayload, Meta, TestEvent&gt;(payloadSchema, metaSchema);

const CREATE: EventPayload = { type: &quot;create&quot; };
const UPDATE: EventPayload = { type: &quot;update&quot;, value: &quot;test&quot; };
const UPDATE2: EventPayload = { type: &quot;update&quot;, value: &quot;test2&quot; };

describe(&quot;EventStream&quot;, () =&gt; {
	const error = new Error(&quot;serialization&quot;);

	const logger = InMemoryLogger();
	const backend = new InMemoryEventStoreBackend();
	const slowBackend = new InMemoryEventStoreBackend({ delayMs: 5 });
	const corruptedBackend = new InMemoryEventStoreBackend({ corrupted: true });
	const store = new EventStore&lt;TestEvent&gt;(logger, backend, camelSnakeSerializer, schema);
	const brokenStore = new EventStore&lt;TestEvent&gt;(logger, backend, neverSerializer, schema);
	const invalidStore = new EventStore&lt;TestEvent&gt;(
		logger,
		backend,
		camelSnakeSerializer,
		neverSchema
	);
	const slowStore = new EventStore&lt;TestEvent&gt;(
		logger,
		slowBackend,
		camelSnakeSerializer,
		schema,
		1,
		1
	);
	const corruptedStore = new EventStore&lt;TestEvent&gt;(
		logger,
		corruptedBackend,
		camelSnakeSerializer,
		schema,
		1,
		1
	);

	beforeEach(() =&gt; {
		logger.logs.length = 0;
	});

	asEventStoreBackend(backend);

	function makeStream&lt;I extends &quot;create&quot; | null&gt;(
		store: EventStore&lt;TestEvent&gt;,
		projectTimeoutMs?: number
	): EventStream&lt;EventPayload, Meta, I, TestEvent&gt; {
		const pk = generateRandomId(&quot;test-event-pk&quot;);
		const createdAt = BigInt(+new Date());
		const meta: Meta = { pk, user: &quot;jkz&quot;, createdAt };
		return new EventStream(logger, &quot;duplex&quot;, store, pk, meta, projectTimeoutMs);
	}

	describe(&quot;save&quot;, () =&gt; {
		test(&quot;saves&quot;, async () =&gt; {
			const stream = makeStream(store);
			await stream.save(CREATE);
			expect(logger.logs.pop()).toMatchObject({ level: log.INFO, msg: &quot;Event saved&quot; });
			const events = await Array.fromAsync(stream.readEvents(&quot;forward&quot;));
			expect(events[0]).toMatchObject({ ...stream.meta, data: CREATE });
		});
		test(&quot;throws ParseError&quot;, async () =&gt; {
			const stream = makeStream(invalidStore);
			await expect(stream.save({} as any)).rejects.toThrowError(ParseError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to instantiate event&quot;,
			});
		});
		test(&quot;throws SerializerError&quot;, async () =&gt; {
			const stream = makeStream(brokenStore);
			await expect(stream.save(CREATE)).rejects.toThrowError(SerializerError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to serialize event&quot;,
			});
		});
		test(&quot;throws Timeout&quot;, async () =&gt; {
			const stream = makeStream(slowStore);
			await expect(stream.save(CREATE)).rejects.toThrowError(Timeout);
			expect(logger.logs.pop()).toMatchObject({ level: log.ERROR, msg: &quot;Timeout&quot; });
		});
	});

	describe(&quot;readEvents&quot;, () =&gt; {
		test(&quot;reads forward&quot;, async () =&gt; {
			const stream = makeStream(store);
			await expect(stream.save(CREATE));
			await expect(stream.save(UPDATE));
			const events = await Array.fromAsync(stream.readEvents(&quot;forward&quot;));
			const payloads = events.map((e) =&gt; e.data);
			expect(payloads).toEqual([CREATE, UPDATE]);
		});
		test(&quot;reads backward&quot;, async () =&gt; {
			const stream = makeStream(store);
			await expect(stream.save(CREATE));
			await expect(stream.save(UPDATE));
			const events = await Array.fromAsync(backend.readEvents(stream.pk, &quot;backward&quot;));
			const payloads = events.map((e) =&gt; e[&quot;data&quot;]);
			expect(payloads).toEqual([UPDATE, CREATE]);
		});
		test(&quot;throws SerializerError&quot;, async () =&gt; {
			const stream = makeStream(brokenStore);
			await backend.addEvent(stream.pk, stream.instantiateEvent(CREATE) as any);
			await expect(stream.readEvents().next()).rejects.toThrowError(SerializerError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to deserialize event&quot;,
			});
		});
		test(&quot;throws ParseError&quot;, async () =&gt; {
			const stream = makeStream(corruptedStore);
			await stream.save(CREATE);
			await expect(stream.readEvents().next()).rejects.toThrowError(ParseError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to parse event&quot;,
			});
		});
		test.skip(&quot;throws Timeout&quot;, async () =&gt; {
			// Timing out on a generator has semantic implications.
		});
	});

	describe(&quot;paginateEvents&quot;, () =&gt; {
		test(&quot;returns empty&quot;, async () =&gt; {
			const stream = makeStream(store);
			await expect(stream.paginateEvents(2, &quot;forward&quot;)).resolves.toMatchObject({
				items: [],
				count: 0,
			});
		});
		test(&quot;paginates forward&quot;, async () =&gt; {
			const stream = makeStream(store);

			await expect(stream.save(CREATE));
			await expect(stream.save(UPDATE));
			await expect(stream.save(UPDATE2));
			let page;

			page = await stream.paginateEvents(2, &quot;forward&quot;);
			expect(page.items.map((x) =&gt; x.data)).toEqual([CREATE, UPDATE]);

			page = await stream.paginateEvents(2, &quot;forward&quot;, page.cursor);
			expect(page.items.map((x) =&gt; x.data)).toEqual([UPDATE2]);
		});
		test(&quot;paginates backward&quot;, async () =&gt; {
			const stream = makeStream(store);

			await expect(stream.save(CREATE));
			await expect(stream.save(UPDATE));
			await expect(stream.save(UPDATE2));
			let page;

			page = await stream.paginateEvents(2, &quot;backward&quot;);
			expect(page.items.map((x) =&gt; x.data)).toEqual([UPDATE2, UPDATE]);

			page = await stream.paginateEvents(2, &quot;backward&quot;, page.cursor);
			expect(page.items.map((x) =&gt; x.data)).toEqual([CREATE]);
		});
		test(&quot;throws SerializationError&quot;, async () =&gt; {
			const stream = makeStream(brokenStore);
			await backend.addEvent(stream.pk, stream.instantiateEvent(CREATE) as any);
			await expect(stream.paginateEvents(1, &quot;forward&quot;)).rejects.toThrowError(SerializerError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to deserialize event&quot;,
			});
		});
		test(&quot;throws ParseError&quot;, async () =&gt; {
			const stream = makeStream(corruptedStore);
			await stream.save(CREATE);
			await expect(stream.paginateEvents(1, &quot;forward&quot;)).rejects.toThrowError(ParseError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to parse event&quot;,
			});
		});
		test(&quot;throws Timeout&quot;, async () =&gt; {
			const stream = makeStream(slowStore);
			await expect(stream.paginateEvents(1, &quot;forward&quot;)).rejects.toThrowError(Timeout);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Timed out paginating events&quot;,
			});
		});
	});

	describe(&quot;project&quot;, () =&gt; {
		// TODO I didn&apos;t test all the error cases exhaustively yet. Maybe we should, but I want
		// to get some code merged first @jkz 2024-11-07
		for (const [name, projectors] of [
			[&quot;with initializer event&quot;, projectorsWithInitializer],
			[&quot;with empty state&quot;, projectorsWithEmptyState],
		] as const) {
			describe(name, () =&gt; {
				test(&quot;projects&quot;, async () =&gt; {
					const stream = makeStream(store);

					await stream.save&lt;Create&gt;(CREATE);
					await stream.save&lt;Update&gt;(UPDATE);

					await expect(stream.project(projectors)).resolves.toMatchObject({
						value: &quot;test&quot;,
						count: 2,
						createdBy: &quot;jkz&quot;,
					});

					await stream.save&lt;Update&gt;(UPDATE2);

					await expect(stream.project(projectors)).resolves.toMatchObject({
						value: &quot;test2&quot;,
						count: 3,
						createdBy: &quot;jkz&quot;,
					});
					expect(logger.logs[0]).toMatchObject({ level: log.INFO, msg: &quot;Event saved&quot; });
				});
				test(&quot;throws ParseError&quot;, async () =&gt; {
					const stream = makeStream(corruptedStore);
					await stream.save(CREATE);
					await stream.save(UPDATE);
					await expect(stream.project(projectors)).rejects.toThrowError(ParseError);
					expect(logger.logs.pop()).toMatchObject({
						level: log.ERROR,
						msg: &quot;Failed to parse event&quot;,
					});
				});
				test(&quot;throws SerializerError&quot;, async () =&gt; {
					const stream = makeStream(brokenStore);
					await backend.addEvent(stream.pk, stream.instantiateEvent(CREATE) as any);
					await expect(stream.project(projectors)).rejects.toThrowError(SerializerError);
					expect(logger.logs.pop()).toMatchObject({
						level: log.ERROR,
						msg: &quot;Failed to deserialize event&quot;,
					});
				});
				test(&quot;throws Timeout&quot;, async () =&gt; {
					const stream = makeStream(slowStore, 1);
					await expect(stream.project(projectors)).rejects.toThrowError(Timeout);
					expect(logger.logs.pop()).toMatchObject({ level: log.ERROR, msg: &quot;Timeout&quot; });
				});
			});
		}
	});
});</file><file path="src/events.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import { z } from &quot;zod&quot;;
import { ParseError, SerializerError, Timeout, timeout } from &quot;./errors.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;
import {
	getZod,
	type Schema,
	type SchemaLike,
	toSchema,
	toUnionSchema,
	type UnionSchema,
	type UnionSchemaDefinition,
} from &quot;./schema.ts&quot;;
import type { Serializer } from &quot;./serializer.ts&quot;;
import type { Pagination, POJO, SortDirection, TupleOfUnion } from &quot;./types.ts&quot;;
// TODO For some weird reason the import below causes:
// SyntaxError: Export named &apos;SortDirection&apos; not found in module &apos;$REPO/lib/ts/lib-platform/src/types.ts&apos;.
// import { SortDirection } from &quot;./types.ts&quot;;

// -- EVENT STORE ----------------------------------------------------------------

export interface ReadEventStoreBackend {
	/**
	 * Fetch events by primrary key
	 */
	readEvents(pk: string, direction: SortDirection): AsyncGenerator&lt;POJO&gt;;
	/**
	 * Ideally a stateless pagination implementation, since the event store presents append-only semantics.
	 * That means that it _should_ be possible to guarantee this with a cursor
	 */
	paginateEvents(
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string,
		filter?: EventFilter
	): Promise&lt;Pagination&lt;POJO&gt;&gt;;
}
export interface WriteEventStoreBackend {
	/**
	 * This promise should resolve when the event has been committed to storage.
	 */
	addEvent(pk: string, e: POJO): Promise&lt;void&gt;;
}

/**
 * The actual operations a backend must implement to be considered an event storage.
 * Our first implementation is DynamoDB, but we could easily swap out for another storage backend.
 */
export interface EventStoreBackend extends ReadEventStoreBackend, WriteEventStoreBackend {}

/**
 * Event storage provides the following guarantees:
 * - Primary Key: Streams are grouped by a primary key
 * - Fire and forget: Events are written to storage and then processed asynchronously.
 * - Strict ordering: Events are processed in the order they are written.
 * - Schema validated: Events are validated against a schema before being written and when read
 *
 * And the following limitations:
 * - Event bodies are opaque to the storage layer
 * - Event bodies can be no more than 400KB (This may be lifted in the future)
 *
 * We use mostly readonly interfaces in this file to emphasise the immutability of events.
 */
export interface EventStore&lt;E&gt; {
	readonly schema: Schema&lt;E&gt;;
	readonly serializer: Serializer&lt;E, POJO&gt;;
	/**
	 * Read all events for the primary key in the given direction. (forward by default)
	 *
	 * Throws:
	 * - NoEventsFound
	 * - BackendError
	 * - Timeout (TODO)
	 */
	readEvents(pk: string, direction?: SortDirection): AsyncGenerator&lt;E&gt;;
	/**
	 * Throws:
	 * - NoEventsFound
	 * - BackendError
	 * - Timeout (TODO)
	 * - ValidationError
	 */
	saveEvent(pk: string, e: E): Promise&lt;void&gt;;

	/**
	 * Fetch a sequence of events from the event store. Since the event store is append-only,
	 * this pagination can be stateless
	 *
	 * @param limit The maximum number of events to return per page
	 * @param direction Read events &quot;forward&quot; or &quot;backward&quot; in time
	 * @param cursor An opaque string produced and interpreted by the backend to paginate events.
	 */
	paginateEvents(
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string,
		filter?: EventFilter
	): Promise&lt;Pagination&lt;E&gt;&gt;;
}

export class EventStore&lt;E&gt; {
	constructor(
		public readonly logger: Logger,
		private readonly backend: EventStoreBackend,
		public readonly serializer: Serializer&lt;E, POJO&gt;,
		public readonly schema: Schema&lt;E&gt;,
		public readonly saveEventTimeoutMs: number = 1000,
		public readonly paginateEventsTimeoutMs: number = 2000
	) {}

	async *readEvents(pk: string, direction: SortDirection): AsyncGenerator&lt;E&gt; {
		for await (const stored of this.backend.readEvents(pk, direction)) {
			const deserialized = this.serializer.fromStore(stored);
			const parsed = this.schema.parse(deserialized);
			yield parsed;
		}
	}

	@timeout
	async paginateEvents(
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string,
		filter?: EventFilter
	): Promise&lt;Pagination&lt;E&gt;&gt; {
		const page = await this.backend.paginateEvents(pk, limit, direction, cursor);
		const items = page.items.map(this.serializer.fromStore);
		return {
			items: items.map(this.schema.parse),
			count: page.count,
			...(page.cursor ? { cursor: page.cursor } : undefined),
		};
	}

	@timeout
	async saveEvent(pk: string, event: E): Promise&lt;void&gt; {
		try {
			const created = this.schema.create(event);
			const storable = this.serializer.toStore(created);
			await this.backend.addEvent(pk, storable);
		} catch (err) {
			const msg =
				err instanceof ParseError
					? &quot;Failed to create event&quot;
					: err instanceof SerializerError
						? &quot;Failed to serialize event&quot;
						: &quot;Failed to save event&quot;;
			this.logger.error({ msg, method: &quot;save&quot;, err, pk, event });
			throw err;
		}

		this.logger.info({ msg: &quot;Event saved&quot;, pk, event });
	}
}

// -- EVENT STREAM ---------------------------------------------------------------

/**
 * A reader for events with a specific primary key. When we don&apos;t have all the requisite metadata,
 * we can&apos;t write events, but we could still read them
 */
export interface EventStreamReader&lt;
	P extends Payload,
	M extends Metadata,
	I extends P[&quot;type&quot;] | null,
	E extends Event&lt;P, M&gt; = Event&lt;P, M&gt;,
&gt; {
	readonly pk: string;

	readEvents(): AsyncGenerator&lt;E&gt;;
	project&lt;R&gt;(projectors: Projectors&lt;R, P, M, I, E&gt;): Promise&lt;R&gt;;
	paginateEvents(
		limit: number,
		direction: SortDirection,
		cursor?: string
	): Promise&lt;Pagination&lt;E&gt;&gt;;
	// TODO support rolling up from existing state
	// project&lt;R&gt;(projectors: Projectors&lt;R, P&gt;, state: R | null): Promise&lt;R&gt;;
}

export interface EventStreamWriter&lt;
	P extends Payload,
	M extends Metadata,
	E extends Event&lt;P, M&gt; = Event&lt;P, M&gt;,
&gt; {
	/**
	 * Metadata for the event stream. This is added to each event.
	 */
	readonly meta: M;
	/**
	 * Store a new event in event storage.
	 */
	save&lt;T extends P&gt;(payload: T): Promise&lt;void&gt;;
}

/**
 * A two-way event stream, which can both read and write events.
 */
export interface EventStreamDuplex&lt;
	P extends Payload,
	M extends Metadata,
	I extends P[&quot;type&quot;] | null,
	E extends Event&lt;P, M&gt; = Event&lt;P, M&gt;,
&gt; extends EventStreamReader&lt;P, M, I, E&gt;,
		EventStreamWriter&lt;P, M, E&gt; {}

/**
 * An event stream models the collection of events for a single &quot;topic&quot;, indicated by a primary key.
 */
export class EventStream&lt;
		P extends Payload,
		M extends Metadata,
		I extends P[&quot;type&quot;] | null,
		E extends Event&lt;P, M&gt;,
	&gt;
	implements
		EventStreamReader&lt;P, M, I, E&gt;,
		EventStreamWriter&lt;P, M, E&gt;,
		EventStreamDuplex&lt;P, M, I, E&gt;
{
	public readonly logger: Logger;
	constructor(
		logger: Logger,
		private readonly mode: Mode,
		// TODO split into read and write store? :D We&apos;ve already discussed how
		//      we may want s3 as a &quot;cold-storage&quot;, which would function as a read-only store.
		private readonly store: EventStore&lt;E&gt;,
		/**
		 * The primary key of the event stream instance. This is used to query events.
		 */
		public readonly pk: string,
		private readonly _meta?: M,
		public readonly projectTimeoutMs: number = 5000
	) {
		this.logger = logger.child({ pk, meta: _meta, cls: &quot;EventStream&quot; });
		// TODO validate metadata?
		if (_meta === undefined) {
			switch (mode) {
				case &quot;read-only&quot;:
					break;
				case &quot;write-only&quot;:
					// TODO specific error message
					// TODO logger child on event stream
					throw new Error(&quot;[EventStream] Metadata required in write-only mode&quot;);
				case &quot;duplex&quot;:
					throw new Error(&quot;[EventStream] Metadata required in duplex mode&quot;);
				default:
					throw new Error(`[EventStream] Invalid mode: ${mode}`);
			}
		}
	}

	/**
	 * Metadata for the event stream. This is added to each event.
	 */
	// There is some ugliness in this class around the metadata.
	// We don&apos;t always have access to the metadata, but other times it is required.
	get meta(): M {
		switch (this.mode) {
			case &quot;read-only&quot;:
				throw new Error(&quot;[EventStream] Cannot access metadata in reader mode&quot;);
			case &quot;duplex&quot;:
			case &quot;write-only&quot;:
				if (this._meta === undefined) {
					throw new Error(&quot;[EventStream] Metadata not set&quot;);
				}
				return this._meta;
		}
	}

	async *readEvents(direction: SortDirection = &quot;forward&quot;): AsyncGenerator&lt;E&gt; {
		try {
			for await (const event of this.store.readEvents(this.pk, direction)) {
				yield event;
			}
		} catch (err) {
			const msg =
				err instanceof ParseError
					? &quot;Failed to parse event&quot;
					: err instanceof SerializerError
						? &quot;Failed to deserialize event&quot;
						: &quot;Failed to read events&quot;;
			this.logger.error({ msg, method: &quot;readEvents&quot;, direction, err });
			throw err;
		}
	}

	async paginateEvents(
		limit: number,
		direction: SortDirection,
		cursor?: string,
		filter?: EventFilter
	): Promise&lt;Pagination&lt;E&gt;&gt; {
		try {
			return await this.store.paginateEvents(this.pk, limit, direction, cursor);
		} catch (err) {
			const msg =
				err instanceof ParseError
					? &quot;Failed to parse event&quot;
					: err instanceof SerializerError
						? &quot;Failed to deserialize event&quot;
						: err instanceof Timeout
							? &quot;Timed out paginating events&quot;
							: &quot;Failed to paginate events&quot;;
			this.logger.error({ msg, method: &quot;paginateEvents&quot;, direction, err });
			throw err;
		}
	}

	@timeout
	async project&lt;TProjection&gt;(
		handlers: Projectors&lt;TProjection, P, M, I, E&gt;
	): Promise&lt;TProjection&gt; {
		// Since rollups are sync at the moment, we read all events first
		// I would like to see an event stream large enough before we optimize this
		const events = await Array.fromAsync(this.readEvents(&quot;forward&quot;));

		try {
			return project(handlers, null, events);
		} catch (err) {
			this.logger.error({ msg: &quot;Failed to project events&quot;, method: &quot;project&quot;, err });
			throw err;
		}
	}

	/**
	 * Creates an event but doesn&apos;t store it yet
	 *
	 * Throws:
	 * - ValidationError
	 */
	instantiateEvent&lt;T extends P&gt;(data: T): E {
		// TODO decide on our timestamp format
		// Right now we end up validating twice, but we could optimize this later
		return this.store.schema.parse({
			created_at: BigInt(+new Date()) * 1_000_000n, // TODO: PR https://github.com/cohelm/platform/pull/1890/ will fix this
			...this.meta,
			data,
		});
	}

	/**
	 * Store a new event in event storage.
	 */
	async save&lt;T extends P&gt;(payload: T): Promise&lt;void&gt; {
		let event;
		try {
			event = this.instantiateEvent(payload);
		} catch (err) {
			this.logger.error({ msg: &quot;Failed to instantiate event&quot;, method: &quot;save&quot;, err });
			throw err;
		}

		// Rely on error handling/logging in addEvent. Moved them there as the single point of entry
		await this.store.saveEvent(this.pk, event);
	}
}

// -- EVENT PROJECTION -------------------------------------------------------------

export const DEFAULT = Symbol(&quot;DEFAULT&quot;);
export const EMPTY = Symbol(&quot;EMPTY&quot;);
export const FILTER = Symbol(&quot;FILTER&quot;);
export const AFTER_EACH_HANDLER = Symbol(&quot;AFTER_EACH_HANDLER&quot;);
export const SCHEMA = Symbol(&quot;SCHEMA&quot;);

/**
 * This type helps you specify rollup handlers as a single object.
 * Each key is an event type, which is a constant string, part of the event payload.
 * Each value is a function that takes the current rollup state, the event, and the event metadata and updates the state with the event.
 * For example: { &quot;set-name-event.v1&quot;: (state, event, meta) =&gt; { state.name = event.name } }
 * It being a simple object (not a class) encourages pure handlers and composability.
 * There are a few special keys that can be used to control the behavior of the rollup:
 * - [EMPTY]:
 * 		If there is avalid empty state: this is a function that returns the empty state.
 * 		Else, this is a list of the event types that can initialize a new projection.
 * 		The schema will throw if the object is not valid after all events have been processed.
 * - [SCHEMA]: This provides runtime type safety.
 * - [DEFAULT] (optional): This is called for every event that doesn&apos;t have an explicit handler.
 * - [FILTER] (optional): This can check whether an event should be handled for a given rollup.
 * - [AFTER_EACH_HANDLER] (optional): This is called for every event after it has been handled.
 * These keys are symbols, so to access them, you need to import them from this module.
 * The symbols are a little bit extra, but they provide a clear separation between the special keys and the event types.
 */
export type Projectors&lt;
	T,
	P extends Payload,
	M extends Metadata,
	I extends P[&quot;type&quot;] | null,
	E extends Event&lt;P, M&gt; = Event&lt;P, M&gt;,
&gt; = {
	[K in Exclude&lt;P[&quot;type&quot;], I&gt;]: (
		projection: T,
		event: E &amp; { data: Extract&lt;P, { type: K }&gt; }
	) =&gt; void;
} &amp; {
	[SCHEMA]: SchemaLike&lt;T&gt;;
	[AFTER_EACH_HANDLER]?: (projection: T, event: E) =&gt; void;
	[DEFAULT]?: (projection: T, event: E) =&gt; void;
	[FILTER]?: (projection: T, event: E) =&gt; boolean;
} &amp; (I extends string
		? // If we specify an initializer event, it gets a special signature
			{ [K in I]: (event: Event&lt;Extract&lt;P, { type: K }&gt;, M&gt;) =&gt; T } &amp; {
				[EMPTY]: TupleOfUnion&lt;I&gt;;
			}
		: // Otherwise we use the EMPTY symbol
			{ [EMPTY]: () =&gt; T });

/**
 * Process a single event with the given handler map. This is where the special keys are used.
 */
export function handleEvent&lt;
	T,
	P extends Payload,
	M extends Metadata,
	I extends P[&quot;type&quot;] | null,
	E extends Event&lt;P, M&gt;,
&gt;(event: E, mapper: Projectors&lt;T, P, M, I, E&gt;, state: T): void {
	const type = event.data.type;

	// The event is a initializer event, which creates a new state object
	if (Array.isArray(mapper[EMPTY]) &amp;&amp; mapper[EMPTY].includes(type)) {
		throw new Error(`Tried to handle an initializer event ${type} on existing state`);
	}

	if (mapper[FILTER]?.(state, event)) {
		return;
	}

	// TODO again, copping out of any
	const handler = (mapper as any)[type] ?? mapper[DEFAULT];

	if (handler) {
		handler(state, event);
		mapper[AFTER_EACH_HANDLER]?.(state, event);
	}
}

/**
 * Used by the event stream class to do the actual events rollup,
 * but also available as a simple standalone function.
 */
export function project&lt;
	R,
	P extends Payload,
	M extends Metadata,
	I extends P[&quot;type&quot;] | null,
	E extends Event&lt;P, M&gt;,
&gt;(handlers: Projectors&lt;R, P, M, I, E&gt;, state: R | null, events: Generator&lt;E&gt; | E[]): R {
	const arr = Array.from(events);
	const schema = toSchema(handlers[SCHEMA]);

	// Either the projection has an empty state defined
	if (state === null &amp;&amp; typeof handlers[EMPTY] === &quot;function&quot;) {
		state = schema.create(handlers[EMPTY]());
		// Or the first event must be an initializer
	} else {
		const first = arr.shift();
		if (first === undefined) {
			throw new Error(&quot;No events to project&quot;);
		} else if (!Array.isArray(handlers[EMPTY])) {
			throw new Error(&quot;No initializer events specified in handlers&quot;);
		} else if (!handlers[EMPTY].includes(first.data.type)) {
			throw new Error(`First event &apos;${first.data.type}&apos; is not an initializer`);
		}
		// TODO didn&apos;t go down the rabbithole of typing this properly, but would be nice
		const handler = (handlers as any)[first.data.type] as any;
		// The schema create brings back type safety
		state = schema.create(handler(first));
		// TODO decide whether we want to call AFTER_EACH_HANDLER here
	}

	for (const event of arr) {
		handleEvent(event, handlers, state);
	}

	// TODO perhaps we want a debug mode where we parse every time so we can catch errors
	// as soon as they happen
	return toSchema(handlers[SCHEMA]).parse(state);
}

/**
 * Event stream can be interacted with in three modes.
 */
export type Mode = &quot;read-only&quot; | &quot;write-only&quot; | &quot;duplex&quot;;

/**
 * Event payloads have no requirements other than an eventType field. This is the discriminator for event type unions,
 * and used to route events to the correct handlers. The type is part of the payload, not of the metadata.
 */
// TODO: `eventType` instead?
export interface Payload {
	readonly type: string;
}

/**
 * Metadata contains at least a primary key. All metadata fields are added to the top level of each event
 */
// NOTE: We previously had the PK as a type parameter, but that added too much complexity
// export type Metadata&lt;PK extends string&gt; = {readonly [key in PK]: string}
export interface Metadata {}

/**
 * A full event object, with the payload in a data field and the rest at the toplevel.
 *
 * We intersect the type so we can define the metadata in isolation.
 * We _could_ use interfaces and have Event extend Metadata
 */
// TODO Do we want an eventId or no?
export type Event&lt;P extends Payload, M extends Metadata&gt; = {
	readonly created_at: bigint; // TODO `eventTime` instead? Should it be a `Date`?
	readonly data: P; // TODO: `payload` instead?
} &amp; M;

export function toEventPayloadSchema&lt;P extends Payload&gt;(
	definition: UnionSchemaDefinition&lt;&quot;type&quot;, P&gt;
): UnionSchema&lt;&quot;type&quot;, P&gt; {
	return toUnionSchema(&quot;type&quot;, definition);
}

export function toEventSchema&lt;P extends Payload, M extends Metadata, E extends Event&lt;P, M&gt;&gt;(
	payload: UnionSchema&lt;&quot;type&quot;, P&gt;,
	meta: SchemaLike&lt;M&gt;
): Schema&lt;E&gt; {
	return toSchema(
		z.intersection(
			z.object({
				created_at: z.coerce.bigint(),
				data: getZod(payload), // TODO work out how we want to compose schemas
			}),
			getZod(meta)
		)
	) as Schema&lt;E&gt;;
}

/**
 * For now we only support event[key] in [...values] filters
 */
export type EventFilter = { [K: string]: any[] };</file><file path="src/flows.test.ts">import &quot;./shims/Array.fromAsync.ts&quot;;

import { describe, expect, test } from &quot;vitest&quot;;
import { z } from &quot;zod&quot;;
import { ParseError } from &quot;./errors.ts&quot;;
import { toFlowSchema } from &quot;./flows.ts&quot;;

describe(&quot;toFlowSchema&quot;, () =&gt; {
	const paramsSchema = z.object({ flowName: z.string(), foo: z.string() });
	const flowSchema = toFlowSchema(paramsSchema);

	test(&quot;parses valid schemas&quot;, () =&gt; {
		expect(
			flowSchema.parse({
				workflow_name: &quot;bar&quot;,
				deduplication_id: &quot;dedupe&quot;,
				parameters: { flow_input: { flowName: &quot;bar&quot;, foo: &quot;baz&quot; } },
			})
		).toEqual({
			workflow_name: &quot;bar&quot;,
			deduplication_id: &quot;dedupe&quot;,
			parameters: { flow_input: { flowName: &quot;bar&quot;, foo: &quot;baz&quot; } },
		});
	});
	test(&quot;throws for mismatched flowName&quot;, () =&gt; {
		expect(() =&gt;
			flowSchema.parse({
				workflow_name: &quot;not-bar&quot;,
				deduplication_id: &quot;dedupe&quot;,
				parameters: { flow_input: { flowName: &quot;bar&quot;, foo: &quot;baz&quot; } },
			})
		).toThrowError(ParseError);
	});
});</file><file path="src/flows.ts">import { z } from &quot;zod&quot;;
import { ParseError } from &quot;./errors.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;
import type { MessageQueue } from &quot;./queue.ts&quot;;
import { getZod, type Schema, type SchemaLike, toSchema } from &quot;./schema.ts&quot;;

export interface FlowParams {
	readonly flowName: string;
}

export interface FlowEnvelope&lt;P extends FlowParams&gt; {
	// This TS code is currently only compatible with prefect. Brrr support to follow. - 2025/05.
	readonly workflow_engine: &quot;prefect&quot;;
	readonly workflow_name: string;
	readonly deduplication_id: string;
	readonly flow_run_name?: string;
	readonly parameters: {
		readonly flow_input: P;
	};
}

// TODO we are in the process of moving the workflow name into the flow params,
// so they are tightly coupled. That will also help with setting up the discriminated union
export function toFlowSchema&lt;P extends FlowParams&gt;(
	schemaLike: SchemaLike&lt;P&gt;
): Schema&lt;FlowEnvelope&lt;P&gt;&gt; {
	return toSchema(
		z
			.object({
				workflow_engine: z.enum([&quot;prefect&quot;]),
				workflow_name: z.string(),
				deduplication_id: z.string(),
				flow_run_name: z.string().optional(),
				parameters: z.object({
					flow_input: getZod(schemaLike),
				}),
			})
			.refine((x) =&gt; {
				if (!x.parameters || x.workflow_name !== x.parameters.flow_input?.flowName) {
					throw new ParseError(
						`workflow_name ${x.workflow_name} must match parameters.flowName
						&apos;${x.parameters?.flow_input?.flowName}&apos;`
					);
				}
				return x;
			})
	) as Schema&lt;FlowEnvelope&lt;P&gt;&gt;;
}

export class FlowScheduler&lt;P extends FlowParams&gt; {
	private readonly flowSchema: Schema&lt;FlowEnvelope&lt;P&gt;&gt;;
	constructor(
		private readonly logger: Logger,
		private readonly queue: MessageQueue&lt;FlowEnvelope&lt;P&gt;&gt;,
		private readonly paramsSchema: SchemaLike&lt;P&gt;
	) {
		this.flowSchema = toFlowSchema(this.paramsSchema);
	}

	async schedule&lt;TParams extends P&gt;(parameters: TParams): Promise&lt;void&gt; {
		const flowEnvelope = this.flowSchema.create({
			workflow_engine: &quot;prefect&quot;,
			workflow_name: parameters.flowName,
			flow_run_name: parameters.flowName,
			deduplication_id: `${parameters.flowName} ${new Date().toISOString()}`,
			parameters: {
				// ...parameters,
				flow_input: parameters,
			},
		});
		try {
			await this.queue.send(flowEnvelope);
		} catch (err) {
			this.logger.error({ message: &quot;Failed to schedule flow&quot;, err, flowEnvelope });
			throw err;
		}

		this.logger.info({ message: &quot;Scheduled flow&quot;, flowEnvelope });
	}
}</file><file path="src/ids.test.ts">import { expect, test } from &quot;vitest&quot;;
import { generateDeterministicId, generateRandomId } from &quot;./ids.ts&quot;;

test(&quot;generateRandomId&quot;, () =&gt; {
	expect(generateRandomId(&quot;abc&quot;)).toMatch(/abc_[a-f0-9]{10}_[a-f0-9]{10}/);
	expect(generateRandomId(&quot;abc&quot;, &quot;test-prefix.&quot;)).toMatch(
		/test-prefix\.abc_[a-f0-9]{10}_[a-f0-9]{10}/
	);
});
test(&quot;generateDeterministicId&quot;, () =&gt; {
	expect(generateDeterministicId(&quot;cse&quot;, &quot;test-hash-key&quot;)).toBe(&quot;cse_153abdb0c4_16df2a1975&quot;);
	expect(generateDeterministicId(&quot;cse&quot;, &quot;test-hash-key&quot;, &quot;test-prefix.&quot;)).toBe(
		&quot;test-prefix.cse_153abdb0c4_16df2a1975&quot;
	);
});</file><file path="src/ids.ts">import { createHash, randomBytes } from &quot;node:crypto&quot;;
import { z } from &quot;./zod.ts&quot;;

// TODO include prefix in type
export type Id&lt;T extends string = string&gt; = `${T}_${string}`;

export const idSchema = &lt;T extends string&gt;(prefix: T) =&gt;
	z.string().regex(new RegExp(`^${prefix}_[a-f0-9]{10}_[a-f0-9]{10}$`)) as z.ZodType&lt;Id&lt;T&gt;&gt;;

function toId&lt;T extends string&gt;(hex: string, typeIdentifier: T, prefix = &quot;&quot;): Id&lt;T&gt; {
	return `${prefix}${typeIdentifier}_${hex.slice(0, 10)}_${hex.slice(10, 20)}` as Id&lt;T&gt;;
}

export function generateDeterministicId&lt;T extends string&gt;(
	typeIdentifier: T,
	hashKey: string,
	prefix?: string
): Id&lt;T&gt; {
	return toId(createHash(&quot;sha256&quot;).update(hashKey).digest(&quot;hex&quot;), typeIdentifier, prefix);
}

export function generateRandomId&lt;T extends string&gt;(typeIdentifier: T, prefix?: string): Id&lt;T&gt; {
	return toId(randomBytes(20).toString(&quot;hex&quot;), typeIdentifier, prefix);
}</file><file path="src/in-memory.test.ts">import { describe } from &quot;vitest&quot;;
import { asAssocStoreBackend } from &quot;./assoc.contract.ts&quot;;
import { asBlobStoreBackend } from &quot;./blob.contract.ts&quot;;
import { asEventStoreBackend } from &quot;./events.contract.ts&quot;;
import {
	InMemoryAssocStoreBackend,
	InMemoryBlobStoreBackend,
	InMemoryEventStoreBackend,
	InMemoryKVStoreBackend,
} from &quot;./in-memory.ts&quot;;
import { asKVStoreBackend } from &quot;./kv.contract.ts&quot;;

describe(&quot;InMemoryKVStoreBackend&quot;, () =&gt; {
	const kv = new InMemoryKVStoreBackend();
	asKVStoreBackend(kv);
});

describe(&quot;InMemoryBlobStoreBackend&quot;, () =&gt; {
	const blobs = new InMemoryBlobStoreBackend();
	asBlobStoreBackend(blobs);
});

describe(&quot;InMemoryEventStoreBackend&quot;, () =&gt; {
	const events = new InMemoryEventStoreBackend();
	asEventStoreBackend(events);
});

describe(&quot;InMemoryAssocStoreBackend&quot;, () =&gt; {
	const entities = new InMemoryAssocStoreBackend();
	asAssocStoreBackend(entities);
});</file><file path="src/in-memory.ts">/**
 * These serve as reference implementations for storage interfaces that can be used in development,
 * testing and ephemeral use cases.
 */
import &quot;./shims/Array.fromAsync.ts&quot;;

import pino from &quot;pino&quot;;
import { Writable } from &quot;stream&quot;;
import type { AssocStoreBackend } from &quot;./assoc.ts&quot;;
import type { BlobStoreBackend, SignedPostUrlInfo } from &quot;./blob.ts&quot;;
import { Blob } from &quot;./blob.ts&quot;;
import type { EventFilter, EventStoreBackend, ReadEventStoreBackend } from &quot;./events.ts&quot;;
import json from &quot;./json.ts&quot;;
import type { KVStoreBackend } from &quot;./kv.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;
import { only } from &quot;./object.ts&quot;;
import type { MessageQueueBackend } from &quot;./queue.ts&quot;;
import type { Pagination, POJO, SortDirection } from &quot;./types.ts&quot;;

/**
 * We can introduce arbitrary delays in the storages to simulate network latency.
 */
export const delay = (ms: number) =&gt; new Promise((resolve) =&gt; setTimeout(resolve, ms));

export function InMemoryLogger(): Logger &amp; { logs: unknown[] } {
	const logs: unknown[] = [];
	const logger = pino(
		new Writable({
			write(chunk, encoding, callback) {
				// Push each log message into the array
				logs.push(json.parse(chunk.toString()));
				callback();
			},
		})
	);
	(logger as any).logs = logs;
	return logger as any;
}

interface SimulationOptions {
	delayMs?: number | undefined;
	fail?: Error | undefined;
	corrupted?: boolean | undefined;
}

export class InMemoryBackend {
	public readonly delayMs?: number;
	public readonly fail?: Error;
	public readonly corrupted?: boolean;
	constructor(options: SimulationOptions = {}) {
		Object.assign(this, options);
	}
	async simulate() {
		if (this.delayMs) {
			await new Promise((resolve) =&gt; setTimeout(resolve, this.delayMs));
		}
		if (this.fail) {
			throw this.fail;
		}
	}
}

export class InMemoryEventStoreBackend extends InMemoryBackend implements EventStoreBackend {
	public readonly _events = new Map&lt;string, string[]&gt;();
	public get events() {
		return this._events;
	}

	private serialize(event: POJO): string {
		const asJson = json.serialize(event);
		return this.corrupted ? `{&quot;corrupted&quot;: ${asJson}}` : asJson;
	}

	protected deserialize(event: string): POJO {
		return json.parse(event) as POJO;
	}
	async *readEvents(pk: string, direction: SortDirection): AsyncGenerator&lt;POJO&gt; {
		await this.simulate();
		const events = Array.from(this.events.get(pk) ?? []);
		if (direction === &quot;backward&quot;) {
			events.reverse();
		}
		for (const event of events) {
			yield this.deserialize(event);
		}
	}

	async addEvent(pk: string, event: POJO): Promise&lt;void&gt; {
		await this.simulate();
		let events = this.events.get(pk);
		const asJson = this.serialize(event);
		if (events === undefined) {
			this.events.set(pk, [asJson]);
		} else {
			events.push(asJson);
		}
	}

	// TODO note that this is not a real pagination implementation, it breaks when inserting while
	// paginating
	//
	// Also, this assumes the events are already in the correct order, without an sk
	async paginateEvents(
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string,
		filter?: EventFilter
	): Promise&lt;Pagination&lt;POJO&gt;&gt; {
		await this.simulate();
		let events = await Array.fromAsync(this.readEvents(pk, direction));
		if (filter) {
			events = events.filter((event) =&gt; {
				for (const [key, value] of Object.entries(filter)) {
					if (!value.includes(event[key])) {
						return false;
					}
				}
				return true;
			});
		}
		const start = cursor ? parseInt(cursor) : 0;
		const end = start + limit;
		const nextCursor = `${end}`;
		const items = events.slice(start, end).map((e) =&gt; e);
		return { items, cursor: nextCursor, count: items.length };
	}
}

export class InMemeryEventStoreIndex
	extends InMemoryEventStoreBackend
	implements ReadEventStoreBackend
{
	/**
	 * This is a hideous implementation @jkz
	 */
	public override get events() {
		const indexed = new Map&lt;string, string[]&gt;();
		// Reindex the entire event backend on the fly
		for (const events of Array.from(this.backend.events.values())) {
			for (const event of events) {
				const deserialized = this.deserialize(event);
				const index = this.getIndex(deserialized);
				if (typeof index !== &quot;string&quot;) continue;
				const existingArr = indexed.get(index);
				if (existingArr) {
					existingArr.push(event);
				} else {
					indexed.set(index, [event]);
				}
			}
		}
		return indexed;
	}

	constructor(
		private readonly backend: InMemoryEventStoreBackend,
		private readonly getIndex: (e: POJO) =&gt; string
	) {
		super(only(backend, &quot;delayMs&quot;, &quot;fail&quot;, &quot;corrupted&quot;));
	}
}

export class InMemoryAssocStoreBackend extends InMemoryBackend implements AssocStoreBackend {
	/**
	 * Map&lt;assocName, Map&lt;pk, string[]&gt;&gt;
	 */
	public readonly assocs = new Map&lt;string, Map&lt;string, string[]&gt;&gt;();

	private serialize(event: { e: POJO; sk: string }): string {
		const asJson = json.serialize(event);
		return this.corrupted ? `corrupted:${asJson}` : asJson;
	}

	private deserialize(event: string): { e: POJO; sk: string } {
		return json.parse(event) as { e: POJO; sk: string };
	}

	private getAssocs(assocName: string, pk: string, direction: SortDirection): POJO[] {
		const assocs = Array.from(this.assocs.get(assocName)?.get(pk) ?? []);

		if (direction === &quot;backward&quot;) {
			assocs.reverse();
		}

		return Array.from(assocs.map(this.deserialize).map(({ e }) =&gt; e));
	}

	async *readAssocs(
		assocName: string,
		pk: string,
		direction: SortDirection
	): AsyncGenerator&lt;POJO&gt; {
		await this.simulate();
		for (const assoc of this.getAssocs(assocName, pk, direction)) {
			yield assoc;
		}
	}
	async paginateAssocs(
		assocName: string,
		pk: string,
		limit: number,
		direction: SortDirection,
		cursor?: string
	): Promise&lt;Pagination&lt;POJO&gt;&gt; {
		await this.simulate();
		const assocs = this.getAssocs(assocName, pk, direction);
		const start = cursor ? parseInt(cursor) : 0;
		const end = start + limit;
		const nextCursor = `${end}`;
		const items = assocs.slice(start, end);
		return { items, cursor: nextCursor, count: items.length };
	}
	async addAssoc(assocName: string, pks: string[], sk: string, e: POJO = {}): Promise&lt;void&gt; {
		await this.simulate();

		let assocsByPk = this.assocs.get(assocName);
		if (!assocsByPk) {
			assocsByPk = new Map&lt;string, string[]&gt;();
			this.assocs.set(assocName, assocsByPk);
		}

		for (const pk of pks) {
			let assocs = assocsByPk.get(pk);
			if (!assocs) {
				assocs = [];
				assocsByPk.set(pk, assocs);
			}

			assocs.push(this.serialize({ sk, e }));
		}
	}
}

export class InMemoryKVStoreBackend extends InMemoryBackend implements KVStoreBackend {
	public readonly store = new Map&lt;string, string&gt;();
	private serialize(event: POJO): string {
		const asJson = json.serialize(event);
		return this.corrupted ? `corrupted:${asJson}` : asJson;
	}

	private deserialize(event: string): POJO {
		return json.parse(event) as POJO;
	}

	async get(key: string): Promise&lt;POJO | undefined&gt; {
		await this.simulate();
		const raw = this.store.get(key);
		if (raw !== undefined) {
			return this.deserialize(raw);
		}
		return;
	}

	async set(key: string, value: POJO): Promise&lt;void&gt; {
		await this.simulate();
		this.store.set(key, this.serialize(value));
	}

	async delete(key: string): Promise&lt;void&gt; {
		await this.simulate();
		this.store.delete(key);
	}
}

export class InMemoryBlobStoreBackend extends InMemoryBackend implements BlobStoreBackend {
	private readonly store = new Map&lt;string, Blob&gt;();
	constructor({ ...options }: SimulationOptions = {}) {
		super(options);
	}

	async getSignedUrl(key: string): Promise&lt;SignedPostUrlInfo&gt; {
		await this.simulate();
		throw new Error(&quot;Method not implemented.&quot;);
	}

	async fetchBlob(key: string): Promise&lt;Blob | undefined&gt; {
		await this.simulate();
		return this.store.get(key);
	}

	async storeBlob(key: string, blob: Blob): Promise&lt;string&gt; {
		await this.simulate();
		this.store.set(key, blob);
		// We probably want the concept of a bucket or folder to support something like this.
		return `blobs://TODO-In-Memory-urls/${key}`;
	}

	async deleteBlob(key: string): Promise&lt;void&gt; {
		await this.simulate();
		this.store.delete(key);
	}
}

export class InMemoryMessageQueueBackend&lt;T&gt;
	extends InMemoryBackend
	implements MessageQueueBackend&lt;T&gt;
{
	public readonly queue: T[] = [];

	async send(message: T): Promise&lt;void&gt; {
		await this.simulate();
		this.queue.push(message);
	}

	// TODO model sqs semantics? (visibility timeout, etc) Not going there because it is not clear
	// we want this abstraction here anyway (if you asked Robin)
	//
	// Right now we use it for testing/demo purposes
	receive(): T | undefined {
		return this.queue.shift();
	}
}</file><file path="src/invariant.test.ts">import { describe, expect, it } from &quot;vitest&quot;;
import invariant from &quot;./invariant.ts&quot;;

describe(&quot;invariant&quot;, () =&gt; {
	it(&quot;should throw an error if the condition is false&quot;, () =&gt; {
		expect(() =&gt; {
			invariant(false, &quot;This should throw an error&quot;);
		}).toThrowError(&quot;Invariant failed: This should throw an error&quot;);
	});

	it(&quot;should not throw an error if the condition is true&quot;, () =&gt; {
		expect(() =&gt; {
			invariant(true, &quot;This should not throw an error&quot;);
		}).not.toThrowError();
	});
});</file><file path="src/invariant.ts">// Adapted from https://github.com/alexreardon/tiny-invariant/blob/b5587cf4ba45e257bce49053c3345b6614324252/src/tiny-invariant.ts

import { logger as baseLogger, type Logger } from &quot;./log.ts&quot;;

const prefix: string = &quot;Invariant failed&quot;;

/**
 * `invariant` is used to [assert](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-3-7.html#assertion-functions) that the `condition` is [truthy](https://github.com/getify/You-Dont-Know-JS/blob/bdbe570600d4e1107d0b131787903ca1c9ec8140/up%20%26%20going/ch2.md#truthy--falsy).
 *
 * 💥 `invariant` will `throw` an `Error` if the `condition` is [falsey](https://github.com/getify/You-Dont-Know-JS/blob/bdbe570600d4e1107d0b131787903ca1c9ec8140/up%20%26%20going/ch2.md#truthy--falsy)
 *
 * @example
 *
 * ```ts
 * const value: Person | null = { name: &apos;Alex&apos; };
 * invariant(value, &apos;Expected value to be a person&apos;);
 * // type of `value`` has been narrowed to `Person`
 * ```
 */
export default function invariant(
	condition: unknown,
	/**
	 * Can provide a string, or a function that returns a string for cases where
	 * the message takes a fair amount of effort to compute
	 */
	message: string | (() =&gt; string),
	/**
	 * Can provide a record of attributes to be logged alongside the error message
	 */
	attributes?: Record&lt;string, unknown&gt; | (() =&gt; Record&lt;string, unknown&gt;),
	/**
	 * A logger to use for logging the error message
	 */
	logger?: Logger
): asserts condition {
	if (condition) {
		return;
	}
	// Condition not passed
	const providedMessage = typeof message === &quot;function&quot; ? message() : message;

	if (logger === undefined) {
		logger = baseLogger;
	}

	const errorMessage = `${prefix}: ${providedMessage}`;
	if (attributes) {
		logger.error(attributes, errorMessage);
	} else {
		logger.error(errorMessage);
	}
	throw new Error(errorMessage);
}</file><file path="src/json.test.ts">import { describe, expect, it } from &quot;vitest&quot;;
import json, { type ParseReviver } from &quot;./json.ts&quot;;

const { serialize, parse } = json;

const serializeTests: { input: any; expected: string }[] = [
	{ input: 1n, expected: &apos;&quot;1&quot;&apos; },
	{ input: 1, expected: &quot;1&quot; },
	{
		input: 1111111111111111111111111111111111n,
		expected: &apos;&quot;1111111111111111111111111111111111&quot;&apos;,
	},
	{ input: true, expected: &quot;true&quot; },
	{ input: false, expected: &quot;false&quot; },
	{ input: null, expected: &quot;null&quot; },
	{ input: undefined, expected: &quot;null&quot; },
	{ input: [], expected: &quot;[]&quot; },
	{ input: [1n], expected: &apos;[&quot;1&quot;]&apos; },
	{ input: {}, expected: &quot;{}&quot; },
	{ input: { foo: 3n }, expected: &apos;{&quot;foo&quot;:&quot;3&quot;}&apos; },
	{ input: new Map(), expected: &quot;{}&quot; },
	{ input: new Set(), expected: &quot;[]&quot; },
	{ input: { toJSON: () =&gt; &quot;foobar&quot; }, expected: &apos;&quot;foobar&quot;&apos; },
	{ input: &quot;foobar&quot;, expected: &apos;&quot;foobar&quot;&apos; },
	{ input: &quot;&quot;, expected: &apos;&quot;&quot;&apos; },
];

describe(&quot;json.serialize&quot;, () =&gt; {
	it(&quot;serializes objects as expected&quot;, () =&gt; {
		for (const { input, expected } of serializeTests) {
			expect(serialize(input)).toEqual(expected);
		}
	});

	it(&quot;serializes nested objects, too&quot;, () =&gt; {
		for (const { input, expected } of serializeTests) {
			expect(serialize({ foo: input })).toEqual(`{&quot;foo&quot;:${expected}}`);
			expect(serialize({ foo: { bar: input } })).toEqual(`{&quot;foo&quot;:{&quot;bar&quot;:${expected}}}`);
			expect(serialize([[[input]]])).toEqual(`[[[${expected}]]]`);
		}
	});
});

const parseTests: Array&lt;{
	input: string;
	expected: any;
	reviver: ParseReviver | null;
	expectError?: boolean;
}&gt; = [
	{
		input: &apos;&quot;1&quot;&apos;,
		expected: 1n,
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;BigInt&quot;,
			},
		],
	},
	{
		input: &quot;1&quot;,
		expected: 1,
		reviver: null,
	},
	{
		input: &apos;&quot;1111111111111111111111111111111111&quot;&apos;,
		expected: 1111111111111111111111111111111111n,
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;BigInt&quot;,
			},
		],
	},
	{
		input: &quot;true&quot;,
		expected: true,
		reviver: null,
	},
	{
		input: &quot;false&quot;,
		expected: false,
		reviver: null,
	},
	{
		input: &quot;null&quot;,
		expected: null,
		reviver: null,
	},
	{
		input: &apos;&quot;undefined&quot;&apos;,
		expected: undefined,
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;undefined&quot;,
			},
		],
	},
	{
		input: &quot;[]&quot;,
		expected: [],
		reviver: null,
	},
	{
		input: &apos;[&quot;1&quot;]&apos;,
		expected: [1n],
		reviver: [
			{
				key: &quot;0&quot;,
				type: &quot;BigInt&quot;,
			},
		],
	},
	{
		input: &quot;{}&quot;,
		expected: {},
		reviver: null,
	},
	{
		input: &quot;{}&quot;,
		expected: new Map(),
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;Map&quot;,
			},
		],
	},
	{
		input: &apos;{&quot;foo&quot;:3}&apos;,
		expected: new Map([[&quot;foo&quot;, 3]]),
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;Map&quot;,
			},
		],
	},
	{
		input: &quot;[]&quot;,
		expected: new Set(),
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;Set&quot;,
			},
		],
	},
	{
		input: &quot;[0,1,2]&quot;,
		expected: new Set([0, 1, 2]),
		reviver: [
			{
				key: &quot;&quot;,
				type: &quot;Set&quot;,
			},
		],
	},
	{
		input: `&quot;foobar&quot;`,
		expected: &quot;foobar&quot;,
		reviver: null,
	},
	{
		input: &apos;&quot;&quot;&apos;,
		expected: &quot;&quot;,
		reviver: null,
	},
	{
		input: &apos;{&quot;foo&quot;:&quot;3&quot;}&apos;,
		expected: { foo: 3n },
		reviver: [
			{
				key: &quot;foo&quot;,
				type: &quot;BigInt&quot;,
			},
		],
	},
	{
		input: &apos;{&quot;foo&quot;:{&quot;bar&quot;:&quot;1&quot;}}&apos;,
		expected: { foo: { bar: 1n } },
		reviver: [
			{
				key: &quot;bar&quot;,
				type: &quot;BigInt&quot;,
				fuzzyPath: true,
			},
		],
	},
	{
		input: &apos;{&quot;foo&quot;:{&quot;bar&quot;:&quot;1&quot;},&quot;buzz&quot;:{&quot;bar&quot;:&quot;2&quot;}}&apos;,
		expected: { foo: { bar: 1n }, buzz: { bar: 2n } },
		reviver: [
			{
				key: &quot;bar&quot;,
				type: &quot;BigInt&quot;,
				fuzzyPath: true,
			},
		],
	},
	{
		input: &apos;{&quot;foo&quot;:{&quot;bar&quot;:&quot;1&quot;},&quot;fizz&quot;:{&quot;buzz&quot;:&quot;2&quot;}}&apos;,
		expected: { foo: { bar: 1n }, fizz: { buzz: &quot;2&quot; } },
		reviver: [
			{
				key: &quot;bar&quot;,
				type: &quot;BigInt&quot;,
				fuzzyPath: true,
			},
			{
				key: &quot;buzz&quot;,
				type: &quot;BigInt&quot;,
				// Note that `fuzzyPath` is not provided,
				// so converting &quot;2&quot; to 2n won&apos;t work
			},
		],
	},
	// Testing that we can apply several revival transformations
	{
		input: &apos;{&quot;foo&quot;:{&quot;bar&quot;:&quot;1&quot;}}&apos;,
		expected: { foo: new Map([[&quot;bar&quot;, 1n]]) },
		reviver: [
			{
				key: &quot;foo&quot;,
				type: &quot;Map&quot;,
			},
			{
				key: &quot;bar&quot;,
				type: &quot;BigInt&quot;,
				fuzzyPath: true,
			},
		],
	},
	{
		// This is the same as above, but checking order of revivers don&apos;t matter
		input: &apos;{&quot;foo&quot;:{&quot;bar&quot;:&quot;1&quot;}}&apos;,
		expected: { foo: new Map([[&quot;bar&quot;, 1n]]) },
		reviver: [
			{
				key: &quot;bar&quot;,
				type: &quot;BigInt&quot;,
				fuzzyPath: true,
			},
			{
				key: &quot;foo&quot;,
				type: &quot;Map&quot;,
			},
		],
	},
	{
		input: &apos;{&quot;foo&quot;:[&quot;1&quot;]}&apos;,
		expected: { foo: new Set([1n]) },
		reviver: [
			{
				key: &quot;foo.0&quot;,
				type: &quot;BigInt&quot;,
			},
			{
				key: &quot;foo&quot;,
				type: &quot;Set&quot;,
			},
		],
	},
	{
		// This is the same as above, but checking order of revivers does matter
		// since it doesn&apos;t make sense to access a Set by index
		input: &apos;{&quot;foo&quot;:[&quot;1&quot;]}&apos;,
		expected: { foo: new Set([1n]) },
		reviver: [
			{
				key: &quot;foo&quot;,
				type: &quot;Set&quot;,
			},
			{
				key: &quot;foo.0&quot;,
				type: &quot;BigInt&quot;,
			},
		],
		expectError: true,
	},
];

describe(&quot;json.parse&quot;, () =&gt; {
	it(&quot;parses objects as expected&quot;, () =&gt; {
		for (const { input, expected, reviver, expectError } of parseTests) {
			if (expectError) {
				expect(() =&gt; parse(input, reviver !== null ? reviver : [])).toThrow();
			} else {
				expect(parse(input, reviver !== null ? reviver : [])).toEqual(expected);
			}
		}
	});

	it(&quot;parses nested objects, too&quot;, () =&gt; {
		for (const { input, expected, reviver, expectError } of parseTests) {
			const prependKeyReviver = (prepend: string) =&gt; {
				if (reviver === null) return [];

				return reviver.map(({ key, ...value }) =&gt; {
					// Don&apos;t prepend the key if it&apos;s a fuzzy path
					if (value.fuzzyPath) return { key, ...value };
					// Making a new variable so that multiple empty keys don&apos;t add unnecessary dots
					let prependKey = prepend;
					// We may need to add dot-separation
					if (key !== &quot;&quot;) prependKey += &quot;.&quot;;
					return { key: prependKey + key, ...value };
				});
			};

			const testCases = [
				{
					act: () =&gt; parse(`{ &quot;foo&quot;: ${input} }`, prependKeyReviver(&quot;foo&quot;)),
					expected: {
						foo: expected,
					},
				},
				{
					act: () =&gt;
						parse(`{ &quot;foo&quot;: { &quot;bar&quot;: ${input} } }`, prependKeyReviver(&quot;foo.bar&quot;)),
					expected: {
						foo: { bar: expected },
					},
				},

				{
					act: () =&gt; parse(`[[[${input}]]]`, prependKeyReviver(&quot;0.0.0&quot;)),
					expected: [[[expected]]],
				},
			];

			if (expectError) {
				for (const { act } of testCases) {
					expect(act).toThrow();
				}
			} else {
				for (const { act, expected } of testCases) {
					expect(act()).toEqual(expected);
				}
			}
		}
	});
});</file><file path="src/json.ts">import invariant from &quot;./invariant.ts&quot;;

function isPlainObject(obj: unknown): obj is Record&lt;string, unknown&gt; {
	return typeof obj === &quot;object&quot; &amp;&amp; obj !== null &amp;&amp; !Array.isArray(obj) &amp;&amp; !(obj instanceof Map);
}

function serializeRepeated&lt;T&gt;(
	open: string,
	close: string,
	iter: Iterable&lt;T&gt;,
	serializeOne: (elem: T) =&gt; string
): string {
	let s = open;
	let sep = &quot;&quot;;
	for (const e of iter) {
		s += sep + serializeOne(e);
		sep = &quot;,&quot;;
	}
	return s + close;
}

function serializeIterable(i: Iterable&lt;unknown&gt;): string {
	return serializeRepeated(&quot;[&quot;, &quot;]&quot;, i, serialize);
}

/**
 * Serialize a map-like object.
 *
 * Will throw an error if any key is not a string (which it must be according to
 * the JSON spec).
 */
function serializeMapping(entries: Iterable&lt;[string, unknown]&gt;): string {
	return serializeRepeated(&quot;{&quot;, &quot;}&quot;, entries, ([k, v]) =&gt; {
		if (typeof k !== &quot;string&quot;) {
			throw new Error(`Unexpected non-string type ${typeof k} for JSON object key`);
		}
		return JSON.stringify(k) + &quot;:&quot; + serialize(v);
	});
}

/**
 * Serialize arbitrary object to JSON on the given write stream.
 *
 * Basically a reimplementation of JSON.stringify. Why? Mostly to support some
 * custom datatypes which you absolutely cannot do with JSON.stringify, no
 * matter how hard you try: bigint, Map, and Set.
 */
function serialize(o: unknown): string {
	if (o === null || o === undefined) {
		return &quot;null&quot;;
	}
	switch (typeof o) {
		case &quot;string&quot;:
		case &quot;number&quot;:
		case &quot;boolean&quot;:
			return JSON.stringify(o);
		case &quot;bigint&quot;:
			// This is the real reason we&apos;re reimplementing JSON.stringify.
			// Making it is a string so that when parsing, we maintain the same precision
			return JSON.stringify(o.toString());
	}
	if (Array.isArray(o) || o instanceof Set) {
		return serializeIterable(o);
	}
	if (o instanceof Map) {
		// This is another reason we&apos;re doing this.
		return serializeMapping(o.entries());
	}
	// If it isn&apos;t an object by now then I don&apos;t know what it is and it&apos;s fine by
	// me if it crashes and burns.
	if (&quot;toJSON&quot; in (o as any)) {
		return serialize((o as any).toJSON());
	}
	return serializeMapping(Object.entries(o as any));
}

type ParseReviverOptions = {
	key: string;
	type: &quot;undefined&quot; | &quot;BigInt&quot; | &quot;Map&quot; | &quot;Set&quot;;
	fuzzyPath?: boolean;
};
type ParseReviver = Array&lt;ParseReviverOptions&gt;;

type Parent = Record&lt;string, unknown&gt; | unknown[] | Set&lt;unknown&gt; | Map&lt;unknown, unknown&gt;;
/**
 * Get the value at the given path in the given object.
 * Cannot give an empty path to this function.
 */
function getValueByPath(
	value: unknown,
	path: string
): { parent: Parent; key: string; value: unknown } | null {
	invariant(path !== &quot;&quot;, &quot;Path cannot be empty, handle empty path separately&quot;, { path, value });

	const parts = path.split(&quot;.&quot;);
	let current: unknown = value;
	let parent: Parent | null = null;
	let lastKey = &quot;&quot;;

	for (const [i, part] of parts.entries()) {
		// Check that we are still going to be able to access the value
		// If not, early return because this path is not applicable
		if (current === undefined || current === null) {
			return null;
		}

		// Handle array index access
		if (/^\d+$/.test(part)) {
			const index = parseInt(part, 10); // Force to base 10
			invariant(!(current instanceof Set), &quot;Cannot access array index on a Set&quot;, {
				part,
			});
			// Can&apos;t access array index on non-array
			if (!Array.isArray(current) || isNaN(index) || index &gt;= current.length) {
				return null;
			}
			// Reached the last part of the `path`, return the value found
			if (i === parts.length - 1) {
				return { parent: current, key: part, value: current[index] };
			}
			// Continue to the next part of the path
			parent = current;
			current = current[index];
			lastKey = part;
			continue;
		}

		if (current instanceof Map) {
			if (!current.has(part)) {
				return null;
			}
			if (i === parts.length - 1) {
				return { parent: current, key: part, value: current.get(part) };
			}
			// Continue to the next part of the path
			parent = current;
			current = current.get(part);
			lastKey = part;
			continue;
		}

		// Can&apos;t access object property on non-object
		if (!isPlainObject(current) || !(part in current)) {
			return null;
		}
		// Reached the last part of the `path`, return the value found
		if (i === parts.length - 1) {
			return { parent: current, key: part, value: current[part] };
		}
		// Continue to the next part of the path
		parent = current;
		current = current[part];
		lastKey = part;
		continue;
	}

	invariant(parent !== null, &quot;Unhandled edge case where path is not in value&quot;, { path, value });
	return { parent: parent, key: lastKey, value: current };
}

/**
 * The actual transformation to the expected type.
 */
function transformValue(value: unknown, parseType: ParseReviverOptions[&quot;type&quot;]) {
	if (parseType === &quot;undefined&quot;) {
		return undefined;
	}

	if (parseType === &quot;BigInt&quot; &amp;&amp; typeof value === &quot;string&quot;) {
		return BigInt(value);
	}

	if (parseType === &quot;Map&quot; &amp;&amp; isPlainObject(value)) {
		return new Map(Object.entries(value));
	}

	if (parseType === &quot;Set&quot; &amp;&amp; Array.isArray(value)) {
		return new Set(value);
	}

	return value;
}

/**
 * Parse arbitrary JSON string to JS objects / primitives.
 *
 * Basically a reimplementation of JSON.parse. Why? Mostly to support some
 * custom datatypes which you absolutely cannot do with JSON.parse, no
 * matter how hard you try: bigint, Map, and Set.
 *
 * The chosen API for reviving is different from the built-in reviver method exposed by JSON.parse.
 * The correspondng `serialize` method does NOT save any metadata about which fields are BigInts, Maps, or Sets.
 * This means that just calling `parse` without any additional metadata cannot restore the original object.
 *
 * So, when calling `parse` when the original object contains BigInts, Maps, or Sets, you need to pass the metadata as well.
 * You may not care where a certain property is located, and in those cases, you can set `fuzzyPath` to true.
 * The order of revivers can matter, especially because you can&apos;t access a Set by index.
 *
 * @example
 * ```ts
 * json.parse(&apos;&quot;{&quot;foo&quot;: { &quot;createdAt&quot;: &quot;1732104382713939039&quot; } }&quot;&apos;, [{ &quot;key&quot;: &quot;createdAt&quot;, &quot;type&quot;: &quot;BigInt&quot;, &quot;fuzzyPath&quot;: true }])
 *
 * json.parse(&apos;&quot;{&quot;foo&quot;: { &quot;createdAt&quot;: &quot;1732104382713939039&quot; } }&quot;&apos;, [{ &quot;key&quot;: &quot;foo.createdAt&quot;, &quot;type&quot;: &quot;BigInt&quot; }])
 *
 * json.parse(&apos;&quot;{&quot;fizz&quot;: &quot;buzz&quot; }&quot;&apos;, [{ &quot;key&quot;: &quot;&quot;, &quot;type&quot;: &quot;Map&quot; }])
 * ```
 */
function parse(s: string, parseReviver: ParseReviver = []): unknown {
	return JSON.parse(s, (key: string, value: unknown) =&gt; {
		for (const { key: parseKey, ...parseOptions } of parseReviver) {
			// Handle fuzzy path matching
			if (parseOptions?.fuzzyPath &amp;&amp; key === parseKey) {
				return transformValue(value, parseOptions.type);
			}
			// We&apos;re asked to do exact path matching, for which we need the entire parsed JS object
			// JSON.parse callback only gives the entire parsed JS object if the key is &quot;&quot;
			else if (!parseOptions?.fuzzyPath &amp;&amp; key === &quot;&quot;) {
				// If the value to change is the root object, transform it directly
				if (parseKey === &quot;&quot;) {
					return transformValue(value, parseOptions.type);
				}

				const pathResult = getValueByPath(value, parseKey);
				// The path doesn&apos;t exist in this value, so skip this parseKey
				if (!pathResult) continue;

				const transformedValue = transformValue(pathResult.value, parseOptions.type);

				// Update the value in the original object if there is a parent
				if (Array.isArray(pathResult.parent)) {
					const index = parseInt(pathResult.key, 10); // Force to base 10
					invariant(!isNaN(index), &quot;Index is not a number&quot;, {
						index,
						originalIndex: pathResult.key,
					});
					invariant(
						index &lt; pathResult.parent.length,
						&quot;Index out of bounds of parent array&quot;,
						{ parent: pathResult.parent, index, originalIndex: pathResult.key }
					);
					pathResult.parent[index] = transformedValue;
				} else if (pathResult.parent instanceof Map) {
					pathResult.parent.set(pathResult.key, transformedValue);
				} else if (isPlainObject(pathResult.parent)) {
					pathResult.parent[pathResult.key] = transformedValue;
				} else if (pathResult.parent instanceof Set) {
					pathResult.parent.delete(pathResult.value);
					pathResult.parent.add(transformedValue);
				}
			}
		}

		return value;
	});
}

export type { ParseReviver };
export default {
	serialize,
	parse,
};</file><file path="src/jwt.ts">import { appApiV1RefreshToken200Response } from &quot;@anterior/openapi/client/version1/version1.zod&quot;;
import type { Context as HonoContext, Next } from &quot;hono&quot;;
import { HTTPException } from &quot;hono/http-exception&quot;;
import { jwt, verify } from &quot;hono/jwt&quot;;
import { Unauthorized } from &quot;./errors.ts&quot;;
import { logger } from &quot;./log.ts&quot;;

export type JWTPayload = {
	entr: Record&lt;string, string&gt;; // &lt;enterpriseUid, role&gt;
	wksp: Record&lt;string, string&gt;; // &lt;workspaceUid, enterpriseUid&gt;
	sub: string;
};

export interface JWTConfig {
	tokenRefreshEndpoint: string;
	jwtSecret: string;
	redirectUrl?: string;
}

/**
 * Adds a jwtPayload to the context.
 */
export function jwtMiddleware(config: JWTConfig) {
	return async function (c: HonoContext, next: Next) {
		const honoJwtMiddleware = jwt({
			secret: config.jwtSecret,
			cookie: &quot;access_token&quot;,
		});
		try {
			return await honoJwtMiddleware(c, next);
		} catch (e) {
			if (e instanceof HTTPException &amp;&amp; e.status === 401) {
				// Refresh the token by calling API.
				const reqCookie = c.req.header(&quot;Cookie&quot;);
				if (reqCookie) {
					await fetch(config.tokenRefreshEndpoint, {
						method: &quot;POST&quot;,
						headers: new Headers({ Cookie: reqCookie }),
					})
						.then((rsp) =&gt; {
							const setCkRspHdr = rsp.headers.get(&quot;Set-Cookie&quot;);
							if (setCkRspHdr) {
								c.res.headers.append(&quot;Set-Cookie&quot;, setCkRspHdr);
							}
							return rsp.json();
						})
						.then((data) =&gt; appApiV1RefreshToken200Response.parse(data))
						.then((tok) =&gt; verify(tok.access_token, config.jwtSecret))
						.then((payload) =&gt; c.set(&quot;jwtPayload&quot;, payload))
						.catch((e) =&gt; {
							logger.error(e, &quot;failed to refresh token&quot;);
							if (config.redirectUrl) {
								return c.redirect(config.redirectUrl);
							}
							throw new Unauthorized(&quot;failed to refresh token, unauthorized&quot;);
						});
				} else {
					// Respond with unauthorized if no cookies are present
					if (config.redirectUrl) {
						return c.redirect(config.redirectUrl);
					}
					throw new Unauthorized(&quot;not unauthorized&quot;);
				}
				await next();
			} else {
				throw e;
			}
		}
	};
}</file><file path="src/kv.contract.ts">import { beforeAll, describe, expect, test } from &quot;./test.ts&quot;;

import { generateRandomId } from &quot;./ids.ts&quot;;
import type { KVStoreBackend } from &quot;./kv.ts&quot;;

export function asKVStoreBackend(kv: KVStoreBackend, beforeHandler?: () =&gt; Promise&lt;void&gt;) {
	describe(&quot;as KVStoreBackend&quot;, () =&gt; {
		if (beforeHandler) {
			beforeAll(beforeHandler);
		}

		test(&quot;returns empty for missing&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-kv&quot;);
			await expect(kv.get(key)).resolves.toBeUndefined();
		});

		test(&quot;returns values&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-kv&quot;);
			const value = { value: 1, k: key };
			await kv.set(key, value);
			await expect(kv.get(key)).resolves.toEqual(value);
		});

		test(&quot;deletes values&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-kv&quot;);
			const value = { value: 1, k: key };
			await kv.set(key, value);
			await expect(kv.get(key)).resolves.toEqual(value);
			await kv.delete(key);
			await expect(kv.get(key)).resolves.toBeUndefined();
		});

		test(&quot;overwrites values&quot;, async () =&gt; {
			const key = generateRandomId(&quot;test-kv&quot;);
			const value1 = { value: 1, k: key };
			const value2 = { value: 2, k: key };
			await kv.set(key, value1);
			await kv.set(key, value2);
			await expect(kv.get(key)).resolves.toEqual(value2);
		});
	});
}</file><file path="src/kv.test.ts">import { beforeEach, describe, expect, test } from &quot;vitest&quot;;
import { z } from &quot;zod&quot;;
import { KeyMissingError, ParseError, SerializerError, Timeout } from &quot;./errors.ts&quot;;
import { generateRandomId } from &quot;./ids.ts&quot;;
import { InMemoryKVStoreBackend, InMemoryLogger } from &quot;./in-memory.ts&quot;;
import { asKVStoreBackend } from &quot;./kv.contract.ts&quot;;
import { KVStore } from &quot;./kv.ts&quot;;
import * as log from &quot;./log.ts&quot;;
import { neverSchema, type Schema, toSchema } from &quot;./schema.ts&quot;;
import { neverSerializer, typedIdentitySerializer } from &quot;./serializer.ts&quot;;

interface KVType {
	readonly value: string;
}
const schema = toSchema(z.object({ value: z.string() })) satisfies Schema&lt;KVType&gt;;

describe(&quot;KVStore&quot;, () =&gt; {
	const fail = new Error(&quot;FAIL&quot;);
	const logger = InMemoryLogger();
	const backend = new InMemoryKVStoreBackend();
	const failBackend = new InMemoryKVStoreBackend({ fail });
	const slowBackend = new InMemoryKVStoreBackend({ delayMs: 10 });
	const serializer = typedIdentitySerializer&lt;KVType&gt;();
	const slowKV = new KVStore&lt;KVType&gt;(logger, slowBackend, serializer, schema, 1, 1, 1);
	const kv = new KVStore(logger, backend, serializer, schema);
	const corruptedKV = new KVStore(logger, backend, neverSerializer, schema);
	const invalidKV = new KVStore(logger, backend, serializer, neverSchema);
	const failKV = new KVStore(logger, failBackend, serializer, schema);
	const v1 = { value: &quot;v1&quot; };
	let key: string;

	beforeEach(() =&gt; {
		key = generateRandomId(&quot;test-kv&quot;);
		logger.logs.length = 0;
	});

	asKVStoreBackend(backend);

	describe(&quot;get&quot;, () =&gt; {
		test(&quot;reads&quot;, async () =&gt; {
			backend.set(key, v1);
			await expect(kv.get(key)).resolves.toMatchObject(v1);
		});
		test(&quot;throws KeyMissingError&quot;, async () =&gt; {
			await expect(kv.get(key)).rejects.toThrowError(KeyMissingError);
			expect(logger.logs.pop()).toMatchObject({ level: log.ERROR, msg: &quot;Key missing&quot; });
		});
		test(&quot;throws SerializerError&quot;, async () =&gt; {
			await backend.set(key, v1);
			await expect(corruptedKV.get(key)).rejects.toThrow(SerializerError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to deserialize value&quot;,
			});
		});
		test(&quot;throws Timeout&quot;, async () =&gt; {
			await backend.set(key, v1);
			await expect(slowKV.get(key)).rejects.toThrow(Timeout);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Timeout&quot;,
				method: &quot;get&quot;,
			});
		});
		test(&quot;throws ParseError&quot;, async () =&gt; {
			await backend.set(key, v1);
			await expect(invalidKV.get(key)).rejects.toThrow(ParseError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to parse value&quot;,
				method: &quot;get&quot;,
			});
		});
		test(&quot;throws Errors&quot;, async () =&gt; {
			await expect(failKV.get(key)).rejects.toThrow(fail);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to read value&quot;,
				method: &quot;get&quot;,
			});
		});
	});

	describe(&quot;set&quot;, () =&gt; {
		test(&quot;writes&quot;, async () =&gt; {
			await expect(kv.set(key, v1)).resolves.toBeUndefined();
			expect(logger.logs.pop()).toMatchObject({ level: log.INFO, msg: &quot;Key written&quot;, key });
		});
		test(&quot;throws SerializerError&quot;, async () =&gt; {
			await expect(corruptedKV.set(key, v1)).rejects.toThrow(SerializerError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to serialize value&quot;,
			});
		});
		test(&quot;throws Timeout&quot;, async () =&gt; {
			await expect(slowKV.set(key, v1)).rejects.toThrow(Timeout);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Timeout&quot;,
				method: &quot;set&quot;,
			});
		});
		test(&quot;throws ParseError&quot;, async () =&gt; {
			await expect(invalidKV.set(key, v1)).rejects.toThrow(ParseError);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to parse value&quot;,
				method: &quot;set&quot;,
			});
		});
		test(&quot;throws errors&quot;, async () =&gt; {
			await expect(failKV.set(key, v1)).rejects.toThrow(fail);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to write value&quot;,
				method: &quot;set&quot;,
			});
		});
	});

	describe(&quot;delete&quot;, () =&gt; {
		test(&quot;deletes&quot;, async () =&gt; {
			await backend.set(key, v1);
			await expect(kv.delete(key)).resolves.toBeUndefined();
			expect(logger.logs.pop()).toMatchObject({ level: log.INFO, msg: &quot;Key deleted&quot;, key });
		});
		test(&quot;delete&quot;, async () =&gt; {
			await backend.set(key, v1);
			await expect(slowKV.delete(key)).rejects.toThrow(Timeout);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Timeout&quot;,
				method: &quot;delete&quot;,
			});
		});
		test(&quot;delete&quot;, async () =&gt; {
			await expect(failKV.delete(key)).rejects.toThrow(fail);
			expect(logger.logs.pop()).toMatchObject({
				level: log.ERROR,
				msg: &quot;Failed to delete value&quot;,
				method: &quot;delete&quot;,
			});
		});
	});
	test(&quot;reads, writes, overwrites and deletes&quot;, async () =&gt; {
		const v1 = { value: &quot;v1&quot; };

		await kv.set(key, v1);
		expect(logger.logs.pop()).toMatchObject({ level: log.INFO, msg: &quot;Key written&quot;, key });

		await expect(kv.get(key)).resolves.toEqual(v1);

		const v2 = { value: &quot;v2&quot; };

		await kv.set(key, v2);
		expect(logger.logs.pop()).toMatchObject({ level: log.INFO, msg: &quot;Key written&quot;, key });

		await expect(kv.get(key)).resolves.toEqual(v2);

		await kv.delete(key);
		expect(logger.logs.pop()).toMatchObject({ level: log.INFO, msg: &quot;Key deleted&quot;, key });

		await expect(kv.get(key)).rejects.toThrowError(KeyMissingError);
	});
});</file><file path="src/kv.ts">import { KeyMissingError, ParseError, SerializerError, timeout } from &quot;./errors.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;
import type { Schema } from &quot;./schema.ts&quot;;
import type { Serializer } from &quot;./serializer.ts&quot;;
import type { POJO } from &quot;./types.ts&quot;;

/**
 * A Key Value store provides a simple key-value abstraction.
 * It provides the following guarantees:
 * - Schema validated: Values are validated against
 * - Stores valid json objects
 *
 * And the following defaults. We can figure out what the right numbers should be for these.
 * - Get Timeout: 50ms
 * - Set Timeout: 100ms
 *
 * Features we may want:
 * - Immutable values
 * - Versioned values
 *
 * The KV provides validation and proper error signaling.
 * To make this possible, every KV needs to be instantiated with its own validator.
 * We can do this be several ways:
 * - Writing them by hand and exposing them in libraries
 * - Generating them based on schemas
 * - Using TypeScript&apos;s type system and don&apos;t actually validate behind the scenes
 *
 * Deletes are currently omitted by the interface. We can discuss adding them
 * if a production use case arises.
 */
export interface KVStore&lt;T&gt; {
	readonly schema: Schema&lt;T&gt;;
	readonly serializer: Serializer&lt;T, POJO&gt;;
	/**
	 * Throws:
	 * - KeyMissingError
	 * - Timeout
	 * - BackendError
	 * - ParseError
	 */
	get(key: string): Promise&lt;T&gt;;
	/**
	 * Throws:
	 * - Timeout
	 * - BackendError
	 * - ParseError
	 */
	set(key: string, val: T): Promise&lt;void&gt;;
}

export class KVStore&lt;T&gt; {
	constructor(
		public readonly logger: Logger,
		private readonly backend: KVStoreBackend,
		public readonly serializer: Serializer&lt;T, POJO&gt;,
		public readonly schema: Schema&lt;T&gt;,
		public getTimeoutMs: number = 50,
		public setTimeoutMs: number = 100,
		public deleteTimeoutMs: number = 50
	) {}

	@timeout
	async get(key: string): Promise&lt;T&gt; {
		try {
			const stored = await this.backend.get(key);
			if (stored === undefined) {
				throw new KeyMissingError();
			}
			const deserialized = this.serializer.fromStore(stored);
			const parsed = this.schema.parse(deserialized);
			return parsed;
		} catch (err) {
			const msg =
				err instanceof SerializerError
					? &quot;Failed to deserialize value&quot;
					: err instanceof KeyMissingError
						? &quot;Key missing&quot;
						: err instanceof ParseError
							? &quot;Failed to parse value&quot;
							: &quot;Failed to read value&quot;;

			this.logger.error({ msg, method: &quot;get&quot;, key, err });
			throw err;
		}
	}

	@timeout
	async set(key: string, value: T): Promise&lt;void&gt; {
		try {
			const parsed = this.schema.parse(value);
			const serialized = this.serializer.toStore(parsed);
			await this.backend.set(key, serialized);
		} catch (err) {
			const msg =
				err instanceof SerializerError
					? &quot;Failed to serialize value&quot;
					: err instanceof ParseError
						? &quot;Failed to parse value&quot;
						: &quot;Failed to write value&quot;;
			this.logger.error({ msg, method: &quot;set&quot;, key, err });
			throw err;
		}

		this.logger.info({ msg: &quot;Key written&quot;, method: &quot;set&quot;, key });
	}

	@timeout
	async delete(key: string): Promise&lt;void&gt; {
		try {
			await this.backend.delete(key);
		} catch (err) {
			this.logger.error({ msg: &quot;Failed to delete value&quot;, method: &quot;delete&quot;, key, err });
			throw err;
		}

		this.logger.info({ msg: &quot;Key deleted&quot;, method: &quot;delete&quot;, key });
	}
}

/**
 * The Backend provides the raw infra abstraction for the KV.
 * Setting it up this way separates the internal interface, using KV in our codebase,
 * from the infra implemenations, so we can swap out backends without touching any business code.
 *
 * One difference is that the KVStore backend does no validation, while the KV backend does.
 */
export interface KVStoreBackend {
	get(key: string): Promise&lt;POJO | undefined&gt;;
	set(key: string, val: POJO): Promise&lt;void&gt;;
	delete(key: string): Promise&lt;void&gt;;
}</file><file path="src/log.ts">import pino from &quot;pino&quot;;

/**
 * The logger is not an actual service, but a standardised way to print to stdout.
 * The service containers ingest these logs and forward them to the appropriate log management system.
 *
 * Please add loggers everywhere, including children on class instances to add additional context
 */
export const logger = pino();
export type Logger = typeof logger;

export function Logger&lt;T extends Record&lt;string, any&gt;&gt;(ctx: T): pino.Logger {
	return logger.child(ctx);
}

export type LogLevel = &quot;trace&quot; | &quot;debug&quot; | &quot;info&quot; | &quot;warn&quot; | &quot;error&quot; | &quot;fatal&quot;;

export const ERROR = 50;
export const INFO = 30;</file><file path="src/object.test.ts">import { describe, expect, test } from &quot;vitest&quot;;

import { DuplicateKeyError } from &quot;./errors.ts&quot;;

import {
	camelCase,
	fromEach,
	fromEachArrayKey,
	only,
	recursiveCamelCase,
	recursiveSnakeCase,
	snakeCase,
} from &quot;./object.ts&quot;;

describe(&quot;narrow&quot;, () =&gt; {
	test(&quot;it narrows an object to the specified keys&quot;, () =&gt; {
		const obj = { a: 1, b: 2, c: 3 };
		expect(only(obj, &quot;a&quot;, &quot;b&quot;)).toEqual({ a: 1, b: 2 });
	});

	test(&quot;it sets missing keys to undefined&quot;, () =&gt; {
		const obj: { a: number; b: number; c?: number } = { a: 1, b: 2 };
		expect(only(obj, &quot;a&quot;, &quot;c&quot;)).toEqual({ a: 1, c: undefined });
	});

	test(&quot;it creates a shallow copy of the object&quot;, () =&gt; {
		const obj = { a: 1, b: { x: 2 }, c: 3 };
		const narrowed = only(obj, &quot;a&quot;, &quot;b&quot;);
		narrowed.a = 4;
		expect(obj.a).toBe(1);
		narrowed.b.x = 4;
		expect(narrowed.b.x).toBe(4);
	});
});

describe(&quot;camelCase&quot;, () =&gt; {
	test(&quot;it converts snake_case to camelCase&quot;, () =&gt; {
		expect(camelCase(&quot;foo_bar&quot;)).toBe(&quot;fooBar&quot;);
		expect(camelCase(&quot;foo_bar_baz&quot;)).toBe(&quot;fooBarBaz&quot;);
	});
	test(&quot;it converts kebab-case to camelCase&quot;, () =&gt; {
		expect(camelCase(&quot;foo-bar&quot;)).toBe(&quot;fooBar&quot;);
		expect(camelCase(&quot;foo-bar-baz&quot;)).toBe(&quot;fooBarBaz&quot;);
	});
	test(&quot;it leaves camelCase as is&quot;, () =&gt; {
		expect(camelCase(&quot;fooBar&quot;)).toBe(&quot;fooBar&quot;);
		expect(camelCase(&quot;fooBarBaz&quot;)).toBe(&quot;fooBarBaz&quot;);
	});
	test(&quot;it leaves PascalCase as is&quot;, () =&gt; {
		expect(camelCase(&quot;FooBar&quot;)).toBe(&quot;FooBar&quot;);
		expect(camelCase(&quot;FooBarBaz&quot;)).toBe(&quot;FooBarBaz&quot;);
	});
	test(&quot;it leaves numbers as is&quot;, () =&gt; {
		expect(camelCase(&quot;foo1&quot;)).toBe(&quot;foo1&quot;);
		expect(camelCase(&quot;foo1Bar&quot;)).toBe(&quot;foo1Bar&quot;);
	});
	test(&quot;it leaves special characters as is&quot;, () =&gt; {
		expect(camelCase(&quot;foo_bar!&quot;)).toBe(&quot;fooBar!&quot;);
		expect(camelCase(&quot;foo_bar!_baz&quot;)).toBe(&quot;fooBar!Baz&quot;);
	});
	test(&quot;it leaves empty strings as is&quot;, () =&gt; {
		expect(camelCase(&quot;&quot;)).toBe(&quot;&quot;);
	});
	test(&quot;it leaves whitespace as is&quot;, () =&gt; {
		expect(camelCase(&quot;foo bar-baz&quot;)).toBe(&quot;foo barBaz&quot;);
	});
});

describe(&quot;snakeCase&quot;, () =&gt; {
	test(&quot;it converts camelCase to snake_case&quot;, () =&gt; {
		expect(snakeCase(&quot;fooBar&quot;)).toBe(&quot;foo_bar&quot;);
		expect(snakeCase(&quot;fooBarBaz&quot;)).toBe(&quot;foo_bar_baz&quot;);
	});
	test(&quot;it converts kebab-case to snake_case&quot;, () =&gt; {
		expect(snakeCase(&quot;foo-bar&quot;)).toBe(&quot;foo_bar&quot;);
		expect(snakeCase(&quot;foo-bar-baz&quot;)).toBe(&quot;foo_bar_baz&quot;);
	});
	test(&quot;it leaves snake_case as is&quot;, () =&gt; {
		expect(snakeCase(&quot;foo_bar&quot;)).toBe(&quot;foo_bar&quot;);
		expect(snakeCase(&quot;foo_bar_baz&quot;)).toBe(&quot;foo_bar_baz&quot;);
	});
	test(&quot;it converts PascalCase to snake_case&quot;, () =&gt; {
		expect(snakeCase(&quot;FooBar&quot;)).toBe(&quot;foo_bar&quot;);
		expect(snakeCase(&quot;FooBarBaz&quot;)).toBe(&quot;foo_bar_baz&quot;);
	});
	test(&quot;it leaves numbers as is&quot;, () =&gt; {
		expect(snakeCase(&quot;foo1&quot;)).toBe(&quot;foo1&quot;);
		expect(snakeCase(&quot;foo1Bar&quot;)).toBe(&quot;foo1_bar&quot;);
	});
	test(&quot;it leaves special characters as is&quot;, () =&gt; {
		expect(snakeCase(&quot;fooBar!&quot;)).toBe(&quot;foo_bar!&quot;);
		expect(snakeCase(&quot;fooBar!Baz&quot;)).toBe(&quot;foo_bar!_baz&quot;);
	});
	test(&quot;it leaves empty strings as is&quot;, () =&gt; {
		expect(snakeCase(&quot;&quot;)).toBe(&quot;&quot;);
	});
	test(&quot;it leaves whitespace as is&quot;, () =&gt; {
		expect(snakeCase(&quot;foo bar-baz&quot;)).toBe(&quot;foo bar_baz&quot;);
	});
});

describe(&quot;recursiveCamelCase&quot;, () =&gt; {
	test(&quot;it converts snake_case to camelCase&quot;, () =&gt; {
		const obj = { foo_bar: &quot;baz&quot; };
		expect(recursiveCamelCase(obj)).toEqual({ fooBar: &quot;baz&quot; });
	});
	test(&quot;it converts nested snake_case to camelCase&quot;, () =&gt; {
		const obj = { foo_bar: { baz_qux: &quot;quux&quot; } };
		expect(recursiveCamelCase(obj)).toEqual({ fooBar: { bazQux: &quot;quux&quot; } });
	});
	test(&quot;it converts arrays of snake_case to camelCase&quot;, () =&gt; {
		const obj = [{ foo_bar: &quot;baz&quot; }, { baz_qux: &quot;quux&quot; }];
		expect(recursiveCamelCase(obj)).toEqual([{ fooBar: &quot;baz&quot; }, { bazQux: &quot;quux&quot; }]);
	});
	test(&quot;it converts arrays of nested snake_case to camelCase&quot;, () =&gt; {
		const obj = [{ foo_bar: { baz_qux: &quot;quux&quot; } }, { baz_qux: { quux_quuz: &quot;quuz&quot; } }];
		expect(recursiveCamelCase(obj)).toEqual([
			{ fooBar: { bazQux: &quot;quux&quot; } },
			{ bazQux: { quuxQuuz: &quot;quuz&quot; } },
		]);
	});
	test(&quot;it leaves camelCase as is&quot;, () =&gt; {
		const obj = { fooBar: &quot;baz&quot; };
		expect(recursiveCamelCase(obj)).toEqual({ fooBar: &quot;baz&quot; });
	});
});

describe(&quot;recursiveSnakeCase&quot;, () =&gt; {
	test(&quot;it converts camelCase to snake_case&quot;, () =&gt; {
		const obj = { fooBar: &quot;baz&quot; };
		expect(recursiveSnakeCase(obj)).toEqual({ foo_bar: &quot;baz&quot; });
	});
	test(&quot;it converts nested camelCase to snake_case&quot;, () =&gt; {
		const obj = { fooBar: { bazQux: &quot;quux&quot; } };
		expect(recursiveSnakeCase(obj)).toEqual({ foo_bar: { baz_qux: &quot;quux&quot; } });
	});
	test(&quot;it converts arrays of camelCase to snake_case&quot;, () =&gt; {
		const obj = [{ fooBar: &quot;baz&quot; }, { bazQux: &quot;quux&quot; }];
		expect(recursiveSnakeCase(obj)).toEqual([{ foo_bar: &quot;baz&quot; }, { baz_qux: &quot;quux&quot; }]);
	});
	test(&quot;it converts arrays of nested camelCase to snake_case&quot;, () =&gt; {
		const obj = [{ fooBar: { bazQux: &quot;quux&quot; } }, { bazQux: { quuxQuuz: &quot;quuz&quot; } }];
		expect(recursiveSnakeCase(obj)).toEqual([
			{ foo_bar: { baz_qux: &quot;quux&quot; } },
			{ baz_qux: { quux_quuz: &quot;quuz&quot; } },
		]);
	});
});

describe(&quot;fromEach&quot;, () =&gt; {
	test(&quot;it creates objects from arrays&quot;, () =&gt; {
		const arr = [&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;];
		expect(fromEach(arr, (v) =&gt; v)).toEqual({ 0: &quot;foo&quot;, 1: &quot;bar&quot;, 2: &quot;baz&quot; });
	});
	test(&quot;it creates objects from requests&quot;, () =&gt; {
		const req = new Request(&quot;http://example.com&quot;, {
			headers: { &quot;x-foo&quot;: &quot;bar&quot;, &quot;x-bar&quot;: &quot;123&quot; },
		});
		expect(fromEach(req.headers)).toEqual({ &quot;x-foo&quot;: &quot;bar&quot;, &quot;x-bar&quot;: &quot;123&quot; });
	});
	test(&quot;it creates objects from queries&quot;, () =&gt; {
		const url = new URL(&quot;http://example.com?foo=bar&amp;bar=123&quot;);
		expect(fromEach(url.searchParams)).toEqual({ foo: &quot;bar&quot;, bar: &quot;123&quot; });
	});
});

describe(&quot;fromEachArrayKey&quot;, () =&gt; {
	// ── 1. scalar → scalar  (current behaviour you already rely on)
	test(&quot;it uses the last occurence in queries&quot;, () =&gt; {
		const url = new URL(&quot;http://example.com?foo=first-bar&amp;bar=123&amp;foo=second-bar&quot;);
		expect(fromEachArrayKey(url.searchParams)).toEqual({ foo: &quot;second-bar&quot;, bar: &quot;123&quot; });
	});
	// ── 2. array → array  (append)
	test(&quot;it appends repeated foo[] values&quot;, () =&gt; {
		const url = new URL(&quot;http://example.com?foo[]=a&amp;bar=123&amp;foo[]=b&quot;);
		expect(fromEachArrayKey(url.searchParams)).toEqual({ foo: [&quot;a&quot;, &quot;b&quot;], bar: &quot;123&quot; });
	});

	// ── 3. scalar → array  (scalar folded into array)
	test(&quot;it folds an earlier scalar into a later foo[] array&quot;, () =&gt; {
		const url = new URL(&quot;http://example.com?foo=first&amp;foo[]=second&amp;foo[]=third&quot;);
		expect(fromEachArrayKey(url.searchParams)).toEqual({ foo: [&quot;first&quot;, &quot;second&quot;, &quot;third&quot;] });
	});

	// ── 4. array → scalar  (scalar overwrites array)
	test(&quot;it overwrites a foo[] array with a final scalar&quot;, () =&gt; {
		const url = new URL(&quot;http://example.com?foo[]=a&amp;foo[]=b&amp;foo=last&quot;);
		expect(fromEachArrayKey(url.searchParams)).toEqual({ foo: &quot;last&quot; });
	});

	// ── New error case: throws when mixing foo and foo[] without clear order
	test(&quot;it throws DuplicateKeyError when mixing key and array notation&quot;, () =&gt; {
		const obj = {
			forEach: (fn: (value: string, key: string) =&gt; void) =&gt; {
				fn(&quot;a&quot;, &quot;foo[]&quot;);
				fn(&quot;b&quot;, &quot;foo&quot;);
			},
		};
		expect(() =&gt; fromEachArrayKey(obj)).toThrow(DuplicateKeyError);
	});

	// ── New error case for URL params
	test(&quot;it throws DuplicateKeyError for URL params with mixed key formats&quot;, () =&gt; {
		const url = new URL(&quot;http://example.com?foo[]=a&amp;bar=123&amp;foo=b&quot;);
		expect(() =&gt; fromEachArrayKey(url.searchParams)).toThrow(DuplicateKeyError);
	});

	// ── New error case for FormData
	test(&quot;it throws DuplicateKeyError for FormData with mixed key formats&quot;, () =&gt; {
		const fd = new FormData();
		fd.append(&quot;foo[]&quot;, &quot;a&quot;);
		fd.append(&quot;foo&quot;, &quot;b&quot;);

		expect(() =&gt; fromEachArrayKey(fd)).toThrow(DuplicateKeyError);
		expect(() =&gt; fromEach(fd)).toThrow(
			&apos;Ambiguous use of both &quot;foo&quot; and &quot;foo[]&quot; keys is not allowed&apos;
		);
	});
});</file><file path="src/object.ts">import { DuplicateKeyError } from &quot;./errors.ts&quot;;
/**
 * A helper to shrink an object down to only the specified keys.
 * At the moment, this sets missing keys to undefined. Arguably it should throw.
 */
export function only&lt;T, K1 extends keyof T&gt;(obj: T, k1: K1): { [key in K1]: T[K1] };
export function only&lt;T, K1 extends keyof T, K2 extends keyof T&gt;(
	object: T,
	k1: K1,
	k2: K2
): { [key in K1]: T[K1] } &amp; { [key in K2]: T[K2] };
export function only&lt;T, K1 extends keyof T, K2 extends keyof T, K3 extends keyof T&gt;(
	object: T,
	k1: K1,
	k2: K2,
	k3: K3
): { [key in K1]: T[K1] } &amp; { [key in K2]: T[K2] } &amp; { [key in K3]: T[K3] };
export function only&lt;
	T,
	K1 extends keyof T,
	K2 extends keyof T,
	K3 extends keyof T,
	K4 extends keyof T,
&gt;(
	object: T,
	k1: K1,
	k2: K2,
	k3: K3,
	k4: K4
): { [key in K1]: T[K1] } &amp; { [key in K2]: T[K2] } &amp; { [key in K3]: T[K3] } &amp; {
	[key in K4]: T[K4];
};
export function only&lt;
	T,
	K1 extends keyof T,
	K2 extends keyof T,
	K3 extends keyof T,
	K4 extends keyof T,
	K5 extends keyof T,
&gt;(
	object: T,
	k1: K1,
	k2: K2,
	k3: K3,
	k4: K4,
	k5: K5
): { [key in K1]: T[K1] } &amp; { [key in K2]: T[K2] } &amp; { [key in K3]: T[K3] } &amp; {
	[key in K4]: T[K4];
} &amp; { [key in K5]: T[K5] };
export function only&lt;
	T,
	K1 extends keyof T,
	K2 extends keyof T,
	K3 extends keyof T,
	K4 extends keyof T,
	K5 extends keyof T,
	K6 extends keyof T,
&gt;(
	object: T,
	k1: K1,
	k2: K2,
	k3: K3,
	k4: K4,
	k5: K5,
	k6: K6
): { [key in K1]: T[K1] } &amp; { [key in K2]: T[K2] } &amp; { [key in K3]: T[K3] } &amp; {
	[key in K4]: T[K4];
} &amp; { [key in K5]: T[K5] } &amp; { [key in K6]: T[K6] };
export function only(obj: any, ...keys: string[]): object {
	const result = {} as any;
	for (const k of keys) {
		result[k] = obj[k];
	}
	return result;
}

export function without&lt;T extends string, O extends Record&lt;string, any&gt;&gt;(
	obj: O,
	...keys: T[]
): Omit&lt;O, T&gt; {
	const result = { ...obj };
	for (const k of keys) {
		delete result[k];
	}
	return result;
}

type SnakeToCamelCase&lt;S extends string&gt; = S extends `${infer T}_${infer U}`
	? `${T}${Capitalize&lt;SnakeToCamelCase&lt;U&gt;&gt;}`
	: S;

type CamelCaseKeys&lt;T&gt; =
	T extends Array&lt;any&gt;
		? Array&lt;CamelCaseKeys&lt;T[number]&gt;&gt;
		: T extends object
			? {
					[K in keyof T as SnakeToCamelCase&lt;string &amp; K&gt;]: CamelCaseKeys&lt;T[K]&gt;;
				}
			: T;

type CamelToSnakeCase&lt;S extends string&gt; = S extends `${infer T}${infer U}`
	? `${T extends Capitalize&lt;T&gt; ? &quot;_&quot; : &quot;&quot;}${Lowercase&lt;T&gt;}${CamelToSnakeCase&lt;U&gt;}`
	: &quot;&quot;;

type SnakeCaseKeys&lt;T&gt; =
	T extends Array&lt;any&gt;
		? Array&lt;SnakeCaseKeys&lt;T[number]&gt;&gt;
		: T extends object
			? {
					[K in keyof T as CamelToSnakeCase&lt;`${string &amp; K}`&gt;]: SnakeCaseKeys&lt;T[K]&gt;;
				}
			: T;

// &quot;Test cases&quot;
type foo = SnakeCaseKeys&lt;{ fooBar: string; barBaz: { quxQuux: string } }&gt;;
type bar = CamelCaseKeys&lt;{ foo_bar: string; bar_baz: { qux_quux: string } }&gt;;

export function camelCase(str: string): string {
	return str.replace(/([-_][a-z])/g, (group) =&gt;
		group.toUpperCase().replace(&quot;-&quot;, &quot;&quot;).replace(&quot;_&quot;, &quot;&quot;)
	);
}

export function snakeCase(str: string): string {
	str = str.replace(/^[A-Z]/g, (letter) =&gt; `${letter.toLowerCase()}`);
	str = str.replace(/[A-Z]/g, (letter) =&gt; `_${letter.toLowerCase()}`);
	return str.replace(/-/g, &quot;_&quot;);
}

export function recursiveCamelCase&lt;T&gt;(data: T): CamelCaseKeys&lt;T&gt; {
	if (Array.isArray(data)) {
		return data.map(recursiveCamelCase) as CamelCaseKeys&lt;T&gt;;
	} else if (typeof data === &quot;object&quot; &amp;&amp; data !== null) {
		const newObj: any = {};
		for (const key in data) {
			if (Object.prototype.hasOwnProperty.call(data, key)) {
				newObj[camelCase(key)] = recursiveCamelCase(data[key]);
			}
		}
		return newObj;
	}
	return data as CamelCaseKeys&lt;T&gt;;
}

export function recursiveSnakeCase&lt;T&gt;(data: T): SnakeCaseKeys&lt;T&gt; {
	if (Array.isArray(data)) {
		return data.map(recursiveSnakeCase) as SnakeCaseKeys&lt;T&gt;;
	} else if (typeof data === &quot;object&quot; &amp;&amp; data !== null) {
		const newObj: any = {};
		for (const key in data) {
			if (Object.prototype.hasOwnProperty.call(data, key)) {
				newObj[snakeCase(key)] = recursiveSnakeCase(data[key]);
			}
		}
		return newObj;
	}
	return data as SnakeCaseKeys&lt;T&gt;;
}

export function mapValues&lt;K extends string, T, U&gt;(o: Record&lt;K, T&gt;, f: (a: T) =&gt; U): Record&lt;K, U&gt; {
	// TS doesn&apos;t understand galaxy brain
	return Object.fromEntries(Object.entries(o).map(([k, v]) =&gt; [k, f(v as any)])) as any;
}

/**
 * Converts any `.forEach` source (FormData, URLSearchParams, Headers, Map)
 * into a plain object, following browser-standard duplicate-key handling.
 * This version also checks for ambiguous use of both &apos;foo&apos; and &apos;foo[]&apos; keys.
 *
 * ┌────────────────────────────────────────────────────────────────┐
 * │ Duplicate-key rules                                            │
 * ├───────────────────┬──────────────────────┬─────────────────────┤
 * │ Sequence of keys  │ Outcome (k)          │ Why                 │
 * ├───────────────────┼──────────────────────┼─────────────────────┤
 * │ foo, foo          │ k = last value       │ URLSearchParams.set │
 * │ foo[], foo[]      │ k = [...values]      │ FormData.append     │
 * │ foo, foo[]        │ ERROR                │ Ambiguous usage     │
 * │ foo[], foo        │ ERROR                │ Ambiguous usage     │
 * └───────────────────┴──────────────────────┴─────────────────────┘
 *
 * Note: Using both &quot;foo&quot; and &quot;foo[]&quot; in the same collection will throw a
 * DuplicateKeyError as this is ambiguous and error-prone.
 *
 * Values are left untouched (File, Blob, etc.).
 */
export function fromEachArrayKey&lt;K extends PropertyKey, T, U = T&gt;(
	o: { forEach: (f: (v: T, k: K) =&gt; void) =&gt; void; },
	t: (v: T, k: K) =&gt; U = (x) =&gt; x as unknown as U
): Record&lt;string, U | U[]&gt; {
	// First collect all entries to validate and avoid FormData consumption issues
	const entries: Array&lt;[K, T]&gt; = [];
	const seenKeys = new Set&lt;string&gt;();
	const seenArrayKeys = new Set&lt;string&gt;();

	// Scan for conflicting keys first
	o.forEach((v, k) =&gt; {
		entries.push([k, v]);

		const isArr = typeof k === &quot;string&quot; &amp;&amp; k.endsWith(&quot;[]&quot;);
		const baseKey = isArr ? k.slice(0, -2) : k;

		if (typeof baseKey === &quot;string&quot;) {
			if (isArr) {
				seenArrayKeys.add(baseKey);
				if (seenKeys.has(baseKey)) {
					throw new DuplicateKeyError(baseKey);
				}
			} else {
				seenKeys.add(baseKey);
				if (seenArrayKeys.has(baseKey)) {
					throw new DuplicateKeyError(baseKey);
				}
			}
		}
	});

	// Process entries after validation
	const result = {} as Record&lt;string, U | U[]&gt;;

	for (const [k, v] of entries) {
		const isArr = typeof k === &quot;string&quot; &amp;&amp; k.endsWith(&quot;[]&quot;);
		const key = isArr ? k.slice(0, -2) : k;
		const val = t(v, k as K);

		if (isArr) {
			const prev = result[key as string];
			result[key as string] =
				prev === undefined ? [val] :
				Array.isArray(prev) ? [...prev, val] :
				[prev, val];
		} else {
			result[k as string] = val;
		}
	}

	return result;
}

export function fromEach&lt;K extends number | string | symbol, T, U = T&gt;(o: {
	forEach: (f: (v: T, k: K) =&gt; void) =&gt; void;
}): Record&lt;K, T&gt;;
export function fromEach&lt;K extends number | string | symbol, T, U&gt;(
	o: { forEach: (f: (v: T, k: K) =&gt; void) =&gt; void },
	t: (v: T, k: K) =&gt; U
): Record&lt;K, U&gt;;
export function fromEach&lt;K extends number | string | symbol, T, U&gt;(
	o: { forEach: (f: (v: T, k: K) =&gt; void) =&gt; void },
	t: (...args: any[]) =&gt; U = (x) =&gt; x
): Record&lt;K, U&gt; {
	const result = {} as Record&lt;K, U&gt;;
	o.forEach((v, k) =&gt; {
		result[k] = t(v, k);
	});
	return result;
}</file><file path="src/queue.ts">import { timeout } from &quot;./errors.ts&quot;;
import { Logger } from &quot;./log.ts&quot;;
import type { Schema } from &quot;./schema.ts&quot;;
import type { StoreSerializer } from &quot;./serializer.ts&quot;;

// TODO haven&apos;t got too deep on this one, since we&apos;re not sure
// whether we want this abstraction exposed. It might also be one
// abstraction too many.

/**
 * A message queue takes messages for asynchronous processing.
 *
 * Guarantees:
 * - At least once delivery
 * - Timeout for long running messages
 * - Dead letter queue for failed messages
 * - Schema validation
 *
 * Only implementing the enqueue end for now.
 */
export interface MessageQueue&lt;T&gt; {
	/**
	 * Throws:
	 * - Timeout (only for enqueueing, not for processing)
	 * - BackendError
	 * - SerializationError
	 * - ValidationError
	 */
	send(message: T): Promise&lt;void&gt;;
}

export interface MessageQueueBackend&lt;T&gt; {
	send(message: T): Promise&lt;void&gt;;
}

export class MessageQueueImpl&lt;TValue, TStore&gt; {
	constructor(
		public readonly logger: Logger,
		private readonly backend: MessageQueueBackend&lt;TStore&gt;,
		private readonly serializer: StoreSerializer&lt;TStore&gt;,
		private readonly schema: Schema&lt;TValue&gt;,
		public readonly sendTimeoutMs: number = 1000
	) {}

	@timeout
	async send(message: TValue): Promise&lt;void&gt; {
		const parsed = this.schema.parse(message);
		const sendable = this.serializer.toStore(parsed);
		await this.backend.send(sendable);
	}
}</file><file path="src/schema.test.ts">import { describe, expect, test } from &quot;vitest&quot;;
import { z } from &quot;zod&quot;;
import { ParseError } from &quot;./errors.ts&quot;;
import { toCreator, toParser, toUnionSchema, type UnionSchemaDefinition } from &quot;./schema.ts&quot;;

interface A {
	readonly type: &quot;a&quot;;
	readonly foo: number;
}

interface B {
	readonly type: &quot;b&quot;;
	readonly bar: number;
}

interface C {
	readonly type: &quot;c&quot;;
	readonly zoop: number;
}

type TestUnion = A | B | C;

const ZODS: UnionSchemaDefinition&lt;&quot;type&quot;, TestUnion&gt; = {
	a: z.object({ type: z.literal(&quot;a&quot;), foo: z.number() }),
	b: z.object({ type: z.literal(&quot;b&quot;), bar: z.number() }),
	c: z.object({ type: z.literal(&quot;c&quot;), zoop: z.number() }),
};

const unionSchema = toUnionSchema&lt;&quot;type&quot;, TestUnion&gt;(&quot;type&quot;, ZODS);

describe(&quot;toUnionSchema&quot;, () =&gt; {
	test(&quot;parses union&quot;, () =&gt; {
		expect(unionSchema.parse({ type: &quot;a&quot;, foo: 1 })).toEqual({ type: &quot;a&quot;, foo: 1 });
		expect(unionSchema.parse({ type: &quot;b&quot;, bar: 1 })).toEqual({ type: &quot;b&quot;, bar: 1 });
	});
	test(&quot;throws union&quot;, () =&gt; {
		expect(() =&gt; unionSchema.parse({ type: &quot;a&quot;, zoop: 1 })).toThrowError(ParseError);
	});
	test(&quot;parses members&quot;, () =&gt; {
		expect(unionSchema[&quot;a&quot;].parse({ type: &quot;a&quot;, foo: 1 })).toEqual({ type: &quot;a&quot;, foo: 1 });
		expect(unionSchema[&quot;b&quot;].parse({ type: &quot;b&quot;, bar: 1 })).toEqual({ type: &quot;b&quot;, bar: 1 });
		expect(unionSchema[&quot;c&quot;].parse({ type: &quot;c&quot;, zoop: 1 })).toEqual({ type: &quot;c&quot;, zoop: 1 });
	});
	test(&quot;throws members&quot;, () =&gt; {
		expect(() =&gt; unionSchema[&quot;a&quot;].parse({ type: &quot;a&quot;, bar: 1 })).toThrowError(ParseError);
		expect(() =&gt; unionSchema[&quot;b&quot;].parse({ type: &quot;b&quot;, foo: 1 })).toThrowError(ParseError);
		expect(() =&gt; unionSchema[&quot;c&quot;].parse({ type: &quot;c&quot;, foo: 1 })).toThrowError(ParseError);
	});
});

describe(&quot;toParser&quot;, () =&gt; {
	const parse = toParser(ZODS.a);
	test(&quot;parses&quot;, () =&gt; {
		expect(parse({ type: &quot;a&quot;, foo: 1 })).toEqual({ type: &quot;a&quot;, foo: 1 });
	});
	test(&quot;parses extra keys&quot;, () =&gt; {
		expect(parse({ type: &quot;a&quot;, foo: 1, bar: 2 })).toEqual({ type: &quot;a&quot;, foo: 1 });
	});
	test(&quot;parser throws for undefined&quot;, () =&gt; {
		expect(() =&gt; parse(undefined)).toThrowError();
	});
	test(&quot;parser throws for null&quot;, () =&gt; {
		expect(() =&gt; parse(null)).toThrowError();
	});
	test(&quot;parser throws for empty&quot;, () =&gt; {
		expect(() =&gt; parse({})).toThrowError();
	});
	test(&quot;parser throws for missing keys&quot;, () =&gt; {
		expect(() =&gt; parse({ type: &quot;a&quot; })).toThrowError();
	});
	test(&quot;parser throws for wrong type&quot;, () =&gt; {
		expect(() =&gt; parse({ type: 1, bar: &quot;2&quot; })).toThrowError();
	});
	test(&quot;parser throws for wrong type&quot;, () =&gt; {
		expect(() =&gt; parse({ type: 1, bar: &quot;2&quot; })).toThrowError();
	});
});

describe(&quot;toCreater&quot;, () =&gt; {
	const create = toCreator(toParser(ZODS.a));

	test(&quot;flags extra properties&quot;, () =&gt; {
		expect(() =&gt; {
			// @ts-expect-error
			create({ type: &quot;a&quot;, foo: 1, bar: 2 });
		});
	});
	test(&quot;flags missing properties&quot;, () =&gt; {
		expect(() =&gt; {
			// @ts-expect-error
			create({ type: &quot;a&quot; });
		});
	});

	test(&quot;creates&quot;, () =&gt; {
		expect(create({ type: &quot;a&quot;, foo: 1 })).toEqual({ type: &quot;a&quot;, foo: 1 });
	});
});</file><file path="src/schema.ts">import { z, type ZodDiscriminatedUnionOption, ZodObject, ZodType, type ZodTypeDef } from &quot;zod&quot;;
import { ParseError } from &quot;./errors.ts&quot;;
import { mapValues } from &quot;./object.ts&quot;;

/**
 * All this type nonsense gives us the following:
 *
 * A single schema per discriminable union type with the following methods:
 * - parse: take an unknown object and return a valid object, or throw an error
 * 		e.g. taking external input
 * - create: take a presumed-valid object, and throw if it isn&apos;t, then return a valid object
 * 		e.g. creating events, when you want your IDE to help you
 *
 * Notably, it does not include
 * - validate: take an unknown object and throw if it isn&apos;t valid
 * 		e.g. internal input, when you don&apos;t want to create a new object
 *
 * See https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/ for an explanation of why we don&apos;t validate
 * The validation code is commented out in this file pending discussion
 *
 * We can use these flexibly in various places, for runtime type safety
 *
 * A few things to note:
 * - The zod definition (or the parser) shouldn&apos;t infer or convert types, and take them verbatim. We may enforce this later on, but for now we don&apos;t
 *
 *
 * TODO decide whether we allow inference on the parser
 * TODO decide whether we want to use zod, or something stricter, or with more jsonschema support, for instance
 * TODO consider throwing our own errors
 */
export interface Schema&lt;T&gt; {
	readonly [ZOD]: ZodType&lt;T&gt;;
	readonly parse: Parser&lt;T&gt;;
	readonly create: Creator&lt;T&gt;;
	// validate: Validator&lt;T&gt;
}

export type SchemaLike&lt;T&gt; = SchemaDefinition&lt;T&gt; | Schema&lt;T&gt;;

// These types illustrate platform semantics for their namesake terms.
// They should be used liberally throughout the codebase, to make it clear
// what is being done with the data at any given time.

/**
 * Return an object of the given type, or throw.
 * Returns a deep copy of the object
 *
 * Throws
 * - ParseError
 */
export type Parser&lt;T&gt; = (obj: unknown) =&gt; T;

// /**
//  * Throw if the given unknown object isn&apos;t of the given type
//  * The original object remains untouched
//  */
// export type Validator&lt;T&gt; = (obj: unknown) =&gt; asserts obj is T
export type Validator&lt;T&gt; = never;

/**
 * Throw if the given object isn&apos;t of the given type, then return it.
 * The original object remains untouched
 * This is useful for compile time type checking with runtime guarantees
 * (As an optimisation we _could_ remove the checks at runtime)
 */
export type Creator&lt;T&gt; = (obj: T) =&gt; T;

/**
 * With this symbol we discourage relying on zod directly outside of the schema definitions,
 * but offer the escape hatch if you really want to use it
 */
const ZOD = Symbol(&quot;ZOD&quot;);
export function getZod&lt;T&gt;(schema: SchemaLike&lt;T&gt;): ZodType&lt;T&gt; {
	// TODO what to do if this is a lie?
	return toSchema(schema)[ZOD];
}

/**
 * Specifically one with a descriminator. May do a find replace later for a more expressive name
 */
export type Union&lt;D extends string, U extends { readonly [K in D]: U[D] }&gt; = {
	readonly [K in D]: U[D];
};

/**
 * Right now we use zod under the hood, but we don&apos;t rely on it heavily, so we could swap it out.
 * More important are the semantics of the types, and the methods we provide.
 *
 * The triple T should make it so we don&apos;t allow any inference or conversion of types
 */
export type Zod&lt;T&gt; = ZodType&lt;T, ZodTypeDef, T&gt;;
export type SchemaDefinition&lt;T&gt; = Zod&lt;T&gt; | { fromJSON(obj: unknown): T };

export type UnionSchemaDefinition&lt;D extends string, U extends Union&lt;D, U&gt;&gt; = {
	readonly [K in U[D]]: SchemaDefinition&lt;Extract&lt;U, { [_ in D]: K }&gt;&gt;;
};
type Foo = UnionSchemaDefinition&lt;&quot;type&quot;, { type: &quot;a&quot;; a: string } | { type: &quot;b&quot;; b: number }&gt;;
const foo: Foo = {
	a: z.object({ type: z.literal(&quot;a&quot;), a: z.string() }),
	b: z.object({ type: z.literal(&quot;b&quot;), b: z.number() }),
};

export type UnionSchema&lt;D extends string, U extends Union&lt;D, U&gt;&gt; =
	// &amp; {[ZOD]?: ZodDiscriminatedUnion&lt;D, [ZodDiscriminatedUnionOption&lt;D&gt;, ...ZodDiscriminatedUnionOption&lt;D&gt;[]]&gt; }
	Schema&lt;U&gt; &amp; { [K in U[D]]: Schema&lt;Extract&lt;U, { [_ in D]: K }&gt;&gt; };

export function isZodType(obj: unknown): obj is ZodType&lt;unknown&gt; {
	// TODO better please
	return !!obj &amp;&amp; Object.hasOwn(obj, &quot;_def&quot;);
}

export function toSchema&lt;T&gt;(definition: SchemaLike&lt;T&gt;): Schema&lt;T&gt; {
	if (isZodType(definition)) {
		const parse = toParser(definition);
		return {
			[ZOD]: definition,
			parse,
			create: toCreator(parse),
			// validate: toValidator(parse)
		};
	} else if (definition &amp;&amp; &quot;parse&quot; in definition &amp;&amp; &quot;create&quot; in definition) {
		return definition;
	} else {
		throw new Error(`Invalid schema definition: ${definition}`);
	}
}
export function toParser&lt;T&gt;(definition: SchemaDefinition&lt;T&gt;): Parser&lt;T&gt; {
	const parser = isZodType(definition)
		? definition instanceof ZodObject
			? // Remove extra keys on objects
				(definition.strip().parse as Parser&lt;T&gt;)
			: definition.parse
		: &quot;fromJSON&quot; in definition
			? definition.fromJSON
			: () =&gt; {
					throw new Error(`Invalid schema definition: ${definition}`);
				};
	return (obj: unknown) =&gt; {
		try {
			return parser(obj);
		} catch (cause) {
			throw new ParseError(cause);
		}
	};
}
export function toCreator&lt;T&gt;(parser: Parser&lt;T&gt;): Creator&lt;T&gt; {
	return (obj: T) =&gt; parser(obj);
}
// function toValidator&lt;T&gt;(parser: Parser&lt;T&gt;): Validator&lt;T&gt; {
// 	return (obj: unknown) =&gt; {
// 		parser(obj)
// 	}
// }

export function toUnionSchema&lt;D extends string, U extends Union&lt;D, U&gt;&gt;(
	discriminator: D,
	definition: UnionSchemaDefinition&lt;D, U&gt;
): UnionSchema&lt;D, U&gt; {
	// Cheating a bit, but we have enough type and test coverage to be confident
	const zod: Schema&lt;U&gt; = z.discriminatedUnion(
		discriminator,
		Object.values(definition) as [
			ZodDiscriminatedUnionOption&lt;D&gt;,
			...ZodDiscriminatedUnionOption&lt;D&gt;[],
		]
	) as any;
	// TODO Work out this type stuff.
	// const zod = z.object({}).passthrough() as any;
	return {
		...toSchema(zod),
		...mapValues(definition, toSchema),
	} as UnionSchema&lt;D, U&gt;;
}

export const neverSchema = toSchema(z.never()) as any as Schema&lt;any&gt;;</file><file path="src/secrets.test.ts">import { describe, expect, test, vi } from &quot;vitest&quot;;
import { Secret, getSecrets } from &quot;./secrets.ts&quot;;

describe(&quot;Secret&quot;, () =&gt; {
	const secret = new Secret(&quot;I eat bags of gummy bears in one sitting&quot;);
	test(&quot;getValue&quot;, () =&gt; {
		expect(secret.getValue()).toBe(&quot;I eat bags of gummy bears in one sitting&quot;);
	});
	test(&quot;hides value in strings&quot;, () =&gt; {
		expect(`Don&apos;t tell anyone: ${secret}`).toBe(&quot;Don&apos;t tell anyone: Secret&quot;);
	});
});

describe(&quot;getSecrets&quot;, () =&gt; {
	test(&quot;returns secrets&quot;, () =&gt; {
		vi.stubEnv(&quot;MY_SECRET&quot;, &quot;I listen to Ariana Grande&quot;);
		const secrets = getSecrets([&quot;MY_SECRET&quot;]);
		expect(secrets.MY_SECRET).toBeInstanceOf(Secret);
		expect(secrets.MY_SECRET.getValue()).toBe(&quot;I listen to Ariana Grande&quot;);
	});

	test(&quot;throws if secret is missing&quot;, () =&gt; {
		expect(() =&gt; getSecrets([&quot;MISSING_SECRET&quot;])).toThrowError(
			&quot;Missing secrets: MISSING_SECRET&quot;
		);
	});

	test(&quot;throws if multiple secrets are missing&quot;, () =&gt; {
		expect(() =&gt; getSecrets([&quot;MISSING_SECRET&quot;, &quot;ANOTHER_MISSING_SECRET&quot;])).toThrowError(
			&quot;Missing secrets: MISSING_SECRET, ANOTHER_MISSING_SECRET&quot;
		);
	});

	test(&quot;throws for empty values&quot;, () =&gt; {
		vi.stubEnv(&quot;EMPTY_SECRET&quot;, &quot;&quot;);
		expect(() =&gt; getSecrets([&quot;EMPTY_SECRET&quot;])).toThrowError(&quot;Empty secrets: EMPTY_SECRET&quot;);
	});

	test(&quot;throws for multiple problems&quot;, () =&gt; {
		vi.stubEnv(&quot;EMPTY_SECRET&quot;, &quot;&quot;);
		expect(() =&gt; getSecrets([&quot;MISSING_SECRET&quot;, &quot;EMPTY_SECRET&quot;])).toThrowError(
			&quot;Missing secrets: MISSING_SECRET\nEmpty secrets: EMPTY_SECRET&quot;
		);
	});
});</file><file path="src/secrets.ts">/**
 * Current implementation uses env vars, but this should be replaced with a proper secret manager using files.
 */
export class Secret {
	constructor(private readonly _value: string) {}

	getValue() {
		return this._value;
	}
	toString() {
		return &quot;Secret&quot;;
	}
}
export function getSecrets&lt;K extends string&gt;(keys: K[]): { [key in K]: Secret } {
	// TODO proper errors or use zod or something off the shelf
	const entries = keys.map((key) =&gt; [key, process.env[key]]);
	const missing = entries.filter(([_, val]) =&gt; val === undefined).map(([key, _]) =&gt; key);
	const empty = entries.filter(([_, val]) =&gt; val === &quot;&quot;).map(([key, _]) =&gt; key);
	const problems: string[] = [];
	if (missing.length &gt; 0) {
		problems.push(`Missing secrets: ${missing.join(&quot;, &quot;)}`);
	}
	if (empty.length &gt; 0) {
		problems.push(`Empty secrets: ${empty.join(&quot;, &quot;)}`);
	}
	if (problems.length &gt; 0) {
		throw new Error(problems.join(&quot;\n&quot;));
	}
	return Object.fromEntries(entries.map(([key, val]) =&gt; [key, new Secret(val as string)])) as any;
}</file><file path="src/serializer.ts">import { SerializerError } from &quot;./errors.ts&quot;;
import json from &quot;./json.ts&quot;;
import { recursiveCamelCase, recursiveSnakeCase } from &quot;./object.ts&quot;;
import type { POJO } from &quot;./types.ts&quot;;

/**
 * The serializer&apos;s only job is to convert between a value&apos;s type and the store&apos;s backend representation.
 * No validation needs be done here. Examples would be JSON serialization or dynamo db objects.
 *
 * Throws SerializerError
 */
export class Serializer&lt;TValue, TStore&gt; {
	constructor(private implementation: TypedStoreSerializer&lt;TValue, TStore&gt;) {
		// Bind the methods to the instance so we can have nice maps
		this.toStore = this.toStore.bind(this);
		this.fromStore = this.fromStore.bind(this);
	}
	toStore(val: TValue): TStore {
		try {
			return this.implementation.toStore(val);
		} catch (error) {
			throw new SerializerError(error);
		}
	}
	fromStore(val: TStore): TValue {
		try {
			return this.implementation.fromStore(val);
		} catch (error) {
			throw new SerializerError(error);
		}
	}
}

export interface TypedStoreSerializer&lt;TValue, TStore&gt; {
	toStore(val: TValue): TStore;
	fromStore(val: TStore): TValue;
}

export interface StoreSerializer&lt;TStore&gt; extends TypedStoreSerializer&lt;unknown, TStore&gt; {}

// TODO Eh.. this one is mainly for testing, although it could aslo be used as the default serializer
// It is a bit of a type cheat, so let&apos;s revisit when we have more concrete use cases.
export const identitySerializer = new Serializer({
	toStore: (val: unknown): unknown =&gt; val,
	fromStore: (val: unknown): unknown =&gt; val,
});

export const typedIdentitySerializer = &lt;T&gt;() =&gt; identitySerializer as Serializer&lt;T, POJO&gt;;

// 🐫 🐍 🤪
export const camelSnakeSerializer: Serializer&lt;any, POJO&gt; = new Serializer({
	fromStore: recursiveCamelCase,
	toStore: recursiveSnakeCase,
});

export const jsonSerializer = new Serializer({
	fromStore: json.parse,
	toStore: json.serialize,
});

export const composeSerializers = &lt;TValue, TStore, TIntermediate&gt;(
	a: Serializer&lt;TValue, TIntermediate&gt;,
	b: Serializer&lt;TIntermediate, TStore&gt;
): Serializer&lt;TValue, TStore&gt; =&gt;
	new Serializer({
		toStore: (val: TValue) =&gt; b.toStore(a.toStore(val)),
		fromStore: (val: TStore) =&gt; a.fromStore(b.fromStore(val)),
	});

/**
 * This one is for testing!
 */
export const neverSerializer = new Serializer&lt;never, never&gt;({
	toStore: () =&gt; {
		throw new Error(&quot;toStore&quot;);
	},
	fromStore: () =&gt; {
		throw new Error(&quot;fromStore&quot;);
	},
});</file><file path="src/tasks.ts">import { Logger } from &quot;./log.ts&quot;;
import type { MessageQueue } from &quot;./queue.ts&quot;;
import { type Schema } from &quot;./schema.ts&quot;;
import type { POJO } from &quot;./types.ts&quot;;

export class TaskScheduler&lt;TRaw&gt; {
	constructor(
		private readonly logger: Logger,
		private readonly queue: MessageQueue&lt;POJO&gt;,
		private readonly taskSchema: Schema&lt;TRaw&gt;,
		private readonly serializer: (task: TRaw) =&gt; POJO
	) {}

	async schedule&lt;TTask extends TRaw&gt;(task: TTask): Promise&lt;void&gt; {
		try {
			const parsed = this.taskSchema.parse(task);
			const serialized = this.serializer(parsed);
			await this.queue.send(serialized);
		} catch (err) {
			// Not too happy about logging the whole task, but we can&apos;t assume any properties - jkz
			this.logger.error({ message: &quot;Failed to schedule task&quot;, err, task });
			throw err;
		}

		// Not too happy about logging the whole task, but we can&apos;t assume any properties - jkz
		this.logger.info({ message: &quot;Scheduled task&quot;, task });
	}
}</file><file path="src/test.ts">import * as vitest from &quot;vitest&quot;;
import { DeliberatelyUnimplementedError } from &quot;./errors.ts&quot;;

export { beforeAll, describe, expect } from &quot;vitest&quot;;
export function test(label: string, fn: vitest.TestFunction) {
	vitest.test(label, async (ctx) =&gt; {
		try {
			await fn(ctx);
		} catch (e) {
			if (e instanceof DeliberatelyUnimplementedError) {
				ctx.skip();
			} else {
				throw e;
			}
		}
	});
}</file><file path="src/types.ts">/**
 * Describes the context variables which are required/guaranteed by the platform.
 *
 * Used for several purposes, like:
 * - Logging
 * - Storing events
 * - Starting workflows
 */
export interface RequestVars {
	traceId: string;
	idempotencyKey: string;
}

/**
 * A single way to describe simple objects that can be json serialized
 */
export type POJO = Record&lt;string, unknown&gt;; // TODO decide on how we want to type this

// Parsers turn unknown into typed data, and throw if the input is invalid
export type Parser&lt;T&gt; =
	// Matches Zod objects
	{ parse(input: unknown): T };

// By GPT:
/**
 * This helper type takes a union and returns a tuple that contains each type exactly once.
 */
type UnionToIntersection&lt;U&gt; = (U extends any ? (x: U) =&gt; void : never) extends (x: infer R) =&gt; void
	? R
	: never;
type LastInUnion&lt;U&gt; =
	UnionToIntersection&lt;U extends any ? (x: U) =&gt; void : never&gt; extends (x: infer L) =&gt; void
		? L
		: never;
export type TupleOfUnion&lt;U, T extends any[] = []&gt; = [U] extends [never]
	? T
	: TupleOfUnion&lt;Exclude&lt;U, LastInUnion&lt;U&gt;&gt;, [LastInUnion&lt;U&gt;, ...T]&gt;;

// Inline validation of the type
type MyUnion = &quot;a&quot; | &quot;b&quot; | &quot;c&quot;;
type MyTuple = TupleOfUnion&lt;MyUnion&gt;;
const correct: MyTuple = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;];
// @ts-expect-error
const withMissing: MyTuple = [&quot;a&quot;, &quot;b&quot;];
// @ts-expect-error
const outOfOrder: MyTuple = [&quot;a&quot;, &quot;c&quot;, &quot;b&quot;];

export type Pagination&lt;T&gt; = {
	items: T[];
	count: number;
	cursor?: string;
};

// &apos;forward&apos; indicates chronological order, moving from past to future
// SQL equivalent: ORDER BY timestamp ASC
export type SortDirection = &quot;forward&quot; | &quot;backward&quot;;</file><file path="src/utils.ts">export function delay(milliseconds: number) {
	return new Promise((resolve) =&gt; setTimeout(resolve, milliseconds));
}</file><file path="src/worker.ts">import { Logger } from &quot;./log.ts&quot;;
import { delay } from &quot;./utils.ts&quot;;

/**
 * A worker is a long-running process that performs some kind of work in Noggin
 * that is NOT part of the request/response lifecycle. Such as, pushing items to
 * a queue, processing background jobs, handling asynchronous task, or
 * performing computations (eg. creating Projections).
 *
 * Defaults to running forever with 5000 msBetweenPolls.
 *
 */
export class Worker {
	public isRunning: boolean = false;
	public msBetweenPolls: number = 5000;
	public logger: Logger;

	constructor(logger: Logger) {
		this.setupSignalHandlers();
		this.logger = logger.child({ service: this.constructor.name });
	}

	/**
	 * This method is intended to be overridden in a subclass.
	 * It is called when the worker object is instantiated.
	 * It should be used to do any initialization before the worker starts running.
	 */
	public async init(): Promise&lt;void&gt; {}

	/**
	 * This method is intended to be overridden in a subclass.
	 * It is called when the worker is shutting down.
	 * It should be used to do any clean up before the process exits.
	 */
	public async exit(): Promise&lt;void&gt; {}

	/**
	 * This method is intended to be overridden in a subclass.
	 * It contains the main logic of the worker.
	 */
	public async run(): Promise&lt;void&gt; {
		this.NotImplementedError(&quot;run&quot;);
	}

	/**
	 * This method sets up signal handlers for SIGINT and SIGTERM so that the worker
	 * can shutdown gracefully.
	 */
	private setupSignalHandlers() {
		process.on(&quot;SIGINT&quot;, () =&gt; {
			this.logger.info(&quot;Received SIGINT. Graceful shutdown initiated...&quot;);
			this.isRunning = false;
			this.exit();
		});

		process.on(&quot;SIGTERM&quot;, () =&gt; {
			this.logger.info(&quot;Received SIGTERM. Graceful shutdown initiated...&quot;);
			this.isRunning = false;
			this.exit();
		});
	}

	/**
	 * This method starts the worker.
	 * If runOnce is true, the worker will run once, then die (useful for one off tasks).
	 */
	public async start(runOnce: boolean = false): Promise&lt;void&gt; {
		this.logger.info(&quot;Starting...&quot;);
		this.isRunning = true;
		this.logger.info(&quot;Initalizing...&quot;);
		await this.init();
		this.logger.info(&quot;Initialization complete.&quot;);
		try {
			while (this.isRunning) {
				this.logger.info(&quot;Running...&quot;);
				await this.run();
				await delay(this.msBetweenPolls);
				if (runOnce) {
					// breaks while loop after the first run
					// ensuring the worker runs once
					break;
				}
			}
		} catch (error) {
			this.logger.error(`Unhandled error in ${this.constructor.name}:`, error);
		} finally {
			await this.exit();
			this.logger.info(&quot;Worker exited.&quot;);
			process.exit(1);
		}
	}

	protected NotImplementedError(methodName: string): never {
		throw new Error(`${methodName} method not implemented in ${this.constructor.name}.`);
	}
}</file><file path="src/zod.ts">export * from &quot;zod&quot;;</file><file path="vitest.config.ts">import * as process from &quot;process&quot;;
import { defineConfig } from &quot;vitest/config&quot;;

export default defineConfig(({ mode }) =&gt; ({
	// This ensures vitest cache is always in a writable directory which is important for building and
	// running tests in Nix.  However it does of course mean tests now lose cache every machine reboot
	// for local devs: if that becomes a problem we can add a better heuristic here to only use TMP in
	// ephemeral build contexts.
	cacheDir: (process.env.TMPDIR || &quot;.&quot;) + &quot;/vitest-cache&quot;,
}));</file><file path="vitest.config.ts">import * as process from &quot;process&quot;;
import { defineConfig } from &quot;vitest/config&quot;;

export default defineConfig(({ mode }) =&gt; ({
	// This ensures vitest cache is always in a writable directory which is important for building and
	// running tests in Nix.  However it does of course mean tests now lose cache every machine reboot
	// for local devs: if that becomes a problem we can add a better heuristic here to only use TMP in
	// ephemeral build contexts.
	cacheDir: (process.env.TMPDIR || &quot;.&quot;) + &quot;/vitest-cache&quot;,
}));</file></files></repomix>